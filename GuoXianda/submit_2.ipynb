{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/vy/jjxqkfb15pj9bsyl9n2zxyww0000gn/T/jieba.cache\n",
      "Loading model cost 0.648 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import codecs\n",
    "import jieba.posseg as pseg\n",
    "import re\n",
    "\n",
    "def ad_detect(discuss):\n",
    "\treturn discuss\n",
    "def splitWord(query,stopwords):#分词返回字符串，停用词表是自己找的删除了一些程度描述词\n",
    "\tif len(query)==0:\n",
    "\t\tquery='空信息'\n",
    "\t\n",
    "\t# wordList = []\n",
    "\t# for line in query:\n",
    "\t#     line = line.strip()\n",
    "\t#     words = jieba.cut(line)\n",
    "\t#     wordList.append(words)\n",
    "\n",
    "\twordList = jieba.lcut(query)\n",
    "\tnum = 0\n",
    "\tresult = ''\n",
    "\tfor word in wordList:\n",
    "\t\tword = word.rstrip()\n",
    "\t\tword = word.rstrip('\"')\n",
    "\t\tif word not in stopwords:\n",
    "\t\t\tif num == 0:\n",
    "\t\t\t\tresult = word\n",
    "\t\t\t\tnum = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tresult = result + ' ' + word\n",
    "\treturn result\n",
    "def preprocess(data):\n",
    "\tstopwords = {}\n",
    "\tfor line in codecs.open('./data/stop.txt','r','utf-8'):\n",
    "\t\tstopwords[line.rstrip()]=1\n",
    "\tdata['split_word'] = data['content'].map(lambda x:splitWord(x,stopwords))\n",
    "\treturn data\n",
    "def select_cn(discuss):\n",
    "\t#print(discuss)\n",
    "\tdiscuss = re.sub(\"[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\?\\？\\、\\。\\，\\；\\’\\【\\】\\·\\~\\！\\@\\#\\￥\\%\\……\\&\\*\\（\\）\\》\\《\\：\\“\\”\\{\\}\\<\\/\\>\\.\\ \\;\\；]\", \"\", discuss)\n",
    "\treturn discuss\n",
    "if __name__ == '__main__':\n",
    "\ttrain_df = pd.read_csv('./data/data_train.csv',header=None,sep='\\t',encoding='gbk')\n",
    "\ttest_df = pd.read_csv('./data/data_test.csv',header=None,sep='\\t',encoding='gbk')\n",
    "\ttrain_df.columns=['id','class','content','label']\n",
    "\ttest_df.columns=['id','class','content']\n",
    "\ttrain_df['content']=train_df['content'].fillna('空')\n",
    "\ttest_df['content']=test_df['content'].fillna('空') \n",
    "\ttrain_df['content'] = train_df['content'].apply(select_cn)\n",
    "\ttest_df['content'] = test_df['content'].apply(select_cn)\n",
    "\ttrain_df = preprocess(train_df)\n",
    "\ttest_df = preprocess(test_df)\n",
    "\ttrain_df.to_csv('./data/train.csv',index=None,encoding='gbk')\n",
    "\ttest_df.to_csv('./data/test.csv',index=None,encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用word2vek训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用wiki训练的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "in_path='./data/seged_wiki.txt'\n",
    "_size = 300\n",
    "model=word2vec.Word2Vec(sentences=LineSentence(in_path), size=_size,sg=1,min_count=5,window=10,workers=30)\n",
    "model.save('./data/wiki'+str(_size)+'_model.m')\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用训练集测试集训练的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "def allget():\n",
    "\tdict1={}\n",
    "\tallword=open('./data/all_fin.csv','w+')\n",
    "\tfile=['./data/train.csv','./data/test.csv']\n",
    "\tfor filename in file:\n",
    "\t\tnum=0\n",
    "\t\twith open(filename,encoding='gbk') as fn:\n",
    "\t\t\tif 'train' in filename:\n",
    "\t\t\t\tfor line in fn:\n",
    "\t\t\t\t\tnum+=1\n",
    "\t\t\t\t\tif num==1:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif line.strip().split(',')[4] in dict1:\n",
    "\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tdict1[line.strip().split(',')[4]]='1'\n",
    "\t\t\t\t\t\t\tallword.write(line.strip().split(',')[4]+'\\n')\n",
    "\t\t\t\t\t\t\tallword.flush()\n",
    "\t\t\tif 'test' in filename:\n",
    "\t\t\t\tfor line in fn:\n",
    "\t\t\t\t\tnum+=1\n",
    "\t\t\t\t\tif num==1:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif line.strip().split(',')[3] in dict1:\n",
    "\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tdict1[line.strip().split(',')[3]]='1'\n",
    "\t\t\t\t\t\t\tallword.write(line.strip().split(',')[3]+'\\n')\n",
    "\t\t\t\t\t\t\tallword.flush()\n",
    "\tallword.close()\n",
    "#allget()\n",
    "in_path='./data/all_fin.csv'\n",
    "_size = 50\n",
    "model=word2vec.Word2Vec(sentences=LineSentence(in_path), size=_size,sg=1,min_count=5,window=10,workers=30)\n",
    "model.save('./data/all_fin_train'+str(_size)+'_model.m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用wiki跟训练集测试集一块训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "in_path='./data/ronghe/seged_wiki_all.txt'\n",
    "_size = 250\n",
    "model=word2vec.Word2Vec(sentences=LineSentence(in_path), size=_size,sg=1,min_count=5,window=10,workers=30)\n",
    "model.save('./data/wiki_and_train'+str(_size)+'_model.m')\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习训练模型（训练得到cnn_Bilstm_unions701_300_100_wiki）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/wiki_and_train300_model.m.wv.vectors.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-97a4be65fe58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgetVec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gbk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gbk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-97a4be65fe58>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mModel1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/wiki_and_train300_model.m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgetVec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \"\"\"\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mcfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s recursively from %s.* with mmap=%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__numpys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/wiki_and_train300_model.m.wv.vectors.npy'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from  keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GlobalMaxPool1D,GRU, Embedding,Bidirectional, Flatten,LSTM, BatchNormalization,Conv1D,MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import *\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import re\n",
    "from keras.layers import Input, Concatenate\n",
    "import numpy as np\n",
    "import gensim\n",
    "from keras.utils import np_utils\n",
    "class Model1:\n",
    "\tdef __init__(self):\n",
    "\t\tself.models = gensim.models.Word2Vec.load('./data/wiki_and_train300_model.m')\n",
    "\tdef getVec(self,msg):\n",
    "\t\treturn self.models[msg]\n",
    "new_model = Model1()\n",
    "train = pd.read_csv('./data/train.csv',encoding='gbk')\n",
    "test = pd.read_csv('./data/test.csv',encoding='gbk')\n",
    "\n",
    "\n",
    "maxlen = 60 #每句话最多留多少词\n",
    "embed_size = 300   #词向量维度\n",
    "train.split_word.fillna('_na_',inplace=True) # inplace=true 在原数据上改\n",
    "test.split_word.fillna('_na_',inplace=True)\n",
    "comment_text = np.hstack([train.split_word.values,test.split_word.values]) # 往后连接起来\n",
    "tok_raw = Tokenizer() # 对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示\n",
    "tok_raw.fit_on_texts(comment_text)  # fit_on_text(texts) 使用一系列文档来生成token词典，texts为list类，每个元素为一个文档\n",
    "train['Discuss_seq'] = tok_raw.texts_to_sequences(train.split_word.values) #texts_to_sequences(texts) 将多个文档转换为word下标的向量形式,shape为[len(texts)，len(text)] -- (文档数，每条文档的长度)\n",
    "word_index = tok_raw.word_index #word_index 一个dict，保存所有word对应的编号id，从1开始\n",
    "test['Discuss_seq'] = tok_raw.texts_to_sequences(test.split_word.values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_keras_data(dataset): \n",
    "\tX={\n",
    "\t\t'Discuss_seq_pre':pad_sequences(dataset.Discuss_seq,maxlen=maxlen)\n",
    "        # 将dataset.Discuss_seq 规范化成长度为maxlen，长度不够的补0，长度过长的截断\n",
    "        # keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) \n",
    "        # 将长为nb_samples的序列（标量序列）转化为形如(nb_samples,nb_timesteps)2D numpy array。\n",
    "        #如果提供了参数maxlen，nb_timesteps=maxlen，否则其值为最长序列的长度。\n",
    "        \n",
    "\t}\n",
    "\treturn X\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "def cnn_lstm():\n",
    "\t#Inputs\n",
    "\tcomment_seq_pre = Input(shape=[maxlen],name='Discuss_seq_pre') #用来实例化一个keras张量\n",
    "\temb_comment1 =Embedding(len(word_index) + 1, embed_size,weights=[embedding_matrix])(comment_seq_pre)\n",
    "\tcon = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_comment1)\n",
    "\tmaxp = MaxPooling1D(pool_size=2)(con)\n",
    "\tlst_1=Dropout(0.5)(LSTM(units=64, activation='tanh')(maxp))\n",
    "\t#lst_1=(LSTM(units=64, activation='tanh',recurrent_dropout=0.2)(maxp))\n",
    "\toutput = Dense(3,activation='softmax')(lst_1)\n",
    "\tmodel = Model(inputs=[comment_seq_pre],outputs=[output])\n",
    "\tadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", fmeasure])\n",
    "\treturn model\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "\n",
    "def cnn_Bilstm():\n",
    "\tdef attention_3d_block(inputs):\n",
    "\t    #input_dim = int(inputs.shape[2])\n",
    "\t    a = Permute((2, 1))(inputs)\n",
    "\t    a = Dense(int(maxlen/2), activation='softmax')(a)\n",
    "\t    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "\t    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "\t    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "\t    return output_attention_mul\n",
    "\n",
    "\t# build RNN model with attention\n",
    "\tcomment_seq_pre = Input(shape=[maxlen],name='Discuss_seq_pre')\n",
    "\temb_comment1 =Embedding(len(word_index) + 1, embed_size,weights=[embedding_matrix])(comment_seq_pre)\n",
    "\tcon = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(emb_comment1) #60\n",
    "\tmaxp = MaxPooling1D(pool_size=2)(con)\n",
    "\tlstm_out = Bidirectional(LSTM(units=16, return_sequences=True,kernel_regularizer=regularizers.l1(0.01)), name='bilstm')(maxp)\n",
    "\tattention_mul = attention_3d_block(lstm_out)\n",
    "\tattention_flatten = Flatten()(attention_mul)\n",
    "\tdrop2 = Dropout(0.5)(attention_flatten)\n",
    "\t#fla = Dense(50)(drop2)\n",
    "\toutput = Dense(3,activation='softmax')(drop2)\n",
    "\tmodel = Model(inputs=[comment_seq_pre],outputs=[output])\n",
    "\tadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", fmeasure]) # fmeasure 评价指标函数\n",
    "\treturn model\n",
    "def cnn():\n",
    "\tcomment_seq_pre = Input(shape=[maxlen],name='Discuss_seq_pre')\n",
    "\temb_comment1 =Embedding(len(word_index) + 1, embed_size,weights=[embedding_matrix])(comment_seq_pre)\n",
    "\tcon1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_comment1)\n",
    "\tmaxp1 = MaxPooling1D(pool_size=2)(con1)\n",
    "\n",
    "#    con2 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(maxp1)\n",
    "#    maxp2 = MaxPooling1D(pool_size=2)(con2)\n",
    "#    con3 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(maxp2)\n",
    "#    maxp3 = MaxPooling1D(pool_size=2)(con3)\n",
    "\n",
    "\tx = Flatten()(maxp1)  #平滑\n",
    "\toutput = Dense(3,activation='softmax')(x)\n",
    "\tmodel = Model(inputs=[comment_seq_pre],outputs=[output])\n",
    "\tadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", fmeasure])\n",
    "\treturn model\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "\ttry:\n",
    "\t\tembedding_vector = new_model.getVec(word)\n",
    "\t\tif embedding_vector is not None:\n",
    "\t\t\tembedding_matrix[i] = embedding_vector #所有词的词向量\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\t\t\n",
    "X_train =get_keras_data(train)\n",
    "X_test = get_keras_data(test)\n",
    "y_train = np_utils.to_categorical(train.label.values) #转换成one-hot编码\n",
    "batch_size = 128#512 #300\n",
    "epochs = 10\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "from keras.models import load_model\n",
    "callbacks_list = [early_stopping]\n",
    "#model1 = cnn_lstm()\n",
    "model1 = cnn_Bilstm()\n",
    "#model1 = cnn()\n",
    "model1.summary()\n",
    "model1.fit(X_train, y_train,\n",
    "\t\tvalidation_split=0.1,#zuihou %10作为验证集   十折交叉验证\n",
    "\t\tbatch_size=batch_size, \n",
    "\t\tepochs=epochs, \n",
    "\t\tshuffle = True,\n",
    "\t\tcallbacks=callbacks_list)\n",
    "\n",
    "# json_string = model1.to_json()\n",
    "# open('../model/cnn_lstm.json','w').write(json_string)\n",
    "\n",
    "# model1.save_weights('../model/cnn_lstm_weights.h5')\n",
    "\n",
    "      \n",
    "preds = model1.predict(X_test)\n",
    "print(preds)\n",
    "result = []\n",
    "with open(\"./result/result_cnn_Bilstm_unions701_300_100_wiki.txt\",\"w\") as ipt:\n",
    "    for line in preds:\n",
    "        for i in line:\n",
    "            ipt.write(str(i)+\" \")\n",
    "        ipt.write('\\n')\n",
    "for i in preds:\n",
    "\tr = i.tolist().index(max(i.tolist()))\n",
    "\tresult.append(r)\n",
    "a = range(1,len(result)+1)\n",
    "a = np.array(a)\n",
    "df = pd.DataFrame({'ind':a,'pre':result})\n",
    "df.to_csv('./result/cnn_Bilstm_unions701_300_100_wiki.csv',index = False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习训练模型（训练得到result_cnn_Bilstm_616）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Discuss_seq_pre (InputLayer)    (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 40, 50)       3622150     Discuss_seq_pre[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 40, 256)      38656       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 20, 256)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bilstm (Bidirectional)          (None, 20, 32)       34944       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 32, 20)       0           bilstm[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32, 20)       420         permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 20, 32)       0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 20, 32)       0           bilstm[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 640)          0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 640)          0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            1923        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,698,093\n",
      "Trainable params: 3,698,093\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 73822 samples, validate on 8203 samples\n",
      "Epoch 1/10\n",
      "73822/73822 [==============================] - 52s 704us/step - loss: 2.2821 - mean_absolute_error: 0.1456 - fmeasure: 0.8421 - val_loss: 0.3420 - val_mean_absolute_error: 0.1313 - val_fmeasure: 0.9431\n",
      "Epoch 2/10\n",
      "73822/73822 [==============================] - 49s 670us/step - loss: 0.2842 - mean_absolute_error: 0.0832 - fmeasure: 0.9219 - val_loss: 0.2574 - val_mean_absolute_error: 0.0819 - val_fmeasure: 0.9492\n",
      "Epoch 3/10\n",
      "73822/73822 [==============================] - 49s 667us/step - loss: 0.2408 - mean_absolute_error: 0.0687 - fmeasure: 0.9321 - val_loss: 0.2427 - val_mean_absolute_error: 0.0698 - val_fmeasure: 0.9478\n",
      "Epoch 4/10\n",
      "73822/73822 [==============================] - 50s 671us/step - loss: 0.2088 - mean_absolute_error: 0.0586 - fmeasure: 0.9415 - val_loss: 0.2432 - val_mean_absolute_error: 0.0664 - val_fmeasure: 0.9437\n",
      "Epoch 5/10\n",
      "73822/73822 [==============================] - 50s 671us/step - loss: 0.1840 - mean_absolute_error: 0.0498 - fmeasure: 0.9519 - val_loss: 0.3165 - val_mean_absolute_error: 0.0833 - val_fmeasure: 0.9167\n",
      "Epoch 6/10\n",
      "73822/73822 [==============================] - 50s 673us/step - loss: 0.1647 - mean_absolute_error: 0.0433 - fmeasure: 0.9590 - val_loss: 0.2684 - val_mean_absolute_error: 0.0597 - val_fmeasure: 0.9306\n",
      "[[6.8212831e-03 1.6155674e-01 8.3162200e-01]\n",
      " [1.4114757e-02 3.3233288e-01 6.5355229e-01]\n",
      " [5.4788947e-02 7.0111465e-01 2.4409646e-01]\n",
      " ...\n",
      " [4.5224722e-03 6.9032222e-02 9.2644525e-01]\n",
      " [9.3049335e-01 5.3832065e-02 1.5674554e-02]\n",
      " [3.3781072e-04 4.5358948e-03 9.9512625e-01]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# import fasttext\n",
    "from  keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GlobalMaxPool1D,GRU, Embedding,Bidirectional, Flatten,LSTM, BatchNormalization,Conv1D,MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import *\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import re\n",
    "from keras.layers import Input, Concatenate\n",
    "import numpy as np\n",
    "import gensim\n",
    "from keras.utils import np_utils\n",
    "class Model1:\n",
    "\tdef __init__(self):\n",
    "\t\tself.models = gensim.models.Word2Vec.load('./data/wiki_and_train50_model.m')\n",
    "        #self.models = gensim.models.Word2Vec.load('./data/wiki_and_train300_model.m')\n",
    "\tdef getVec(self,msg):\n",
    "\t\treturn self.models[msg]\n",
    "new_model = Model1()\n",
    "train = pd.read_csv('./data/train.csv',encoding='gbk')\n",
    "test = pd.read_csv('./data/test.csv',encoding='gbk')\n",
    "\n",
    "\n",
    "maxlen = 40\n",
    "embed_size = 50\n",
    "train.split_word.fillna('_na_',inplace=True)\n",
    "test.split_word.fillna('_na_',inplace=True)\n",
    "comment_text = np.hstack([train.split_word.values,test.split_word.values])\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(comment_text)\n",
    "train['Discuss_seq'] = tok_raw.texts_to_sequences(train.split_word.values)\n",
    "word_index = tok_raw.word_index\n",
    "test['Discuss_seq'] = tok_raw.texts_to_sequences(test.split_word.values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_keras_data(dataset): \n",
    "\tX={\n",
    "\t\t'Discuss_seq_pre':pad_sequences(dataset.Discuss_seq,maxlen=maxlen)\n",
    "\t}\n",
    "\treturn X\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "def cnn_lstm():\n",
    "\t#Inputs\n",
    "\tcomment_seq_pre = Input(shape=[maxlen],name='Discuss_seq_pre')\n",
    "\temb_comment1 =Embedding(len(word_index) + 1, embed_size,weights=[embedding_matrix])(comment_seq_pre)\n",
    "\tcon = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_comment1)\n",
    "\tmaxp = MaxPooling1D(pool_size=2)(con)\n",
    "\tlst_1=Dropout(0.5)(LSTM(units=64, activation='tanh')(maxp))\n",
    "\t#lst_1=(LSTM(units=64, activation='tanh',recurrent_dropout=0.2)(maxp))\n",
    "\toutput = Dense(3,activation='softmax')(lst_1)\n",
    "\tmodel = Model(inputs=[comment_seq_pre],outputs=[output])\n",
    "\tadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", fmeasure])\n",
    "\treturn model\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "\n",
    "def cnn_Bilstm():\n",
    "\tdef attention_3d_block(inputs):\n",
    "\t    #input_dim = int(inputs.shape[2])\n",
    "\t    a = Permute((2, 1))(inputs)\n",
    "\t    a = Dense(int(maxlen/2), activation='softmax')(a)\n",
    "\t    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "\t    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "\t    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "\t    return output_attention_mul\n",
    "\n",
    "\t# build RNN model with attention\n",
    "\tcomment_seq_pre = Input(shape=[maxlen],name='Discuss_seq_pre')\n",
    "\temb_comment1 =Embedding(len(word_index) + 1, embed_size,weights=[embedding_matrix])(comment_seq_pre)\n",
    "\tcon = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(emb_comment1) #60\n",
    "\tmaxp = MaxPooling1D(pool_size=2)(con)\n",
    "\tlstm_out = Bidirectional(LSTM(units=16, return_sequences=True,kernel_regularizer=regularizers.l1(0.01)), name='bilstm')(maxp)\n",
    "\tattention_mul = attention_3d_block(lstm_out)\n",
    "\tattention_flatten = Flatten()(attention_mul)\n",
    "\tdrop2 = Dropout(0.5)(attention_flatten)\n",
    "\t#fla = Dense(50)(drop2)\n",
    "\toutput = Dense(3,activation='softmax')(drop2)\n",
    "\tmodel = Model(inputs=[comment_seq_pre],outputs=[output])\n",
    "\tadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", fmeasure])\n",
    "\treturn model\n",
    "def cnn():\n",
    "\tcomment_seq_pre = Input(shape=[maxlen],name='Discuss_seq_pre')\n",
    "\temb_comment1 =Embedding(len(word_index) + 1, embed_size,weights=[embedding_matrix])(comment_seq_pre)\n",
    "\tcon1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(emb_comment1)\n",
    "\tmaxp1 = MaxPooling1D(pool_size=2)(con1)\n",
    "\n",
    "#    con2 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(maxp1)\n",
    "#    maxp2 = MaxPooling1D(pool_size=2)(con2)\n",
    "#    con3 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(maxp2)\n",
    "#    maxp3 = MaxPooling1D(pool_size=2)(con3)\n",
    "\n",
    "\tx = Flatten()(maxp1)\n",
    "\toutput = Dense(3,activation='softmax')(x)\n",
    "\tmodel = Model(inputs=[comment_seq_pre],outputs=[output])\n",
    "\tadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", fmeasure])\n",
    "\treturn model\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "\ttry:\n",
    "\t\tembedding_vector = new_model.getVec(word)\n",
    "\t\tif embedding_vector is not None:\n",
    "\t\t\tembedding_matrix[i] = embedding_vector\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\t\t\n",
    "X_train =get_keras_data(train)\n",
    "X_test = get_keras_data(test)\n",
    "y_train = np_utils.to_categorical(train.label.values)\n",
    "batch_size = 128#512 #300\n",
    "epochs = 10\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "from keras.models import load_model\n",
    "callbacks_list = [early_stopping]\n",
    "#model1 = cnn_lstm()\n",
    "model1 = cnn_Bilstm()\n",
    "#model1 = cnn()\n",
    "model1.summary()\n",
    "model1.fit(X_train, y_train,\n",
    "\t\tvalidation_split=0.1,#zuihou %10作为验证集\n",
    "\t\tbatch_size=batch_size, \n",
    "\t\tepochs=epochs, \n",
    "\t\tshuffle = True,\n",
    "\t\tcallbacks=callbacks_list)\n",
    "\n",
    "# json_string = model1.to_json()\n",
    "# open('../model/cnn_lstm.json','w').write(json_string)\n",
    "\n",
    "# model1.save_weights('../model/cnn_lstm_weights.h5')\n",
    "\n",
    "      \n",
    "preds = model1.predict(X_test)\n",
    "print(preds)\n",
    "result = []\n",
    "with open(\"./result/result_cnn_Bilstm_616.txt\",\"w\") as ipt:\n",
    "    for line in preds:\n",
    "        for i in line:\n",
    "            ipt.write(str(i)+\" \")\n",
    "        ipt.write('\\n')\n",
    "for i in preds:\n",
    "\tr = i.tolist().index(max(i.tolist()))\n",
    "\tresult.append(r)\n",
    "a = range(1,len(result)+1)\n",
    "a = np.array(a)\n",
    "df = pd.DataFrame({'ind':a,'pre':result})\n",
    "df.to_csv('./result/cnn_Bilstm_616.csv',index = False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':[1,2,3],'b':[4,5,6]}\n",
    "import pandas as pd\n",
    "import copy\n",
    "b = pd.DataFrame(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  9  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = copy.deepcopy(b)\n",
    "c['a'][1] =  9\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-11-3f83be8f96fc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-3f83be8f96fc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    'test'+str(0) = b\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "'test'+str(0) = b\n",
    "'test'+str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
