{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "train_file = './data1/data_train1.csv'\n",
    "test_file = './data1/data_test1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "def seg(path,train):\n",
    "    oral_csv = pd.read_csv(path,delimiter = '\\t',header = None, index_col=False)\n",
    "    com = oral_csv.loc[:,2]\n",
    "    docs = np.array(com)\n",
    "\n",
    "    seged_corpus = []\n",
    "    if train:\n",
    "        seged_file = open('./data1/train_seg.txt','w')\n",
    "    else:\n",
    "        seged_file = open('./data1/test_seg.txt','w')\n",
    "    for line in docs:\n",
    "        line = str(line).strip()\n",
    "        words = jieba.cut(line)\n",
    "        words = \" \".join(words)\n",
    "        seged_file.write(words.encode('utf-8'))\n",
    "        seged_file.write('\\n')\n",
    "        seged_corpus.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg(test_file,False)\n",
    "seg(train_file,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入类别信息生成的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named lightgbm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a4af7dbc92da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msen_model_cate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yinhao/code/比赛/sen_submit/sen_model_cate.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named lightgbm"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sen_model_cate import *\n",
    "from sklearn import preprocessing\n",
    "from util import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "class CONFIG(object):\n",
    "    METHOD = 'lr'\n",
    "    SEED_POS = './data1/seed/sum-positive.txt'  # 正向情感词典\n",
    "    SEED_NEG = './data1/seed/sum-negative.txt'  # 负向情感词典\n",
    "    SPT = '\\t'\n",
    "    GEN_FEAT_METHOD = 'seed,unigram,bigram'\n",
    "    N_FEAT = 5000\n",
    "    FREQ = False\n",
    "seed_pos = gen_seed_dic(CONFIG.SEED_POS)\n",
    "seed_neg = gen_seed_dic(CONFIG.SEED_NEG)\n",
    "param = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'regression_l2',\n",
    "          'metric': 'mse',\n",
    "          'min_child_weight': 1.5,\n",
    "          'num_leaves': 2**5,\n",
    "          'lambda_l2': 10,\n",
    "          'subsample': 0.7,\n",
    "          'colsample_bytree': 0.5,\n",
    "          'colsample_bylevel': 0.5,\n",
    "          'learning_rate': 0.1,\n",
    "          'scale_pos_weight': 20,\n",
    "          'seed': 2018,\n",
    "          'nthread': 4,\n",
    "          'silent': True,\n",
    "}\n",
    "test_file = './data1/data_test1.csv'\n",
    "test_csv = pd.read_csv(test_file,delimiter = '\\t',header = None, index_col=False)\n",
    "test_com = test_csv.loc[:,1].values\n",
    "test_doc = np.array(test_com)\n",
    "\n",
    "train_file = './data1/data_train1.csv'\n",
    "train_csv = pd.read_csv(train_file,delimiter = '\\t',header = None, index_col=False)\n",
    "train_target = train_csv.loc[:,3].values\n",
    "train_com = train_csv.loc[:,1].values\n",
    "train_doc = np.array(train_com)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()  # 标准化标签\n",
    "label_encoder.fit(train_doc)\n",
    "\n",
    "train_cate = label_encoder.transform(train_doc)\n",
    "train_cate = train_cate.reshape((len(train_cate),1))\n",
    "\n",
    "train_docs = []\n",
    "with open('./data1/train_seg.txt', 'r') as ipt:\n",
    "    for line in ipt.readlines():\n",
    "        items = line.strip().split(' ')\n",
    "        train_docs.append(items)\n",
    "\n",
    "test_docs = []\n",
    "with open('./data1/test_seg.txt', 'r') as ipt:\n",
    "    for line in ipt.readlines():\n",
    "        items = line.strip().split(' ')\n",
    "        test_docs.append(items)\n",
    "\n",
    "s_model = sen_model(method='lr',\n",
    "                    docs=train_docs,\n",
    "                    cate=train_cate,\n",
    "                    target=train_target,\n",
    "                    test=test_docs,\n",
    "                    seed_pos=seed_pos,\n",
    "                    seed_neg=seed_neg,\n",
    "                    gen_feat_method=CONFIG.GEN_FEAT_METHOD,\n",
    "                    n_feat=CONFIG.N_FEAT,\n",
    "                    freq=CONFIG.FREQ,\n",
    "                    param=param)\n",
    "test = s_model.test\n",
    "test_cate = label_encoder.transform(test_doc)\n",
    "test_cate = test_cate.reshape((len(test_cate),1))\n",
    "test = np.hstack((test,test_cate))\n",
    "lr_result = s_model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写入预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./result/result_lr_5000_cate.txt\",\"w\") as ipt:\n",
    "    for line in lr_result:\n",
    "        for i in line:\n",
    "            ipt.write(str(i)+\" \")\n",
    "        ipt.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 未加入类别信息生成的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start init...\n",
      "start gen unigram feat..\n",
      "gen unigram feat successed..\n",
      "start init...\n",
      "start gen bigram feat..\n",
      "gen bigram feat successed..\n",
      "start init...\n",
      "start gen seed feat..\n",
      "start training model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict...\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sen_model import *\n",
    "from sklearn import preprocessing\n",
    "from util import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "class CONFIG(object):\n",
    "    METHOD = 'lr'\n",
    "    SEED_POS = './data/seed/sum-positive.txt'\n",
    "    SEED_NEG = './data/seed/sum-negative.txt'\n",
    "    SPT = '\\t'\n",
    "    GEN_FEAT_METHOD = 'seed,unigram,bigram'\n",
    "    N_FEAT = 5000  # 前五千个作为特征\n",
    "    FREQ = False   # 有ngram特征为1，没有为0    True时是指 频率，出现的次数\n",
    "seed_pos = gen_seed_dic(CONFIG.SEED_POS)\n",
    "seed_neg = gen_seed_dic(CONFIG.SEED_NEG)\n",
    "param = {     #lightgbm 参数\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'regression_l2',\n",
    "          'metric': 'mse',\n",
    "          'min_child_weight': 1.5,\n",
    "          'num_leaves': 2**5,\n",
    "          'lambda_l2': 10,\n",
    "          'subsample': 0.7,\n",
    "          'colsample_bytree': 0.5,\n",
    "          'colsample_bylevel': 0.5,\n",
    "          'learning_rate': 0.1,\n",
    "          'scale_pos_weight': 20,\n",
    "          'seed': 2018,\n",
    "          'nthread': 4,\n",
    "          'silent': True,\n",
    "}\n",
    "test_file = './data/data_test1.csv'\n",
    "test_csv = pd.read_csv(test_file,delimiter = '\\t',header = None, index_col=False,encoding='gbk')\n",
    "\n",
    "train_file = './data/data_train1.csv'\n",
    "train_csv = pd.read_csv(train_file,delimiter = '\\t',header = None, index_col=False,encoding='gbk')\n",
    "train_target = train_csv.loc[:,3].values\n",
    "\n",
    "train_docs = []\n",
    "with open('./data/train_seg.txt', 'r') as ipt:\n",
    "    for line in ipt.readlines():\n",
    "        items = line.strip().split(' ')\n",
    "        train_docs.append(items)\n",
    "\n",
    "test_docs = []\n",
    "with open('./data/test_seg.txt', 'r') as ipt:\n",
    "    for line in ipt.readlines():\n",
    "        items = line.strip().split(' ')\n",
    "        test_docs.append(items)\n",
    "\n",
    "s_model = sen_model(method='lr',\n",
    "                    docs=train_docs,\n",
    "                    target=train_target,\n",
    "                    test=test_docs,\n",
    "                    seed_pos=seed_pos,\n",
    "                    seed_neg=seed_neg,\n",
    "                    gen_feat_method=CONFIG.GEN_FEAT_METHOD,\n",
    "                    n_feat=CONFIG.N_FEAT,\n",
    "                    freq=CONFIG.FREQ,\n",
    "                    param=param)\n",
    "test = s_model.test\n",
    "lr_result = s_model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写入预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./result/result_lr_5000.txt\",\"w\") as ipt:\n",
    "    for line in lr_result:\n",
    "        for i in line:\n",
    "            ipt.write(str(i)+\" \")\n",
    "        ipt.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合生成ronghe_cnnBilstm701_300_100_wiki_lr_cate文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./result/result_lr_5000_cate.txt') as ipt:\n",
    "    lr = []\n",
    "    for line in ipt:\n",
    "        line = line.strip().split()\n",
    "        lr.append(line)\n",
    "with open('./result/result_cnn_Bilstm_unions701_300_100_wiki.txt','r') as f:\n",
    "    cnn_lstm = []\n",
    "    for ls in f:\n",
    "        ls = ls.strip().split()\n",
    "        cnn_lstm.append(ls)\n",
    "jg = []\n",
    "for j in range(len(lr)):\n",
    "    jg1 = []\n",
    "    for k in range(len(lr[0])):\n",
    "        c = float(lr[j][k])+float(cnn_lstm[j][k])\n",
    "        jg1.append(c)\n",
    "    jg.append(jg1)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "result = []\n",
    "for i in jg:\n",
    "    r = i.index(max(i))\n",
    "    result.append(r)\n",
    "a = range(1,len(result)+1)\n",
    "a = np.array(a)\n",
    "df = pd.DataFrame({'ind':a,'pre':result})\n",
    "df.to_csv('./result/ronghe_cnnBilstm701_300_100_wiki_lr_cate.csv',index = False,header=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合生成ronghe_cnnBilstm616_lr_cate文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./result/result_lr_5000_cate.txt') as ipt:\n",
    "    lr = []\n",
    "    for line in ipt:\n",
    "        line = line.strip().split()\n",
    "        lr.append(line)\n",
    "with open('./result/result_cnn_Bilstm_616.txt','r') as f:\n",
    "    cnn_lstm = []\n",
    "    for ls in f:\n",
    "        ls = ls.strip().split()\n",
    "        cnn_lstm.append(ls)\n",
    "jg = []\n",
    "for j in range(len(lr)):\n",
    "    jg1 = []\n",
    "    for k in range(len(lr[0])):\n",
    "        c = float(lr[j][k])+float(cnn_lstm[j][k])\n",
    "        jg1.append(c)\n",
    "    jg.append(jg1)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "result = []\n",
    "for i in jg:\n",
    "    r = i.index(max(i))\n",
    "    result.append(r)\n",
    "a = range(1,len(result)+1)\n",
    "a = np.array(a)\n",
    "df = pd.DataFrame({'ind':a,'pre':result})\n",
    "df.to_csv('./result/ronghe_cnnBilstm616_lr_cate.csv',index = False,header=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合生成ronghe_cnnBilstm616_lr文件 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./result/result_lr_5000.txt') as ipt:\n",
    "    lr = []\n",
    "    for line in ipt:\n",
    "        line = line.strip().split()\n",
    "        lr.append(line)\n",
    "with open('./result/result_cnn_Bilstm_616.txt','r') as f:\n",
    "    cnn_lstm = []\n",
    "    for ls in f:\n",
    "        ls = ls.strip().split()\n",
    "        cnn_lstm.append(ls)\n",
    "jg = []\n",
    "for j in range(len(lr)):\n",
    "    jg1 = []\n",
    "    for k in range(len(lr[0])):\n",
    "        c = float(lr[j][k])+float(cnn_lstm[j][k])\n",
    "        jg1.append(c)\n",
    "    jg.append(jg1)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "result = []\n",
    "for i in jg:\n",
    "    r = i.index(max(i))\n",
    "    result.append(r)\n",
    "a = range(1,len(result)+1)\n",
    "a = np.array(a)\n",
    "df = pd.DataFrame({'ind':a,'pre':result})\n",
    "df.to_csv('./result/ronghe_cnnBilstm616_lr.csv',index = False,header=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最终的结果融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "result1 = './result/ronghe_cnnBilstm616_lr_cate.csv'\n",
    "result2 = './result/ronghe_cnnBilstm616_lr.csv'\n",
    "result3 = './result/ronghe_cnnBilstm701_300_100_wiki_lr_cate.csv'\n",
    "result1_csv = pd.read_csv(result1,delimiter = ',',header = None, index_col=False)\n",
    "result2_csv = pd.read_csv(result2,delimiter = ',',header = None, index_col=False)\n",
    "result3_csv = pd.read_csv(result3,delimiter = ',',header = None, index_col=False)\n",
    "result1_result = result1_csv.loc[:,1].values\n",
    "result2_result = result2_csv.loc[:,1].values\n",
    "result3_result = result3_csv.loc[:,1].values\n",
    "result_2 = ((result1_result==2) + 0)+ ((result2_result==2) + 0) +  ((result3_result==2) + 0)\n",
    "result_0 =((result1_result==0) + 0)+ ((result2_result==0) + 0) +  ((result3_result==0) + 0)\n",
    "result = list(map(lambda x,y:2 if x >= 2 else(0 if y >=2 else 1),result_2,result_0))\n",
    "# 写入结果\n",
    "with open(\"./result/result.csv\",\"w+\") as outer:\n",
    "    writer = csv.writer(outer)\n",
    "    for j in range(len(result)):\n",
    "        writer.writerow([j+1, result[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python27",
   "language": "python",
   "name": "python27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
