{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Glove\n",
    "from glove import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GlobalMaxPool1D,GRU, Embedding,Bidirectional, Flatten,LSTM, BatchNormalization,Conv1D,MaxPooling1D\n",
    "from keras.layers import GlobalMaxPooling1D,Concatenate,Activation, Lambda,SimpleRNN,LSTM,GRU,Bidirectional,TimeDistributed\n",
    "from keras.layers import concatenate, GlobalMaxPooling1D,SpatialDropout1D,GlobalAveragePooling1D,GlobalAveragePooling1D\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "def fmeasure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(_filters,_kernel,_drop):\n",
    "    comment_seq_pre = Input(shape=[maxlen],name='Discuss_seq_pre')\n",
    "    emb_comment1 =Embedding(len(word_index) + 1, embed_size,weights=[embedding_matrix])(comment_seq_pre)\n",
    "    con1 = Conv1D(filters=_filters, kernel_size=_kernel, padding='same', activation='relu')(emb_comment1)\n",
    "    maxp1 = MaxPooling1D(pool_size=2)(con1)\n",
    "    fla = Flatten()(maxp1)\n",
    "    drop = Dropout(_drop)(fla)\n",
    "    output = Dense(3,activation='softmax')(drop)\n",
    "    model = Model(inputs=[comment_seq_pre],outputs=[output])\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", fmeasure])\n",
    "    return model\n",
    "\n",
    "def cnn_gru(_filters, _kernel, _neurons, _drop):\n",
    "    comment_seq_pre = Input(shape=[maxlen])\n",
    "    emb_comment = Embedding(len(word_index) + 1,embed_size,weights=[embedding_matrix],input_length=maxlen)(comment_seq_pre)\n",
    "    con1 = Conv1D(filters=_filters, kernel_size=_kernel, padding='same', activation='relu')(emb_comment)\n",
    "    maxp1 = MaxPooling1D(pool_size=2)(con1)\n",
    "    gru = GRU(units=_neurons, activation='tanh')(maxp1)\n",
    "    drop1 = Dropout(_drop)(gru)\n",
    "    output = Dense(3, activation=\"softmax\")(drop1)\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model = Model(inputs=comment_seq_pre, outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=[\"mae\", fmeasure])\n",
    "    return model\n",
    "\n",
    "def cnn_lstm(_filters, _kernel, _neurons, _drop):\n",
    "    comment_seq_pre = Input(shape=[maxlen])\n",
    "    emb_comment1 =Embedding(len(word_index) + 1, embed_size,weights=[embedding_matrix])(comment_seq_pre)\n",
    "    con = Conv1D(filters=_filters, kernel_size=_kernel, padding='same', activation='relu')(emb_comment1)\n",
    "    maxp = MaxPooling1D(pool_size=2)(con)\n",
    "    lst_1=Dropout(_drop)(LSTM(units=_neurons, activation='tanh')(maxp))\n",
    "    output = Dense(3,activation='softmax')(lst_1)\n",
    "    model = Model(inputs=[comment_seq_pre],outputs=[output])\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", fmeasure])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>买这套系统本来是用来做我们公司的公众号第三方平台代运营的，没想到还有app，而且每个都很方便...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>烤鸭还是不错的，别的菜没什么特殊的</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>使用说明看不懂！不会用，很多操作没详细标明！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>越来越不好了，菜品也少了，服务也不及时。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>是在是不知道该吃什么好、就来了</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id class                                            content  label\n",
       "0   1  食品餐饮  买这套系统本来是用来做我们公司的公众号第三方平台代运营的，没想到还有app，而且每个都很方便...      2\n",
       "1   2  食品餐饮                                  烤鸭还是不错的，别的菜没什么特殊的      1\n",
       "2   3  食品餐饮                             使用说明看不懂！不会用，很多操作没详细标明！      0\n",
       "3   4  食品餐饮                               越来越不好了，菜品也少了，服务也不及时。      0\n",
       "4   5  食品餐饮                                    是在是不知道该吃什么好、就来了      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/data_train.csv\",header=None,sep='\\t',encoding='gbk')\n",
    "data.columns = ['id','class','content','label']\n",
    "data['content'] = data['content'].fillna('空')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "停用词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "stopwords = []\n",
    "with open(\"./data/stopwords.txt\",encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        stopwords.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_map(arr):\n",
    "    res = \"\"\n",
    "    for c in arr:\n",
    "        if c not in stopwords and c != ' ' and c != '\\xa0'and c != '\\n' and c != '\\ufeff' and c != '\\r':\n",
    "            res += c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到字符级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char(arr):\n",
    "    res = []\n",
    "    for c in arr:\n",
    "        res.append(c)\n",
    "    return list(res)\n",
    "data['char'] = data.content.map(lambda x: filter_map(x))\n",
    "data['char'] = data.content.map(lambda x: get_char(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到词语级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/nt/gh88gm3j7xndfrl3xyqv6zy40000gn/T/jieba.cache\n",
      "Loading model cost 0.687 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "data['word'] = data['content'].apply(lambda x:list(jieba.cut(x)))\n",
    "data.to_csv(\"./data/data.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10405</th>\n",
       "      <td>10406</td>\n",
       "      <td>食品餐饮</td>\n",
       "      <td>环境挺好的，地方很大，蛋糕挺好吃的，不错</td>\n",
       "      <td>2</td>\n",
       "      <td>['环境', '挺', '好', '的', '，', '地方', '很大', '，', '蛋...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37421</th>\n",
       "      <td>37422</td>\n",
       "      <td>金融服务</td>\n",
       "      <td>我一个人在外地上班，刚开始嘛也没有挣多少钱，朋友推荐我了这个网站，我用了一次，全款买了个电脑...</td>\n",
       "      <td>2</td>\n",
       "      <td>['我', '一个', '人', '在', '外地', '上班', '，', '刚', '开...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68427</th>\n",
       "      <td>68428</td>\n",
       "      <td>物流快递</td>\n",
       "      <td>马氏鼎好物流公司，还可以，我们公司发空运的货物流全部找他们，速度快，价格合理，关键是查货方便...</td>\n",
       "      <td>2</td>\n",
       "      <td>['马氏鼎', '好', '物流', '公司', '，', '还', '可以', '，', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80884</th>\n",
       "      <td>80885</td>\n",
       "      <td>物流快递</td>\n",
       "      <td>时效很快，服务很好，双清关派送到门，对得起这个价格，赞！</td>\n",
       "      <td>2</td>\n",
       "      <td>['时效', '很快', '，', '服务', '很', '好', '，', '双清', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25191</th>\n",
       "      <td>25192</td>\n",
       "      <td>旅游住宿</td>\n",
       "      <td>其实旅游网都差不多，途牛好一点的是售后，不像有些，交完钱他就是大爷，对你不管不问。去欧洲回来...</td>\n",
       "      <td>2</td>\n",
       "      <td>['其实', '旅游网', '都', '差不多', '，', '途牛', '好', '一点'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id class                                            content  label  \\\n",
       "10405  10406  食品餐饮                               环境挺好的，地方很大，蛋糕挺好吃的，不错      2   \n",
       "37421  37422  金融服务  我一个人在外地上班，刚开始嘛也没有挣多少钱，朋友推荐我了这个网站，我用了一次，全款买了个电脑...      2   \n",
       "68427  68428  物流快递  马氏鼎好物流公司，还可以，我们公司发空运的货物流全部找他们，速度快，价格合理，关键是查货方便...      2   \n",
       "80884  80885  物流快递                       时效很快，服务很好，双清关派送到门，对得起这个价格，赞！      2   \n",
       "25191  25192  旅游住宿  其实旅游网都差不多，途牛好一点的是售后，不像有些，交完钱他就是大爷，对你不管不问。去欧洲回来...      2   \n",
       "\n",
       "                                                    word  \n",
       "10405  ['环境', '挺', '好', '的', '，', '地方', '很大', '，', '蛋...  \n",
       "37421  ['我', '一个', '人', '在', '外地', '上班', '，', '刚', '开...  \n",
       "68427  ['马氏鼎', '好', '物流', '公司', '，', '还', '可以', '，', ...  \n",
       "80884  ['时效', '很快', '，', '服务', '很', '好', '，', '双清', '...  \n",
       "25191  ['其实', '旅游网', '都', '差不多', '，', '途牛', '好', '一点'...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all = pd.read_csv('./data/data.csv')\n",
    "\n",
    "#随机扰乱\n",
    "data_all = shuffle(data_all,random_state=0)\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " data = data_all[data_all['class'] != '金融服务']  #四个类别的数据集  调参用\n",
    "#data = data_all # 五个类别的数据集  验证模型适用于不同领域的隶属度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_char = []\n",
    "for s in data[\"char\"]:\n",
    "    line_char.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_word = []\n",
    "for s in data['word']:\n",
    "    line_word.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练词向量  _size维度:50 100 200 300       _type:word char         _embedding:word2vec fasttext glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(_size,_type,_embeding):\n",
    "    if _embeding == 'word2vec':\n",
    "        if _type == 'char':\n",
    "            word2vec_model = Word2Vec(line_char, size=_size,min_count=1,window=5,workers=4, iter=15)\n",
    "            word2vec_model.save('./embeding/word2vec_char_'+str(_size)+'.vector')\n",
    "        else:\n",
    "            word2vec_model = Word2Vec(line_word, size=_size,min_count=1,window=5,workers=4, iter=15)\n",
    "            word2vec_model.save('./embeding/word2vec_word_'+str(_size)+'.vector')\n",
    "    elif _embeding == 'fasttext':\n",
    "        if _type == 'char':\n",
    "            fasttext_model = FastText(line_char, size=_size,window=10,min_count=1,iter=15)\n",
    "            fasttext_model.save('./embeding/fasttext_char_'+str(_size)+'.vector')\n",
    "        else:\n",
    "            fasttext_model = FastText(line_word, size=_size,window=10,min_count=1,iter=15)\n",
    "            fasttext_model.save('./embeding/fasttext_word_'+str(_size)+'.vector')\n",
    "    elif _embeding =='glove':\n",
    "        if _type == 'char':\n",
    "            corpus_model = Corpus()\n",
    "            corpus_model.fit(line_char, window=10)\n",
    "            glove = Glove(no_components=_size, learning_rate=0.05)\n",
    "            glove.fit(corpus_model.matrix, epochs=10,no_threads=1, verbose=True)\n",
    "            glove.add_dictionary(corpus_model.dictionary)\n",
    "            glove.save('./embeding/glove_char_'+str(_size)+'.model')\n",
    "            corpus_model.save('./embeding/glove_corpus_char_'+str(_size)+'.model')\n",
    "        else:\n",
    "            corpus_model = Corpus()\n",
    "            corpus_model.fit(line_word, window=10)\n",
    "            glove = Glove(no_components=_size, learning_rate=0.05)\n",
    "            glove.fit(corpus_model.matrix, epochs=10,no_threads=1, verbose=True)\n",
    "            glove.add_dictionary(corpus_model.dictionary)\n",
    "            glove.save('./embeding/glove_word_'+str(_size)+'.model')\n",
    "            corpus_model.save('./embeding/glove_corpus_word_'+str(_size)+'.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding(100,'word','word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量维度:embed_size   截取长度:maxlen  line:line_word  line_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_embedding(embed_size,_type,_embeding,line,maxlen):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(line)\n",
    "    data['Discuss_seq'] = tokenizer.texts_to_sequences(line)\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "    if _embeding == 'word2vec':\n",
    "        if _type == 'char':\n",
    "            model = Word2Vec.load(\"./embeding/word2vec_char_\"+str(embed_size)+\".vector\")\n",
    "        else:\n",
    "           # model = Word2Vec.load(\"./embeding/word2vec_word_\"+str(embed_size)+\".vector\")\n",
    "            model = Word2Vec.load(\"./embeding/word2vec_word_\"+str(embed_size)+\"_skip.txt\")\n",
    "    elif _embeding == 'fasttext':\n",
    "        if _type == 'char':\n",
    "            model = FastText.load('./embeding/fasttext_char_'+str(embed_size)+'.vector')\n",
    "        else:\n",
    "            model = FastText.load('./embeding/fasttext_word_'+str(embed_size)+'.vector')\n",
    "    elif _embeding == 'glove':\n",
    "        if _type == 'char':\n",
    "            model = Glove.load('../vector/glove_char_'+str(embed_size)+'.model')\n",
    "        else:\n",
    "            model = Glove.load('../vector/glove_word_'+str(embed_size)+'.model')\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            pass\n",
    "    return word_index, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def convert_embedding2(embed_size,line,maxlen):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(line)\n",
    "    data['Discuss_seq'] = tokenizer.texts_to_sequences(line)\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "    model =   KeyedVectors.load_word2vec_format(\"./embeding/word2vec_word_\"+str(embed_size)+\"_skip.txt\",binary=False)\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            pass\n",
    "    return word_index, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "embed_size = 300\n",
    "maxlen = 100\n",
    "word_index, embedding_matrix = convert_embedding2(embed_size,line_word,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_file(obj,file):\n",
    "    with open(file,'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "def load_file(file):\n",
    "    with open(file,'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(embedding_matrix,'./data/embedding_matrix_300d.pkl')\n",
    "\n",
    "save_file(word_index, './data/word_index_300d.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train  = data[:int(len(data) * 0.9)]#训练集\n",
    "data_test = data[int(len(data) * 0.9):]#测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(_filters, _kernel, _neurons, _drop, epoch=1):\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "    callbacks_list = [early_stopping]\n",
    "    _data_train = shuffle(data_train)\n",
    "    X_train = pad_sequences(_data_train.Discuss_seq,maxlen=maxlen)\n",
    "    y_train = np_utils.to_categorical(_data_train.label.values)\n",
    "    final_model = cnn_lstm(_filters, _kernel, _neurons, _drop)\n",
    "    final_model.summary()\n",
    "    history = final_model.fit(X_train, y_train,\n",
    "            validation_split=0.1, #%10作为验证集\n",
    "            batch_size=128, \n",
    "            epochs= epoch, \n",
    "            shuffle = True,\n",
    "            callbacks=callbacks_list)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试集结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    X_test = pad_sequences(data_test.Discuss_seq,maxlen=maxlen)\n",
    "    preds = model.predict(X_test)\n",
    "    result = []\n",
    "    for i in preds:\n",
    "        r = i.tolist().index(max(i.tolist()))\n",
    "        result.append(r)\n",
    "#     a = range(1,len(result)+1)\n",
    "#     a = np.array(a)\n",
    "#     df = pd.DataFrame({'ind':a,'pre':result})\n",
    "#     df.to_csv('./data/result.csv',index = False,header=False)\n",
    "    return preds, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正向f1值计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(y_true, y_pred):\n",
    "    #res = []\n",
    "    true_positives = pd.DataFrame(y_true + y_pred)['label'].value_counts()[4]\n",
    "    predicted_positives = pd.DataFrame(y_pred)[0].value_counts()[2]\n",
    "    p = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "   # true_positives = pd.DataFrame(y_true + y_pred)['label'].value_counts()[4]\n",
    "    possible_positives = y_true.value_counts()[2]\n",
    "    r = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    f1 = 2* (p * r) / ( p + r + K.epsilon())\n",
    "    #res.append([p,r,f1])\n",
    "    return [p,r,f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def eval_(y_true,y_pred):\n",
    "    p_pos = precision_score(y_true, y_pred, labels=[2], average = 'micro')\n",
    "    p_neg = precision_score(y_true, y_pred, labels=[0], average='micro')\n",
    "\n",
    "    r_pos = recall_score(y_true, y_pred, labels=[2], average='micro')\n",
    "    r_neg = recall_score(y_true, y_pred, labels=[0], average='micro')\n",
    "\n",
    "    f1_pos = f1_score(y_true, y_pred, labels=[2], average = 'micro')\n",
    "    f1_neg = f1_score(y_true, y_pred, labels=[0], average = 'micro')\n",
    "\n",
    "    pos_rate = 6560/(6560+1224)\n",
    "    #neg_rate = 12240/(65598+12240)\n",
    "\n",
    "    p_w = p_pos*pos_rate+ p_neg*(1-pos_rate)\n",
    "    r_w = r_pos*pos_rate+ r_neg*(1-pos_rate)\n",
    "    f1_w = f1_pos * pos_rate + f1_neg * (1 - pos_rate)\n",
    "    return p_w,r_w,f1_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(_repeats, _filters, _kernel, _neurons, _drop):\n",
    "    evaluate_result = []\n",
    "    sum_preds = np.zeros((len(data_test), 3))\n",
    "    for i in range(_repeats):\n",
    "        model = train_model(_filters, _kernel, _neurons, _drop)\n",
    "        preds, result = predict(model)\n",
    "        sum_preds += preds\n",
    "        res = f_score(data_test.label,result)\n",
    "        evaluate_result.append(res)\n",
    "    sum_preds /= _repeats\n",
    "    with open(\"./data/result.txt\",\"w\") as ipt:\n",
    "        for line in sum_preds:\n",
    "            for i in line:\n",
    "                ipt.write(str(i)+\" \")\n",
    "            ipt.write('\\n')\n",
    "    return np.array(evaluate_result).mean(axis=0),sum_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调整参数 filters, kernel, neurons, drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [8,16,24,32,48,64,96,128,192,256,384,512]\n",
    "def train_filters(variable):\n",
    "    all_result = []\n",
    "    for i in variable:\n",
    "        error_result,sum_preds = experiment(2, i, 3, 128, 0.1)\n",
    "        all_result.append([i,error_result])\n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = [1,2,3,4,5]\n",
    "def train_kernel(variable):\n",
    "    all_result = []\n",
    "    for i in variable:\n",
    "        f1_result,sum_preds = experiment(2, 128, i, 128, 0.1)\n",
    "        all_result.append([i,f1_result])\n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [8,16,24,32,48,64,96,128,192,256]\n",
    "def train_filters(variable):\n",
    "    all_result = []\n",
    "    for i in variable:\n",
    "        f1_result,sum_preds = experiment(2, 128, 3, i, 0.1)\n",
    "        all_result.append([i,f1_result])\n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "def train_drop(variable):\n",
    "    all_result = []\n",
    "    for i in variable:\n",
    "        f1_result,sum_preds = experiment(2, 128, 3, 128, i)\n",
    "        all_result.append([i,f1_result])\n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 100)          6125600   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 100, 256)          77056     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 6,400,163\n",
      "Trainable params: 6,400,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 66439 samples, validate on 7383 samples\n",
      "Epoch 1/1\n",
      "66439/66439 [==============================] - 113s 2ms/step - loss: 0.2670 - mean_absolute_error: 0.0941 - fmeasure: 0.8998 - val_loss: 0.1822 - val_mean_absolute_error: 0.0680 - val_fmeasure: 0.9348\n"
     ]
    }
   ],
   "source": [
    "f1_result,sum_preds = experiment(_repeats=1, _filters=256, _kernel=3, _neurons=128, _drop=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
