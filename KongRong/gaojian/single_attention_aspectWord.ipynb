{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version:  2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"tf version: \", tf.__version__)\n",
    "\n",
    "import tensorflow.keras.backend  as K\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#type(word_index)  dict字典\n",
    "import pickle \n",
    "def save_obj(obj, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(file ):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1  读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv( './data/train_location.csv', usecols=['content']).values\n",
    "valid_x = pd.read_csv( './data/valid_location.csv', usecols=['content']).values\n",
    "\n",
    "train_x = [line[0].strip().split() for line in train_x]\n",
    "valid_x = [line[0].strip().split() for line in valid_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = pd.read_csv('./data/train_location.csv', usecols=['label']).values+2\n",
    "# valid_y = pd.read_csv('./data/valid_location.csv', usecols=['label']).values+2\n",
    "\n",
    "# train_y = pd.read_csv('./data/train_service.csv', usecols=['label']).values+2\n",
    "# valid_y = pd.read_csv('./data/valid_service.csv', usecols=['label']).values+2\n",
    "\n",
    "# train_y = pd.read_csv('./data/train_price.csv', usecols=['label']).values+2\n",
    "# valid_y = pd.read_csv('./data/valid_price.csv', usecols=['label']).values+2\n",
    "\n",
    "# train_y = pd.read_csv('./data/train_environment.csv', usecols=['label']).values+2\n",
    "# valid_y = pd.read_csv('./data/valid_environment.csv', usecols=['label']).values+2\n",
    "\n",
    "train_y = pd.read_csv('./data/train_dish.csv', usecols=['label']).values+2\n",
    "valid_y = pd.read_csv('./data/valid_dish.csv', usecols=['label']).values+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_x = pd.read_csv('./testSetSales/chunla_filted.csv', usecols=['comment']).values\n",
    "dxy_x = pd.read_csv('./testSetSales/dingxiangyuan_filted.csv', usecols=['comment']).values\n",
    "jld_x = pd.read_csv('./testSetSales/jialide_filted.csv', usecols=['comment']).values\n",
    "jsz_x = pd.read_csv('./testSetSales/jianshazui_filted.csv', usecols=['comment']).values\n",
    "jf_x = pd.read_csv('./testSetSales/jiefu_filted.csv', usecols=['comment']).values\n",
    "kl_x = pd.read_csv('./testSetSales/kuaileai_filted.csv', usecols=['comment']).values\n",
    "nzn_x = pd.read_csv('./testSetSales/niuzhongniu_filted.csv', usecols=['comment']).values\n",
    "sr_x = pd.read_csv('./testSetSales/shouergong_filted.csv', usecols=['comment']).values\n",
    "xlk_x = pd.read_csv('./testSetSales/xiaolongkan_filted.csv', usecols=['comment']).values\n",
    "zh_x = pd.read_csv('./testSetSales/zhenghuangqi_filted.csv', usecols=['comment']).values\n",
    "\n",
    "cl_x = [line[0].strip().split() for line in cl_x]\n",
    "dxy_x = [line[0].strip().split() for line in dxy_x]\n",
    "jld_x = [line[0].strip().split() for line in jld_x]\n",
    "jsz_x = [line[0].strip().split() for line in jsz_x]\n",
    "jf_x = [line[0].strip().split() for line in jf_x]\n",
    "kl_x = [line[0].strip().split() for line in kl_x]\n",
    "nzn_x = [line[0].strip().split() for line in nzn_x]\n",
    "sr_x = [line[0].strip().split() for line in sr_x]\n",
    "xlk_x = [line[0].strip().split() for line in xlk_x]\n",
    "zh_x = [line[0].strip().split() for line in zh_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137489,)\n"
     ]
    }
   ],
   "source": [
    "comment_text = np.hstack([train_x, valid_x,\n",
    "                          cl_x, dxy_x, jld_x, jsz_x, jf_x, kl_x, nzn_x, sr_x, xlk_x, zh_x])\n",
    "print(comment_text.shape)\n",
    "\n",
    "tok_raw = tf.keras.preprocessing.text.Tokenizer()\n",
    "tok_raw.fit_on_texts(comment_text)\n",
    "word_index = tok_raw.word_index #语料总所有出现过词的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token padding\n",
    "maxlen = 200 #训练集平均词长 100\n",
    "\n",
    "train_x = tok_raw.texts_to_sequences(train_x)\n",
    "valid_x = tok_raw.texts_to_sequences(valid_x)\n",
    "\n",
    "train_x = tf.keras.preprocessing.sequence.pad_sequences(sequences=train_x,maxlen=maxlen ,padding='post',truncating='post',value=0.0)\n",
    "valid_x = tf.keras.preprocessing.sequence.pad_sequences(sequences=valid_x,maxlen=maxlen ,padding='post',truncating='post',value=0.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2  aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspect转为 index , aspect_len\n",
    "# 先人工选择种子词，腾讯词向量加载最相似的20个词， 每个aspect 再人工筛选20词,aspect_len 设为10\n",
    "# 筛选纯 方面词\n",
    "# 方面描述：\n",
    "# loc = ['位置', '交通', '距离', '商圈']\n",
    "# ser = ['服务', '排队', '等候', '态度', '点菜', '上菜', '停车']\n",
    "# pri = ['价格', '性价比', '折扣']\n",
    "# env = ['环境', ‘装修’, ‘卫生’]\n",
    "# dis = ['菜品', ‘分量’, '口感', '外观']\n",
    "\n",
    "# loc = ['位置', '交通', '距离', '商圈', '城市核心', '商区','周边商业','交通枢纽','周边交通','城市中心',\n",
    "#        '繁华商圈', '大商圈', '大型商圈', '繁华区域', '成熟商圈', '热门商圈', '繁华商业', '交通便利性', '交通四通八达', '便利交通']\n",
    "\n",
    "\n",
    "# ser = ['服务', '排队', '等候', '态度', '点菜', '上菜', '停车', '点餐', '排队等候', '取餐',\n",
    "#        '排长队', '上菜慢', '上菜快', '上菜速度快', '等半天', '主动招呼', '加塞儿', '长队', '排长龙', '热门餐厅']\n",
    "\n",
    "\n",
    "# pri = ['价格', '性价比', '折扣', '价格方面', '价位', '售价', '整体性价比', '产品的价格','定价', '价格比较',\n",
    "#        '优惠', '价格优势', '降价', '性价比极高', '价格低', '高性价比', '便宜', '性价比很高', '亲民价格', '超高性价比']\n",
    "\n",
    "\n",
    "# env = ['环境','装修','卫生', '整体环境', '周边环境', '卫生环境', '卫生方面', '周边的环境','装修环境', '卫生条件',\n",
    "#        '干净的环境', '整洁', '干净整洁', '环境干净', '干净卫生', '舒适的环境','脏乱的环境', '脏乱', '安静整洁', '整洁卫生']\n",
    "\n",
    "\n",
    "# dis = ['菜品','分量','口感','外观', '口味', '味道', '风味', '口感口味', '菜品味道','菜品口感', \n",
    "#        '味道浓郁', '口感丰富', '口感细腻', '味道丰富', '分量足', '味道香浓', '口味浓郁','味道适中', '味道浓厚', '口感浓郁']\n",
    "\n",
    "\n",
    "loc = ['位置', '交通', '距离', '商圈', '城市核心', '商区','周边商业','交通枢纽','周边交通',\n",
    "        '城市中心', '区域', '商圈中心', '城市交通枢纽', '周边商圈', '交通方面', '城市商圈', \n",
    "        '交通配套', '商业区域', '市中心区域', '商业核心']\n",
    "\n",
    "\n",
    "ser = ['服务', '排队', '等候', '态度', '点菜', '上菜', '停车', '点餐', '排队等候', '取餐',\n",
    "        '用餐', '客人', '上餐', '结账', '服务员', '付钱', '排队等待', '叫号', '排队等位',\n",
    "        '等餐']\n",
    "\n",
    "pri = ['价格', '性价比', '折扣', '价格方面', '价位', '售价', '整体性价比', '产品的价格',\n",
    "       '定价', '价格比较', '价格定位', '最终价格', '购买价格', '优惠力度', '产品价格',\n",
    "       '综合性价比', '市场价', '官方价格', '价格比', '价'] \n",
    "\n",
    "\n",
    "env = ['环境','装修','卫生', '整体环境', '周边环境', '卫生环境', '卫生方面', '周边的环境',\n",
    "       '装修环境', '卫生条件', '卫生状况', '室内装修', '饮食环境', '室内的装修', '餐饮环境',\n",
    "       '餐厅环境', '总体环境', '环境和卫生', '卫生情况', '装修布置']\n",
    "\n",
    "dis = ['菜品','分量','口感','外观', '口味', '味道', '风味', '口感口味', '菜品味道',\n",
    "       '菜品口感', '吃起来', '口味口感', '菜品口味', '嚼劲', '配菜', '配料','酱料', \n",
    "       '咀嚼感', '招牌菜', '食材']\n",
    "\n",
    "\n",
    "aspect_len = 20\n",
    "\n",
    "assert len(loc) == len(ser) == len(pri) == len(env) == len(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "def transform(lst):\n",
    "    res = []\n",
    "    for x in tok_raw.texts_to_sequences(lst):\n",
    "        if x: res.append(x[0])\n",
    "        else: res.append(0)\n",
    "    return res\n",
    "\n",
    "loc = transform(loc)\n",
    "ser = transform(ser)\n",
    "pri = transform(pri)\n",
    "env = transform(env)\n",
    "dis = transform(dis)\n",
    "\n",
    "print(len(loc))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "class single_attention_aspect( tf.keras.Model): \n",
    "    def __init__(self, maxlen, embedding_matrix, aspect, embedding_dim, aspect_len, hidden_size, activation, output_size):\n",
    "        super(single_attention_aspect, self).__init__(name = 'single_attention_aspect')\n",
    "        self.output_size = output_size\n",
    "        self.maxlen = maxlen\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_maxtrix = tf.compat.v1.get_variable( name='embedding', initializer=embedding_matrix,\n",
    "                                                          dtype=tf.float64, trainable=True)\n",
    "        self.aspect = aspect\n",
    "        self.aspect_len = aspect_len\n",
    " \n",
    "        self.lstm_x =  tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                units = self.hidden_size,\n",
    "                return_sequences = True, \n",
    "                return_state = True,\n",
    "                activation = 'tanh',\n",
    "                kernel_regularizer = tf.keras.regularizers.l1_l2(l1=0.02,l2=0.0004),\n",
    "                recurrent_regularizer = tf.keras.regularizers.l1_l2(l1=0.02,l2=0.0004),\n",
    "                bias_regularizer = tf.keras.regularizers.l1_l2(l1=0.02,l2=0.0004),\n",
    "                activity_regularizer = tf.keras.regularizers.l1_l2(l1=0.02,l2=0.0004),\n",
    "                dropout = 0.1, recurrent_dropout = 0.1, stateful=True\n",
    "                )\n",
    "         )\n",
    "        self.lstm_aspect = tf.keras.layers.LSTM(\n",
    "            units=self.hidden_size,return_sequences=True, return_state=True,activation='tanh',stateful=True\n",
    "        )#stateful=True\n",
    "        '''\n",
    "                                                                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.02,l2=0.0004), \n",
    "                                                                          recurrent_regularizer= tf.keras.regularizers.l1_l2(l1=0.02,l2=0.0004),\n",
    "                                                                          bias_regularizer=tf.keras.regularizers.l1_l2(l1=0.02,l2=0.0004),\n",
    "                                                                          activity_regularizer= tf.keras.regularizers.l1_l2(l1=0.02,l2=0.0004),\n",
    "                                                                          dropout=0,\n",
    "                                                                          recurrent_dropout= 0,\n",
    "                                                                           )# [batch, feature]->[batch,hidden_size]\n",
    "        '''\n",
    "        \n",
    "        self.dense_aspect_qw = tf.keras.layers.Dense(units = self.hidden_size*2, use_bias = True)\n",
    "        self.dense_aspect_kw = tf.keras.layers.Dense(units = self.hidden_size*2, use_bias = True)\n",
    "        self.dense_aspect_vw = tf.keras.layers.Dense(units = self.hidden_size*2, use_bias = True)\n",
    "\n",
    "        # self.flat = tf.keras.layers.Flatten()\n",
    "        self.drop = tf.keras.layers.Dropout(rate=0.3, seed = 2020)\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units = self.output_size,\n",
    "            activation= 'softmax',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='glorot_uniform')\n",
    "            \n",
    "  \n",
    "    def call(self,inputs): #定义前向传播过程\n",
    "        x_contex = tf.nn.embedding_lookup(params=self.embedding_maxtrix , ids= inputs) # (batch,maxlen, embding_size)\n",
    "        x_aspect = tf.nn.embedding_lookup(params=self.embedding_maxtrix , ids= self.aspect) # (aspect_len, embding_size)\n",
    "        \n",
    "        #lstm1 存放的就是全部时间步的 hidden state\n",
    "        x_contex = self.lstm_x(x_contex)[0] # 隐层 (batch,maxlen, h_size*2) 单个LSTM细胞\n",
    "        \n",
    "        x_aspect = tf.keras.backend.expand_dims(x_aspect,axis = 0 ) # (1, aspect_len, embding_size)\n",
    "        x_aspect,tmp1,tmp2 = self.lstm_aspect(x_aspect) #隐层 (1,aspect_len, h_size)\n",
    "        \n",
    "#         x_aspect_self_qw = self.dense_selfatten_qw(x_aspect) # (1,aspect_len, h_size)\n",
    "#         x_aspect_self_kw = self.dense_selfatten_kw(x_aspect) # (1,aspect_len, h_size)\n",
    "#         x_aspect_self_vw = self.dense_selfatten_vw(x_aspect) # (1,aspect_len, h_size)\n",
    "        \n",
    "#         # self attention\n",
    "#         a = tf.einsum('blh,blh->blh',x_aspect_self_qw,x_aspect_self_kw) # (1,aspect_len, h_size)\n",
    "#         a = K.softmax( a/(self.hidden_size**0.5)) #(1,aspect_len, h_size)\n",
    "#         a = tf.einsum('blh,blh->blh', a, x_aspect_self_vw) #(1,aspect_len, h_size)\n",
    "        \n",
    "        # aspect attention\n",
    "        x_contex_kw = self.dense_aspect_kw(x_contex) \n",
    "        x_contex_vw = self.dense_aspect_vw(x_contex) # (batch,maxlen, h_size*2) \n",
    "        x_contex_qw = self.dense_aspect_qw(x_aspect) # (1,aspect_len, h_size*2) aspect_len==20, maxlen==200\n",
    "        x_contex_qw = tf.tile(x_contex_qw, tf.constant([128,1,1], tf.int32)) # (batch, aspect_len, h_size*2) \n",
    "        x_contex_kw = K.reshape(x_contex_kw, (-1, K.shape(x_contex_kw)[-1], K.shape(x_contex_kw)[1])) # (batch, h_size*2, maxlen)\n",
    "        b =  tf.einsum('bah,bhl->bal', x_contex_qw, x_contex_kw) # (batch, aspect_len, maxlen)\n",
    "        b = K.softmax( b/ ( (self.hidden_size*2) **0.5) ) # (batch, aspect_len, maxlen)\n",
    "        b =  tf.einsum('bal,blh->bah', b, x_contex_vw) # (batch, aspect_len, h_size*2)\n",
    "        b = K.reshape( b, (-1, K.shape(b)[1] * K.shape(b)[-1])) #(batch, aspect_len * maxlen)\n",
    "        b = self.drop(b)\n",
    "        out = self.dense(b)\n",
    "        return out\n",
    "\n",
    "    def predict(self,\n",
    "              x,\n",
    "              batch_size=128,\n",
    "              verbose=0,\n",
    "              steps=None,\n",
    "              callbacks=None,\n",
    "              max_queue_size=10,\n",
    "              workers=1,\n",
    "              use_multiprocessing=False): #父类该方法batch_size默认参数是32，所以后面调用该方法的时候，需要传入一个个性化参数\n",
    "        logits = self(x) \n",
    "        fixed = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "        return logits ,fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数：\n",
    "# dataset: 元组(train_x, train_y)的形式\n",
    "class eval_of_experiment():\n",
    "    def __init__(self, dataset, batch_size, model):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "        self.precision_0 = tf.metrics.Precision(thresholds=0.5,top_k=None,class_id=0) # micro-precision\n",
    "        self.precision_1 = tf.metrics.Precision(thresholds=0.5,top_k=None,class_id=1)\n",
    "        self.precision_2 = tf.metrics.Precision(thresholds=0.5,top_k=None,class_id=2)\n",
    "        self.precision_3 = tf.metrics.Precision(thresholds=0.5,top_k=None,class_id=3)\n",
    "        self.recall_0 = tf.keras.metrics.Recall(thresholds=0.5,top_k=None,class_id=0) # micro-recall\n",
    "        self.recall_1 = tf.keras.metrics.Recall(thresholds=0.5,top_k=None,class_id=1)\n",
    "        self.recall_2 = tf.keras.metrics.Recall(thresholds=0.5,top_k=None,class_id=2)\n",
    "        self.recall_3 = tf.keras.metrics.Recall(thresholds=0.5,top_k=None,class_id=3)\n",
    "        self.y_true_label_0_num = 0\n",
    "        self.y_true_label_1_num = 0\n",
    "        self.y_true_label_2_num = 0\n",
    "        self.y_true_label_3_num = 0\n",
    "\n",
    "    def eval_of_valid(self):\n",
    "        data = tf.data.Dataset.from_tensor_slices(self.dataset).shuffle(buffer_size=10000).batch(self.batch_size,drop_remainder=True)\n",
    "        for x,y in data:\n",
    "            y_prob , y_fixed = self.model.predict(x, batch_size = self.batch_size) #only predict, no training process\n",
    "            self.sparse_categorical_accuracy.update_state(y_true = y, y_pred= y_prob)\n",
    "            self.precision_0.update_state(y_true =tf.keras.utils.to_categorical(y), y_pred= y_prob) #必须是one-hot编码的形式\n",
    "            self.precision_1.update_state(y_true =tf.keras.utils.to_categorical(y), y_pred= y_prob)\n",
    "            self.precision_2.update_state(y_true =tf.keras.utils.to_categorical(y), y_pred= y_prob)\n",
    "            self.precision_3.update_state(y_true =tf.keras.utils.to_categorical(y), y_pred= y_prob)\n",
    "            self.recall_0.update_state(y_true =tf.keras.utils.to_categorical(y), y_pred= y_prob) \n",
    "            self.recall_1.update_state(y_true =tf.keras.utils.to_categorical(y), y_pred= y_prob) \n",
    "            self.recall_2.update_state(y_true =tf.keras.utils.to_categorical(y), y_pred= y_prob) \n",
    "            self.recall_3.update_state(y_true =tf.keras.utils.to_categorical(y), y_pred= y_prob)\n",
    "            \n",
    "            self.y_true_label_0_num += tf.reduce_sum(tf.cast(tf.math.equal(y,0),dtype= tf.int32)).numpy()\n",
    "            self.y_true_label_1_num += tf.reduce_sum(tf.cast(tf.math.equal(y,1),dtype= tf.int32)).numpy()\n",
    "            self.y_true_label_2_num += tf.reduce_sum(tf.cast(tf.math.equal(y,2),dtype= tf.int32)).numpy()\n",
    "            self.y_true_label_3_num += tf.reduce_sum(tf.cast(tf.math.equal(y,3),dtype= tf.int32)).numpy()\n",
    "    \n",
    "    def predict_of_test(self):\n",
    "        data = tf.data.Dataset.from_tensor_slices(self.dataset).batch(self.batch_size,drop_remainder=True)#不打乱\n",
    "        prob=[]\n",
    "        label_predic = []\n",
    "        for x in data:\n",
    "            y_prob , y_fixed = self.model.predict(x,batch_size=self.batch_size) # y_prob(batch,4), y_fixed(batch,)\n",
    "            prob += y_prob.numpy()#tensor 转np.ndarrays\n",
    "            label_predic += y_fixed.numpy()\n",
    "        return prob, label_predic\n",
    "            \n",
    "    \n",
    "    def result(self):\n",
    "        total = self.y_true_label_0_num + self.y_true_label_1_num + self.y_true_label_2_num+ self.y_true_label_3_num\n",
    "        p0 = self.y_true_label_0_num/total\n",
    "        p1 = self.y_true_label_1_num/total\n",
    "        p2 = self.y_true_label_2_num/total\n",
    "        p3 = self.y_true_label_3_num/total\n",
    "        print('y_true_label_0_num: %d ; rate: %f'%(self.y_true_label_0_num, p0))\n",
    "        print('y_true_label_1_num: %d ; rate: %f'%(self.y_true_label_1_num,p1))\n",
    "        print('y_true_label_2_num: %d ; rate: %f'%(self.y_true_label_2_num,p2))\n",
    "        print('y_true_label_3_num: %d ; rate: %f'%(self.y_true_label_3_num,p3))\n",
    "        print('y_true_label_num_total: %d'% (total))\n",
    "        \n",
    "        avg_precision = p0*self.precision_0.result()+ p1*self.precision_1.result()+ p2*self.precision_2.result()+p3*self.precision_3.result()\n",
    "        avg_recall = p0*self.recall_0.result()+ p1*self.recall_1.result()+ p2*self.recall_2.result()+ p3*self.recall_3.result()\n",
    "        f_0 = 2*self.precision_0.result()*self.recall_0.result()/(self.precision_0.result()+self.recall_0.result()+tf.keras.backend.epsilon())\n",
    "        f_1 = 2*self.precision_1.result()*self.recall_1.result()/(self.precision_1.result()+self.recall_1.result()+tf.keras.backend.epsilon())\n",
    "        f_2 = 2*self.precision_2.result()*self.recall_2.result()/(self.precision_2.result()+self.recall_2.result()+tf.keras.backend.epsilon())\n",
    "        f_3 = 2*self.precision_3.result()*self.recall_3.result()/(self.precision_3.result()+self.recall_3.result()+tf.keras.backend.epsilon())\n",
    "        avg_f1 = p0*f_0 + p1*f_1 + p2*f_2 + p3*f_3\n",
    "        print(\"valid accuracy: %f\" % (self.sparse_categorical_accuracy.result()))\n",
    "        print(\"valid avg_precision: %f\" % (avg_precision))\n",
    "        print(\"valid avg_recall: %f\" % (avg_recall))\n",
    "        print(\"valid avg_f1: %f\" % (avg_f1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4  location 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "embedding_matrix = load_obj('./data2/train_sales_embedding_matrix.pkl')\n",
    "\n",
    "model = single_attention_aspect(\n",
    "                maxlen=maxlen, \n",
    "                embedding_matrix=embedding_matrix, \n",
    "                aspect=loc,\n",
    "                embedding_dim=200,\n",
    "                aspect_len=20,\n",
    "                hidden_size=100,\n",
    "                activation=None,\n",
    "                output_size=4\n",
    "            )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "epoch 1; batch 128; loss 1.397185\n",
      "epoch:1; batch 128; train accuracy: 0.046875\n",
      "epoch 1; batch 256; loss 1.292021\n",
      "epoch:1; batch 256; train accuracy: 0.308594\n",
      "epoch 1; batch 384; loss 1.177844\n",
      "epoch:1; batch 384; train accuracy: 0.414062\n",
      "epoch 1; batch 512; loss 1.044830\n",
      "epoch:1; batch 512; train accuracy: 0.457031\n",
      "epoch 1; batch 640; loss 1.036733\n",
      "epoch:1; batch 640; train accuracy: 0.465625\n",
      "epoch 1; batch 768; loss 0.945003\n",
      "epoch:1; batch 768; train accuracy: 0.481771\n",
      "epoch 1; batch 896; loss 1.000474\n",
      "epoch:1; batch 896; train accuracy: 0.494420\n",
      "epoch 1; batch 1024; loss 0.945917\n",
      "epoch:1; batch 1024; train accuracy: 0.497070\n",
      "epoch 1; batch 1152; loss 0.887314\n",
      "epoch:1; batch 1152; train accuracy: 0.505208\n",
      "epoch 1; batch 1280; loss 0.890262\n",
      "epoch:1; batch 1280; train accuracy: 0.515625\n",
      "epoch 1; batch 1408; loss 1.043950\n",
      "epoch:1; batch 1408; train accuracy: 0.518466\n",
      "epoch 1; batch 1536; loss 0.920705\n",
      "epoch:1; batch 1536; train accuracy: 0.522786\n",
      "epoch 1; batch 1664; loss 0.932112\n",
      "epoch:1; batch 1664; train accuracy: 0.528846\n",
      "epoch 1; batch 1792; loss 0.899095\n",
      "epoch:1; batch 1792; train accuracy: 0.531808\n",
      "epoch 1; batch 1920; loss 0.955681\n",
      "epoch:1; batch 1920; train accuracy: 0.535417\n",
      "epoch 1; batch 2048; loss 0.887248\n",
      "epoch:1; batch 2048; train accuracy: 0.542480\n",
      "epoch 1; batch 2176; loss 0.867660\n",
      "epoch:1; batch 2176; train accuracy: 0.543658\n",
      "epoch 1; batch 2304; loss 0.950744\n",
      "epoch:1; batch 2304; train accuracy: 0.542969\n",
      "epoch 1; batch 2432; loss 0.912805\n",
      "epoch:1; batch 2432; train accuracy: 0.542352\n",
      "epoch 1; batch 2560; loss 0.853254\n",
      "epoch:1; batch 2560; train accuracy: 0.548047\n",
      "epoch 1; batch 2688; loss 1.005686\n",
      "epoch:1; batch 2688; train accuracy: 0.547247\n",
      "epoch 1; batch 2816; loss 0.983560\n",
      "epoch:1; batch 2816; train accuracy: 0.546165\n",
      "epoch 1; batch 2944; loss 0.857318\n",
      "epoch:1; batch 2944; train accuracy: 0.546875\n",
      "epoch 1; batch 3072; loss 1.048894\n",
      "epoch:1; batch 3072; train accuracy: 0.545247\n",
      "epoch 1; batch 3200; loss 0.883066\n",
      "epoch:1; batch 3200; train accuracy: 0.551250\n",
      "epoch 1; batch 3328; loss 0.957436\n",
      "epoch:1; batch 3328; train accuracy: 0.549579\n",
      "epoch 1; batch 3456; loss 0.865272\n",
      "epoch:1; batch 3456; train accuracy: 0.550926\n",
      "epoch 1; batch 3584; loss 0.757285\n",
      "epoch:1; batch 3584; train accuracy: 0.553850\n",
      "epoch 1; batch 3712; loss 0.887304\n",
      "epoch:1; batch 3712; train accuracy: 0.554149\n",
      "epoch 1; batch 3840; loss 0.874935\n",
      "epoch:1; batch 3840; train accuracy: 0.553906\n",
      "epoch 1; batch 3968; loss 0.836845\n",
      "epoch:1; batch 3968; train accuracy: 0.558216\n",
      "epoch 1; batch 4096; loss 0.787310\n",
      "epoch:1; batch 4096; train accuracy: 0.563721\n",
      "epoch 1; batch 4224; loss 0.832868\n",
      "epoch:1; batch 4224; train accuracy: 0.566761\n",
      "epoch 1; batch 4352; loss 0.751345\n",
      "epoch:1; batch 4352; train accuracy: 0.569853\n",
      "epoch 1; batch 4480; loss 0.711772\n",
      "epoch:1; batch 4480; train accuracy: 0.573884\n",
      "epoch 1; batch 4608; loss 0.804293\n",
      "epoch:1; batch 4608; train accuracy: 0.577474\n",
      "epoch 1; batch 4736; loss 0.805904\n",
      "epoch:1; batch 4736; train accuracy: 0.582137\n",
      "epoch 1; batch 4864; loss 0.835508\n",
      "epoch:1; batch 4864; train accuracy: 0.585938\n",
      "epoch 1; batch 4992; loss 0.738884\n",
      "epoch:1; batch 4992; train accuracy: 0.591546\n",
      "epoch 1; batch 5120; loss 0.798430\n",
      "epoch:1; batch 5120; train accuracy: 0.594922\n",
      "epoch 1; batch 5248; loss 0.877131\n",
      "epoch:1; batch 5248; train accuracy: 0.596227\n",
      "epoch 1; batch 5376; loss 0.905079\n",
      "epoch:1; batch 5376; train accuracy: 0.597284\n",
      "epoch 1; batch 5504; loss 0.724839\n",
      "epoch:1; batch 5504; train accuracy: 0.600836\n",
      "epoch 1; batch 5632; loss 0.674374\n",
      "epoch:1; batch 5632; train accuracy: 0.604048\n",
      "epoch 1; batch 5760; loss 0.639610\n",
      "epoch:1; batch 5760; train accuracy: 0.606597\n",
      "epoch 1; batch 5888; loss 0.861390\n",
      "epoch:1; batch 5888; train accuracy: 0.606488\n",
      "epoch 1; batch 6016; loss 0.700159\n",
      "epoch:1; batch 6016; train accuracy: 0.608710\n",
      "epoch 1; batch 6144; loss 0.615929\n",
      "epoch:1; batch 6144; train accuracy: 0.612956\n",
      "epoch 1; batch 6272; loss 0.714693\n",
      "epoch:1; batch 6272; train accuracy: 0.615274\n",
      "epoch 1; batch 6400; loss 0.670600\n",
      "epoch:1; batch 6400; train accuracy: 0.617344\n",
      "epoch 1; batch 6528; loss 0.657542\n",
      "epoch:1; batch 6528; train accuracy: 0.619792\n",
      "epoch 1; batch 6656; loss 0.683256\n",
      "epoch:1; batch 6656; train accuracy: 0.623347\n",
      "epoch 1; batch 6784; loss 0.583808\n",
      "epoch:1; batch 6784; train accuracy: 0.626621\n",
      "epoch 1; batch 6912; loss 0.615227\n",
      "epoch:1; batch 6912; train accuracy: 0.629774\n",
      "epoch 1; batch 7040; loss 0.742324\n",
      "epoch:1; batch 7040; train accuracy: 0.631108\n",
      "epoch 1; batch 7168; loss 0.653723\n",
      "epoch:1; batch 7168; train accuracy: 0.633092\n",
      "epoch 1; batch 7296; loss 0.691662\n",
      "epoch:1; batch 7296; train accuracy: 0.635280\n",
      "epoch 1; batch 7424; loss 0.606999\n",
      "epoch:1; batch 7424; train accuracy: 0.638066\n",
      "epoch 1; batch 7552; loss 0.475925\n",
      "epoch:1; batch 7552; train accuracy: 0.641287\n",
      "epoch 1; batch 7680; loss 0.563395\n",
      "epoch:1; batch 7680; train accuracy: 0.643490\n",
      "epoch 1; batch 7808; loss 0.865542\n",
      "epoch:1; batch 7808; train accuracy: 0.644467\n",
      "epoch 1; batch 7936; loss 0.729659\n",
      "epoch:1; batch 7936; train accuracy: 0.646043\n",
      "epoch 1; batch 8064; loss 0.704683\n",
      "epoch:1; batch 8064; train accuracy: 0.647569\n",
      "epoch 1; batch 8192; loss 0.648262\n",
      "epoch:1; batch 8192; train accuracy: 0.649536\n",
      "epoch 1; batch 8320; loss 0.623791\n",
      "epoch:1; batch 8320; train accuracy: 0.651442\n",
      "epoch 1; batch 8448; loss 0.475539\n",
      "epoch:1; batch 8448; train accuracy: 0.654119\n",
      "epoch 1; batch 8576; loss 0.631731\n",
      "epoch:1; batch 8576; train accuracy: 0.655784\n",
      "epoch 1; batch 8704; loss 0.656074\n",
      "epoch:1; batch 8704; train accuracy: 0.656939\n",
      "epoch 1; batch 8832; loss 0.514248\n",
      "epoch:1; batch 8832; train accuracy: 0.658967\n",
      "epoch 1; batch 8960; loss 0.540398\n",
      "epoch:1; batch 8960; train accuracy: 0.661161\n",
      "epoch 1; batch 9088; loss 0.879602\n",
      "epoch:1; batch 9088; train accuracy: 0.661642\n",
      "epoch 1; batch 9216; loss 0.754346\n",
      "epoch:1; batch 9216; train accuracy: 0.662652\n",
      "epoch 1; batch 9344; loss 0.559657\n",
      "epoch:1; batch 9344; train accuracy: 0.664170\n",
      "epoch 1; batch 9472; loss 0.649512\n",
      "epoch:1; batch 9472; train accuracy: 0.665435\n",
      "epoch 1; batch 9600; loss 0.687511\n",
      "epoch:1; batch 9600; train accuracy: 0.666250\n",
      "epoch 1; batch 9728; loss 0.579441\n",
      "epoch:1; batch 9728; train accuracy: 0.667866\n",
      "epoch 1; batch 9856; loss 0.584273\n",
      "epoch:1; batch 9856; train accuracy: 0.669338\n",
      "epoch 1; batch 9984; loss 0.699286\n",
      "epoch:1; batch 9984; train accuracy: 0.670773\n",
      "epoch 1; batch 10112; loss 0.587937\n",
      "epoch:1; batch 10112; train accuracy: 0.672073\n",
      "epoch 1; batch 10240; loss 0.519412\n",
      "epoch:1; batch 10240; train accuracy: 0.673926\n",
      "epoch 1; batch 10368; loss 0.798147\n",
      "epoch:1; batch 10368; train accuracy: 0.674672\n",
      "epoch 1; batch 10496; loss 0.617062\n",
      "epoch:1; batch 10496; train accuracy: 0.675876\n",
      "epoch 1; batch 10624; loss 0.720657\n",
      "epoch:1; batch 10624; train accuracy: 0.676487\n",
      "epoch 1; batch 10752; loss 0.659225\n",
      "epoch:1; batch 10752; train accuracy: 0.677827\n",
      "epoch 1; batch 10880; loss 0.524376\n",
      "epoch:1; batch 10880; train accuracy: 0.679596\n",
      "epoch 1; batch 11008; loss 0.681506\n",
      "epoch:1; batch 11008; train accuracy: 0.680233\n",
      "epoch 1; batch 11136; loss 0.546162\n",
      "epoch:1; batch 11136; train accuracy: 0.682112\n",
      "epoch 1; batch 11264; loss 0.522537\n",
      "epoch:1; batch 11264; train accuracy: 0.683327\n",
      "epoch 1; batch 11392; loss 0.566984\n",
      "epoch:1; batch 11392; train accuracy: 0.684954\n",
      "epoch 1; batch 11520; loss 0.676723\n",
      "epoch:1; batch 11520; train accuracy: 0.685503\n",
      "epoch 1; batch 11648; loss 0.733761\n",
      "epoch:1; batch 11648; train accuracy: 0.686041\n",
      "epoch 1; batch 11776; loss 0.692874\n",
      "epoch:1; batch 11776; train accuracy: 0.686906\n",
      "epoch 1; batch 11904; loss 0.670667\n",
      "epoch:1; batch 11904; train accuracy: 0.687836\n",
      "epoch 1; batch 12032; loss 0.521905\n",
      "epoch:1; batch 12032; train accuracy: 0.689162\n",
      "epoch 1; batch 12160; loss 0.470573\n",
      "epoch:1; batch 12160; train accuracy: 0.690872\n",
      "epoch 1; batch 12288; loss 0.526998\n",
      "epoch:1; batch 12288; train accuracy: 0.692139\n",
      "epoch 1; batch 12416; loss 0.613161\n",
      "epoch:1; batch 12416; train accuracy: 0.692816\n",
      "epoch 1; batch 12544; loss 0.588007\n",
      "epoch:1; batch 12544; train accuracy: 0.693878\n",
      "epoch 1; batch 12672; loss 0.548642\n",
      "epoch:1; batch 12672; train accuracy: 0.694918\n",
      "epoch 1; batch 12800; loss 0.634273\n",
      "epoch:1; batch 12800; train accuracy: 0.696016\n",
      "epoch 1; batch 12928; loss 0.514381\n",
      "epoch:1; batch 12928; train accuracy: 0.696937\n",
      "epoch 1; batch 13056; loss 0.494225\n",
      "epoch:1; batch 13056; train accuracy: 0.698223\n",
      "epoch 1; batch 13184; loss 0.508539\n",
      "epoch:1; batch 13184; train accuracy: 0.699181\n",
      "epoch 1; batch 13312; loss 0.725158\n",
      "epoch:1; batch 13312; train accuracy: 0.699745\n",
      "epoch 1; batch 13440; loss 0.708963\n",
      "epoch:1; batch 13440; train accuracy: 0.700298\n",
      "epoch 1; batch 13568; loss 0.642190\n",
      "epoch:1; batch 13568; train accuracy: 0.700988\n",
      "epoch 1; batch 13696; loss 0.563796\n",
      "epoch:1; batch 13696; train accuracy: 0.701957\n",
      "epoch 1; batch 13824; loss 0.470292\n",
      "epoch:1; batch 13824; train accuracy: 0.703414\n",
      "epoch 1; batch 13952; loss 0.535800\n",
      "epoch:1; batch 13952; train accuracy: 0.704415\n",
      "epoch 1; batch 14080; loss 0.583472\n",
      "epoch:1; batch 14080; train accuracy: 0.705043\n",
      "epoch 1; batch 14208; loss 0.632059\n",
      "epoch:1; batch 14208; train accuracy: 0.705729\n",
      "epoch 1; batch 14336; loss 0.490033\n",
      "epoch:1; batch 14336; train accuracy: 0.707031\n",
      "epoch 1; batch 14464; loss 0.542070\n",
      "epoch:1; batch 14464; train accuracy: 0.707619\n",
      "epoch 1; batch 14592; loss 0.494415\n",
      "epoch:1; batch 14592; train accuracy: 0.708745\n",
      "epoch 1; batch 14720; loss 0.611671\n",
      "epoch:1; batch 14720; train accuracy: 0.709375\n",
      "epoch 1; batch 14848; loss 0.410283\n",
      "epoch:1; batch 14848; train accuracy: 0.710668\n",
      "epoch 1; batch 14976; loss 0.608340\n",
      "epoch:1; batch 14976; train accuracy: 0.711472\n",
      "epoch 1; batch 15104; loss 0.501032\n",
      "epoch:1; batch 15104; train accuracy: 0.712527\n",
      "epoch 1; batch 15232; loss 0.592113\n",
      "epoch:1; batch 15232; train accuracy: 0.713170\n",
      "epoch 1; batch 15360; loss 0.586110\n",
      "epoch:1; batch 15360; train accuracy: 0.713867\n",
      "epoch 1; batch 15488; loss 0.494748\n",
      "epoch:1; batch 15488; train accuracy: 0.714941\n",
      "epoch 1; batch 15616; loss 0.530447\n",
      "epoch:1; batch 15616; train accuracy: 0.715932\n",
      "epoch 1; batch 15744; loss 0.547970\n",
      "epoch:1; batch 15744; train accuracy: 0.716527\n",
      "epoch 1; batch 15872; loss 0.402311\n",
      "epoch:1; batch 15872; train accuracy: 0.717553\n",
      "epoch 1; batch 16000; loss 0.583810\n",
      "epoch:1; batch 16000; train accuracy: 0.718250\n",
      "epoch 1; batch 16128; loss 0.647937\n",
      "epoch:1; batch 16128; train accuracy: 0.718874\n",
      "epoch 1; batch 16256; loss 0.560757\n",
      "epoch:1; batch 16256; train accuracy: 0.719734\n",
      "epoch 1; batch 16384; loss 0.574467\n",
      "epoch:1; batch 16384; train accuracy: 0.720459\n",
      "epoch 1; batch 16512; loss 0.563312\n",
      "epoch:1; batch 16512; train accuracy: 0.721051\n",
      "epoch 1; batch 16640; loss 0.284156\n",
      "epoch:1; batch 16640; train accuracy: 0.722416\n",
      "epoch 1; batch 16768; loss 0.555478\n",
      "epoch:1; batch 16768; train accuracy: 0.722925\n",
      "epoch 1; batch 16896; loss 0.468349\n",
      "epoch:1; batch 16896; train accuracy: 0.723781\n",
      "epoch 1; batch 17024; loss 0.524948\n",
      "epoch:1; batch 17024; train accuracy: 0.724330\n",
      "epoch 1; batch 17152; loss 0.604831\n",
      "epoch:1; batch 17152; train accuracy: 0.724755\n",
      "epoch 1; batch 17280; loss 0.444536\n",
      "epoch:1; batch 17280; train accuracy: 0.725637\n",
      "epoch 1; batch 17408; loss 0.516854\n",
      "epoch:1; batch 17408; train accuracy: 0.726218\n",
      "epoch 1; batch 17536; loss 0.576934\n",
      "epoch:1; batch 17536; train accuracy: 0.726905\n",
      "epoch 1; batch 17664; loss 0.562176\n",
      "epoch:1; batch 17664; train accuracy: 0.727525\n",
      "epoch 1; batch 17792; loss 0.511418\n",
      "epoch:1; batch 17792; train accuracy: 0.728249\n",
      "epoch 1; batch 17920; loss 0.507526\n",
      "epoch:1; batch 17920; train accuracy: 0.729074\n",
      "epoch 1; batch 18048; loss 0.576721\n",
      "epoch:1; batch 18048; train accuracy: 0.729499\n",
      "epoch 1; batch 18176; loss 0.484574\n",
      "epoch:1; batch 18176; train accuracy: 0.730084\n",
      "epoch 1; batch 18304; loss 0.593949\n",
      "epoch:1; batch 18304; train accuracy: 0.730496\n",
      "epoch 1; batch 18432; loss 0.475807\n",
      "epoch:1; batch 18432; train accuracy: 0.731174\n",
      "epoch 1; batch 18560; loss 0.357365\n",
      "epoch:1; batch 18560; train accuracy: 0.732328\n",
      "epoch 1; batch 18688; loss 0.399052\n",
      "epoch:1; batch 18688; train accuracy: 0.733251\n",
      "epoch 1; batch 18816; loss 0.688699\n",
      "epoch:1; batch 18816; train accuracy: 0.733525\n",
      "epoch 1; batch 18944; loss 0.399038\n",
      "epoch:1; batch 18944; train accuracy: 0.734375\n",
      "epoch 1; batch 19072; loss 0.424765\n",
      "epoch:1; batch 19072; train accuracy: 0.735161\n",
      "epoch 1; batch 19200; loss 0.422794\n",
      "epoch:1; batch 19200; train accuracy: 0.735937\n",
      "epoch 1; batch 19328; loss 0.620104\n",
      "epoch:1; batch 19328; train accuracy: 0.736445\n",
      "epoch 1; batch 19456; loss 0.332802\n",
      "epoch:1; batch 19456; train accuracy: 0.737664\n",
      "epoch 1; batch 19584; loss 0.562571\n",
      "epoch:1; batch 19584; train accuracy: 0.737847\n",
      "epoch 1; batch 19712; loss 0.876770\n",
      "epoch:1; batch 19712; train accuracy: 0.737774\n",
      "epoch 1; batch 19840; loss 0.544448\n",
      "epoch:1; batch 19840; train accuracy: 0.738306\n",
      "epoch 1; batch 19968; loss 0.581305\n",
      "epoch:1; batch 19968; train accuracy: 0.738882\n",
      "epoch 1; batch 20096; loss 0.433446\n",
      "epoch:1; batch 20096; train accuracy: 0.739749\n",
      "epoch 1; batch 20224; loss 0.619865\n",
      "epoch:1; batch 20224; train accuracy: 0.740160\n",
      "epoch 1; batch 20352; loss 0.531853\n",
      "epoch:1; batch 20352; train accuracy: 0.740566\n",
      "epoch 1; batch 20480; loss 0.458586\n",
      "epoch:1; batch 20480; train accuracy: 0.741016\n",
      "epoch 1; batch 20608; loss 0.457278\n",
      "epoch:1; batch 20608; train accuracy: 0.741605\n",
      "epoch 1; batch 20736; loss 0.323203\n",
      "epoch:1; batch 20736; train accuracy: 0.742622\n",
      "epoch 1; batch 20864; loss 0.389548\n",
      "epoch:1; batch 20864; train accuracy: 0.743290\n",
      "epoch 1; batch 20992; loss 0.527970\n",
      "epoch:1; batch 20992; train accuracy: 0.743712\n",
      "epoch 1; batch 21120; loss 0.545723\n",
      "epoch:1; batch 21120; train accuracy: 0.744318\n",
      "epoch 1; batch 21248; loss 0.572353\n",
      "epoch:1; batch 21248; train accuracy: 0.744776\n",
      "epoch 1; batch 21376; loss 0.636784\n",
      "epoch:1; batch 21376; train accuracy: 0.744948\n",
      "epoch 1; batch 21504; loss 0.444317\n",
      "epoch:1; batch 21504; train accuracy: 0.745443\n",
      "epoch 1; batch 21632; loss 0.432940\n",
      "epoch:1; batch 21632; train accuracy: 0.746117\n",
      "epoch 1; batch 21760; loss 0.502180\n",
      "epoch:1; batch 21760; train accuracy: 0.746507\n",
      "epoch 1; batch 21888; loss 0.636589\n",
      "epoch:1; batch 21888; train accuracy: 0.746711\n",
      "epoch 1; batch 22016; loss 0.589024\n",
      "epoch:1; batch 22016; train accuracy: 0.746957\n",
      "epoch 1; batch 22144; loss 0.434185\n",
      "epoch:1; batch 22144; train accuracy: 0.747516\n",
      "epoch 1; batch 22272; loss 0.587531\n",
      "epoch:1; batch 22272; train accuracy: 0.747845\n",
      "epoch 1; batch 22400; loss 0.591451\n",
      "epoch:1; batch 22400; train accuracy: 0.748125\n",
      "epoch 1; batch 22528; loss 0.478681\n",
      "epoch:1; batch 22528; train accuracy: 0.748402\n",
      "epoch 1; batch 22656; loss 0.522968\n",
      "epoch:1; batch 22656; train accuracy: 0.748852\n",
      "epoch 1; batch 22784; loss 0.511779\n",
      "epoch:1; batch 22784; train accuracy: 0.748991\n",
      "epoch 1; batch 22912; loss 0.484547\n",
      "epoch:1; batch 22912; train accuracy: 0.749345\n",
      "epoch 1; batch 23040; loss 0.459654\n",
      "epoch:1; batch 23040; train accuracy: 0.749609\n",
      "epoch 1; batch 23168; loss 0.549351\n",
      "epoch:1; batch 23168; train accuracy: 0.749871\n",
      "epoch 1; batch 23296; loss 0.422976\n",
      "epoch:1; batch 23296; train accuracy: 0.750386\n",
      "epoch 1; batch 23424; loss 0.493468\n",
      "epoch:1; batch 23424; train accuracy: 0.750683\n",
      "epoch 1; batch 23552; loss 0.532281\n",
      "epoch:1; batch 23552; train accuracy: 0.751104\n",
      "epoch 1; batch 23680; loss 0.493570\n",
      "epoch:1; batch 23680; train accuracy: 0.751605\n",
      "epoch 1; batch 23808; loss 0.497596\n",
      "epoch:1; batch 23808; train accuracy: 0.752100\n",
      "epoch 1; batch 23936; loss 0.291123\n",
      "epoch:1; batch 23936; train accuracy: 0.752883\n",
      "epoch 1; batch 24064; loss 0.534092\n",
      "epoch:1; batch 24064; train accuracy: 0.753241\n",
      "epoch 1; batch 24192; loss 0.500762\n",
      "epoch:1; batch 24192; train accuracy: 0.753720\n",
      "epoch 1; batch 24320; loss 0.533654\n",
      "epoch:1; batch 24320; train accuracy: 0.754030\n",
      "epoch 1; batch 24448; loss 0.476222\n",
      "epoch:1; batch 24448; train accuracy: 0.754499\n",
      "epoch 1; batch 24576; loss 0.599420\n",
      "epoch:1; batch 24576; train accuracy: 0.754720\n",
      "epoch 1; batch 24704; loss 0.538894\n",
      "epoch:1; batch 24704; train accuracy: 0.755060\n",
      "epoch 1; batch 24832; loss 0.433610\n",
      "epoch:1; batch 24832; train accuracy: 0.755638\n",
      "epoch 1; batch 24960; loss 0.444489\n",
      "epoch:1; batch 24960; train accuracy: 0.756170\n",
      "epoch 1; batch 25088; loss 0.410816\n",
      "epoch:1; batch 25088; train accuracy: 0.756736\n",
      "epoch 1; batch 25216; loss 0.561076\n",
      "epoch:1; batch 25216; train accuracy: 0.757099\n",
      "epoch 1; batch 25344; loss 0.408680\n",
      "epoch:1; batch 25344; train accuracy: 0.757615\n",
      "epoch 1; batch 25472; loss 0.437086\n",
      "epoch:1; batch 25472; train accuracy: 0.758205\n",
      "epoch 1; batch 25600; loss 0.690658\n",
      "epoch:1; batch 25600; train accuracy: 0.758125\n",
      "epoch 1; batch 25728; loss 0.492763\n",
      "epoch:1; batch 25728; train accuracy: 0.758473\n",
      "epoch 1; batch 25856; loss 0.625246\n",
      "epoch:1; batch 25856; train accuracy: 0.758663\n",
      "epoch 1; batch 25984; loss 0.560182\n",
      "epoch:1; batch 25984; train accuracy: 0.758890\n",
      "epoch 1; batch 26112; loss 0.500813\n",
      "epoch:1; batch 26112; train accuracy: 0.759153\n",
      "epoch 1; batch 26240; loss 0.430697\n",
      "epoch:1; batch 26240; train accuracy: 0.759604\n",
      "epoch 1; batch 26368; loss 0.553182\n",
      "epoch:1; batch 26368; train accuracy: 0.759709\n",
      "epoch 1; batch 26496; loss 0.497086\n",
      "epoch:1; batch 26496; train accuracy: 0.759737\n",
      "epoch 1; batch 26624; loss 0.425774\n",
      "epoch:1; batch 26624; train accuracy: 0.760329\n",
      "epoch 1; batch 26752; loss 0.490338\n",
      "epoch:1; batch 26752; train accuracy: 0.760766\n",
      "epoch 1; batch 26880; loss 0.531581\n",
      "epoch:1; batch 26880; train accuracy: 0.761049\n",
      "epoch 1; batch 27008; loss 0.394734\n",
      "epoch:1; batch 27008; train accuracy: 0.761478\n",
      "epoch 1; batch 27136; loss 0.397584\n",
      "epoch:1; batch 27136; train accuracy: 0.761940\n",
      "epoch 1; batch 27264; loss 0.440165\n",
      "epoch:1; batch 27264; train accuracy: 0.762287\n",
      "epoch 1; batch 27392; loss 0.613860\n",
      "epoch:1; batch 27392; train accuracy: 0.762412\n",
      "epoch 1; batch 27520; loss 0.539667\n",
      "epoch:1; batch 27520; train accuracy: 0.762718\n",
      "epoch 1; batch 27648; loss 0.484515\n",
      "epoch:1; batch 27648; train accuracy: 0.763057\n",
      "epoch 1; batch 27776; loss 0.362699\n",
      "epoch:1; batch 27776; train accuracy: 0.763609\n",
      "epoch 1; batch 27904; loss 0.429064\n",
      "epoch:1; batch 27904; train accuracy: 0.764012\n",
      "epoch 1; batch 28032; loss 0.474822\n",
      "epoch:1; batch 28032; train accuracy: 0.764198\n",
      "epoch 1; batch 28160; loss 0.484632\n",
      "epoch:1; batch 28160; train accuracy: 0.764453\n",
      "epoch 1; batch 28288; loss 0.445819\n",
      "epoch:1; batch 28288; train accuracy: 0.764918\n",
      "epoch 1; batch 28416; loss 0.535745\n",
      "epoch:1; batch 28416; train accuracy: 0.765203\n",
      "epoch 1; batch 28544; loss 0.427529\n",
      "epoch:1; batch 28544; train accuracy: 0.765625\n",
      "epoch 1; batch 28672; loss 0.446169\n",
      "epoch:1; batch 28672; train accuracy: 0.766009\n",
      "epoch 1; batch 28800; loss 0.421428\n",
      "epoch:1; batch 28800; train accuracy: 0.766493\n",
      "epoch 1; batch 28928; loss 0.472833\n",
      "epoch:1; batch 28928; train accuracy: 0.766662\n",
      "epoch 1; batch 29056; loss 0.541571\n",
      "epoch:1; batch 29056; train accuracy: 0.766898\n",
      "epoch 1; batch 29184; loss 0.418850\n",
      "epoch:1; batch 29184; train accuracy: 0.767304\n",
      "epoch 1; batch 29312; loss 0.402151\n",
      "epoch:1; batch 29312; train accuracy: 0.767604\n",
      "epoch 1; batch 29440; loss 0.533429\n",
      "epoch:1; batch 29440; train accuracy: 0.767833\n",
      "epoch 1; batch 29568; loss 0.420333\n",
      "epoch:1; batch 29568; train accuracy: 0.768229\n",
      "epoch 1; batch 29696; loss 0.553562\n",
      "epoch:1; batch 29696; train accuracy: 0.768353\n",
      "epoch 1; batch 29824; loss 0.426367\n",
      "epoch:1; batch 29824; train accuracy: 0.768777\n",
      "epoch 1; batch 29952; loss 0.366160\n",
      "epoch:1; batch 29952; train accuracy: 0.769231\n",
      "epoch 1; batch 30080; loss 0.512906\n",
      "epoch:1; batch 30080; train accuracy: 0.769348\n",
      "epoch 1; batch 30208; loss 0.494191\n",
      "epoch:1; batch 30208; train accuracy: 0.769597\n",
      "epoch 1; batch 30336; loss 0.428737\n",
      "epoch:1; batch 30336; train accuracy: 0.769976\n",
      "epoch 1; batch 30464; loss 0.524086\n",
      "epoch:1; batch 30464; train accuracy: 0.770056\n",
      "epoch 1; batch 30592; loss 0.438904\n",
      "epoch:1; batch 30592; train accuracy: 0.770332\n",
      "epoch 1; batch 30720; loss 0.266729\n",
      "epoch:1; batch 30720; train accuracy: 0.770964\n",
      "epoch 1; batch 30848; loss 0.445458\n",
      "epoch:1; batch 30848; train accuracy: 0.771298\n",
      "epoch 1; batch 30976; loss 0.533091\n",
      "epoch:1; batch 30976; train accuracy: 0.771597\n",
      "epoch 1; batch 31104; loss 0.466108\n",
      "epoch:1; batch 31104; train accuracy: 0.771862\n",
      "epoch 1; batch 31232; loss 0.456353\n",
      "epoch:1; batch 31232; train accuracy: 0.772157\n",
      "epoch 1; batch 31360; loss 0.410859\n",
      "epoch:1; batch 31360; train accuracy: 0.772481\n",
      "epoch 1; batch 31488; loss 0.418370\n",
      "epoch:1; batch 31488; train accuracy: 0.772707\n",
      "epoch 1; batch 31616; loss 0.521794\n",
      "epoch:1; batch 31616; train accuracy: 0.772995\n",
      "epoch 1; batch 31744; loss 0.490252\n",
      "epoch:1; batch 31744; train accuracy: 0.773154\n",
      "epoch 1; batch 31872; loss 0.431576\n",
      "epoch:1; batch 31872; train accuracy: 0.773532\n",
      "epoch 1; batch 32000; loss 0.475123\n",
      "epoch:1; batch 32000; train accuracy: 0.773656\n",
      "epoch 1; batch 32128; loss 0.487533\n",
      "epoch:1; batch 32128; train accuracy: 0.773873\n",
      "epoch 1; batch 32256; loss 0.522507\n",
      "epoch:1; batch 32256; train accuracy: 0.774058\n",
      "epoch 1; batch 32384; loss 0.380511\n",
      "epoch:1; batch 32384; train accuracy: 0.774457\n",
      "epoch 1; batch 32512; loss 0.414937\n",
      "epoch:1; batch 32512; train accuracy: 0.774914\n",
      "epoch 1; batch 32640; loss 0.517640\n",
      "epoch:1; batch 32640; train accuracy: 0.775092\n",
      "epoch 1; batch 32768; loss 0.332771\n",
      "epoch:1; batch 32768; train accuracy: 0.775574\n",
      "epoch 1; batch 32896; loss 0.361228\n",
      "epoch:1; batch 32896; train accuracy: 0.775930\n",
      "epoch 1; batch 33024; loss 0.477666\n",
      "epoch:1; batch 33024; train accuracy: 0.776163\n",
      "epoch 1; batch 33152; loss 0.505076\n",
      "epoch:1; batch 33152; train accuracy: 0.776394\n",
      "epoch 1; batch 33280; loss 0.386831\n",
      "epoch:1; batch 33280; train accuracy: 0.776653\n",
      "epoch 1; batch 33408; loss 0.423256\n",
      "epoch:1; batch 33408; train accuracy: 0.776880\n",
      "epoch 1; batch 33536; loss 0.374290\n",
      "epoch:1; batch 33536; train accuracy: 0.777254\n",
      "epoch 1; batch 33664; loss 0.669893\n",
      "epoch:1; batch 33664; train accuracy: 0.777091\n",
      "epoch 1; batch 33792; loss 0.532292\n",
      "epoch:1; batch 33792; train accuracy: 0.777196\n",
      "epoch 1; batch 33920; loss 0.510008\n",
      "epoch:1; batch 33920; train accuracy: 0.777417\n",
      "epoch 1; batch 34048; loss 0.344940\n",
      "epoch:1; batch 34048; train accuracy: 0.777872\n",
      "epoch 1; batch 34176; loss 0.460863\n",
      "epoch:1; batch 34176; train accuracy: 0.778061\n",
      "epoch 1; batch 34304; loss 0.453209\n",
      "epoch:1; batch 34304; train accuracy: 0.778277\n",
      "epoch 1; batch 34432; loss 0.345015\n",
      "epoch:1; batch 34432; train accuracy: 0.778578\n",
      "epoch 1; batch 34560; loss 0.528801\n",
      "epoch:1; batch 34560; train accuracy: 0.778733\n",
      "epoch 1; batch 34688; loss 0.402362\n",
      "epoch:1; batch 34688; train accuracy: 0.778973\n",
      "epoch 1; batch 34816; loss 0.452262\n",
      "epoch:1; batch 34816; train accuracy: 0.779182\n",
      "epoch 1; batch 34944; loss 0.440337\n",
      "epoch:1; batch 34944; train accuracy: 0.779418\n",
      "epoch 1; batch 35072; loss 0.321922\n",
      "epoch:1; batch 35072; train accuracy: 0.779824\n",
      "epoch 1; batch 35200; loss 0.391110\n",
      "epoch:1; batch 35200; train accuracy: 0.780114\n",
      "epoch 1; batch 35328; loss 0.411240\n",
      "epoch:1; batch 35328; train accuracy: 0.780401\n",
      "epoch 1; batch 35456; loss 0.482863\n",
      "epoch:1; batch 35456; train accuracy: 0.780601\n",
      "epoch 1; batch 35584; loss 0.488156\n",
      "epoch:1; batch 35584; train accuracy: 0.780857\n",
      "epoch 1; batch 35712; loss 0.386974\n",
      "epoch:1; batch 35712; train accuracy: 0.781194\n",
      "epoch 1; batch 35840; loss 0.488762\n",
      "epoch:1; batch 35840; train accuracy: 0.781473\n",
      "epoch 1; batch 35968; loss 0.513421\n",
      "epoch:1; batch 35968; train accuracy: 0.781584\n",
      "epoch 1; batch 36096; loss 0.385932\n",
      "epoch:1; batch 36096; train accuracy: 0.781859\n",
      "epoch 1; batch 36224; loss 0.365233\n",
      "epoch:1; batch 36224; train accuracy: 0.782271\n",
      "epoch 1; batch 36352; loss 0.390347\n",
      "epoch:1; batch 36352; train accuracy: 0.782625\n",
      "epoch 1; batch 36480; loss 0.415644\n",
      "epoch:1; batch 36480; train accuracy: 0.782895\n",
      "epoch 1; batch 36608; loss 0.465057\n",
      "epoch:1; batch 36608; train accuracy: 0.783053\n",
      "epoch 1; batch 36736; loss 0.464072\n",
      "epoch:1; batch 36736; train accuracy: 0.783292\n",
      "epoch 1; batch 36864; loss 0.471909\n",
      "epoch:1; batch 36864; train accuracy: 0.783474\n",
      "epoch 1; batch 36992; loss 0.534535\n",
      "epoch:1; batch 36992; train accuracy: 0.783629\n",
      "epoch 1; batch 37120; loss 0.426008\n",
      "epoch:1; batch 37120; train accuracy: 0.783836\n",
      "epoch 1; batch 37248; loss 0.380428\n",
      "epoch:1; batch 37248; train accuracy: 0.784096\n",
      "epoch 1; batch 37376; loss 0.571746\n",
      "epoch:1; batch 37376; train accuracy: 0.784086\n",
      "epoch 1; batch 37504; loss 0.336735\n",
      "epoch:1; batch 37504; train accuracy: 0.784396\n",
      "epoch 1; batch 37632; loss 0.307875\n",
      "epoch:1; batch 37632; train accuracy: 0.784731\n",
      "epoch 1; batch 37760; loss 0.317606\n",
      "epoch:1; batch 37760; train accuracy: 0.785064\n",
      "epoch 1; batch 37888; loss 0.364936\n",
      "epoch:1; batch 37888; train accuracy: 0.785315\n",
      "epoch 1; batch 38016; loss 0.318194\n",
      "epoch:1; batch 38016; train accuracy: 0.785590\n",
      "epoch 1; batch 38144; loss 0.586222\n",
      "epoch:1; batch 38144; train accuracy: 0.785681\n",
      "epoch 1; batch 38272; loss 0.409759\n",
      "epoch:1; batch 38272; train accuracy: 0.785875\n",
      "epoch 1; batch 38400; loss 0.375010\n",
      "epoch:1; batch 38400; train accuracy: 0.786120\n",
      "epoch 1; batch 38528; loss 0.425581\n",
      "epoch:1; batch 38528; train accuracy: 0.786363\n",
      "epoch 1; batch 38656; loss 0.532875\n",
      "epoch:1; batch 38656; train accuracy: 0.786346\n",
      "epoch 1; batch 38784; loss 0.550964\n",
      "epoch:1; batch 38784; train accuracy: 0.786407\n",
      "epoch 1; batch 38912; loss 0.352057\n",
      "epoch:1; batch 38912; train accuracy: 0.786724\n",
      "epoch 1; batch 39040; loss 0.457489\n",
      "epoch:1; batch 39040; train accuracy: 0.786860\n",
      "epoch 1; batch 39168; loss 0.345660\n",
      "epoch:1; batch 39168; train accuracy: 0.787173\n",
      "epoch 1; batch 39296; loss 0.517739\n",
      "epoch:1; batch 39296; train accuracy: 0.787332\n",
      "epoch 1; batch 39424; loss 0.537734\n",
      "epoch:1; batch 39424; train accuracy: 0.787363\n",
      "epoch 1; batch 39552; loss 0.549895\n",
      "epoch:1; batch 39552; train accuracy: 0.787394\n",
      "epoch 1; batch 39680; loss 0.442969\n",
      "epoch:1; batch 39680; train accuracy: 0.787651\n",
      "epoch 1; batch 39808; loss 0.378426\n",
      "epoch:1; batch 39808; train accuracy: 0.787932\n",
      "epoch 1; batch 39936; loss 0.391821\n",
      "epoch:1; batch 39936; train accuracy: 0.788236\n",
      "epoch 1; batch 40064; loss 0.425834\n",
      "epoch:1; batch 40064; train accuracy: 0.788488\n",
      "epoch 1; batch 40192; loss 0.413988\n",
      "epoch:1; batch 40192; train accuracy: 0.788714\n",
      "epoch 1; batch 40320; loss 0.542670\n",
      "epoch:1; batch 40320; train accuracy: 0.788690\n",
      "epoch 1; batch 40448; loss 0.471548\n",
      "epoch:1; batch 40448; train accuracy: 0.788964\n",
      "epoch 1; batch 40576; loss 0.437839\n",
      "epoch:1; batch 40576; train accuracy: 0.789210\n",
      "epoch 1; batch 40704; loss 0.489779\n",
      "epoch:1; batch 40704; train accuracy: 0.789234\n",
      "epoch 1; batch 40832; loss 0.477085\n",
      "epoch:1; batch 40832; train accuracy: 0.789430\n",
      "epoch 1; batch 40960; loss 0.373218\n",
      "epoch:1; batch 40960; train accuracy: 0.789746\n",
      "epoch 1; batch 41088; loss 0.583393\n",
      "epoch:1; batch 41088; train accuracy: 0.789866\n",
      "epoch 1; batch 41216; loss 0.441436\n",
      "epoch:1; batch 41216; train accuracy: 0.790057\n",
      "epoch 1; batch 41344; loss 0.385797\n",
      "epoch:1; batch 41344; train accuracy: 0.790344\n",
      "epoch 1; batch 41472; loss 0.385156\n",
      "epoch:1; batch 41472; train accuracy: 0.790606\n",
      "epoch 1; batch 41600; loss 0.579106\n",
      "epoch:1; batch 41600; train accuracy: 0.790625\n",
      "epoch 1; batch 41728; loss 0.475092\n",
      "epoch:1; batch 41728; train accuracy: 0.790788\n",
      "epoch 1; batch 41856; loss 0.492303\n",
      "epoch:1; batch 41856; train accuracy: 0.790807\n",
      "epoch 1; batch 41984; loss 0.448121\n",
      "epoch:1; batch 41984; train accuracy: 0.790944\n",
      "epoch 1; batch 42112; loss 0.424387\n",
      "epoch:1; batch 42112; train accuracy: 0.791105\n",
      "epoch 1; batch 42240; loss 0.433875\n",
      "epoch:1; batch 42240; train accuracy: 0.791193\n",
      "epoch 1; batch 42368; loss 0.501882\n",
      "epoch:1; batch 42368; train accuracy: 0.791305\n",
      "epoch 1; batch 42496; loss 0.385384\n",
      "epoch:1; batch 42496; train accuracy: 0.791486\n",
      "epoch 1; batch 42624; loss 0.337935\n",
      "epoch:1; batch 42624; train accuracy: 0.791831\n",
      "epoch 1; batch 42752; loss 0.440859\n",
      "epoch:1; batch 42752; train accuracy: 0.791986\n",
      "epoch 1; batch 42880; loss 0.314521\n",
      "epoch:1; batch 42880; train accuracy: 0.792374\n",
      "epoch 1; batch 43008; loss 0.487688\n",
      "epoch:1; batch 43008; train accuracy: 0.792504\n",
      "epoch 1; batch 43136; loss 0.447149\n",
      "epoch:1; batch 43136; train accuracy: 0.792609\n",
      "epoch 1; batch 43264; loss 0.367789\n",
      "epoch:1; batch 43264; train accuracy: 0.792876\n",
      "epoch 1; batch 43392; loss 0.488750\n",
      "epoch:1; batch 43392; train accuracy: 0.793003\n",
      "epoch 1; batch 43520; loss 0.412676\n",
      "epoch:1; batch 43520; train accuracy: 0.793084\n",
      "epoch 1; batch 43648; loss 0.312304\n",
      "epoch:1; batch 43648; train accuracy: 0.793370\n",
      "epoch 1; batch 43776; loss 0.423286\n",
      "epoch:1; batch 43776; train accuracy: 0.793448\n",
      "epoch 1; batch 43904; loss 0.239131\n",
      "epoch:1; batch 43904; train accuracy: 0.793846\n",
      "epoch 1; batch 44032; loss 0.480800\n",
      "epoch:1; batch 44032; train accuracy: 0.793900\n",
      "epoch 1; batch 44160; loss 0.469075\n",
      "epoch:1; batch 44160; train accuracy: 0.794067\n",
      "epoch 1; batch 44288; loss 0.480769\n",
      "epoch:1; batch 44288; train accuracy: 0.794188\n",
      "epoch 1; batch 44416; loss 0.463913\n",
      "epoch:1; batch 44416; train accuracy: 0.794353\n",
      "epoch 1; batch 44544; loss 0.299611\n",
      "epoch:1; batch 44544; train accuracy: 0.794652\n",
      "epoch 1; batch 44672; loss 0.436226\n",
      "epoch:1; batch 44672; train accuracy: 0.794883\n",
      "epoch 1; batch 44800; loss 0.412200\n",
      "epoch:1; batch 44800; train accuracy: 0.795089\n",
      "epoch 1; batch 44928; loss 0.366185\n",
      "epoch:1; batch 44928; train accuracy: 0.795317\n",
      "epoch 1; batch 45056; loss 0.359016\n",
      "epoch:1; batch 45056; train accuracy: 0.795588\n",
      "epoch 1; batch 45184; loss 0.404275\n",
      "epoch:1; batch 45184; train accuracy: 0.795835\n",
      "epoch 1; batch 45312; loss 0.390233\n",
      "epoch:1; batch 45312; train accuracy: 0.796036\n",
      "epoch 1; batch 45440; loss 0.452051\n",
      "epoch:1; batch 45440; train accuracy: 0.796171\n",
      "epoch 1; batch 45568; loss 0.469638\n",
      "epoch:1; batch 45568; train accuracy: 0.796217\n",
      "epoch 1; batch 45696; loss 0.368556\n",
      "epoch:1; batch 45696; train accuracy: 0.796437\n",
      "epoch 1; batch 45824; loss 0.378857\n",
      "epoch:1; batch 45824; train accuracy: 0.796613\n",
      "epoch 1; batch 45952; loss 0.473397\n",
      "epoch:1; batch 45952; train accuracy: 0.796723\n",
      "epoch 1; batch 46080; loss 0.416920\n",
      "epoch:1; batch 46080; train accuracy: 0.796810\n",
      "epoch 1; batch 46208; loss 0.295756\n",
      "epoch:1; batch 46208; train accuracy: 0.797091\n",
      "epoch 1; batch 46336; loss 0.410992\n",
      "epoch:1; batch 46336; train accuracy: 0.797220\n",
      "epoch 1; batch 46464; loss 0.315218\n",
      "epoch:1; batch 46464; train accuracy: 0.797521\n",
      "epoch 1; batch 46592; loss 0.394830\n",
      "epoch:1; batch 46592; train accuracy: 0.797755\n",
      "epoch 1; batch 46720; loss 0.424556\n",
      "epoch:1; batch 46720; train accuracy: 0.797881\n",
      "epoch 1; batch 46848; loss 0.493166\n",
      "epoch:1; batch 46848; train accuracy: 0.797942\n",
      "epoch 1; batch 46976; loss 0.412467\n",
      "epoch:1; batch 46976; train accuracy: 0.798131\n",
      "epoch 1; batch 47104; loss 0.330345\n",
      "epoch:1; batch 47104; train accuracy: 0.798382\n",
      "epoch 1; batch 47232; loss 0.341578\n",
      "epoch:1; batch 47232; train accuracy: 0.798632\n",
      "epoch 1; batch 47360; loss 0.441832\n",
      "epoch:1; batch 47360; train accuracy: 0.798754\n",
      "epoch 1; batch 47488; loss 0.393334\n",
      "epoch:1; batch 47488; train accuracy: 0.798960\n",
      "epoch 1; batch 47616; loss 0.459305\n",
      "epoch:1; batch 47616; train accuracy: 0.799122\n",
      "epoch 1; batch 47744; loss 0.351137\n",
      "epoch:1; batch 47744; train accuracy: 0.799326\n",
      "epoch 1; batch 47872; loss 0.299760\n",
      "epoch:1; batch 47872; train accuracy: 0.799591\n",
      "epoch 1; batch 48000; loss 0.404593\n",
      "epoch:1; batch 48000; train accuracy: 0.799771\n",
      "epoch 1; batch 48128; loss 0.458263\n",
      "epoch:1; batch 48128; train accuracy: 0.799950\n",
      "epoch 1; batch 48256; loss 0.457091\n",
      "epoch:1; batch 48256; train accuracy: 0.800025\n",
      "epoch 1; batch 48384; loss 0.330163\n",
      "epoch:1; batch 48384; train accuracy: 0.800244\n",
      "epoch 1; batch 48512; loss 0.512506\n",
      "epoch:1; batch 48512; train accuracy: 0.800400\n",
      "epoch 1; batch 48640; loss 0.530296\n",
      "epoch:1; batch 48640; train accuracy: 0.800452\n",
      "epoch 1; batch 48768; loss 0.330245\n",
      "epoch:1; batch 48768; train accuracy: 0.800709\n",
      "epoch 1; batch 48896; loss 0.501652\n",
      "epoch:1; batch 48896; train accuracy: 0.800802\n",
      "epoch 1; batch 49024; loss 0.443061\n",
      "epoch:1; batch 49024; train accuracy: 0.800832\n",
      "epoch 1; batch 49152; loss 0.366479\n",
      "epoch:1; batch 49152; train accuracy: 0.801005\n",
      "epoch 1; batch 49280; loss 0.371409\n",
      "epoch:1; batch 49280; train accuracy: 0.801136\n",
      "epoch 1; batch 49408; loss 0.380437\n",
      "epoch:1; batch 49408; train accuracy: 0.801287\n",
      "epoch 1; batch 49536; loss 0.425752\n",
      "epoch:1; batch 49536; train accuracy: 0.801417\n",
      "epoch 1; batch 49664; loss 0.354596\n",
      "epoch:1; batch 49664; train accuracy: 0.801647\n",
      "epoch 1; batch 49792; loss 0.469102\n",
      "epoch:1; batch 49792; train accuracy: 0.801675\n",
      "epoch 1; batch 49920; loss 0.395891\n",
      "epoch:1; batch 49920; train accuracy: 0.801843\n",
      "epoch 1; batch 50048; loss 0.454188\n",
      "epoch:1; batch 50048; train accuracy: 0.801910\n",
      "epoch 1; batch 50176; loss 0.463550\n",
      "epoch:1; batch 50176; train accuracy: 0.801997\n",
      "epoch 1; batch 50304; loss 0.406467\n",
      "epoch:1; batch 50304; train accuracy: 0.802203\n",
      "epoch 1; batch 50432; loss 0.307410\n",
      "epoch:1; batch 50432; train accuracy: 0.802467\n",
      "epoch 1; batch 50560; loss 0.346143\n",
      "epoch:1; batch 50560; train accuracy: 0.802571\n",
      "epoch 1; batch 50688; loss 0.367641\n",
      "epoch:1; batch 50688; train accuracy: 0.802734\n",
      "epoch 1; batch 50816; loss 0.391997\n",
      "epoch:1; batch 50816; train accuracy: 0.802857\n",
      "epoch 1; batch 50944; loss 0.345083\n",
      "epoch:1; batch 50944; train accuracy: 0.803078\n",
      "epoch 1; batch 51072; loss 0.432263\n",
      "epoch:1; batch 51072; train accuracy: 0.803160\n",
      "epoch 1; batch 51200; loss 0.549399\n",
      "epoch:1; batch 51200; train accuracy: 0.803184\n",
      "epoch 1; batch 51328; loss 0.392440\n",
      "epoch:1; batch 51328; train accuracy: 0.803343\n",
      "epoch 1; batch 51456; loss 0.306178\n",
      "epoch:1; batch 51456; train accuracy: 0.803580\n",
      "epoch 1; batch 51584; loss 0.426981\n",
      "epoch:1; batch 51584; train accuracy: 0.803699\n",
      "epoch 1; batch 51712; loss 0.521258\n",
      "epoch:1; batch 51712; train accuracy: 0.803721\n",
      "epoch 1; batch 51840; loss 0.389170\n",
      "epoch:1; batch 51840; train accuracy: 0.803897\n",
      "epoch 1; batch 51968; loss 0.456718\n",
      "epoch:1; batch 51968; train accuracy: 0.803899\n",
      "epoch 1; batch 52096; loss 0.351753\n",
      "epoch:1; batch 52096; train accuracy: 0.804092\n",
      "epoch 1; batch 52224; loss 0.324493\n",
      "epoch:1; batch 52224; train accuracy: 0.804324\n",
      "epoch 1; batch 52352; loss 0.428687\n",
      "epoch:1; batch 52352; train accuracy: 0.804439\n",
      "epoch 1; batch 52480; loss 0.348456\n",
      "epoch:1; batch 52480; train accuracy: 0.804649\n",
      "epoch 1; batch 52608; loss 0.397021\n",
      "epoch:1; batch 52608; train accuracy: 0.804802\n",
      "epoch 1; batch 52736; loss 0.613447\n",
      "epoch:1; batch 52736; train accuracy: 0.804782\n",
      "epoch 1; batch 52864; loss 0.389718\n",
      "epoch:1; batch 52864; train accuracy: 0.804971\n",
      "epoch 1; batch 52992; loss 0.383875\n",
      "epoch:1; batch 52992; train accuracy: 0.805065\n",
      "epoch 1; batch 53120; loss 0.336567\n",
      "epoch:1; batch 53120; train accuracy: 0.805233\n",
      "epoch 1; batch 53248; loss 0.447405\n",
      "epoch:1; batch 53248; train accuracy: 0.805326\n",
      "epoch 1; batch 53376; loss 0.337072\n",
      "epoch:1; batch 53376; train accuracy: 0.805456\n",
      "epoch 1; batch 53504; loss 0.466163\n",
      "epoch:1; batch 53504; train accuracy: 0.805510\n",
      "epoch 1; batch 53632; loss 0.437243\n",
      "epoch:1; batch 53632; train accuracy: 0.805638\n",
      "epoch 1; batch 53760; loss 0.392684\n",
      "epoch:1; batch 53760; train accuracy: 0.805748\n",
      "epoch 1; batch 53888; loss 0.466921\n",
      "epoch:1; batch 53888; train accuracy: 0.805857\n",
      "epoch 1; batch 54016; loss 0.371886\n",
      "epoch:1; batch 54016; train accuracy: 0.805983\n",
      "epoch 1; batch 54144; loss 0.325531\n",
      "epoch:1; batch 54144; train accuracy: 0.806184\n",
      "epoch 1; batch 54272; loss 0.411030\n",
      "epoch:1; batch 54272; train accuracy: 0.806254\n",
      "epoch 1; batch 54400; loss 0.518993\n",
      "epoch:1; batch 54400; train accuracy: 0.806342\n",
      "epoch 1; batch 54528; loss 0.406767\n",
      "epoch:1; batch 54528; train accuracy: 0.806430\n",
      "epoch 1; batch 54656; loss 0.432807\n",
      "epoch:1; batch 54656; train accuracy: 0.806481\n",
      "epoch 1; batch 54784; loss 0.403442\n",
      "epoch:1; batch 54784; train accuracy: 0.806549\n",
      "epoch 1; batch 54912; loss 0.547971\n",
      "epoch:1; batch 54912; train accuracy: 0.806545\n",
      "epoch 1; batch 55040; loss 0.387769\n",
      "epoch:1; batch 55040; train accuracy: 0.806613\n",
      "epoch 1; batch 55168; loss 0.340943\n",
      "epoch:1; batch 55168; train accuracy: 0.806736\n",
      "epoch 1; batch 55296; loss 0.403503\n",
      "epoch:1; batch 55296; train accuracy: 0.806876\n",
      "epoch 1; batch 55424; loss 0.317863\n",
      "epoch:1; batch 55424; train accuracy: 0.807051\n",
      "epoch 1; batch 55552; loss 0.518880\n",
      "epoch:1; batch 55552; train accuracy: 0.807136\n",
      "epoch 1; batch 55680; loss 0.522497\n",
      "epoch:1; batch 55680; train accuracy: 0.807148\n",
      "epoch 1; batch 55808; loss 0.342544\n",
      "epoch:1; batch 55808; train accuracy: 0.807268\n",
      "epoch 1; batch 55936; loss 0.269086\n",
      "epoch:1; batch 55936; train accuracy: 0.807548\n",
      "epoch 1; batch 56064; loss 0.498768\n",
      "epoch:1; batch 56064; train accuracy: 0.807595\n",
      "epoch 1; batch 56192; loss 0.431925\n",
      "epoch:1; batch 56192; train accuracy: 0.807855\n",
      "epoch 1; batch 56320; loss 0.519788\n",
      "epoch:1; batch 56320; train accuracy: 0.807830\n",
      "epoch 1; batch 56448; loss 0.471682\n",
      "epoch:1; batch 56448; train accuracy: 0.807912\n",
      "epoch 1; batch 56576; loss 0.453971\n",
      "epoch:1; batch 56576; train accuracy: 0.807851\n",
      "epoch 1; batch 56704; loss 0.424904\n",
      "epoch:1; batch 56704; train accuracy: 0.808056\n",
      "epoch 1; batch 56832; loss 0.478372\n",
      "epoch:1; batch 56832; train accuracy: 0.808136\n",
      "epoch 1; batch 56960; loss 0.370205\n",
      "epoch:1; batch 56960; train accuracy: 0.808286\n",
      "epoch 1; batch 57088; loss 0.337002\n",
      "epoch:1; batch 57088; train accuracy: 0.808471\n",
      "epoch 1; batch 57216; loss 0.564831\n",
      "epoch:1; batch 57216; train accuracy: 0.808393\n",
      "epoch 1; batch 57344; loss 0.424906\n",
      "epoch:1; batch 57344; train accuracy: 0.808472\n",
      "epoch 1; batch 57472; loss 0.328379\n",
      "epoch:1; batch 57472; train accuracy: 0.808620\n",
      "epoch 1; batch 57600; loss 0.438946\n",
      "epoch:1; batch 57600; train accuracy: 0.808646\n",
      "epoch 1; batch 57728; loss 0.409606\n",
      "epoch:1; batch 57728; train accuracy: 0.808793\n",
      "epoch 1; batch 57856; loss 0.523212\n",
      "epoch:1; batch 57856; train accuracy: 0.808784\n",
      "epoch 1; batch 57984; loss 0.465740\n",
      "epoch:1; batch 57984; train accuracy: 0.808861\n",
      "epoch 1; batch 58112; loss 0.328433\n",
      "epoch:1; batch 58112; train accuracy: 0.809007\n",
      "epoch 1; batch 58240; loss 0.362361\n",
      "epoch:1; batch 58240; train accuracy: 0.809169\n",
      "epoch 1; batch 58368; loss 0.391175\n",
      "epoch:1; batch 58368; train accuracy: 0.809296\n",
      "epoch 1; batch 58496; loss 0.362168\n",
      "epoch:1; batch 58496; train accuracy: 0.809457\n",
      "epoch 1; batch 58624; loss 0.393474\n",
      "epoch:1; batch 58624; train accuracy: 0.809600\n",
      "epoch 1; batch 58752; loss 0.534229\n",
      "epoch:1; batch 58752; train accuracy: 0.809555\n",
      "epoch 1; batch 58880; loss 0.460737\n",
      "epoch:1; batch 58880; train accuracy: 0.809681\n",
      "epoch 1; batch 59008; loss 0.297640\n",
      "epoch:1; batch 59008; train accuracy: 0.809839\n",
      "epoch 1; batch 59136; loss 0.376846\n",
      "epoch:1; batch 59136; train accuracy: 0.809997\n",
      "epoch 1; batch 59264; loss 0.415072\n",
      "epoch:1; batch 59264; train accuracy: 0.810104\n",
      "epoch 1; batch 59392; loss 0.397740\n",
      "epoch:1; batch 59392; train accuracy: 0.810193\n",
      "epoch 1; batch 59520; loss 0.485469\n",
      "epoch:1; batch 59520; train accuracy: 0.810265\n",
      "epoch 1; batch 59648; loss 0.430053\n",
      "epoch:1; batch 59648; train accuracy: 0.810371\n",
      "epoch 1; batch 59776; loss 0.320263\n",
      "epoch:1; batch 59776; train accuracy: 0.810509\n",
      "epoch 1; batch 59904; loss 0.368026\n",
      "epoch:1; batch 59904; train accuracy: 0.810597\n",
      "epoch 1; batch 60032; loss 0.418514\n",
      "epoch:1; batch 60032; train accuracy: 0.810684\n",
      "epoch 1; batch 60160; loss 0.569572\n",
      "epoch:1; batch 60160; train accuracy: 0.810688\n",
      "epoch 1; batch 60288; loss 0.359616\n",
      "epoch:1; batch 60288; train accuracy: 0.810841\n",
      "epoch 1; batch 60416; loss 0.458152\n",
      "epoch:1; batch 60416; train accuracy: 0.810812\n",
      "epoch 1; batch 60544; loss 0.543406\n",
      "epoch:1; batch 60544; train accuracy: 0.810881\n",
      "epoch 1; batch 60672; loss 0.317682\n",
      "epoch:1; batch 60672; train accuracy: 0.810984\n",
      "epoch 1; batch 60800; loss 0.352201\n",
      "epoch:1; batch 60800; train accuracy: 0.811069\n",
      "epoch 1; batch 60928; loss 0.294903\n",
      "epoch:1; batch 60928; train accuracy: 0.811203\n",
      "epoch 1; batch 61056; loss 0.334944\n",
      "epoch:1; batch 61056; train accuracy: 0.811370\n",
      "epoch 1; batch 61184; loss 0.279674\n",
      "epoch:1; batch 61184; train accuracy: 0.811568\n",
      "epoch 1; batch 61312; loss 0.491368\n",
      "epoch:1; batch 61312; train accuracy: 0.811668\n",
      "epoch 1; batch 61440; loss 0.398241\n",
      "epoch:1; batch 61440; train accuracy: 0.811768\n",
      "epoch 1; batch 61568; loss 0.540991\n",
      "epoch:1; batch 61568; train accuracy: 0.811737\n",
      "epoch 1; batch 61696; loss 0.224235\n",
      "epoch:1; batch 61696; train accuracy: 0.811981\n",
      "epoch 1; batch 61824; loss 0.353111\n",
      "epoch:1; batch 61824; train accuracy: 0.812160\n",
      "epoch 1; batch 61952; loss 0.442899\n",
      "epoch:1; batch 61952; train accuracy: 0.812242\n",
      "epoch 1; batch 62080; loss 0.410243\n",
      "epoch:1; batch 62080; train accuracy: 0.812323\n",
      "epoch 1; batch 62208; loss 0.343700\n",
      "epoch:1; batch 62208; train accuracy: 0.812387\n",
      "epoch 1; batch 62336; loss 0.382004\n",
      "epoch:1; batch 62336; train accuracy: 0.812516\n",
      "epoch 1; batch 62464; loss 0.397454\n",
      "epoch:1; batch 62464; train accuracy: 0.812564\n",
      "epoch 1; batch 62592; loss 0.445687\n",
      "epoch:1; batch 62592; train accuracy: 0.812676\n",
      "epoch 1; batch 62720; loss 0.332474\n",
      "epoch:1; batch 62720; train accuracy: 0.812771\n",
      "epoch 1; batch 62848; loss 0.438684\n",
      "epoch:1; batch 62848; train accuracy: 0.812882\n",
      "epoch 1; batch 62976; loss 0.461247\n",
      "epoch:1; batch 62976; train accuracy: 0.812865\n",
      "epoch 1; batch 63104; loss 0.364909\n",
      "epoch:1; batch 63104; train accuracy: 0.812975\n",
      "epoch 1; batch 63232; loss 0.417576\n",
      "epoch:1; batch 63232; train accuracy: 0.813022\n",
      "epoch 1; batch 63360; loss 0.436017\n",
      "epoch:1; batch 63360; train accuracy: 0.813100\n",
      "epoch 1; batch 63488; loss 0.399680\n",
      "epoch:1; batch 63488; train accuracy: 0.813225\n",
      "epoch 1; batch 63616; loss 0.368798\n",
      "epoch:1; batch 63616; train accuracy: 0.813317\n",
      "epoch 1; batch 63744; loss 0.343387\n",
      "epoch:1; batch 63744; train accuracy: 0.813504\n",
      "epoch 1; batch 63872; loss 0.398877\n",
      "epoch:1; batch 63872; train accuracy: 0.813549\n",
      "epoch 1; batch 64000; loss 0.383479\n",
      "epoch:1; batch 64000; train accuracy: 0.813703\n",
      "epoch 1; batch 64128; loss 0.345755\n",
      "epoch:1; batch 64128; train accuracy: 0.813810\n",
      "epoch 1; batch 64256; loss 0.391287\n",
      "epoch:1; batch 64256; train accuracy: 0.813870\n",
      "epoch 1; batch 64384; loss 0.382418\n",
      "epoch:1; batch 64384; train accuracy: 0.813960\n",
      "epoch 1; batch 64512; loss 0.413471\n",
      "epoch:1; batch 64512; train accuracy: 0.813988\n",
      "epoch 1; batch 64640; loss 0.304750\n",
      "epoch:1; batch 64640; train accuracy: 0.814124\n",
      "epoch 1; batch 64768; loss 0.506995\n",
      "epoch:1; batch 64768; train accuracy: 0.814137\n",
      "epoch 1; batch 64896; loss 0.324419\n",
      "epoch:1; batch 64896; train accuracy: 0.814287\n",
      "epoch 1; batch 65024; loss 0.365010\n",
      "epoch:1; batch 65024; train accuracy: 0.814376\n",
      "epoch 1; batch 65152; loss 0.446511\n",
      "epoch:1; batch 65152; train accuracy: 0.814388\n",
      "epoch 1; batch 65280; loss 0.561006\n",
      "epoch:1; batch 65280; train accuracy: 0.814369\n",
      "epoch 1; batch 65408; loss 0.421190\n",
      "epoch:1; batch 65408; train accuracy: 0.814472\n",
      "epoch 1; batch 65536; loss 0.413114\n",
      "epoch:1; batch 65536; train accuracy: 0.814545\n",
      "epoch 1; batch 65664; loss 0.366019\n",
      "epoch:1; batch 65664; train accuracy: 0.814678\n",
      "epoch 1; batch 65792; loss 0.432950\n",
      "epoch:1; batch 65792; train accuracy: 0.814689\n",
      "epoch 1; batch 65920; loss 0.357802\n",
      "epoch:1; batch 65920; train accuracy: 0.814866\n",
      "epoch 1; batch 66048; loss 0.364817\n",
      "epoch:1; batch 66048; train accuracy: 0.814968\n",
      "epoch 1; batch 66176; loss 0.473283\n",
      "epoch:1; batch 66176; train accuracy: 0.815039\n",
      "epoch 1; batch 66304; loss 0.535735\n",
      "epoch:1; batch 66304; train accuracy: 0.815004\n",
      "epoch 1; batch 66432; loss 0.256758\n",
      "epoch:1; batch 66432; train accuracy: 0.815164\n",
      "epoch 1; batch 66560; loss 0.489809\n",
      "epoch:1; batch 66560; train accuracy: 0.815264\n",
      "epoch 1; batch 66688; loss 0.367279\n",
      "epoch:1; batch 66688; train accuracy: 0.815379\n",
      "epoch 1; batch 66816; loss 0.500228\n",
      "epoch:1; batch 66816; train accuracy: 0.815433\n",
      "epoch 1; batch 66944; loss 0.388144\n",
      "epoch:1; batch 66944; train accuracy: 0.815473\n",
      "epoch 1; batch 67072; loss 0.352980\n",
      "epoch:1; batch 67072; train accuracy: 0.815571\n",
      "epoch 1; batch 67200; loss 0.443077\n",
      "epoch:1; batch 67200; train accuracy: 0.815640\n",
      "epoch 1; batch 67328; loss 0.451537\n",
      "epoch:1; batch 67328; train accuracy: 0.815708\n",
      "epoch 1; batch 67456; loss 0.434484\n",
      "epoch:1; batch 67456; train accuracy: 0.815776\n",
      "epoch 1; batch 67584; loss 0.275204\n",
      "epoch:1; batch 67584; train accuracy: 0.815992\n",
      "epoch 1; batch 67712; loss 0.421830\n",
      "epoch:1; batch 67712; train accuracy: 0.816030\n",
      "epoch 1; batch 67840; loss 0.318723\n",
      "epoch:1; batch 67840; train accuracy: 0.816156\n",
      "epoch 1; batch 67968; loss 0.313889\n",
      "epoch:1; batch 67968; train accuracy: 0.816340\n",
      "epoch 1; batch 68096; loss 0.304962\n",
      "epoch:1; batch 68096; train accuracy: 0.816494\n",
      "epoch 1; batch 68224; loss 0.435114\n",
      "epoch:1; batch 68224; train accuracy: 0.816516\n",
      "epoch 1; batch 68352; loss 0.513724\n",
      "epoch:1; batch 68352; train accuracy: 0.816567\n",
      "epoch 1; batch 68480; loss 0.426565\n",
      "epoch:1; batch 68480; train accuracy: 0.816618\n",
      "epoch 1; batch 68608; loss 0.497724\n",
      "epoch:1; batch 68608; train accuracy: 0.816683\n",
      "epoch 1; batch 68736; loss 0.330750\n",
      "epoch:1; batch 68736; train accuracy: 0.816806\n",
      "epoch 1; batch 68864; loss 0.618161\n",
      "epoch:1; batch 68864; train accuracy: 0.816769\n",
      "epoch 1; batch 68992; loss 0.430517\n",
      "epoch:1; batch 68992; train accuracy: 0.816848\n",
      "epoch 1; batch 69120; loss 0.444989\n",
      "epoch:1; batch 69120; train accuracy: 0.816884\n",
      "epoch 1; batch 69248; loss 0.419913\n",
      "epoch:1; batch 69248; train accuracy: 0.816962\n",
      "epoch 1; batch 69376; loss 0.327141\n",
      "epoch:1; batch 69376; train accuracy: 0.817127\n",
      "epoch 1; batch 69504; loss 0.384167\n",
      "epoch:1; batch 69504; train accuracy: 0.817205\n",
      "epoch 1; batch 69632; loss 0.425072\n",
      "epoch:1; batch 69632; train accuracy: 0.817239\n",
      "epoch 1; batch 69760; loss 0.336424\n",
      "epoch:1; batch 69760; train accuracy: 0.817288\n",
      "epoch 1; batch 69888; loss 0.431220\n",
      "epoch:1; batch 69888; train accuracy: 0.817322\n",
      "epoch 1; batch 70016; loss 0.470680\n",
      "epoch:1; batch 70016; train accuracy: 0.817356\n",
      "epoch 1; batch 70144; loss 0.501930\n",
      "epoch:1; batch 70144; train accuracy: 0.817361\n",
      "epoch 1; batch 70272; loss 0.358382\n",
      "epoch:1; batch 70272; train accuracy: 0.817523\n",
      "epoch 1; batch 70400; loss 0.561678\n",
      "epoch:1; batch 70400; train accuracy: 0.817528\n",
      "epoch 1; batch 70528; loss 0.302452\n",
      "epoch:1; batch 70528; train accuracy: 0.817675\n",
      "epoch 1; batch 70656; loss 0.499947\n",
      "epoch:1; batch 70656; train accuracy: 0.817666\n",
      "epoch 1; batch 70784; loss 0.373567\n",
      "epoch:1; batch 70784; train accuracy: 0.817741\n",
      "epoch 1; batch 70912; loss 0.534611\n",
      "epoch:1; batch 70912; train accuracy: 0.817690\n",
      "epoch 1; batch 71040; loss 0.377345\n",
      "epoch:1; batch 71040; train accuracy: 0.817765\n",
      "epoch 1; batch 71168; loss 0.427922\n",
      "epoch:1; batch 71168; train accuracy: 0.817825\n",
      "epoch 1; batch 71296; loss 0.388458\n",
      "epoch:1; batch 71296; train accuracy: 0.817942\n",
      "epoch 1; batch 71424; loss 0.385128\n",
      "epoch:1; batch 71424; train accuracy: 0.818044\n",
      "epoch 1; batch 71552; loss 0.383665\n",
      "epoch:1; batch 71552; train accuracy: 0.818090\n",
      "epoch 1; batch 71680; loss 0.434405\n",
      "epoch:1; batch 71680; train accuracy: 0.818136\n",
      "epoch 1; batch 71808; loss 0.456640\n",
      "epoch:1; batch 71808; train accuracy: 0.818182\n",
      "epoch 1; batch 71936; loss 0.412605\n",
      "epoch:1; batch 71936; train accuracy: 0.818269\n",
      "epoch 1; batch 72064; loss 0.421024\n",
      "epoch:1; batch 72064; train accuracy: 0.818287\n",
      "epoch 1; batch 72192; loss 0.428138\n",
      "epoch:1; batch 72192; train accuracy: 0.818332\n",
      "epoch 1; batch 72320; loss 0.493027\n",
      "epoch:1; batch 72320; train accuracy: 0.818390\n",
      "epoch 1; batch 72448; loss 0.347286\n",
      "epoch:1; batch 72448; train accuracy: 0.818449\n",
      "epoch 1; batch 72576; loss 0.271302\n",
      "epoch:1; batch 72576; train accuracy: 0.818604\n",
      "epoch 1; batch 72704; loss 0.386661\n",
      "epoch:1; batch 72704; train accuracy: 0.818662\n",
      "epoch 1; batch 72832; loss 0.349325\n",
      "epoch:1; batch 72832; train accuracy: 0.818747\n",
      "epoch 1; batch 72960; loss 0.341136\n",
      "epoch:1; batch 72960; train accuracy: 0.818901\n",
      "epoch 1; batch 73088; loss 0.290225\n",
      "epoch:1; batch 73088; train accuracy: 0.819067\n",
      "epoch 1; batch 73216; loss 0.489207\n",
      "epoch:1; batch 73216; train accuracy: 0.819097\n",
      "epoch 1; batch 73344; loss 0.348740\n",
      "epoch:1; batch 73344; train accuracy: 0.819181\n",
      "epoch 1; batch 73472; loss 0.433392\n",
      "epoch:1; batch 73472; train accuracy: 0.819237\n",
      "epoch 1; batch 73600; loss 0.525413\n",
      "epoch:1; batch 73600; train accuracy: 0.819212\n",
      "epoch 1; batch 73728; loss 0.299948\n",
      "epoch:1; batch 73728; train accuracy: 0.819350\n",
      "epoch 1; batch 73856; loss 0.502333\n",
      "epoch:1; batch 73856; train accuracy: 0.819297\n",
      "epoch 1; batch 73984; loss 0.267623\n",
      "epoch:1; batch 73984; train accuracy: 0.819447\n",
      "epoch 1; batch 74112; loss 0.273573\n",
      "epoch:1; batch 74112; train accuracy: 0.819584\n",
      "epoch 1; batch 74240; loss 0.466583\n",
      "epoch:1; batch 74240; train accuracy: 0.819585\n",
      "epoch 1; batch 74368; loss 0.417493\n",
      "epoch:1; batch 74368; train accuracy: 0.819667\n",
      "epoch 1; batch 74496; loss 0.340368\n",
      "epoch:1; batch 74496; train accuracy: 0.819762\n",
      "epoch 1; batch 74624; loss 0.385676\n",
      "epoch:1; batch 74624; train accuracy: 0.819870\n",
      "epoch 1; batch 74752; loss 0.411843\n",
      "epoch:1; batch 74752; train accuracy: 0.819938\n",
      "epoch 1; batch 74880; loss 0.396182\n",
      "epoch:1; batch 74880; train accuracy: 0.820005\n",
      "epoch 1; batch 75008; loss 0.441560\n",
      "epoch:1; batch 75008; train accuracy: 0.820019\n",
      "epoch 1; batch 75136; loss 0.403817\n",
      "epoch:1; batch 75136; train accuracy: 0.820060\n",
      "epoch 1; batch 75264; loss 0.528816\n",
      "epoch:1; batch 75264; train accuracy: 0.820153\n",
      "epoch 1; batch 75392; loss 0.373428\n",
      "epoch:1; batch 75392; train accuracy: 0.820233\n",
      "epoch 1; batch 75520; loss 0.441061\n",
      "epoch:1; batch 75520; train accuracy: 0.820233\n",
      "epoch 1; batch 75648; loss 0.400546\n",
      "epoch:1; batch 75648; train accuracy: 0.820286\n",
      "epoch 1; batch 75776; loss 0.438330\n",
      "epoch:1; batch 75776; train accuracy: 0.820326\n",
      "epoch 1; batch 75904; loss 0.400071\n",
      "epoch:1; batch 75904; train accuracy: 0.820405\n",
      "epoch 1; batch 76032; loss 0.403957\n",
      "epoch:1; batch 76032; train accuracy: 0.820484\n",
      "epoch 1; batch 76160; loss 0.367961\n",
      "epoch:1; batch 76160; train accuracy: 0.820575\n",
      "epoch 1; batch 76288; loss 0.443419\n",
      "epoch:1; batch 76288; train accuracy: 0.820588\n",
      "epoch 1; batch 76416; loss 0.387159\n",
      "epoch:1; batch 76416; train accuracy: 0.820627\n",
      "epoch 1; batch 76544; loss 0.480901\n",
      "epoch:1; batch 76544; train accuracy: 0.820613\n",
      "epoch 1; batch 76672; loss 0.420391\n",
      "epoch:1; batch 76672; train accuracy: 0.820704\n",
      "epoch 1; batch 76800; loss 0.290196\n",
      "epoch:1; batch 76800; train accuracy: 0.820846\n",
      "epoch 1; batch 76928; loss 0.520412\n",
      "epoch:1; batch 76928; train accuracy: 0.820832\n",
      "epoch 1; batch 77056; loss 0.255689\n",
      "epoch:1; batch 77056; train accuracy: 0.820948\n",
      "epoch 1; batch 77184; loss 0.436187\n",
      "epoch:1; batch 77184; train accuracy: 0.820973\n",
      "epoch 1; batch 77312; loss 0.441293\n",
      "epoch:1; batch 77312; train accuracy: 0.821037\n",
      "epoch 1; batch 77440; loss 0.417716\n",
      "epoch:1; batch 77440; train accuracy: 0.821074\n",
      "epoch 1; batch 77568; loss 0.437270\n",
      "epoch:1; batch 77568; train accuracy: 0.821112\n",
      "epoch 1; batch 77696; loss 0.394210\n",
      "epoch:1; batch 77696; train accuracy: 0.821149\n",
      "epoch 1; batch 77824; loss 0.349360\n",
      "epoch:1; batch 77824; train accuracy: 0.821238\n",
      "epoch 1; batch 77952; loss 0.449126\n",
      "epoch:1; batch 77952; train accuracy: 0.821275\n",
      "epoch 1; batch 78080; loss 0.417588\n",
      "epoch:1; batch 78080; train accuracy: 0.821311\n",
      "epoch 1; batch 78208; loss 0.312163\n",
      "epoch:1; batch 78208; train accuracy: 0.821450\n",
      "epoch 1; batch 78336; loss 0.361182\n",
      "epoch:1; batch 78336; train accuracy: 0.821551\n",
      "epoch 1; batch 78464; loss 0.366004\n",
      "epoch:1; batch 78464; train accuracy: 0.821638\n",
      "epoch 1; batch 78592; loss 0.232626\n",
      "epoch:1; batch 78592; train accuracy: 0.821788\n",
      "epoch 1; batch 78720; loss 0.299101\n",
      "epoch:1; batch 78720; train accuracy: 0.821875\n",
      "epoch 1; batch 78848; loss 0.258241\n",
      "epoch:1; batch 78848; train accuracy: 0.822012\n",
      "epoch 1; batch 78976; loss 0.405957\n",
      "epoch:1; batch 78976; train accuracy: 0.822073\n",
      "epoch 1; batch 79104; loss 0.509080\n",
      "epoch:1; batch 79104; train accuracy: 0.822070\n",
      "epoch 1; batch 79232; loss 0.501827\n",
      "epoch:1; batch 79232; train accuracy: 0.822117\n",
      "epoch 1; batch 79360; loss 0.359231\n",
      "epoch:1; batch 79360; train accuracy: 0.822215\n",
      "epoch 1; batch 79488; loss 0.392675\n",
      "epoch:1; batch 79488; train accuracy: 0.822275\n",
      "epoch 1; batch 79616; loss 0.440604\n",
      "epoch:1; batch 79616; train accuracy: 0.822322\n",
      "epoch 1; batch 79744; loss 0.285671\n",
      "epoch:1; batch 79744; train accuracy: 0.822407\n",
      "epoch 1; batch 79872; loss 0.386822\n",
      "epoch:1; batch 79872; train accuracy: 0.822478\n",
      "epoch 1; batch 80000; loss 0.476304\n",
      "epoch:1; batch 80000; train accuracy: 0.822462\n",
      "epoch 1; batch 80128; loss 0.319088\n",
      "epoch:1; batch 80128; train accuracy: 0.822497\n",
      "epoch 1; batch 80256; loss 0.430723\n",
      "epoch:1; batch 80256; train accuracy: 0.822493\n",
      "epoch 1; batch 80384; loss 0.307637\n",
      "epoch:1; batch 80384; train accuracy: 0.822614\n",
      "epoch 1; batch 80512; loss 0.411373\n",
      "epoch:1; batch 80512; train accuracy: 0.822710\n",
      "epoch 1; batch 80640; loss 0.317080\n",
      "epoch:1; batch 80640; train accuracy: 0.822805\n",
      "epoch 1; batch 80768; loss 0.498845\n",
      "epoch:1; batch 80768; train accuracy: 0.822776\n",
      "epoch 1; batch 80896; loss 0.404633\n",
      "epoch:1; batch 80896; train accuracy: 0.822810\n",
      "epoch 1; batch 81024; loss 0.418948\n",
      "epoch:1; batch 81024; train accuracy: 0.822892\n",
      "epoch 1; batch 81152; loss 0.574355\n",
      "epoch:1; batch 81152; train accuracy: 0.822839\n",
      "epoch 1; batch 81280; loss 0.314056\n",
      "epoch:1; batch 81280; train accuracy: 0.822933\n",
      "epoch 1; batch 81408; loss 0.403502\n",
      "epoch:1; batch 81408; train accuracy: 0.822990\n",
      "epoch 1; batch 81536; loss 0.358737\n",
      "epoch:1; batch 81536; train accuracy: 0.823109\n",
      "epoch 1; batch 81664; loss 0.269011\n",
      "epoch:1; batch 81664; train accuracy: 0.823227\n",
      "epoch 1; batch 81792; loss 0.381409\n",
      "epoch:1; batch 81792; train accuracy: 0.823283\n",
      "epoch 1; batch 81920; loss 0.385510\n",
      "epoch:1; batch 81920; train accuracy: 0.823340\n",
      "epoch 1; batch 82048; loss 0.459256\n",
      "epoch:1; batch 82048; train accuracy: 0.823384\n",
      "epoch 1; batch 82176; loss 0.382913\n",
      "epoch:1; batch 82176; train accuracy: 0.823464\n",
      "epoch 1; batch 82304; loss 0.536581\n",
      "epoch:1; batch 82304; train accuracy: 0.823459\n",
      "epoch 1; batch 82432; loss 0.297027\n",
      "epoch:1; batch 82432; train accuracy: 0.823600\n",
      "epoch 1; batch 82560; loss 0.439019\n",
      "epoch:1; batch 82560; train accuracy: 0.823668\n",
      "epoch 1; batch 82688; loss 0.363652\n",
      "epoch:1; batch 82688; train accuracy: 0.823699\n",
      "epoch 1; batch 82816; loss 0.554853\n",
      "epoch:1; batch 82816; train accuracy: 0.823694\n",
      "epoch 1; batch 82944; loss 0.331459\n",
      "epoch:1; batch 82944; train accuracy: 0.823797\n",
      "epoch 1; batch 83072; loss 0.325660\n",
      "epoch:1; batch 83072; train accuracy: 0.823876\n",
      "epoch 1; batch 83200; loss 0.525320\n",
      "epoch:1; batch 83200; train accuracy: 0.823810\n",
      "epoch 1; batch 83328; loss 0.271940\n",
      "epoch:1; batch 83328; train accuracy: 0.823985\n",
      "epoch 1; batch 83456; loss 0.261178\n",
      "epoch:1; batch 83456; train accuracy: 0.824111\n",
      "epoch 1; batch 83584; loss 0.453271\n",
      "epoch:1; batch 83584; train accuracy: 0.824141\n",
      "epoch 1; batch 83712; loss 0.357579\n",
      "epoch:1; batch 83712; train accuracy: 0.824219\n",
      "epoch 1; batch 83840; loss 0.318937\n",
      "epoch:1; batch 83840; train accuracy: 0.824308\n",
      "epoch 1; batch 83968; loss 0.483324\n",
      "epoch:1; batch 83968; train accuracy: 0.824338\n",
      "epoch 1; batch 84096; loss 0.461487\n",
      "epoch:1; batch 84096; train accuracy: 0.824344\n",
      "epoch 1; batch 84224; loss 0.458957\n",
      "epoch:1; batch 84224; train accuracy: 0.824373\n",
      "epoch 1; batch 84352; loss 0.444944\n",
      "epoch:1; batch 84352; train accuracy: 0.824462\n",
      "epoch 1; batch 84480; loss 0.389111\n",
      "epoch:1; batch 84480; train accuracy: 0.824467\n",
      "epoch 1; batch 84608; loss 0.423883\n",
      "epoch:1; batch 84608; train accuracy: 0.824520\n",
      "epoch 1; batch 84736; loss 0.346783\n",
      "epoch:1; batch 84736; train accuracy: 0.824620\n",
      "epoch 1; batch 84864; loss 0.379460\n",
      "epoch:1; batch 84864; train accuracy: 0.824720\n",
      "epoch 1; batch 84992; loss 0.416246\n",
      "epoch:1; batch 84992; train accuracy: 0.824760\n",
      "epoch 1; batch 85120; loss 0.402061\n",
      "epoch:1; batch 85120; train accuracy: 0.824812\n",
      "epoch 1; batch 85248; loss 0.328984\n",
      "epoch:1; batch 85248; train accuracy: 0.824887\n",
      "epoch 1; batch 85376; loss 0.392553\n",
      "epoch:1; batch 85376; train accuracy: 0.824927\n",
      "epoch 1; batch 85504; loss 0.478161\n",
      "epoch:1; batch 85504; train accuracy: 0.824944\n",
      "epoch 1; batch 85632; loss 0.395545\n",
      "epoch:1; batch 85632; train accuracy: 0.824984\n",
      "epoch 1; batch 85760; loss 0.395146\n",
      "epoch:1; batch 85760; train accuracy: 0.825023\n",
      "epoch 1; batch 85888; loss 0.531653\n",
      "epoch:1; batch 85888; train accuracy: 0.824981\n",
      "epoch 1; batch 86016; loss 0.391163\n",
      "epoch:1; batch 86016; train accuracy: 0.825021\n",
      "epoch 1; batch 86144; loss 0.664357\n",
      "epoch:1; batch 86144; train accuracy: 0.824909\n",
      "epoch 1; batch 86272; loss 0.456403\n",
      "epoch:1; batch 86272; train accuracy: 0.824903\n",
      "epoch 1; batch 86400; loss 0.338628\n",
      "epoch:1; batch 86400; train accuracy: 0.824988\n",
      "epoch 1; batch 86528; loss 0.336040\n",
      "epoch:1; batch 86528; train accuracy: 0.825062\n",
      "epoch 1; batch 86656; loss 0.438044\n",
      "epoch:1; batch 86656; train accuracy: 0.825044\n",
      "epoch 1; batch 86784; loss 0.384129\n",
      "epoch:1; batch 86784; train accuracy: 0.825152\n",
      "epoch 1; batch 86912; loss 0.246942\n",
      "epoch:1; batch 86912; train accuracy: 0.825283\n",
      "epoch 1; batch 87040; loss 0.390114\n",
      "epoch:1; batch 87040; train accuracy: 0.825310\n",
      "epoch 1; batch 87168; loss 0.398385\n",
      "epoch:1; batch 87168; train accuracy: 0.825383\n",
      "epoch 1; batch 87296; loss 0.447136\n",
      "epoch:1; batch 87296; train accuracy: 0.825433\n",
      "epoch 1; batch 87424; loss 0.308371\n",
      "epoch:1; batch 87424; train accuracy: 0.825551\n",
      "epoch 1; batch 87552; loss 0.306994\n",
      "epoch:1; batch 87552; train accuracy: 0.825646\n",
      "epoch 1; batch 87680; loss 0.377857\n",
      "epoch:1; batch 87680; train accuracy: 0.825741\n",
      "epoch 1; batch 87808; loss 0.385048\n",
      "epoch:1; batch 87808; train accuracy: 0.825802\n",
      "epoch 1; batch 87936; loss 0.432770\n",
      "epoch:1; batch 87936; train accuracy: 0.825851\n",
      "epoch 1; batch 88064; loss 0.364405\n",
      "epoch:1; batch 88064; train accuracy: 0.825922\n",
      "epoch 1; batch 88192; loss 0.465435\n",
      "epoch:1; batch 88192; train accuracy: 0.825880\n",
      "epoch 1; batch 88320; loss 0.251112\n",
      "epoch:1; batch 88320; train accuracy: 0.825985\n",
      "epoch 1; batch 88448; loss 0.342235\n",
      "epoch:1; batch 88448; train accuracy: 0.826067\n",
      "epoch 1; batch 88576; loss 0.457900\n",
      "epoch:1; batch 88576; train accuracy: 0.826048\n",
      "epoch 1; batch 88704; loss 0.332868\n",
      "epoch:1; batch 88704; train accuracy: 0.826130\n",
      "epoch 1; batch 88832; loss 0.341976\n",
      "epoch:1; batch 88832; train accuracy: 0.826189\n",
      "epoch 1; batch 88960; loss 0.357757\n",
      "epoch:1; batch 88960; train accuracy: 0.826203\n",
      "epoch 1; batch 89088; loss 0.405909\n",
      "epoch:1; batch 89088; train accuracy: 0.826250\n",
      "epoch 1; batch 89216; loss 0.538836\n",
      "epoch:1; batch 89216; train accuracy: 0.826186\n",
      "epoch 1; batch 89344; loss 0.348125\n",
      "epoch:1; batch 89344; train accuracy: 0.826267\n",
      "epoch 1; batch 89472; loss 0.489974\n",
      "epoch:1; batch 89472; train accuracy: 0.826247\n",
      "epoch 1; batch 89600; loss 0.437143\n",
      "epoch:1; batch 89600; train accuracy: 0.826250\n",
      "epoch 1; batch 89728; loss 0.433203\n",
      "epoch:1; batch 89728; train accuracy: 0.826230\n",
      "epoch 1; batch 89856; loss 0.393103\n",
      "epoch:1; batch 89856; train accuracy: 0.826278\n",
      "epoch 1; batch 89984; loss 0.349287\n",
      "epoch:1; batch 89984; train accuracy: 0.826358\n",
      "epoch 1; batch 90112; loss 0.393228\n",
      "epoch:1; batch 90112; train accuracy: 0.826383\n",
      "epoch 1; batch 90240; loss 0.370454\n",
      "epoch:1; batch 90240; train accuracy: 0.826485\n",
      "epoch 1; batch 90368; loss 0.367819\n",
      "epoch:1; batch 90368; train accuracy: 0.826520\n",
      "epoch 1; batch 90496; loss 0.458003\n",
      "epoch:1; batch 90496; train accuracy: 0.826501\n",
      "epoch 1; batch 90624; loss 0.368915\n",
      "epoch:1; batch 90624; train accuracy: 0.826536\n",
      "epoch 1; batch 90752; loss 0.363991\n",
      "epoch:1; batch 90752; train accuracy: 0.826659\n",
      "epoch 1; batch 90880; loss 0.363754\n",
      "epoch:1; batch 90880; train accuracy: 0.826684\n",
      "epoch 1; batch 91008; loss 0.390283\n",
      "epoch:1; batch 91008; train accuracy: 0.826730\n",
      "epoch 1; batch 91136; loss 0.407521\n",
      "epoch:1; batch 91136; train accuracy: 0.826775\n",
      "epoch 1; batch 91264; loss 0.488153\n",
      "epoch:1; batch 91264; train accuracy: 0.826766\n",
      "epoch 1; batch 91392; loss 0.363438\n",
      "epoch:1; batch 91392; train accuracy: 0.826845\n",
      "epoch 1; batch 91520; loss 0.497845\n",
      "epoch:1; batch 91520; train accuracy: 0.826825\n",
      "epoch 1; batch 91648; loss 0.416182\n",
      "epoch:1; batch 91648; train accuracy: 0.826859\n",
      "epoch 1; batch 91776; loss 0.458082\n",
      "epoch:1; batch 91776; train accuracy: 0.826872\n",
      "epoch 1; batch 91904; loss 0.381812\n",
      "epoch:1; batch 91904; train accuracy: 0.826895\n",
      "epoch 1; batch 92032; loss 0.419370\n",
      "epoch:1; batch 92032; train accuracy: 0.826919\n",
      "epoch 1; batch 92160; loss 0.329434\n",
      "epoch:1; batch 92160; train accuracy: 0.826997\n",
      "epoch 1; batch 92288; loss 0.476232\n",
      "epoch:1; batch 92288; train accuracy: 0.826976\n",
      "epoch 1; batch 92416; loss 0.317891\n",
      "epoch:1; batch 92416; train accuracy: 0.827065\n",
      "epoch 1; batch 92544; loss 0.266849\n",
      "epoch:1; batch 92544; train accuracy: 0.827174\n",
      "epoch 1; batch 92672; loss 0.426320\n",
      "epoch:1; batch 92672; train accuracy: 0.827219\n",
      "epoch 1; batch 92800; loss 0.311501\n",
      "epoch:1; batch 92800; train accuracy: 0.827328\n",
      "epoch 1; batch 92928; loss 0.397834\n",
      "epoch:1; batch 92928; train accuracy: 0.827404\n",
      "epoch 1; batch 93056; loss 0.407692\n",
      "epoch:1; batch 93056; train accuracy: 0.827448\n",
      "epoch 1; batch 93184; loss 0.567269\n",
      "epoch:1; batch 93184; train accuracy: 0.827438\n",
      "epoch 1; batch 93312; loss 0.358642\n",
      "epoch:1; batch 93312; train accuracy: 0.827514\n",
      "epoch 1; batch 93440; loss 0.235996\n",
      "epoch:1; batch 93440; train accuracy: 0.827643\n",
      "epoch 1; batch 93568; loss 0.423366\n",
      "epoch:1; batch 93568; train accuracy: 0.827719\n",
      "epoch 1; batch 93696; loss 0.333234\n",
      "epoch:1; batch 93696; train accuracy: 0.827783\n",
      "epoch 1; batch 93824; loss 0.402945\n",
      "epoch:1; batch 93824; train accuracy: 0.827741\n",
      "epoch 1; batch 93952; loss 0.396141\n",
      "epoch:1; batch 93952; train accuracy: 0.827795\n",
      "epoch 1; batch 94080; loss 0.453116\n",
      "epoch:1; batch 94080; train accuracy: 0.827785\n",
      "epoch 1; batch 94208; loss 0.310643\n",
      "epoch:1; batch 94208; train accuracy: 0.827870\n",
      "epoch 1; batch 94336; loss 0.376043\n",
      "epoch:1; batch 94336; train accuracy: 0.827945\n",
      "epoch 1; batch 94464; loss 0.467379\n",
      "epoch:1; batch 94464; train accuracy: 0.827913\n",
      "epoch 1; batch 94592; loss 0.387997\n",
      "epoch:1; batch 94592; train accuracy: 0.827988\n",
      "epoch 1; batch 94720; loss 0.534304\n",
      "epoch:1; batch 94720; train accuracy: 0.827946\n",
      "epoch 1; batch 94848; loss 0.535436\n",
      "epoch:1; batch 94848; train accuracy: 0.827904\n",
      "epoch 1; batch 94976; loss 0.500967\n",
      "epoch:1; batch 94976; train accuracy: 0.827946\n",
      "epoch 1; batch 95104; loss 0.358938\n",
      "epoch:1; batch 95104; train accuracy: 0.827999\n",
      "epoch 1; batch 95232; loss 0.453015\n",
      "epoch:1; batch 95232; train accuracy: 0.827999\n",
      "epoch 1; batch 95360; loss 0.413060\n",
      "epoch:1; batch 95360; train accuracy: 0.827978\n",
      "epoch 1; batch 95488; loss 0.385147\n",
      "epoch:1; batch 95488; train accuracy: 0.827989\n",
      "epoch 1; batch 95616; loss 0.456510\n",
      "epoch:1; batch 95616; train accuracy: 0.827979\n",
      "epoch 1; batch 95744; loss 0.381065\n",
      "epoch:1; batch 95744; train accuracy: 0.828010\n",
      "epoch 1; batch 95872; loss 0.331228\n",
      "epoch:1; batch 95872; train accuracy: 0.828104\n",
      "epoch 1; batch 96000; loss 0.337966\n",
      "epoch:1; batch 96000; train accuracy: 0.828167\n",
      "epoch 1; batch 96128; loss 0.421879\n",
      "epoch:1; batch 96128; train accuracy: 0.828229\n",
      "epoch 1; batch 96256; loss 0.524814\n",
      "epoch:1; batch 96256; train accuracy: 0.828198\n",
      "epoch 1; batch 96384; loss 0.444690\n",
      "epoch:1; batch 96384; train accuracy: 0.828187\n",
      "epoch 1; batch 96512; loss 0.256985\n",
      "epoch:1; batch 96512; train accuracy: 0.828322\n",
      "epoch 1; batch 96640; loss 0.349492\n",
      "epoch:1; batch 96640; train accuracy: 0.828322\n",
      "epoch 1; batch 96768; loss 0.278796\n",
      "epoch:1; batch 96768; train accuracy: 0.828435\n",
      "epoch 1; batch 96896; loss 0.574851\n",
      "epoch:1; batch 96896; train accuracy: 0.828414\n",
      "epoch 1; batch 97024; loss 0.456024\n",
      "epoch:1; batch 97024; train accuracy: 0.828393\n",
      "epoch 1; batch 97152; loss 0.372543\n",
      "epoch:1; batch 97152; train accuracy: 0.828424\n",
      "epoch 1; batch 97280; loss 0.391064\n",
      "epoch:1; batch 97280; train accuracy: 0.828444\n",
      "epoch 1; batch 97408; loss 0.319906\n",
      "epoch:1; batch 97408; train accuracy: 0.828464\n",
      "epoch 1; batch 97536; loss 0.355373\n",
      "epoch:1; batch 97536; train accuracy: 0.828545\n",
      "epoch 1; batch 97664; loss 0.438061\n",
      "epoch:1; batch 97664; train accuracy: 0.828576\n",
      "epoch 1; batch 97792; loss 0.407714\n",
      "epoch:1; batch 97792; train accuracy: 0.828626\n",
      "epoch 1; batch 97920; loss 0.434501\n",
      "epoch:1; batch 97920; train accuracy: 0.828676\n",
      "epoch 1; batch 98048; loss 0.371990\n",
      "epoch:1; batch 98048; train accuracy: 0.828706\n",
      "epoch 1; batch 98176; loss 0.339926\n",
      "epoch:1; batch 98176; train accuracy: 0.828767\n",
      "epoch 1; batch 98304; loss 0.364556\n",
      "epoch:1; batch 98304; train accuracy: 0.828857\n",
      "epoch 1; batch 98432; loss 0.282857\n",
      "epoch:1; batch 98432; train accuracy: 0.828938\n",
      "epoch 1; batch 98560; loss 0.318846\n",
      "epoch:1; batch 98560; train accuracy: 0.829018\n",
      "epoch 1; batch 98688; loss 0.305197\n",
      "epoch:1; batch 98688; train accuracy: 0.829098\n",
      "epoch 1; batch 98816; loss 0.310396\n",
      "epoch:1; batch 98816; train accuracy: 0.829137\n",
      "epoch 1; batch 98944; loss 0.341520\n",
      "epoch:1; batch 98944; train accuracy: 0.829206\n",
      "epoch 1; batch 99072; loss 0.416026\n",
      "epoch:1; batch 99072; train accuracy: 0.829235\n",
      "epoch 1; batch 99200; loss 0.559352\n",
      "epoch:1; batch 99200; train accuracy: 0.829224\n",
      "epoch 1; batch 99328; loss 0.373899\n",
      "epoch:1; batch 99328; train accuracy: 0.829273\n",
      "epoch 1; batch 99456; loss 0.310770\n",
      "epoch:1; batch 99456; train accuracy: 0.829342\n",
      "epoch 1; batch 99584; loss 0.403144\n",
      "epoch:1; batch 99584; train accuracy: 0.829390\n",
      "epoch 1; batch 99712; loss 0.328335\n",
      "epoch:1; batch 99712; train accuracy: 0.829429\n",
      "epoch 1; batch 99840; loss 0.530701\n",
      "epoch:1; batch 99840; train accuracy: 0.829407\n",
      "epoch 1; batch 99968; loss 0.440715\n",
      "epoch:1; batch 99968; train accuracy: 0.829365\n",
      "epoch 1; batch 100096; loss 0.362756\n",
      "epoch:1; batch 100096; train accuracy: 0.829404\n",
      "epoch 1; batch 100224; loss 0.444631\n",
      "epoch:1; batch 100224; train accuracy: 0.829402\n",
      "epoch 1; batch 100352; loss 0.496218\n",
      "epoch:1; batch 100352; train accuracy: 0.829410\n",
      "epoch 1; batch 100480; loss 0.502518\n",
      "epoch:1; batch 100480; train accuracy: 0.829369\n",
      "epoch 1; batch 100608; loss 0.286330\n",
      "epoch:1; batch 100608; train accuracy: 0.829497\n",
      "epoch 1; batch 100736; loss 0.457203\n",
      "epoch:1; batch 100736; train accuracy: 0.829535\n",
      "epoch 1; batch 100864; loss 0.265308\n",
      "epoch:1; batch 100864; train accuracy: 0.829632\n",
      "epoch 1; batch 100992; loss 0.391745\n",
      "epoch:1; batch 100992; train accuracy: 0.829660\n",
      "epoch 1; batch 101120; loss 0.415259\n",
      "epoch:1; batch 101120; train accuracy: 0.829638\n",
      "epoch 1; batch 101248; loss 0.289648\n",
      "epoch:1; batch 101248; train accuracy: 0.829715\n",
      "epoch 1; batch 101376; loss 0.389654\n",
      "epoch:1; batch 101376; train accuracy: 0.829762\n",
      "epoch 1; batch 101504; loss 0.306383\n",
      "epoch:1; batch 101504; train accuracy: 0.829859\n",
      "epoch 1; batch 101632; loss 0.520802\n",
      "epoch:1; batch 101632; train accuracy: 0.829817\n",
      "epoch 1; batch 101760; loss 0.365142\n",
      "epoch:1; batch 101760; train accuracy: 0.829864\n",
      "epoch 1; batch 101888; loss 0.327015\n",
      "epoch:1; batch 101888; train accuracy: 0.829951\n",
      "epoch 1; batch 102016; loss 0.382294\n",
      "epoch:1; batch 102016; train accuracy: 0.829997\n",
      "epoch 1; batch 102144; loss 0.389141\n",
      "epoch:1; batch 102144; train accuracy: 0.830034\n",
      "epoch 1; batch 102272; loss 0.484048\n",
      "epoch:1; batch 102272; train accuracy: 0.830041\n",
      "epoch 1; batch 102400; loss 0.425819\n",
      "epoch:1; batch 102400; train accuracy: 0.830029\n",
      "epoch 1; batch 102528; loss 0.492015\n",
      "epoch:1; batch 102528; train accuracy: 0.829988\n",
      "epoch 1; batch 102656; loss 0.373739\n",
      "epoch:1; batch 102656; train accuracy: 0.830034\n",
      "epoch 1; batch 102784; loss 0.401467\n",
      "epoch:1; batch 102784; train accuracy: 0.830061\n",
      "epoch 1; batch 102912; loss 0.378265\n",
      "epoch:1; batch 102912; train accuracy: 0.830136\n",
      "epoch 1; batch 103040; loss 0.443922\n",
      "epoch:1; batch 103040; train accuracy: 0.830144\n",
      "epoch 1; batch 103168; loss 0.387843\n",
      "epoch:1; batch 103168; train accuracy: 0.830160\n",
      "epoch 1; batch 103296; loss 0.501666\n",
      "epoch:1; batch 103296; train accuracy: 0.830158\n",
      "epoch 1; batch 103424; loss 0.436974\n",
      "epoch:1; batch 103424; train accuracy: 0.830213\n",
      "epoch 1; batch 103552; loss 0.312546\n",
      "epoch:1; batch 103552; train accuracy: 0.830307\n",
      "epoch 1; batch 103680; loss 0.308946\n",
      "epoch:1; batch 103680; train accuracy: 0.830411\n",
      "epoch 1; batch 103808; loss 0.411848\n",
      "epoch:1; batch 103808; train accuracy: 0.830418\n",
      "epoch 1; batch 103936; loss 0.314415\n",
      "epoch:1; batch 103936; train accuracy: 0.830473\n",
      "epoch 1; batch 104064; loss 0.383238\n",
      "epoch:1; batch 104064; train accuracy: 0.830489\n",
      "epoch 1; batch 104192; loss 0.409145\n",
      "epoch:1; batch 104192; train accuracy: 0.830505\n",
      "epoch 1; batch 104320; loss 0.392403\n",
      "epoch:1; batch 104320; train accuracy: 0.830550\n",
      "epoch 1; batch 104448; loss 0.249379\n",
      "epoch:1; batch 104448; train accuracy: 0.830653\n",
      "epoch 1; batch 104576; loss 0.400998\n",
      "epoch:1; batch 104576; train accuracy: 0.830678\n",
      "epoch 1; batch 104704; loss 0.427169\n",
      "epoch:1; batch 104704; train accuracy: 0.830665\n",
      "epoch 1; batch 104832; loss 0.217094\n",
      "epoch:1; batch 104832; train accuracy: 0.830758\n",
      "epoch 1; batch 104960; loss 0.314774\n",
      "epoch:1; batch 104960; train accuracy: 0.830821\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 59102 ; rate: 0.563091\n",
      "y_true_label_1_num: 3642 ; rate: 0.034699\n",
      "y_true_label_2_num: 3069 ; rate: 0.029240\n",
      "y_true_label_3_num: 39147 ; rate: 0.372971\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.904830\n",
      "valid avg_precision: 0.911775\n",
      "valid avg_recall: 0.893883\n",
      "valid avg_f1: 0.893244\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 8540 ; rate: 0.570246\n",
      "y_true_label_1_num: 496 ; rate: 0.033120\n",
      "y_true_label_2_num: 425 ; rate: 0.028379\n",
      "y_true_label_3_num: 5515 ; rate: 0.368256\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.867655\n",
      "valid avg_precision: 0.876410\n",
      "valid avg_recall: 0.855569\n",
      "valid avg_f1: 0.855443\n",
      "epoch 2\n",
      "epoch 2; batch 128; loss 0.251510\n",
      "epoch:2; batch 128; train accuracy: 0.830942\n",
      "epoch 2; batch 256; loss 0.326431\n",
      "epoch:2; batch 256; train accuracy: 0.830976\n",
      "epoch 2; batch 384; loss 0.228815\n",
      "epoch:2; batch 384; train accuracy: 0.831077\n",
      "epoch 2; batch 512; loss 0.229673\n",
      "epoch:2; batch 512; train accuracy: 0.831168\n",
      "epoch 2; batch 640; loss 0.327774\n",
      "epoch:2; batch 640; train accuracy: 0.831231\n",
      "epoch 2; batch 768; loss 0.327295\n",
      "epoch:2; batch 768; train accuracy: 0.831265\n",
      "epoch 2; batch 896; loss 0.345405\n",
      "epoch:2; batch 896; train accuracy: 0.831309\n",
      "epoch 2; batch 1024; loss 0.463630\n",
      "epoch:2; batch 1024; train accuracy: 0.831258\n",
      "epoch 2; batch 1152; loss 0.337657\n",
      "epoch:2; batch 1152; train accuracy: 0.831310\n",
      "epoch 2; batch 1280; loss 0.377674\n",
      "epoch:2; batch 1280; train accuracy: 0.831372\n",
      "epoch 2; batch 1408; loss 0.369773\n",
      "epoch:2; batch 1408; train accuracy: 0.831425\n",
      "epoch 2; batch 1536; loss 0.273417\n",
      "epoch:2; batch 1536; train accuracy: 0.831524\n",
      "epoch 2; batch 1664; loss 0.304585\n",
      "epoch:2; batch 1664; train accuracy: 0.831586\n",
      "epoch 2; batch 1792; loss 0.277003\n",
      "epoch:2; batch 1792; train accuracy: 0.831647\n",
      "epoch 2; batch 1920; loss 0.396519\n",
      "epoch:2; batch 1920; train accuracy: 0.831662\n",
      "epoch 2; batch 2048; loss 0.300393\n",
      "epoch:2; batch 2048; train accuracy: 0.831732\n",
      "epoch 2; batch 2176; loss 0.291852\n",
      "epoch:2; batch 2176; train accuracy: 0.831821\n",
      "epoch 2; batch 2304; loss 0.369682\n",
      "epoch:2; batch 2304; train accuracy: 0.831835\n",
      "epoch 2; batch 2432; loss 0.240289\n",
      "epoch:2; batch 2432; train accuracy: 0.831971\n",
      "epoch 2; batch 2560; loss 0.281521\n",
      "epoch:2; batch 2560; train accuracy: 0.832050\n",
      "epoch 2; batch 2688; loss 0.297691\n",
      "epoch:2; batch 2688; train accuracy: 0.832101\n",
      "epoch 2; batch 2816; loss 0.277512\n",
      "epoch:2; batch 2816; train accuracy: 0.832198\n",
      "epoch 2; batch 2944; loss 0.283127\n",
      "epoch:2; batch 2944; train accuracy: 0.832286\n",
      "epoch 2; batch 3072; loss 0.284497\n",
      "epoch:2; batch 3072; train accuracy: 0.832346\n",
      "epoch 2; batch 3200; loss 0.398892\n",
      "epoch:2; batch 3200; train accuracy: 0.832378\n",
      "epoch 2; batch 3328; loss 0.434746\n",
      "epoch:2; batch 3328; train accuracy: 0.832373\n",
      "epoch 2; batch 3456; loss 0.398729\n",
      "epoch:2; batch 3456; train accuracy: 0.832359\n",
      "epoch 2; batch 3584; loss 0.235511\n",
      "epoch:2; batch 3584; train accuracy: 0.832464\n",
      "epoch 2; batch 3712; loss 0.355603\n",
      "epoch:2; batch 3712; train accuracy: 0.832524\n",
      "epoch 2; batch 3840; loss 0.270164\n",
      "epoch:2; batch 3840; train accuracy: 0.832601\n",
      "epoch 2; batch 3968; loss 0.219048\n",
      "epoch:2; batch 3968; train accuracy: 0.832743\n",
      "epoch 2; batch 4096; loss 0.464955\n",
      "epoch:2; batch 4096; train accuracy: 0.832728\n",
      "epoch 2; batch 4224; loss 0.217841\n",
      "epoch:2; batch 4224; train accuracy: 0.832851\n",
      "epoch 2; batch 4352; loss 0.503180\n",
      "epoch:2; batch 4352; train accuracy: 0.832873\n",
      "epoch 2; batch 4480; loss 0.261152\n",
      "epoch:2; batch 4480; train accuracy: 0.832986\n",
      "epoch 2; batch 4608; loss 0.214388\n",
      "epoch:2; batch 4608; train accuracy: 0.833090\n",
      "epoch 2; batch 4736; loss 0.256806\n",
      "epoch:2; batch 4736; train accuracy: 0.833175\n",
      "epoch 2; batch 4864; loss 0.480488\n",
      "epoch:2; batch 4864; train accuracy: 0.833188\n",
      "epoch 2; batch 4992; loss 0.401021\n",
      "epoch:2; batch 4992; train accuracy: 0.833218\n",
      "epoch 2; batch 5120; loss 0.396082\n",
      "epoch:2; batch 5120; train accuracy: 0.833249\n",
      "epoch 2; batch 5248; loss 0.293451\n",
      "epoch:2; batch 5248; train accuracy: 0.833297\n",
      "epoch 2; batch 5376; loss 0.302607\n",
      "epoch:2; batch 5376; train accuracy: 0.833336\n",
      "epoch 2; batch 5504; loss 0.384696\n",
      "epoch:2; batch 5504; train accuracy: 0.833376\n",
      "epoch 2; batch 5632; loss 0.313595\n",
      "epoch:2; batch 5632; train accuracy: 0.833433\n",
      "epoch 2; batch 5760; loss 0.329671\n",
      "epoch:2; batch 5760; train accuracy: 0.833490\n",
      "epoch 2; batch 5888; loss 0.403398\n",
      "epoch:2; batch 5888; train accuracy: 0.833502\n",
      "epoch 2; batch 6016; loss 0.256870\n",
      "epoch:2; batch 6016; train accuracy: 0.833595\n",
      "epoch 2; batch 6144; loss 0.430226\n",
      "epoch:2; batch 6144; train accuracy: 0.833633\n",
      "epoch 2; batch 6272; loss 0.351093\n",
      "epoch:2; batch 6272; train accuracy: 0.833672\n",
      "epoch 2; batch 6400; loss 0.270397\n",
      "epoch:2; batch 6400; train accuracy: 0.833728\n",
      "epoch 2; batch 6528; loss 0.398582\n",
      "epoch:2; batch 6528; train accuracy: 0.833776\n",
      "epoch 2; batch 6656; loss 0.294213\n",
      "epoch:2; batch 6656; train accuracy: 0.833832\n",
      "epoch 2; batch 6784; loss 0.318223\n",
      "epoch:2; batch 6784; train accuracy: 0.833906\n",
      "epoch 2; batch 6912; loss 0.338056\n",
      "epoch:2; batch 6912; train accuracy: 0.833989\n",
      "epoch 2; batch 7040; loss 0.296227\n",
      "epoch:2; batch 7040; train accuracy: 0.834054\n",
      "epoch 2; batch 7168; loss 0.350194\n",
      "epoch:2; batch 7168; train accuracy: 0.834118\n",
      "epoch 2; batch 7296; loss 0.333858\n",
      "epoch:2; batch 7296; train accuracy: 0.834174\n",
      "epoch 2; batch 7424; loss 0.407056\n",
      "epoch:2; batch 7424; train accuracy: 0.834193\n",
      "epoch 2; batch 7552; loss 0.243757\n",
      "epoch:2; batch 7552; train accuracy: 0.834284\n",
      "epoch 2; batch 7680; loss 0.213304\n",
      "epoch:2; batch 7680; train accuracy: 0.834384\n",
      "epoch 2; batch 7808; loss 0.350863\n",
      "epoch:2; batch 7808; train accuracy: 0.834395\n",
      "epoch 2; batch 7936; loss 0.379932\n",
      "epoch:2; batch 7936; train accuracy: 0.834449\n",
      "epoch 2; batch 8064; loss 0.428449\n",
      "epoch:2; batch 8064; train accuracy: 0.834469\n",
      "epoch 2; batch 8192; loss 0.295649\n",
      "epoch:2; batch 8192; train accuracy: 0.834576\n",
      "epoch 2; batch 8320; loss 0.290570\n",
      "epoch:2; batch 8320; train accuracy: 0.834666\n",
      "epoch 2; batch 8448; loss 0.174831\n",
      "epoch:2; batch 8448; train accuracy: 0.834774\n",
      "epoch 2; batch 8576; loss 0.267270\n",
      "epoch:2; batch 8576; train accuracy: 0.834881\n",
      "epoch 2; batch 8704; loss 0.454833\n",
      "epoch:2; batch 8704; train accuracy: 0.834899\n",
      "epoch 2; batch 8832; loss 0.414657\n",
      "epoch:2; batch 8832; train accuracy: 0.834927\n",
      "epoch 2; batch 8960; loss 0.289652\n",
      "epoch:2; batch 8960; train accuracy: 0.834989\n",
      "epoch 2; batch 9088; loss 0.256176\n",
      "epoch:2; batch 9088; train accuracy: 0.835087\n",
      "epoch 2; batch 9216; loss 0.273652\n",
      "epoch:2; batch 9216; train accuracy: 0.835132\n",
      "epoch 2; batch 9344; loss 0.234104\n",
      "epoch:2; batch 9344; train accuracy: 0.835211\n",
      "epoch 2; batch 9472; loss 0.290995\n",
      "epoch:2; batch 9472; train accuracy: 0.835282\n",
      "epoch 2; batch 9600; loss 0.280110\n",
      "epoch:2; batch 9600; train accuracy: 0.835370\n",
      "epoch 2; batch 9728; loss 0.352175\n",
      "epoch:2; batch 9728; train accuracy: 0.835423\n",
      "epoch 2; batch 9856; loss 0.222759\n",
      "epoch:2; batch 9856; train accuracy: 0.835502\n",
      "epoch 2; batch 9984; loss 0.322511\n",
      "epoch:2; batch 9984; train accuracy: 0.835537\n",
      "epoch 2; batch 10112; loss 0.369682\n",
      "epoch:2; batch 10112; train accuracy: 0.835555\n",
      "epoch 2; batch 10240; loss 0.410076\n",
      "epoch:2; batch 10240; train accuracy: 0.835590\n",
      "epoch 2; batch 10368; loss 0.168159\n",
      "epoch:2; batch 10368; train accuracy: 0.835712\n",
      "epoch 2; batch 10496; loss 0.339703\n",
      "epoch:2; batch 10496; train accuracy: 0.835756\n",
      "epoch 2; batch 10624; loss 0.367102\n",
      "epoch:2; batch 10624; train accuracy: 0.835739\n",
      "epoch 2; batch 10752; loss 0.314673\n",
      "epoch:2; batch 10752; train accuracy: 0.835799\n",
      "epoch 2; batch 10880; loss 0.268556\n",
      "epoch:2; batch 10880; train accuracy: 0.835868\n",
      "epoch 2; batch 11008; loss 0.198153\n",
      "epoch:2; batch 11008; train accuracy: 0.835981\n",
      "epoch 2; batch 11136; loss 0.339505\n",
      "epoch:2; batch 11136; train accuracy: 0.835998\n",
      "epoch 2; batch 11264; loss 0.276931\n",
      "epoch:2; batch 11264; train accuracy: 0.836067\n",
      "epoch 2; batch 11392; loss 0.249976\n",
      "epoch:2; batch 11392; train accuracy: 0.836144\n",
      "epoch 2; batch 11520; loss 0.353944\n",
      "epoch:2; batch 11520; train accuracy: 0.836169\n",
      "epoch 2; batch 11648; loss 0.304093\n",
      "epoch:2; batch 11648; train accuracy: 0.836238\n",
      "epoch 2; batch 11776; loss 0.384269\n",
      "epoch:2; batch 11776; train accuracy: 0.836254\n",
      "epoch 2; batch 11904; loss 0.189404\n",
      "epoch:2; batch 11904; train accuracy: 0.836348\n",
      "epoch 2; batch 12032; loss 0.321716\n",
      "epoch:2; batch 12032; train accuracy: 0.836408\n",
      "epoch 2; batch 12160; loss 0.345806\n",
      "epoch:2; batch 12160; train accuracy: 0.836450\n",
      "epoch 2; batch 12288; loss 0.261145\n",
      "epoch:2; batch 12288; train accuracy: 0.836517\n",
      "epoch 2; batch 12416; loss 0.339466\n",
      "epoch:2; batch 12416; train accuracy: 0.836559\n",
      "epoch 2; batch 12544; loss 0.240903\n",
      "epoch:2; batch 12544; train accuracy: 0.836635\n",
      "epoch 2; batch 12672; loss 0.233329\n",
      "epoch:2; batch 12672; train accuracy: 0.836703\n",
      "epoch 2; batch 12800; loss 0.285190\n",
      "epoch:2; batch 12800; train accuracy: 0.836736\n",
      "epoch 2; batch 12928; loss 0.267164\n",
      "epoch:2; batch 12928; train accuracy: 0.836786\n",
      "epoch 2; batch 13056; loss 0.238493\n",
      "epoch:2; batch 13056; train accuracy: 0.836861\n",
      "epoch 2; batch 13184; loss 0.225608\n",
      "epoch:2; batch 13184; train accuracy: 0.836945\n",
      "epoch 2; batch 13312; loss 0.290677\n",
      "epoch:2; batch 13312; train accuracy: 0.836986\n",
      "epoch 2; batch 13440; loss 0.183019\n",
      "epoch:2; batch 13440; train accuracy: 0.837086\n",
      "epoch 2; batch 13568; loss 0.275010\n",
      "epoch:2; batch 13568; train accuracy: 0.837119\n",
      "epoch 2; batch 13696; loss 0.346987\n",
      "epoch:2; batch 13696; train accuracy: 0.837160\n",
      "epoch 2; batch 13824; loss 0.354273\n",
      "epoch:2; batch 13824; train accuracy: 0.837226\n",
      "epoch 2; batch 13952; loss 0.184752\n",
      "epoch:2; batch 13952; train accuracy: 0.837334\n",
      "epoch 2; batch 14080; loss 0.366936\n",
      "epoch:2; batch 14080; train accuracy: 0.837340\n",
      "epoch 2; batch 14208; loss 0.227594\n",
      "epoch:2; batch 14208; train accuracy: 0.837398\n",
      "epoch 2; batch 14336; loss 0.359907\n",
      "epoch:2; batch 14336; train accuracy: 0.837446\n",
      "epoch 2; batch 14464; loss 0.347146\n",
      "epoch:2; batch 14464; train accuracy: 0.837512\n",
      "epoch 2; batch 14592; loss 0.281919\n",
      "epoch:2; batch 14592; train accuracy: 0.837577\n",
      "epoch 2; batch 14720; loss 0.382286\n",
      "epoch:2; batch 14720; train accuracy: 0.837600\n",
      "epoch 2; batch 14848; loss 0.298498\n",
      "epoch:2; batch 14848; train accuracy: 0.837665\n",
      "epoch 2; batch 14976; loss 0.273636\n",
      "epoch:2; batch 14976; train accuracy: 0.837738\n",
      "epoch 2; batch 15104; loss 0.205253\n",
      "epoch:2; batch 15104; train accuracy: 0.837803\n",
      "epoch 2; batch 15232; loss 0.277201\n",
      "epoch:2; batch 15232; train accuracy: 0.837851\n",
      "epoch 2; batch 15360; loss 0.342775\n",
      "epoch:2; batch 15360; train accuracy: 0.837882\n",
      "epoch 2; batch 15488; loss 0.351338\n",
      "epoch:2; batch 15488; train accuracy: 0.837905\n",
      "epoch 2; batch 15616; loss 0.345534\n",
      "epoch:2; batch 15616; train accuracy: 0.837953\n",
      "epoch 2; batch 15744; loss 0.385026\n",
      "epoch:2; batch 15744; train accuracy: 0.837951\n",
      "epoch 2; batch 15872; loss 0.281301\n",
      "epoch:2; batch 15872; train accuracy: 0.838006\n",
      "epoch 2; batch 16000; loss 0.277713\n",
      "epoch:2; batch 16000; train accuracy: 0.838054\n",
      "epoch 2; batch 16128; loss 0.360001\n",
      "epoch:2; batch 16128; train accuracy: 0.838085\n",
      "epoch 2; batch 16256; loss 0.239738\n",
      "epoch:2; batch 16256; train accuracy: 0.838165\n",
      "epoch 2; batch 16384; loss 0.287028\n",
      "epoch:2; batch 16384; train accuracy: 0.838220\n",
      "epoch 2; batch 16512; loss 0.254192\n",
      "epoch:2; batch 16512; train accuracy: 0.838308\n",
      "epoch 2; batch 16640; loss 0.359184\n",
      "epoch:2; batch 16640; train accuracy: 0.838331\n",
      "epoch 2; batch 16768; loss 0.306826\n",
      "epoch:2; batch 16768; train accuracy: 0.838377\n",
      "epoch 2; batch 16896; loss 0.266569\n",
      "epoch:2; batch 16896; train accuracy: 0.838465\n",
      "epoch 2; batch 17024; loss 0.263012\n",
      "epoch:2; batch 17024; train accuracy: 0.838544\n",
      "epoch 2; batch 17152; loss 0.287764\n",
      "epoch:2; batch 17152; train accuracy: 0.838624\n",
      "epoch 2; batch 17280; loss 0.287936\n",
      "epoch:2; batch 17280; train accuracy: 0.838686\n",
      "epoch 2; batch 17408; loss 0.299908\n",
      "epoch:2; batch 17408; train accuracy: 0.838749\n",
      "epoch 2; batch 17536; loss 0.237625\n",
      "epoch:2; batch 17536; train accuracy: 0.838844\n",
      "epoch 2; batch 17664; loss 0.156208\n",
      "epoch:2; batch 17664; train accuracy: 0.838947\n",
      "epoch 2; batch 17792; loss 0.287077\n",
      "epoch:2; batch 17792; train accuracy: 0.838992\n",
      "epoch 2; batch 17920; loss 0.264701\n",
      "epoch:2; batch 17920; train accuracy: 0.839063\n",
      "epoch 2; batch 18048; loss 0.195284\n",
      "epoch:2; batch 18048; train accuracy: 0.839157\n",
      "epoch 2; batch 18176; loss 0.251428\n",
      "epoch:2; batch 18176; train accuracy: 0.839251\n",
      "epoch 2; batch 18304; loss 0.303306\n",
      "epoch:2; batch 18304; train accuracy: 0.839288\n",
      "epoch 2; batch 18432; loss 0.270086\n",
      "epoch:2; batch 18432; train accuracy: 0.839357\n",
      "epoch 2; batch 18560; loss 0.315002\n",
      "epoch:2; batch 18560; train accuracy: 0.839419\n",
      "epoch 2; batch 18688; loss 0.426335\n",
      "epoch:2; batch 18688; train accuracy: 0.839456\n",
      "epoch 2; batch 18816; loss 0.170785\n",
      "epoch:2; batch 18816; train accuracy: 0.839573\n",
      "epoch 2; batch 18944; loss 0.355326\n",
      "epoch:2; batch 18944; train accuracy: 0.839594\n",
      "epoch 2; batch 19072; loss 0.245520\n",
      "epoch:2; batch 19072; train accuracy: 0.839678\n",
      "epoch 2; batch 19200; loss 0.307935\n",
      "epoch:2; batch 19200; train accuracy: 0.839715\n",
      "epoch 2; batch 19328; loss 0.367990\n",
      "epoch:2; batch 19328; train accuracy: 0.839767\n",
      "epoch 2; batch 19456; loss 0.188782\n",
      "epoch:2; batch 19456; train accuracy: 0.839844\n",
      "epoch 2; batch 19584; loss 0.223434\n",
      "epoch:2; batch 19584; train accuracy: 0.839928\n",
      "epoch 2; batch 19712; loss 0.354765\n",
      "epoch:2; batch 19712; train accuracy: 0.839964\n",
      "epoch 2; batch 19840; loss 0.344242\n",
      "epoch:2; batch 19840; train accuracy: 0.839984\n",
      "epoch 2; batch 19968; loss 0.312228\n",
      "epoch:2; batch 19968; train accuracy: 0.840060\n",
      "epoch 2; batch 20096; loss 0.257161\n",
      "epoch:2; batch 20096; train accuracy: 0.840104\n",
      "epoch 2; batch 20224; loss 0.336470\n",
      "epoch:2; batch 20224; train accuracy: 0.840155\n",
      "epoch 2; batch 20352; loss 0.289580\n",
      "epoch:2; batch 20352; train accuracy: 0.840207\n",
      "epoch 2; batch 20480; loss 0.207581\n",
      "epoch:2; batch 20480; train accuracy: 0.840274\n",
      "epoch 2; batch 20608; loss 0.316406\n",
      "epoch:2; batch 20608; train accuracy: 0.840342\n",
      "epoch 2; batch 20736; loss 0.205208\n",
      "epoch:2; batch 20736; train accuracy: 0.840456\n",
      "epoch 2; batch 20864; loss 0.274257\n",
      "epoch:2; batch 20864; train accuracy: 0.840539\n",
      "epoch 2; batch 20992; loss 0.217659\n",
      "epoch:2; batch 20992; train accuracy: 0.840622\n",
      "epoch 2; batch 21120; loss 0.249208\n",
      "epoch:2; batch 21120; train accuracy: 0.840720\n",
      "epoch 2; batch 21248; loss 0.209082\n",
      "epoch:2; batch 21248; train accuracy: 0.840787\n",
      "epoch 2; batch 21376; loss 0.295280\n",
      "epoch:2; batch 21376; train accuracy: 0.840837\n",
      "epoch 2; batch 21504; loss 0.219373\n",
      "epoch:2; batch 21504; train accuracy: 0.840903\n",
      "epoch 2; batch 21632; loss 0.294498\n",
      "epoch:2; batch 21632; train accuracy: 0.840985\n",
      "epoch 2; batch 21760; loss 0.294923\n",
      "epoch:2; batch 21760; train accuracy: 0.841051\n",
      "epoch 2; batch 21888; loss 0.280395\n",
      "epoch:2; batch 21888; train accuracy: 0.841125\n",
      "epoch 2; batch 22016; loss 0.339113\n",
      "epoch:2; batch 22016; train accuracy: 0.841175\n",
      "epoch 2; batch 22144; loss 0.259532\n",
      "epoch:2; batch 22144; train accuracy: 0.841232\n",
      "epoch 2; batch 22272; loss 0.366680\n",
      "epoch:2; batch 22272; train accuracy: 0.841266\n",
      "epoch 2; batch 22400; loss 0.281665\n",
      "epoch:2; batch 22400; train accuracy: 0.841332\n",
      "epoch 2; batch 22528; loss 0.431713\n",
      "epoch:2; batch 22528; train accuracy: 0.841373\n",
      "epoch 2; batch 22656; loss 0.282221\n",
      "epoch:2; batch 22656; train accuracy: 0.841423\n",
      "epoch 2; batch 22784; loss 0.285692\n",
      "epoch:2; batch 22784; train accuracy: 0.841488\n",
      "epoch 2; batch 22912; loss 0.293551\n",
      "epoch:2; batch 22912; train accuracy: 0.841537\n",
      "epoch 2; batch 23040; loss 0.311037\n",
      "epoch:2; batch 23040; train accuracy: 0.841578\n",
      "epoch 2; batch 23168; loss 0.279537\n",
      "epoch:2; batch 23168; train accuracy: 0.841635\n",
      "epoch 2; batch 23296; loss 0.203706\n",
      "epoch:2; batch 23296; train accuracy: 0.841715\n",
      "epoch 2; batch 23424; loss 0.293675\n",
      "epoch:2; batch 23424; train accuracy: 0.841748\n",
      "epoch 2; batch 23552; loss 0.308939\n",
      "epoch:2; batch 23552; train accuracy: 0.841789\n",
      "epoch 2; batch 23680; loss 0.205070\n",
      "epoch:2; batch 23680; train accuracy: 0.841861\n",
      "epoch 2; batch 23808; loss 0.266347\n",
      "epoch:2; batch 23808; train accuracy: 0.841909\n",
      "epoch 2; batch 23936; loss 0.239351\n",
      "epoch:2; batch 23936; train accuracy: 0.842012\n",
      "epoch 2; batch 24064; loss 0.318245\n",
      "epoch:2; batch 24064; train accuracy: 0.842045\n",
      "epoch 2; batch 24192; loss 0.208516\n",
      "epoch:2; batch 24192; train accuracy: 0.842116\n",
      "epoch 2; batch 24320; loss 0.226366\n",
      "epoch:2; batch 24320; train accuracy: 0.842195\n",
      "epoch 2; batch 24448; loss 0.345440\n",
      "epoch:2; batch 24448; train accuracy: 0.842212\n",
      "epoch 2; batch 24576; loss 0.385454\n",
      "epoch:2; batch 24576; train accuracy: 0.842245\n",
      "epoch 2; batch 24704; loss 0.218387\n",
      "epoch:2; batch 24704; train accuracy: 0.842331\n",
      "epoch 2; batch 24832; loss 0.365643\n",
      "epoch:2; batch 24832; train accuracy: 0.842371\n",
      "epoch 2; batch 24960; loss 0.282073\n",
      "epoch:2; batch 24960; train accuracy: 0.842441\n",
      "epoch 2; batch 25088; loss 0.292877\n",
      "epoch:2; batch 25088; train accuracy: 0.842504\n",
      "epoch 2; batch 25216; loss 0.367302\n",
      "epoch:2; batch 25216; train accuracy: 0.842544\n",
      "epoch 2; batch 25344; loss 0.244106\n",
      "epoch:2; batch 25344; train accuracy: 0.842637\n",
      "epoch 2; batch 25472; loss 0.243758\n",
      "epoch:2; batch 25472; train accuracy: 0.842707\n",
      "epoch 2; batch 25600; loss 0.290683\n",
      "epoch:2; batch 25600; train accuracy: 0.842754\n",
      "epoch 2; batch 25728; loss 0.151765\n",
      "epoch:2; batch 25728; train accuracy: 0.842878\n",
      "epoch 2; batch 25856; loss 0.275291\n",
      "epoch:2; batch 25856; train accuracy: 0.842955\n",
      "epoch 2; batch 25984; loss 0.261790\n",
      "epoch:2; batch 25984; train accuracy: 0.843017\n",
      "epoch 2; batch 26112; loss 0.238614\n",
      "epoch:2; batch 26112; train accuracy: 0.843094\n",
      "epoch 2; batch 26240; loss 0.232716\n",
      "epoch:2; batch 26240; train accuracy: 0.843148\n",
      "epoch 2; batch 26368; loss 0.376032\n",
      "epoch:2; batch 26368; train accuracy: 0.843186\n",
      "epoch 2; batch 26496; loss 0.259523\n",
      "epoch:2; batch 26496; train accuracy: 0.843248\n",
      "epoch 2; batch 26624; loss 0.286785\n",
      "epoch:2; batch 26624; train accuracy: 0.843324\n",
      "epoch 2; batch 26752; loss 0.166448\n",
      "epoch:2; batch 26752; train accuracy: 0.843408\n",
      "epoch 2; batch 26880; loss 0.313565\n",
      "epoch:2; batch 26880; train accuracy: 0.843454\n",
      "epoch 2; batch 27008; loss 0.274129\n",
      "epoch:2; batch 27008; train accuracy: 0.843515\n",
      "epoch 2; batch 27136; loss 0.219008\n",
      "epoch:2; batch 27136; train accuracy: 0.843568\n",
      "epoch 2; batch 27264; loss 0.222128\n",
      "epoch:2; batch 27264; train accuracy: 0.843659\n",
      "epoch 2; batch 27392; loss 0.239558\n",
      "epoch:2; batch 27392; train accuracy: 0.843727\n",
      "epoch 2; batch 27520; loss 0.220646\n",
      "epoch:2; batch 27520; train accuracy: 0.843788\n",
      "epoch 2; batch 27648; loss 0.246362\n",
      "epoch:2; batch 27648; train accuracy: 0.843863\n",
      "epoch 2; batch 27776; loss 0.256591\n",
      "epoch:2; batch 27776; train accuracy: 0.843908\n",
      "epoch 2; batch 27904; loss 0.213371\n",
      "epoch:2; batch 27904; train accuracy: 0.843976\n",
      "epoch 2; batch 28032; loss 0.154307\n",
      "epoch:2; batch 28032; train accuracy: 0.844088\n",
      "epoch 2; batch 28160; loss 0.270208\n",
      "epoch:2; batch 28160; train accuracy: 0.844163\n",
      "epoch 2; batch 28288; loss 0.173592\n",
      "epoch:2; batch 28288; train accuracy: 0.844253\n",
      "epoch 2; batch 28416; loss 0.397298\n",
      "epoch:2; batch 28416; train accuracy: 0.844230\n",
      "epoch 2; batch 28544; loss 0.282868\n",
      "epoch:2; batch 28544; train accuracy: 0.844289\n",
      "epoch 2; batch 28672; loss 0.184256\n",
      "epoch:2; batch 28672; train accuracy: 0.844364\n",
      "epoch 2; batch 28800; loss 0.220489\n",
      "epoch:2; batch 28800; train accuracy: 0.844438\n",
      "epoch 2; batch 28928; loss 0.205897\n",
      "epoch:2; batch 28928; train accuracy: 0.844512\n",
      "epoch 2; batch 29056; loss 0.199176\n",
      "epoch:2; batch 29056; train accuracy: 0.844586\n",
      "epoch 2; batch 29184; loss 0.173379\n",
      "epoch:2; batch 29184; train accuracy: 0.844652\n",
      "epoch 2; batch 29312; loss 0.186300\n",
      "epoch:2; batch 29312; train accuracy: 0.844726\n",
      "epoch 2; batch 29440; loss 0.256575\n",
      "epoch:2; batch 29440; train accuracy: 0.844792\n",
      "epoch 2; batch 29568; loss 0.247334\n",
      "epoch:2; batch 29568; train accuracy: 0.844843\n",
      "epoch 2; batch 29696; loss 0.219962\n",
      "epoch:2; batch 29696; train accuracy: 0.844909\n",
      "epoch 2; batch 29824; loss 0.163762\n",
      "epoch:2; batch 29824; train accuracy: 0.845004\n",
      "epoch 2; batch 29952; loss 0.229296\n",
      "epoch:2; batch 29952; train accuracy: 0.845069\n",
      "epoch 2; batch 30080; loss 0.243688\n",
      "epoch:2; batch 30080; train accuracy: 0.845127\n",
      "epoch 2; batch 30208; loss 0.288537\n",
      "epoch:2; batch 30208; train accuracy: 0.845193\n",
      "epoch 2; batch 30336; loss 0.156763\n",
      "epoch:2; batch 30336; train accuracy: 0.845280\n",
      "epoch 2; batch 30464; loss 0.208002\n",
      "epoch:2; batch 30464; train accuracy: 0.845352\n",
      "epoch 2; batch 30592; loss 0.139515\n",
      "epoch:2; batch 30592; train accuracy: 0.845447\n",
      "epoch 2; batch 30720; loss 0.197960\n",
      "epoch:2; batch 30720; train accuracy: 0.845519\n",
      "epoch 2; batch 30848; loss 0.234359\n",
      "epoch:2; batch 30848; train accuracy: 0.845569\n",
      "epoch 2; batch 30976; loss 0.244180\n",
      "epoch:2; batch 30976; train accuracy: 0.845641\n",
      "epoch 2; batch 31104; loss 0.252510\n",
      "epoch:2; batch 31104; train accuracy: 0.845705\n",
      "epoch 2; batch 31232; loss 0.231301\n",
      "epoch:2; batch 31232; train accuracy: 0.845791\n",
      "epoch 2; batch 31360; loss 0.308191\n",
      "epoch:2; batch 31360; train accuracy: 0.845819\n",
      "epoch 2; batch 31488; loss 0.240011\n",
      "epoch:2; batch 31488; train accuracy: 0.845905\n",
      "epoch 2; batch 31616; loss 0.194891\n",
      "epoch:2; batch 31616; train accuracy: 0.845969\n",
      "epoch 2; batch 31744; loss 0.172007\n",
      "epoch:2; batch 31744; train accuracy: 0.846069\n",
      "epoch 2; batch 31872; loss 0.165909\n",
      "epoch:2; batch 31872; train accuracy: 0.846154\n",
      "epoch 2; batch 32000; loss 0.211374\n",
      "epoch:2; batch 32000; train accuracy: 0.846232\n",
      "epoch 2; batch 32128; loss 0.373660\n",
      "epoch:2; batch 32128; train accuracy: 0.846267\n",
      "epoch 2; batch 32256; loss 0.154076\n",
      "epoch:2; batch 32256; train accuracy: 0.846359\n",
      "epoch 2; batch 32384; loss 0.218479\n",
      "epoch:2; batch 32384; train accuracy: 0.846400\n",
      "epoch 2; batch 32512; loss 0.218659\n",
      "epoch:2; batch 32512; train accuracy: 0.846478\n",
      "epoch 2; batch 32640; loss 0.156845\n",
      "epoch:2; batch 32640; train accuracy: 0.846584\n",
      "epoch 2; batch 32768; loss 0.230190\n",
      "epoch:2; batch 32768; train accuracy: 0.846654\n",
      "epoch 2; batch 32896; loss 0.131356\n",
      "epoch:2; batch 32896; train accuracy: 0.846775\n",
      "epoch 2; batch 33024; loss 0.323531\n",
      "epoch:2; batch 33024; train accuracy: 0.846772\n",
      "epoch 2; batch 33152; loss 0.181691\n",
      "epoch:2; batch 33152; train accuracy: 0.846849\n",
      "epoch 2; batch 33280; loss 0.114915\n",
      "epoch:2; batch 33280; train accuracy: 0.846940\n",
      "epoch 2; batch 33408; loss 0.275227\n",
      "epoch:2; batch 33408; train accuracy: 0.846966\n",
      "epoch 2; batch 33536; loss 0.287081\n",
      "epoch:2; batch 33536; train accuracy: 0.847035\n",
      "epoch 2; batch 33664; loss 0.219956\n",
      "epoch:2; batch 33664; train accuracy: 0.847090\n",
      "epoch 2; batch 33792; loss 0.168608\n",
      "epoch:2; batch 33792; train accuracy: 0.847173\n",
      "epoch 2; batch 33920; loss 0.230257\n",
      "epoch:2; batch 33920; train accuracy: 0.847249\n",
      "epoch 2; batch 34048; loss 0.203715\n",
      "epoch:2; batch 34048; train accuracy: 0.847333\n",
      "epoch 2; batch 34176; loss 0.264844\n",
      "epoch:2; batch 34176; train accuracy: 0.847380\n",
      "epoch 2; batch 34304; loss 0.260954\n",
      "epoch:2; batch 34304; train accuracy: 0.847434\n",
      "epoch 2; batch 34432; loss 0.256871\n",
      "epoch:2; batch 34432; train accuracy: 0.847488\n",
      "epoch 2; batch 34560; loss 0.324425\n",
      "epoch:2; batch 34560; train accuracy: 0.847506\n",
      "epoch 2; batch 34688; loss 0.273113\n",
      "epoch:2; batch 34688; train accuracy: 0.847574\n",
      "epoch 2; batch 34816; loss 0.253961\n",
      "epoch:2; batch 34816; train accuracy: 0.847620\n",
      "epoch 2; batch 34944; loss 0.319913\n",
      "epoch:2; batch 34944; train accuracy: 0.847667\n",
      "epoch 2; batch 35072; loss 0.210564\n",
      "epoch:2; batch 35072; train accuracy: 0.847728\n",
      "epoch 2; batch 35200; loss 0.300590\n",
      "epoch:2; batch 35200; train accuracy: 0.847788\n",
      "epoch 2; batch 35328; loss 0.196596\n",
      "epoch:2; batch 35328; train accuracy: 0.847849\n",
      "epoch 2; batch 35456; loss 0.211974\n",
      "epoch:2; batch 35456; train accuracy: 0.847902\n",
      "epoch 2; batch 35584; loss 0.200421\n",
      "epoch:2; batch 35584; train accuracy: 0.847969\n",
      "epoch 2; batch 35712; loss 0.259140\n",
      "epoch:2; batch 35712; train accuracy: 0.848022\n",
      "epoch 2; batch 35840; loss 0.251783\n",
      "epoch:2; batch 35840; train accuracy: 0.848054\n",
      "epoch 2; batch 35968; loss 0.244501\n",
      "epoch:2; batch 35968; train accuracy: 0.848078\n",
      "epoch 2; batch 36096; loss 0.230322\n",
      "epoch:2; batch 36096; train accuracy: 0.848138\n",
      "epoch 2; batch 36224; loss 0.273996\n",
      "epoch:2; batch 36224; train accuracy: 0.848184\n",
      "epoch 2; batch 36352; loss 0.336627\n",
      "epoch:2; batch 36352; train accuracy: 0.848222\n",
      "epoch 2; batch 36480; loss 0.380174\n",
      "epoch:2; batch 36480; train accuracy: 0.848268\n",
      "epoch 2; batch 36608; loss 0.270294\n",
      "epoch:2; batch 36608; train accuracy: 0.848306\n",
      "epoch 2; batch 36736; loss 0.184122\n",
      "epoch:2; batch 36736; train accuracy: 0.848380\n",
      "epoch 2; batch 36864; loss 0.215929\n",
      "epoch:2; batch 36864; train accuracy: 0.848453\n",
      "epoch 2; batch 36992; loss 0.204457\n",
      "epoch:2; batch 36992; train accuracy: 0.848526\n",
      "epoch 2; batch 37120; loss 0.247170\n",
      "epoch:2; batch 37120; train accuracy: 0.848578\n",
      "epoch 2; batch 37248; loss 0.221580\n",
      "epoch:2; batch 37248; train accuracy: 0.848637\n",
      "epoch 2; batch 37376; loss 0.165335\n",
      "epoch:2; batch 37376; train accuracy: 0.848689\n",
      "epoch 2; batch 37504; loss 0.233273\n",
      "epoch:2; batch 37504; train accuracy: 0.848755\n",
      "epoch 2; batch 37632; loss 0.215074\n",
      "epoch:2; batch 37632; train accuracy: 0.848813\n",
      "epoch 2; batch 37760; loss 0.289791\n",
      "epoch:2; batch 37760; train accuracy: 0.848879\n",
      "epoch 2; batch 37888; loss 0.234051\n",
      "epoch:2; batch 37888; train accuracy: 0.848944\n",
      "epoch 2; batch 38016; loss 0.186851\n",
      "epoch:2; batch 38016; train accuracy: 0.849031\n",
      "epoch 2; batch 38144; loss 0.177101\n",
      "epoch:2; batch 38144; train accuracy: 0.849117\n",
      "epoch 2; batch 38272; loss 0.316129\n",
      "epoch:2; batch 38272; train accuracy: 0.849175\n",
      "epoch 2; batch 38400; loss 0.125644\n",
      "epoch:2; batch 38400; train accuracy: 0.849275\n",
      "epoch 2; batch 38528; loss 0.321769\n",
      "epoch:2; batch 38528; train accuracy: 0.849311\n",
      "epoch 2; batch 38656; loss 0.242126\n",
      "epoch:2; batch 38656; train accuracy: 0.849376\n",
      "epoch 2; batch 38784; loss 0.278596\n",
      "epoch:2; batch 38784; train accuracy: 0.849406\n",
      "epoch 2; batch 38912; loss 0.291045\n",
      "epoch:2; batch 38912; train accuracy: 0.849429\n",
      "epoch 2; batch 39040; loss 0.248527\n",
      "epoch:2; batch 39040; train accuracy: 0.849493\n",
      "epoch 2; batch 39168; loss 0.253693\n",
      "epoch:2; batch 39168; train accuracy: 0.849543\n",
      "epoch 2; batch 39296; loss 0.197072\n",
      "epoch:2; batch 39296; train accuracy: 0.849615\n",
      "epoch 2; batch 39424; loss 0.219214\n",
      "epoch:2; batch 39424; train accuracy: 0.849672\n",
      "epoch 2; batch 39552; loss 0.283089\n",
      "epoch:2; batch 39552; train accuracy: 0.849729\n",
      "epoch 2; batch 39680; loss 0.200820\n",
      "epoch:2; batch 39680; train accuracy: 0.849800\n",
      "epoch 2; batch 39808; loss 0.373934\n",
      "epoch:2; batch 39808; train accuracy: 0.849808\n",
      "epoch 2; batch 39936; loss 0.206132\n",
      "epoch:2; batch 39936; train accuracy: 0.849872\n",
      "epoch 2; batch 40064; loss 0.190487\n",
      "epoch:2; batch 40064; train accuracy: 0.849956\n",
      "epoch 2; batch 40192; loss 0.281324\n",
      "epoch:2; batch 40192; train accuracy: 0.849992\n",
      "epoch 2; batch 40320; loss 0.312004\n",
      "epoch:2; batch 40320; train accuracy: 0.850028\n",
      "epoch 2; batch 40448; loss 0.139060\n",
      "epoch:2; batch 40448; train accuracy: 0.850111\n",
      "epoch 2; batch 40576; loss 0.108912\n",
      "epoch:2; batch 40576; train accuracy: 0.850223\n",
      "epoch 2; batch 40704; loss 0.386020\n",
      "epoch:2; batch 40704; train accuracy: 0.850217\n",
      "epoch 2; batch 40832; loss 0.309568\n",
      "epoch:2; batch 40832; train accuracy: 0.850259\n",
      "epoch 2; batch 40960; loss 0.234821\n",
      "epoch:2; batch 40960; train accuracy: 0.850329\n",
      "epoch 2; batch 41088; loss 0.148703\n",
      "epoch:2; batch 41088; train accuracy: 0.850405\n",
      "epoch 2; batch 41216; loss 0.331582\n",
      "epoch:2; batch 41216; train accuracy: 0.850427\n",
      "epoch 2; batch 41344; loss 0.205958\n",
      "epoch:2; batch 41344; train accuracy: 0.850496\n",
      "epoch 2; batch 41472; loss 0.197788\n",
      "epoch:2; batch 41472; train accuracy: 0.850565\n",
      "epoch 2; batch 41600; loss 0.274100\n",
      "epoch:2; batch 41600; train accuracy: 0.850594\n",
      "epoch 2; batch 41728; loss 0.301635\n",
      "epoch:2; batch 41728; train accuracy: 0.850608\n",
      "epoch 2; batch 41856; loss 0.253758\n",
      "epoch:2; batch 41856; train accuracy: 0.850657\n",
      "epoch 2; batch 41984; loss 0.238936\n",
      "epoch:2; batch 41984; train accuracy: 0.850712\n",
      "epoch 2; batch 42112; loss 0.214116\n",
      "epoch:2; batch 42112; train accuracy: 0.850767\n",
      "epoch 2; batch 42240; loss 0.251219\n",
      "epoch:2; batch 42240; train accuracy: 0.850815\n",
      "epoch 2; batch 42368; loss 0.323603\n",
      "epoch:2; batch 42368; train accuracy: 0.850823\n",
      "epoch 2; batch 42496; loss 0.168010\n",
      "epoch:2; batch 42496; train accuracy: 0.850898\n",
      "epoch 2; batch 42624; loss 0.219008\n",
      "epoch:2; batch 42624; train accuracy: 0.850973\n",
      "epoch 2; batch 42752; loss 0.401544\n",
      "epoch:2; batch 42752; train accuracy: 0.850987\n",
      "epoch 2; batch 42880; loss 0.252914\n",
      "epoch:2; batch 42880; train accuracy: 0.851015\n",
      "epoch 2; batch 43008; loss 0.206156\n",
      "epoch:2; batch 43008; train accuracy: 0.851069\n",
      "epoch 2; batch 43136; loss 0.352963\n",
      "epoch:2; batch 43136; train accuracy: 0.851103\n",
      "epoch 2; batch 43264; loss 0.259812\n",
      "epoch:2; batch 43264; train accuracy: 0.851158\n",
      "epoch 2; batch 43392; loss 0.301178\n",
      "epoch:2; batch 43392; train accuracy: 0.851192\n",
      "epoch 2; batch 43520; loss 0.293695\n",
      "epoch:2; batch 43520; train accuracy: 0.851232\n",
      "epoch 2; batch 43648; loss 0.268614\n",
      "epoch:2; batch 43648; train accuracy: 0.851273\n",
      "epoch 2; batch 43776; loss 0.171010\n",
      "epoch:2; batch 43776; train accuracy: 0.851347\n",
      "epoch 2; batch 43904; loss 0.318043\n",
      "epoch:2; batch 43904; train accuracy: 0.851381\n",
      "epoch 2; batch 44032; loss 0.223512\n",
      "epoch:2; batch 44032; train accuracy: 0.851435\n",
      "epoch 2; batch 44160; loss 0.223566\n",
      "epoch:2; batch 44160; train accuracy: 0.851516\n",
      "epoch 2; batch 44288; loss 0.225811\n",
      "epoch:2; batch 44288; train accuracy: 0.851576\n",
      "epoch 2; batch 44416; loss 0.225548\n",
      "epoch:2; batch 44416; train accuracy: 0.851616\n",
      "epoch 2; batch 44544; loss 0.168150\n",
      "epoch:2; batch 44544; train accuracy: 0.851676\n",
      "epoch 2; batch 44672; loss 0.221643\n",
      "epoch:2; batch 44672; train accuracy: 0.851743\n",
      "epoch 2; batch 44800; loss 0.198584\n",
      "epoch:2; batch 44800; train accuracy: 0.851803\n",
      "epoch 2; batch 44928; loss 0.276956\n",
      "epoch:2; batch 44928; train accuracy: 0.851829\n",
      "epoch 2; batch 45056; loss 0.134936\n",
      "epoch:2; batch 45056; train accuracy: 0.851922\n",
      "epoch 2; batch 45184; loss 0.192220\n",
      "epoch:2; batch 45184; train accuracy: 0.851989\n",
      "epoch 2; batch 45312; loss 0.205440\n",
      "epoch:2; batch 45312; train accuracy: 0.852062\n",
      "epoch 2; batch 45440; loss 0.300965\n",
      "epoch:2; batch 45440; train accuracy: 0.852101\n",
      "epoch 2; batch 45568; loss 0.143555\n",
      "epoch:2; batch 45568; train accuracy: 0.852194\n",
      "epoch 2; batch 45696; loss 0.163059\n",
      "epoch:2; batch 45696; train accuracy: 0.852246\n",
      "epoch 2; batch 45824; loss 0.175401\n",
      "epoch:2; batch 45824; train accuracy: 0.852325\n",
      "epoch 2; batch 45952; loss 0.241572\n",
      "epoch:2; batch 45952; train accuracy: 0.852371\n",
      "epoch 2; batch 46080; loss 0.289203\n",
      "epoch:2; batch 46080; train accuracy: 0.852403\n",
      "epoch 2; batch 46208; loss 0.086402\n",
      "epoch:2; batch 46208; train accuracy: 0.852502\n",
      "epoch 2; batch 46336; loss 0.278471\n",
      "epoch:2; batch 46336; train accuracy: 0.852561\n",
      "epoch 2; batch 46464; loss 0.147352\n",
      "epoch:2; batch 46464; train accuracy: 0.852646\n",
      "epoch 2; batch 46592; loss 0.216428\n",
      "epoch:2; batch 46592; train accuracy: 0.852691\n",
      "epoch 2; batch 46720; loss 0.147772\n",
      "epoch:2; batch 46720; train accuracy: 0.852769\n",
      "epoch 2; batch 46848; loss 0.261254\n",
      "epoch:2; batch 46848; train accuracy: 0.852808\n",
      "epoch 2; batch 46976; loss 0.170503\n",
      "epoch:2; batch 46976; train accuracy: 0.852866\n",
      "epoch 2; batch 47104; loss 0.303062\n",
      "epoch:2; batch 47104; train accuracy: 0.852911\n",
      "epoch 2; batch 47232; loss 0.293187\n",
      "epoch:2; batch 47232; train accuracy: 0.852942\n",
      "epoch 2; batch 47360; loss 0.278719\n",
      "epoch:2; batch 47360; train accuracy: 0.852994\n",
      "epoch 2; batch 47488; loss 0.208811\n",
      "epoch:2; batch 47488; train accuracy: 0.853045\n",
      "epoch 2; batch 47616; loss 0.153259\n",
      "epoch:2; batch 47616; train accuracy: 0.853129\n",
      "epoch 2; batch 47744; loss 0.198984\n",
      "epoch:2; batch 47744; train accuracy: 0.853206\n",
      "epoch 2; batch 47872; loss 0.354287\n",
      "epoch:2; batch 47872; train accuracy: 0.853224\n",
      "epoch 2; batch 48000; loss 0.173288\n",
      "epoch:2; batch 48000; train accuracy: 0.853302\n",
      "epoch 2; batch 48128; loss 0.180639\n",
      "epoch:2; batch 48128; train accuracy: 0.853352\n",
      "epoch 2; batch 48256; loss 0.266565\n",
      "epoch:2; batch 48256; train accuracy: 0.853403\n",
      "epoch 2; batch 48384; loss 0.200528\n",
      "epoch:2; batch 48384; train accuracy: 0.853473\n",
      "epoch 2; batch 48512; loss 0.310301\n",
      "epoch:2; batch 48512; train accuracy: 0.853517\n",
      "epoch 2; batch 48640; loss 0.304858\n",
      "epoch:2; batch 48640; train accuracy: 0.853535\n",
      "epoch 2; batch 48768; loss 0.302098\n",
      "epoch:2; batch 48768; train accuracy: 0.853579\n",
      "epoch 2; batch 48896; loss 0.208701\n",
      "epoch:2; batch 48896; train accuracy: 0.853636\n",
      "epoch 2; batch 49024; loss 0.149480\n",
      "epoch:2; batch 49024; train accuracy: 0.853712\n",
      "epoch 2; batch 49152; loss 0.243291\n",
      "epoch:2; batch 49152; train accuracy: 0.853782\n",
      "epoch 2; batch 49280; loss 0.206504\n",
      "epoch:2; batch 49280; train accuracy: 0.853838\n",
      "epoch 2; batch 49408; loss 0.252123\n",
      "epoch:2; batch 49408; train accuracy: 0.853875\n",
      "epoch 2; batch 49536; loss 0.213856\n",
      "epoch:2; batch 49536; train accuracy: 0.853912\n",
      "epoch 2; batch 49664; loss 0.219099\n",
      "epoch:2; batch 49664; train accuracy: 0.853962\n",
      "epoch 2; batch 49792; loss 0.273311\n",
      "epoch:2; batch 49792; train accuracy: 0.853999\n",
      "epoch 2; batch 49920; loss 0.145710\n",
      "epoch:2; batch 49920; train accuracy: 0.854074\n",
      "epoch 2; batch 50048; loss 0.241667\n",
      "epoch:2; batch 50048; train accuracy: 0.854117\n",
      "epoch 2; batch 50176; loss 0.222329\n",
      "epoch:2; batch 50176; train accuracy: 0.854173\n",
      "epoch 2; batch 50304; loss 0.218501\n",
      "epoch:2; batch 50304; train accuracy: 0.854216\n",
      "epoch 2; batch 50432; loss 0.112612\n",
      "epoch:2; batch 50432; train accuracy: 0.854304\n",
      "epoch 2; batch 50560; loss 0.170305\n",
      "epoch:2; batch 50560; train accuracy: 0.854372\n",
      "epoch 2; batch 50688; loss 0.228577\n",
      "epoch:2; batch 50688; train accuracy: 0.854409\n",
      "epoch 2; batch 50816; loss 0.248683\n",
      "epoch:2; batch 50816; train accuracy: 0.854471\n",
      "epoch 2; batch 50944; loss 0.206314\n",
      "epoch:2; batch 50944; train accuracy: 0.854545\n",
      "epoch 2; batch 51072; loss 0.239019\n",
      "epoch:2; batch 51072; train accuracy: 0.854587\n",
      "epoch 2; batch 51200; loss 0.312878\n",
      "epoch:2; batch 51200; train accuracy: 0.854585\n",
      "epoch 2; batch 51328; loss 0.240406\n",
      "epoch:2; batch 51328; train accuracy: 0.854634\n",
      "epoch 2; batch 51456; loss 0.162570\n",
      "epoch:2; batch 51456; train accuracy: 0.854695\n",
      "epoch 2; batch 51584; loss 0.160739\n",
      "epoch:2; batch 51584; train accuracy: 0.854763\n",
      "epoch 2; batch 51712; loss 0.206422\n",
      "epoch:2; batch 51712; train accuracy: 0.854818\n",
      "epoch 2; batch 51840; loss 0.362134\n",
      "epoch:2; batch 51840; train accuracy: 0.854834\n",
      "epoch 2; batch 51968; loss 0.192745\n",
      "epoch:2; batch 51968; train accuracy: 0.854902\n",
      "epoch 2; batch 52096; loss 0.203532\n",
      "epoch:2; batch 52096; train accuracy: 0.854969\n",
      "epoch 2; batch 52224; loss 0.137615\n",
      "epoch:2; batch 52224; train accuracy: 0.855055\n",
      "epoch 2; batch 52352; loss 0.300400\n",
      "epoch:2; batch 52352; train accuracy: 0.855065\n",
      "epoch 2; batch 52480; loss 0.235362\n",
      "epoch:2; batch 52480; train accuracy: 0.855119\n",
      "epoch 2; batch 52608; loss 0.292455\n",
      "epoch:2; batch 52608; train accuracy: 0.855148\n",
      "epoch 2; batch 52736; loss 0.162719\n",
      "epoch:2; batch 52736; train accuracy: 0.855209\n",
      "epoch 2; batch 52864; loss 0.283566\n",
      "epoch:2; batch 52864; train accuracy: 0.855231\n",
      "epoch 2; batch 52992; loss 0.253515\n",
      "epoch:2; batch 52992; train accuracy: 0.855285\n",
      "epoch 2; batch 53120; loss 0.208997\n",
      "epoch:2; batch 53120; train accuracy: 0.855333\n",
      "epoch 2; batch 53248; loss 0.260832\n",
      "epoch:2; batch 53248; train accuracy: 0.855368\n",
      "epoch 2; batch 53376; loss 0.210020\n",
      "epoch:2; batch 53376; train accuracy: 0.855428\n",
      "epoch 2; batch 53504; loss 0.198665\n",
      "epoch:2; batch 53504; train accuracy: 0.855500\n",
      "epoch 2; batch 53632; loss 0.293244\n",
      "epoch:2; batch 53632; train accuracy: 0.855541\n",
      "epoch 2; batch 53760; loss 0.217678\n",
      "epoch:2; batch 53760; train accuracy: 0.855607\n",
      "epoch 2; batch 53888; loss 0.254945\n",
      "epoch:2; batch 53888; train accuracy: 0.855654\n",
      "epoch 2; batch 54016; loss 0.285081\n",
      "epoch:2; batch 54016; train accuracy: 0.855683\n",
      "epoch 2; batch 54144; loss 0.135627\n",
      "epoch:2; batch 54144; train accuracy: 0.855748\n",
      "epoch 2; batch 54272; loss 0.216156\n",
      "epoch:2; batch 54272; train accuracy: 0.855814\n",
      "epoch 2; batch 54400; loss 0.190909\n",
      "epoch:2; batch 54400; train accuracy: 0.855880\n",
      "epoch 2; batch 54528; loss 0.235374\n",
      "epoch:2; batch 54528; train accuracy: 0.855926\n",
      "epoch 2; batch 54656; loss 0.294978\n",
      "epoch:2; batch 54656; train accuracy: 0.855942\n",
      "epoch 2; batch 54784; loss 0.157446\n",
      "epoch:2; batch 54784; train accuracy: 0.856007\n",
      "epoch 2; batch 54912; loss 0.183218\n",
      "epoch:2; batch 54912; train accuracy: 0.856066\n",
      "epoch 2; batch 55040; loss 0.137220\n",
      "epoch:2; batch 55040; train accuracy: 0.856144\n",
      "epoch 2; batch 55168; loss 0.173095\n",
      "epoch:2; batch 55168; train accuracy: 0.856209\n",
      "epoch 2; batch 55296; loss 0.219789\n",
      "epoch:2; batch 55296; train accuracy: 0.856255\n",
      "epoch 2; batch 55424; loss 0.185275\n",
      "epoch:2; batch 55424; train accuracy: 0.856314\n",
      "epoch 2; batch 55552; loss 0.138950\n",
      "epoch:2; batch 55552; train accuracy: 0.856397\n",
      "epoch 2; batch 55680; loss 0.119119\n",
      "epoch:2; batch 55680; train accuracy: 0.856480\n",
      "epoch 2; batch 55808; loss 0.177288\n",
      "epoch:2; batch 55808; train accuracy: 0.856551\n",
      "epoch 2; batch 55936; loss 0.199507\n",
      "epoch:2; batch 55936; train accuracy: 0.856591\n",
      "epoch 2; batch 56064; loss 0.318755\n",
      "epoch:2; batch 56064; train accuracy: 0.856630\n",
      "epoch 2; batch 56192; loss 0.309562\n",
      "epoch:2; batch 56192; train accuracy: 0.856651\n",
      "epoch 2; batch 56320; loss 0.248576\n",
      "epoch:2; batch 56320; train accuracy: 0.856678\n",
      "epoch 2; batch 56448; loss 0.204574\n",
      "epoch:2; batch 56448; train accuracy: 0.856711\n",
      "epoch 2; batch 56576; loss 0.183987\n",
      "epoch:2; batch 56576; train accuracy: 0.856769\n",
      "epoch 2; batch 56704; loss 0.216391\n",
      "epoch:2; batch 56704; train accuracy: 0.856820\n",
      "epoch 2; batch 56832; loss 0.146716\n",
      "epoch:2; batch 56832; train accuracy: 0.856878\n",
      "epoch 2; batch 56960; loss 0.335870\n",
      "epoch:2; batch 56960; train accuracy: 0.856868\n",
      "epoch 2; batch 57088; loss 0.272875\n",
      "epoch:2; batch 57088; train accuracy: 0.856894\n",
      "epoch 2; batch 57216; loss 0.195605\n",
      "epoch:2; batch 57216; train accuracy: 0.856964\n",
      "epoch 2; batch 57344; loss 0.187236\n",
      "epoch:2; batch 57344; train accuracy: 0.857021\n",
      "epoch 2; batch 57472; loss 0.081341\n",
      "epoch:2; batch 57472; train accuracy: 0.857128\n",
      "epoch 2; batch 57600; loss 0.246111\n",
      "epoch:2; batch 57600; train accuracy: 0.857154\n",
      "epoch 2; batch 57728; loss 0.266386\n",
      "epoch:2; batch 57728; train accuracy: 0.857181\n",
      "epoch 2; batch 57856; loss 0.218297\n",
      "epoch:2; batch 57856; train accuracy: 0.857250\n",
      "epoch 2; batch 57984; loss 0.150268\n",
      "epoch:2; batch 57984; train accuracy: 0.857301\n",
      "epoch 2; batch 58112; loss 0.283009\n",
      "epoch:2; batch 58112; train accuracy: 0.857327\n",
      "epoch 2; batch 58240; loss 0.127056\n",
      "epoch:2; batch 58240; train accuracy: 0.857408\n",
      "epoch 2; batch 58368; loss 0.244895\n",
      "epoch:2; batch 58368; train accuracy: 0.857471\n",
      "epoch 2; batch 58496; loss 0.206867\n",
      "epoch:2; batch 58496; train accuracy: 0.857521\n",
      "epoch 2; batch 58624; loss 0.113347\n",
      "epoch:2; batch 58624; train accuracy: 0.857602\n",
      "epoch 2; batch 58752; loss 0.300695\n",
      "epoch:2; batch 58752; train accuracy: 0.857628\n",
      "epoch 2; batch 58880; loss 0.183817\n",
      "epoch:2; batch 58880; train accuracy: 0.857690\n",
      "epoch 2; batch 59008; loss 0.277710\n",
      "epoch:2; batch 59008; train accuracy: 0.857734\n",
      "epoch 2; batch 59136; loss 0.282333\n",
      "epoch:2; batch 59136; train accuracy: 0.857772\n",
      "epoch 2; batch 59264; loss 0.190792\n",
      "epoch:2; batch 59264; train accuracy: 0.857834\n",
      "epoch 2; batch 59392; loss 0.211630\n",
      "epoch:2; batch 59392; train accuracy: 0.857872\n",
      "epoch 2; batch 59520; loss 0.147623\n",
      "epoch:2; batch 59520; train accuracy: 0.857940\n",
      "epoch 2; batch 59648; loss 0.291077\n",
      "epoch:2; batch 59648; train accuracy: 0.857990\n",
      "epoch 2; batch 59776; loss 0.164681\n",
      "epoch:2; batch 59776; train accuracy: 0.858052\n",
      "epoch 2; batch 59904; loss 0.241849\n",
      "epoch:2; batch 59904; train accuracy: 0.858095\n",
      "epoch 2; batch 60032; loss 0.185524\n",
      "epoch:2; batch 60032; train accuracy: 0.858145\n",
      "epoch 2; batch 60160; loss 0.176692\n",
      "epoch:2; batch 60160; train accuracy: 0.858194\n",
      "epoch 2; batch 60288; loss 0.193343\n",
      "epoch:2; batch 60288; train accuracy: 0.858231\n",
      "epoch 2; batch 60416; loss 0.165700\n",
      "epoch:2; batch 60416; train accuracy: 0.858274\n",
      "epoch 2; batch 60544; loss 0.272961\n",
      "epoch:2; batch 60544; train accuracy: 0.858312\n",
      "epoch 2; batch 60672; loss 0.343384\n",
      "epoch:2; batch 60672; train accuracy: 0.858324\n",
      "epoch 2; batch 60800; loss 0.167832\n",
      "epoch:2; batch 60800; train accuracy: 0.858386\n",
      "epoch 2; batch 60928; loss 0.189183\n",
      "epoch:2; batch 60928; train accuracy: 0.858435\n",
      "epoch 2; batch 61056; loss 0.272702\n",
      "epoch:2; batch 61056; train accuracy: 0.858490\n",
      "epoch 2; batch 61184; loss 0.251095\n",
      "epoch:2; batch 61184; train accuracy: 0.858556\n",
      "epoch 2; batch 61312; loss 0.166642\n",
      "epoch:2; batch 61312; train accuracy: 0.858623\n",
      "epoch 2; batch 61440; loss 0.188628\n",
      "epoch:2; batch 61440; train accuracy: 0.858702\n",
      "epoch 2; batch 61568; loss 0.175522\n",
      "epoch:2; batch 61568; train accuracy: 0.858756\n",
      "epoch 2; batch 61696; loss 0.224257\n",
      "epoch:2; batch 61696; train accuracy: 0.858799\n",
      "epoch 2; batch 61824; loss 0.237356\n",
      "epoch:2; batch 61824; train accuracy: 0.858835\n",
      "epoch 2; batch 61952; loss 0.178281\n",
      "epoch:2; batch 61952; train accuracy: 0.858902\n",
      "epoch 2; batch 62080; loss 0.301360\n",
      "epoch:2; batch 62080; train accuracy: 0.858920\n",
      "epoch 2; batch 62208; loss 0.133403\n",
      "epoch:2; batch 62208; train accuracy: 0.859004\n",
      "epoch 2; batch 62336; loss 0.274163\n",
      "epoch:2; batch 62336; train accuracy: 0.859052\n",
      "epoch 2; batch 62464; loss 0.267429\n",
      "epoch:2; batch 62464; train accuracy: 0.859082\n",
      "epoch 2; batch 62592; loss 0.278359\n",
      "epoch:2; batch 62592; train accuracy: 0.859106\n",
      "epoch 2; batch 62720; loss 0.182812\n",
      "epoch:2; batch 62720; train accuracy: 0.859172\n",
      "epoch 2; batch 62848; loss 0.204609\n",
      "epoch:2; batch 62848; train accuracy: 0.859238\n",
      "epoch 2; batch 62976; loss 0.254189\n",
      "epoch:2; batch 62976; train accuracy: 0.859274\n",
      "epoch 2; batch 63104; loss 0.165375\n",
      "epoch:2; batch 63104; train accuracy: 0.859339\n",
      "epoch 2; batch 63232; loss 0.253633\n",
      "epoch:2; batch 63232; train accuracy: 0.859375\n",
      "epoch 2; batch 63360; loss 0.306885\n",
      "epoch:2; batch 63360; train accuracy: 0.859423\n",
      "epoch 2; batch 63488; loss 0.209087\n",
      "epoch:2; batch 63488; train accuracy: 0.859476\n",
      "epoch 2; batch 63616; loss 0.194978\n",
      "epoch:2; batch 63616; train accuracy: 0.859511\n",
      "epoch 2; batch 63744; loss 0.227530\n",
      "epoch:2; batch 63744; train accuracy: 0.859559\n",
      "epoch 2; batch 63872; loss 0.280139\n",
      "epoch:2; batch 63872; train accuracy: 0.859588\n",
      "epoch 2; batch 64000; loss 0.237570\n",
      "epoch:2; batch 64000; train accuracy: 0.859641\n",
      "epoch 2; batch 64128; loss 0.120893\n",
      "epoch:2; batch 64128; train accuracy: 0.859712\n",
      "epoch 2; batch 64256; loss 0.325799\n",
      "epoch:2; batch 64256; train accuracy: 0.859741\n",
      "epoch 2; batch 64384; loss 0.259622\n",
      "epoch:2; batch 64384; train accuracy: 0.859782\n",
      "epoch 2; batch 64512; loss 0.157815\n",
      "epoch:2; batch 64512; train accuracy: 0.859859\n",
      "epoch 2; batch 64640; loss 0.142598\n",
      "epoch:2; batch 64640; train accuracy: 0.859935\n",
      "epoch 2; batch 64768; loss 0.149620\n",
      "epoch:2; batch 64768; train accuracy: 0.860011\n",
      "epoch 2; batch 64896; loss 0.236969\n",
      "epoch:2; batch 64896; train accuracy: 0.860058\n",
      "epoch 2; batch 65024; loss 0.151856\n",
      "epoch:2; batch 65024; train accuracy: 0.860128\n",
      "epoch 2; batch 65152; loss 0.211861\n",
      "epoch:2; batch 65152; train accuracy: 0.860174\n",
      "epoch 2; batch 65280; loss 0.259617\n",
      "epoch:2; batch 65280; train accuracy: 0.860215\n",
      "epoch 2; batch 65408; loss 0.184733\n",
      "epoch:2; batch 65408; train accuracy: 0.860273\n",
      "epoch 2; batch 65536; loss 0.258492\n",
      "epoch:2; batch 65536; train accuracy: 0.860313\n",
      "epoch 2; batch 65664; loss 0.287212\n",
      "epoch:2; batch 65664; train accuracy: 0.860354\n",
      "epoch 2; batch 65792; loss 0.199761\n",
      "epoch:2; batch 65792; train accuracy: 0.860400\n",
      "epoch 2; batch 65920; loss 0.236238\n",
      "epoch:2; batch 65920; train accuracy: 0.860446\n",
      "epoch 2; batch 66048; loss 0.187429\n",
      "epoch:2; batch 66048; train accuracy: 0.860504\n",
      "epoch 2; batch 66176; loss 0.204002\n",
      "epoch:2; batch 66176; train accuracy: 0.860561\n",
      "epoch 2; batch 66304; loss 0.132791\n",
      "epoch:2; batch 66304; train accuracy: 0.860630\n",
      "epoch 2; batch 66432; loss 0.217195\n",
      "epoch:2; batch 66432; train accuracy: 0.860694\n",
      "epoch 2; batch 66560; loss 0.293127\n",
      "epoch:2; batch 66560; train accuracy: 0.860716\n",
      "epoch 2; batch 66688; loss 0.131128\n",
      "epoch:2; batch 66688; train accuracy: 0.860779\n",
      "epoch 2; batch 66816; loss 0.238372\n",
      "epoch:2; batch 66816; train accuracy: 0.860807\n",
      "epoch 2; batch 66944; loss 0.304764\n",
      "epoch:2; batch 66944; train accuracy: 0.860818\n",
      "epoch 2; batch 67072; loss 0.302348\n",
      "epoch:2; batch 67072; train accuracy: 0.860840\n",
      "epoch 2; batch 67200; loss 0.405616\n",
      "epoch:2; batch 67200; train accuracy: 0.860827\n",
      "epoch 2; batch 67328; loss 0.324463\n",
      "epoch:2; batch 67328; train accuracy: 0.860838\n",
      "epoch 2; batch 67456; loss 0.116107\n",
      "epoch:2; batch 67456; train accuracy: 0.860912\n",
      "epoch 2; batch 67584; loss 0.096003\n",
      "epoch:2; batch 67584; train accuracy: 0.860992\n",
      "epoch 2; batch 67712; loss 0.280454\n",
      "epoch:2; batch 67712; train accuracy: 0.860997\n",
      "epoch 2; batch 67840; loss 0.283658\n",
      "epoch:2; batch 67840; train accuracy: 0.861024\n",
      "epoch 2; batch 67968; loss 0.290476\n",
      "epoch:2; batch 67968; train accuracy: 0.861023\n",
      "epoch 2; batch 68096; loss 0.221531\n",
      "epoch:2; batch 68096; train accuracy: 0.861057\n",
      "epoch 2; batch 68224; loss 0.240768\n",
      "epoch:2; batch 68224; train accuracy: 0.861113\n",
      "epoch 2; batch 68352; loss 0.188579\n",
      "epoch:2; batch 68352; train accuracy: 0.861169\n",
      "epoch 2; batch 68480; loss 0.170097\n",
      "epoch:2; batch 68480; train accuracy: 0.861237\n",
      "epoch 2; batch 68608; loss 0.158507\n",
      "epoch:2; batch 68608; train accuracy: 0.861299\n",
      "epoch 2; batch 68736; loss 0.247971\n",
      "epoch:2; batch 68736; train accuracy: 0.861338\n",
      "epoch 2; batch 68864; loss 0.230429\n",
      "epoch:2; batch 68864; train accuracy: 0.861377\n",
      "epoch 2; batch 68992; loss 0.248702\n",
      "epoch:2; batch 68992; train accuracy: 0.861410\n",
      "epoch 2; batch 69120; loss 0.178231\n",
      "epoch:2; batch 69120; train accuracy: 0.861454\n",
      "epoch 2; batch 69248; loss 0.247351\n",
      "epoch:2; batch 69248; train accuracy: 0.861499\n",
      "epoch 2; batch 69376; loss 0.205736\n",
      "epoch:2; batch 69376; train accuracy: 0.861543\n",
      "epoch 2; batch 69504; loss 0.315862\n",
      "epoch:2; batch 69504; train accuracy: 0.861570\n",
      "epoch 2; batch 69632; loss 0.173432\n",
      "epoch:2; batch 69632; train accuracy: 0.861620\n",
      "epoch 2; batch 69760; loss 0.189703\n",
      "epoch:2; batch 69760; train accuracy: 0.861676\n",
      "epoch 2; batch 69888; loss 0.161222\n",
      "epoch:2; batch 69888; train accuracy: 0.861731\n",
      "epoch 2; batch 70016; loss 0.271220\n",
      "epoch:2; batch 70016; train accuracy: 0.861770\n",
      "epoch 2; batch 70144; loss 0.244803\n",
      "epoch:2; batch 70144; train accuracy: 0.861808\n",
      "epoch 2; batch 70272; loss 0.199251\n",
      "epoch:2; batch 70272; train accuracy: 0.861863\n",
      "epoch 2; batch 70400; loss 0.146747\n",
      "epoch:2; batch 70400; train accuracy: 0.861913\n",
      "epoch 2; batch 70528; loss 0.184722\n",
      "epoch:2; batch 70528; train accuracy: 0.861951\n",
      "epoch 2; batch 70656; loss 0.185645\n",
      "epoch:2; batch 70656; train accuracy: 0.862006\n",
      "epoch 2; batch 70784; loss 0.307602\n",
      "epoch:2; batch 70784; train accuracy: 0.862010\n",
      "epoch 2; batch 70912; loss 0.247024\n",
      "epoch:2; batch 70912; train accuracy: 0.862042\n",
      "epoch 2; batch 71040; loss 0.241112\n",
      "epoch:2; batch 71040; train accuracy: 0.862057\n",
      "epoch 2; batch 71168; loss 0.205467\n",
      "epoch:2; batch 71168; train accuracy: 0.862072\n",
      "epoch 2; batch 71296; loss 0.250698\n",
      "epoch:2; batch 71296; train accuracy: 0.862104\n",
      "epoch 2; batch 71424; loss 0.232270\n",
      "epoch:2; batch 71424; train accuracy: 0.862153\n",
      "epoch 2; batch 71552; loss 0.270018\n",
      "epoch:2; batch 71552; train accuracy: 0.862179\n",
      "epoch 2; batch 71680; loss 0.206135\n",
      "epoch:2; batch 71680; train accuracy: 0.862234\n",
      "epoch 2; batch 71808; loss 0.123398\n",
      "epoch:2; batch 71808; train accuracy: 0.862300\n",
      "epoch 2; batch 71936; loss 0.246537\n",
      "epoch:2; batch 71936; train accuracy: 0.862343\n",
      "epoch 2; batch 72064; loss 0.162745\n",
      "epoch:2; batch 72064; train accuracy: 0.862392\n",
      "epoch 2; batch 72192; loss 0.284530\n",
      "epoch:2; batch 72192; train accuracy: 0.862423\n",
      "epoch 2; batch 72320; loss 0.254354\n",
      "epoch:2; batch 72320; train accuracy: 0.862460\n",
      "epoch 2; batch 72448; loss 0.226994\n",
      "epoch:2; batch 72448; train accuracy: 0.862503\n",
      "epoch 2; batch 72576; loss 0.296971\n",
      "epoch:2; batch 72576; train accuracy: 0.862529\n",
      "epoch 2; batch 72704; loss 0.205987\n",
      "epoch:2; batch 72704; train accuracy: 0.862572\n",
      "epoch 2; batch 72832; loss 0.209874\n",
      "epoch:2; batch 72832; train accuracy: 0.862615\n",
      "epoch 2; batch 72960; loss 0.223031\n",
      "epoch:2; batch 72960; train accuracy: 0.862646\n",
      "epoch 2; batch 73088; loss 0.306675\n",
      "epoch:2; batch 73088; train accuracy: 0.862677\n",
      "epoch 2; batch 73216; loss 0.201515\n",
      "epoch:2; batch 73216; train accuracy: 0.862726\n",
      "epoch 2; batch 73344; loss 0.141619\n",
      "epoch:2; batch 73344; train accuracy: 0.862785\n",
      "epoch 2; batch 73472; loss 0.141050\n",
      "epoch:2; batch 73472; train accuracy: 0.862844\n",
      "epoch 2; batch 73600; loss 0.252244\n",
      "epoch:2; batch 73600; train accuracy: 0.862892\n",
      "epoch 2; batch 73728; loss 0.141003\n",
      "epoch:2; batch 73728; train accuracy: 0.862951\n",
      "epoch 2; batch 73856; loss 0.165189\n",
      "epoch:2; batch 73856; train accuracy: 0.863010\n",
      "epoch 2; batch 73984; loss 0.182091\n",
      "epoch:2; batch 73984; train accuracy: 0.863063\n",
      "epoch 2; batch 74112; loss 0.223129\n",
      "epoch:2; batch 74112; train accuracy: 0.863089\n",
      "epoch 2; batch 74240; loss 0.292055\n",
      "epoch:2; batch 74240; train accuracy: 0.863114\n",
      "epoch 2; batch 74368; loss 0.170551\n",
      "epoch:2; batch 74368; train accuracy: 0.863178\n",
      "epoch 2; batch 74496; loss 0.180064\n",
      "epoch:2; batch 74496; train accuracy: 0.863248\n",
      "epoch 2; batch 74624; loss 0.245263\n",
      "epoch:2; batch 74624; train accuracy: 0.863262\n",
      "epoch 2; batch 74752; loss 0.275826\n",
      "epoch:2; batch 74752; train accuracy: 0.863281\n",
      "epoch 2; batch 74880; loss 0.189288\n",
      "epoch:2; batch 74880; train accuracy: 0.863323\n",
      "epoch 2; batch 75008; loss 0.126364\n",
      "epoch:2; batch 75008; train accuracy: 0.863387\n",
      "epoch 2; batch 75136; loss 0.193532\n",
      "epoch:2; batch 75136; train accuracy: 0.863428\n",
      "epoch 2; batch 75264; loss 0.276445\n",
      "epoch:2; batch 75264; train accuracy: 0.863459\n",
      "epoch 2; batch 75392; loss 0.236713\n",
      "epoch:2; batch 75392; train accuracy: 0.863495\n",
      "epoch 2; batch 75520; loss 0.191242\n",
      "epoch:2; batch 75520; train accuracy: 0.863547\n",
      "epoch 2; batch 75648; loss 0.209313\n",
      "epoch:2; batch 75648; train accuracy: 0.863589\n",
      "epoch 2; batch 75776; loss 0.209810\n",
      "epoch:2; batch 75776; train accuracy: 0.863641\n",
      "epoch 2; batch 75904; loss 0.166005\n",
      "epoch:2; batch 75904; train accuracy: 0.863688\n",
      "epoch 2; batch 76032; loss 0.227655\n",
      "epoch:2; batch 76032; train accuracy: 0.863740\n",
      "epoch 2; batch 76160; loss 0.354048\n",
      "epoch:2; batch 76160; train accuracy: 0.863753\n",
      "epoch 2; batch 76288; loss 0.303107\n",
      "epoch:2; batch 76288; train accuracy: 0.863767\n",
      "epoch 2; batch 76416; loss 0.121631\n",
      "epoch:2; batch 76416; train accuracy: 0.863841\n",
      "epoch 2; batch 76544; loss 0.296420\n",
      "epoch:2; batch 76544; train accuracy: 0.863854\n",
      "epoch 2; batch 76672; loss 0.266197\n",
      "epoch:2; batch 76672; train accuracy: 0.863873\n",
      "epoch 2; batch 76800; loss 0.177955\n",
      "epoch:2; batch 76800; train accuracy: 0.863941\n",
      "epoch 2; batch 76928; loss 0.277252\n",
      "epoch:2; batch 76928; train accuracy: 0.863944\n",
      "epoch 2; batch 77056; loss 0.307182\n",
      "epoch:2; batch 77056; train accuracy: 0.863963\n",
      "epoch 2; batch 77184; loss 0.225748\n",
      "epoch:2; batch 77184; train accuracy: 0.863998\n",
      "epoch 2; batch 77312; loss 0.203332\n",
      "epoch:2; batch 77312; train accuracy: 0.864049\n",
      "epoch 2; batch 77440; loss 0.151158\n",
      "epoch:2; batch 77440; train accuracy: 0.864112\n",
      "epoch 2; batch 77568; loss 0.243576\n",
      "epoch:2; batch 77568; train accuracy: 0.864158\n",
      "epoch 2; batch 77696; loss 0.261449\n",
      "epoch:2; batch 77696; train accuracy: 0.864193\n",
      "epoch 2; batch 77824; loss 0.208734\n",
      "epoch:2; batch 77824; train accuracy: 0.864239\n",
      "epoch 2; batch 77952; loss 0.078348\n",
      "epoch:2; batch 77952; train accuracy: 0.864323\n",
      "epoch 2; batch 78080; loss 0.173335\n",
      "epoch:2; batch 78080; train accuracy: 0.864368\n",
      "epoch 2; batch 78208; loss 0.330813\n",
      "epoch:2; batch 78208; train accuracy: 0.864381\n",
      "epoch 2; batch 78336; loss 0.175632\n",
      "epoch:2; batch 78336; train accuracy: 0.864449\n",
      "epoch 2; batch 78464; loss 0.176355\n",
      "epoch:2; batch 78464; train accuracy: 0.864494\n",
      "epoch 2; batch 78592; loss 0.252727\n",
      "epoch:2; batch 78592; train accuracy: 0.864523\n",
      "epoch 2; batch 78720; loss 0.274586\n",
      "epoch:2; batch 78720; train accuracy: 0.864531\n",
      "epoch 2; batch 78848; loss 0.310252\n",
      "epoch:2; batch 78848; train accuracy: 0.864543\n",
      "epoch 2; batch 78976; loss 0.164103\n",
      "epoch:2; batch 78976; train accuracy: 0.864610\n",
      "epoch 2; batch 79104; loss 0.155664\n",
      "epoch:2; batch 79104; train accuracy: 0.864677\n",
      "epoch 2; batch 79232; loss 0.280472\n",
      "epoch:2; batch 79232; train accuracy: 0.864712\n",
      "epoch 2; batch 79360; loss 0.247188\n",
      "epoch:2; batch 79360; train accuracy: 0.864741\n",
      "epoch 2; batch 79488; loss 0.250808\n",
      "epoch:2; batch 79488; train accuracy: 0.864764\n",
      "epoch 2; batch 79616; loss 0.183610\n",
      "epoch:2; batch 79616; train accuracy: 0.864820\n",
      "epoch 2; batch 79744; loss 0.187840\n",
      "epoch:2; batch 79744; train accuracy: 0.864887\n",
      "epoch 2; batch 79872; loss 0.160373\n",
      "epoch:2; batch 79872; train accuracy: 0.864942\n",
      "epoch 2; batch 80000; loss 0.231276\n",
      "epoch:2; batch 80000; train accuracy: 0.864976\n",
      "epoch 2; batch 80128; loss 0.203687\n",
      "epoch:2; batch 80128; train accuracy: 0.865016\n",
      "epoch 2; batch 80256; loss 0.228989\n",
      "epoch:2; batch 80256; train accuracy: 0.865071\n",
      "epoch 2; batch 80384; loss 0.207754\n",
      "epoch:2; batch 80384; train accuracy: 0.865116\n",
      "epoch 2; batch 80512; loss 0.240898\n",
      "epoch:2; batch 80512; train accuracy: 0.865160\n",
      "epoch 2; batch 80640; loss 0.197856\n",
      "epoch:2; batch 80640; train accuracy: 0.865199\n",
      "epoch 2; batch 80768; loss 0.213609\n",
      "epoch:2; batch 80768; train accuracy: 0.865233\n",
      "epoch 2; batch 80896; loss 0.231667\n",
      "epoch:2; batch 80896; train accuracy: 0.865283\n",
      "epoch 2; batch 81024; loss 0.194118\n",
      "epoch:2; batch 81024; train accuracy: 0.865327\n",
      "epoch 2; batch 81152; loss 0.150993\n",
      "epoch:2; batch 81152; train accuracy: 0.865393\n",
      "epoch 2; batch 81280; loss 0.216869\n",
      "epoch:2; batch 81280; train accuracy: 0.865432\n",
      "epoch 2; batch 81408; loss 0.253072\n",
      "epoch:2; batch 81408; train accuracy: 0.865465\n",
      "epoch 2; batch 81536; loss 0.203974\n",
      "epoch:2; batch 81536; train accuracy: 0.865504\n",
      "epoch 2; batch 81664; loss 0.188825\n",
      "epoch:2; batch 81664; train accuracy: 0.865537\n",
      "epoch 2; batch 81792; loss 0.141870\n",
      "epoch:2; batch 81792; train accuracy: 0.865586\n",
      "epoch 2; batch 81920; loss 0.135990\n",
      "epoch:2; batch 81920; train accuracy: 0.865636\n",
      "epoch 2; batch 82048; loss 0.175349\n",
      "epoch:2; batch 82048; train accuracy: 0.865674\n",
      "epoch 2; batch 82176; loss 0.213118\n",
      "epoch:2; batch 82176; train accuracy: 0.865713\n",
      "epoch 2; batch 82304; loss 0.180933\n",
      "epoch:2; batch 82304; train accuracy: 0.865762\n",
      "epoch 2; batch 82432; loss 0.191283\n",
      "epoch:2; batch 82432; train accuracy: 0.865811\n",
      "epoch 2; batch 82560; loss 0.168271\n",
      "epoch:2; batch 82560; train accuracy: 0.865865\n",
      "epoch 2; batch 82688; loss 0.148426\n",
      "epoch:2; batch 82688; train accuracy: 0.865903\n",
      "epoch 2; batch 82816; loss 0.227906\n",
      "epoch:2; batch 82816; train accuracy: 0.865925\n",
      "epoch 2; batch 82944; loss 0.115608\n",
      "epoch:2; batch 82944; train accuracy: 0.865995\n",
      "epoch 2; batch 83072; loss 0.260371\n",
      "epoch:2; batch 83072; train accuracy: 0.866033\n",
      "epoch 2; batch 83200; loss 0.110051\n",
      "epoch:2; batch 83200; train accuracy: 0.866103\n",
      "epoch 2; batch 83328; loss 0.182468\n",
      "epoch:2; batch 83328; train accuracy: 0.866141\n",
      "epoch 2; batch 83456; loss 0.132562\n",
      "epoch:2; batch 83456; train accuracy: 0.866184\n",
      "epoch 2; batch 83584; loss 0.332572\n",
      "epoch:2; batch 83584; train accuracy: 0.866212\n",
      "epoch 2; batch 83712; loss 0.246793\n",
      "epoch:2; batch 83712; train accuracy: 0.866228\n",
      "epoch 2; batch 83840; loss 0.188061\n",
      "epoch:2; batch 83840; train accuracy: 0.866261\n",
      "epoch 2; batch 83968; loss 0.217314\n",
      "epoch:2; batch 83968; train accuracy: 0.866298\n",
      "epoch 2; batch 84096; loss 0.193139\n",
      "epoch:2; batch 84096; train accuracy: 0.866346\n",
      "epoch 2; batch 84224; loss 0.204702\n",
      "epoch:2; batch 84224; train accuracy: 0.866400\n",
      "epoch 2; batch 84352; loss 0.158134\n",
      "epoch:2; batch 84352; train accuracy: 0.866459\n",
      "epoch 2; batch 84480; loss 0.215393\n",
      "epoch:2; batch 84480; train accuracy: 0.866496\n",
      "epoch 2; batch 84608; loss 0.231694\n",
      "epoch:2; batch 84608; train accuracy: 0.866539\n",
      "epoch 2; batch 84736; loss 0.237132\n",
      "epoch:2; batch 84736; train accuracy: 0.866565\n",
      "epoch 2; batch 84864; loss 0.248051\n",
      "epoch:2; batch 84864; train accuracy: 0.866603\n",
      "epoch 2; batch 84992; loss 0.194072\n",
      "epoch:2; batch 84992; train accuracy: 0.866629\n",
      "epoch 2; batch 85120; loss 0.248870\n",
      "epoch:2; batch 85120; train accuracy: 0.866661\n",
      "epoch 2; batch 85248; loss 0.188141\n",
      "epoch:2; batch 85248; train accuracy: 0.866714\n",
      "epoch 2; batch 85376; loss 0.157243\n",
      "epoch:2; batch 85376; train accuracy: 0.866757\n",
      "epoch 2; batch 85504; loss 0.155216\n",
      "epoch:2; batch 85504; train accuracy: 0.866794\n",
      "epoch 2; batch 85632; loss 0.126107\n",
      "epoch:2; batch 85632; train accuracy: 0.866857\n",
      "epoch 2; batch 85760; loss 0.208552\n",
      "epoch:2; batch 85760; train accuracy: 0.866899\n",
      "epoch 2; batch 85888; loss 0.178582\n",
      "epoch:2; batch 85888; train accuracy: 0.866957\n",
      "epoch 2; batch 86016; loss 0.216297\n",
      "epoch:2; batch 86016; train accuracy: 0.866994\n",
      "epoch 2; batch 86144; loss 0.202922\n",
      "epoch:2; batch 86144; train accuracy: 0.867031\n",
      "epoch 2; batch 86272; loss 0.251027\n",
      "epoch:2; batch 86272; train accuracy: 0.867046\n",
      "epoch 2; batch 86400; loss 0.159984\n",
      "epoch:2; batch 86400; train accuracy: 0.867093\n",
      "epoch 2; batch 86528; loss 0.159890\n",
      "epoch:2; batch 86528; train accuracy: 0.867151\n",
      "epoch 2; batch 86656; loss 0.214268\n",
      "epoch:2; batch 86656; train accuracy: 0.867182\n",
      "epoch 2; batch 86784; loss 0.184289\n",
      "epoch:2; batch 86784; train accuracy: 0.867229\n",
      "epoch 2; batch 86912; loss 0.150834\n",
      "epoch:2; batch 86912; train accuracy: 0.867281\n",
      "epoch 2; batch 87040; loss 0.086296\n",
      "epoch:2; batch 87040; train accuracy: 0.867354\n",
      "epoch 2; batch 87168; loss 0.185874\n",
      "epoch:2; batch 87168; train accuracy: 0.867406\n",
      "epoch 2; batch 87296; loss 0.202607\n",
      "epoch:2; batch 87296; train accuracy: 0.867453\n",
      "epoch 2; batch 87424; loss 0.188901\n",
      "epoch:2; batch 87424; train accuracy: 0.867499\n",
      "epoch 2; batch 87552; loss 0.223903\n",
      "epoch:2; batch 87552; train accuracy: 0.867530\n",
      "epoch 2; batch 87680; loss 0.136621\n",
      "epoch:2; batch 87680; train accuracy: 0.867592\n",
      "epoch 2; batch 87808; loss 0.130275\n",
      "epoch:2; batch 87808; train accuracy: 0.867639\n",
      "epoch 2; batch 87936; loss 0.139621\n",
      "epoch:2; batch 87936; train accuracy: 0.867696\n",
      "epoch 2; batch 88064; loss 0.210188\n",
      "epoch:2; batch 88064; train accuracy: 0.867742\n",
      "epoch 2; batch 88192; loss 0.165921\n",
      "epoch:2; batch 88192; train accuracy: 0.867778\n",
      "epoch 2; batch 88320; loss 0.158848\n",
      "epoch:2; batch 88320; train accuracy: 0.867829\n",
      "epoch 2; batch 88448; loss 0.169897\n",
      "epoch:2; batch 88448; train accuracy: 0.867891\n",
      "epoch 2; batch 88576; loss 0.137987\n",
      "epoch:2; batch 88576; train accuracy: 0.867952\n",
      "epoch 2; batch 88704; loss 0.190870\n",
      "epoch:2; batch 88704; train accuracy: 0.867993\n",
      "epoch 2; batch 88832; loss 0.213954\n",
      "epoch:2; batch 88832; train accuracy: 0.868034\n",
      "epoch 2; batch 88960; loss 0.283514\n",
      "epoch:2; batch 88960; train accuracy: 0.868054\n",
      "epoch 2; batch 89088; loss 0.213884\n",
      "epoch:2; batch 89088; train accuracy: 0.868084\n",
      "epoch 2; batch 89216; loss 0.214883\n",
      "epoch:2; batch 89216; train accuracy: 0.868130\n",
      "epoch 2; batch 89344; loss 0.148410\n",
      "epoch:2; batch 89344; train accuracy: 0.868181\n",
      "epoch 2; batch 89472; loss 0.145078\n",
      "epoch:2; batch 89472; train accuracy: 0.868232\n",
      "epoch 2; batch 89600; loss 0.234009\n",
      "epoch:2; batch 89600; train accuracy: 0.868267\n",
      "epoch 2; batch 89728; loss 0.210336\n",
      "epoch:2; batch 89728; train accuracy: 0.868287\n",
      "epoch 2; batch 89856; loss 0.207310\n",
      "epoch:2; batch 89856; train accuracy: 0.868317\n",
      "epoch 2; batch 89984; loss 0.071533\n",
      "epoch:2; batch 89984; train accuracy: 0.868393\n",
      "epoch 2; batch 90112; loss 0.325373\n",
      "epoch:2; batch 90112; train accuracy: 0.868392\n",
      "epoch 2; batch 90240; loss 0.236872\n",
      "epoch:2; batch 90240; train accuracy: 0.868438\n",
      "epoch 2; batch 90368; loss 0.200227\n",
      "epoch:2; batch 90368; train accuracy: 0.868467\n",
      "epoch 2; batch 90496; loss 0.227972\n",
      "epoch:2; batch 90496; train accuracy: 0.868497\n",
      "epoch 2; batch 90624; loss 0.205306\n",
      "epoch:2; batch 90624; train accuracy: 0.868527\n",
      "epoch 2; batch 90752; loss 0.222809\n",
      "epoch:2; batch 90752; train accuracy: 0.868562\n",
      "epoch 2; batch 90880; loss 0.163906\n",
      "epoch:2; batch 90880; train accuracy: 0.868602\n",
      "epoch 2; batch 91008; loss 0.190538\n",
      "epoch:2; batch 91008; train accuracy: 0.868647\n",
      "epoch 2; batch 91136; loss 0.263143\n",
      "epoch:2; batch 91136; train accuracy: 0.868656\n",
      "epoch 2; batch 91264; loss 0.290914\n",
      "epoch:2; batch 91264; train accuracy: 0.868686\n",
      "epoch 2; batch 91392; loss 0.130032\n",
      "epoch:2; batch 91392; train accuracy: 0.868751\n",
      "epoch 2; batch 91520; loss 0.223477\n",
      "epoch:2; batch 91520; train accuracy: 0.868791\n",
      "epoch 2; batch 91648; loss 0.121340\n",
      "epoch:2; batch 91648; train accuracy: 0.868851\n",
      "epoch 2; batch 91776; loss 0.225792\n",
      "epoch:2; batch 91776; train accuracy: 0.868885\n",
      "epoch 2; batch 91904; loss 0.278673\n",
      "epoch:2; batch 91904; train accuracy: 0.868904\n",
      "epoch 2; batch 92032; loss 0.259828\n",
      "epoch:2; batch 92032; train accuracy: 0.868919\n",
      "epoch 2; batch 92160; loss 0.144374\n",
      "epoch:2; batch 92160; train accuracy: 0.868968\n",
      "epoch 2; batch 92288; loss 0.210237\n",
      "epoch:2; batch 92288; train accuracy: 0.869008\n",
      "epoch 2; batch 92416; loss 0.143920\n",
      "epoch:2; batch 92416; train accuracy: 0.869052\n",
      "epoch 2; batch 92544; loss 0.179706\n",
      "epoch:2; batch 92544; train accuracy: 0.869086\n",
      "epoch 2; batch 92672; loss 0.242688\n",
      "epoch:2; batch 92672; train accuracy: 0.869125\n",
      "epoch 2; batch 92800; loss 0.148990\n",
      "epoch:2; batch 92800; train accuracy: 0.869170\n",
      "epoch 2; batch 92928; loss 0.298977\n",
      "epoch:2; batch 92928; train accuracy: 0.869184\n",
      "epoch 2; batch 93056; loss 0.352339\n",
      "epoch:2; batch 93056; train accuracy: 0.869187\n",
      "epoch 2; batch 93184; loss 0.185399\n",
      "epoch:2; batch 93184; train accuracy: 0.869231\n",
      "epoch 2; batch 93312; loss 0.240948\n",
      "epoch:2; batch 93312; train accuracy: 0.869260\n",
      "epoch 2; batch 93440; loss 0.204629\n",
      "epoch:2; batch 93440; train accuracy: 0.869304\n",
      "epoch 2; batch 93568; loss 0.308763\n",
      "epoch:2; batch 93568; train accuracy: 0.869323\n",
      "epoch 2; batch 93696; loss 0.213383\n",
      "epoch:2; batch 93696; train accuracy: 0.869337\n",
      "epoch 2; batch 93824; loss 0.330206\n",
      "epoch:2; batch 93824; train accuracy: 0.869356\n",
      "epoch 2; batch 93952; loss 0.236568\n",
      "epoch:2; batch 93952; train accuracy: 0.869364\n",
      "epoch 2; batch 94080; loss 0.232311\n",
      "epoch:2; batch 94080; train accuracy: 0.869388\n",
      "epoch 2; batch 94208; loss 0.289864\n",
      "epoch:2; batch 94208; train accuracy: 0.869382\n",
      "epoch 2; batch 94336; loss 0.193343\n",
      "epoch:2; batch 94336; train accuracy: 0.869420\n",
      "epoch 2; batch 94464; loss 0.202999\n",
      "epoch:2; batch 94464; train accuracy: 0.869459\n",
      "epoch 2; batch 94592; loss 0.232375\n",
      "epoch:2; batch 94592; train accuracy: 0.869478\n",
      "epoch 2; batch 94720; loss 0.215133\n",
      "epoch:2; batch 94720; train accuracy: 0.869511\n",
      "epoch 2; batch 94848; loss 0.264705\n",
      "epoch:2; batch 94848; train accuracy: 0.869525\n",
      "epoch 2; batch 94976; loss 0.215236\n",
      "epoch:2; batch 94976; train accuracy: 0.869558\n",
      "epoch 2; batch 95104; loss 0.213758\n",
      "epoch:2; batch 95104; train accuracy: 0.869592\n",
      "epoch 2; batch 95232; loss 0.252972\n",
      "epoch:2; batch 95232; train accuracy: 0.869595\n",
      "epoch 2; batch 95360; loss 0.230400\n",
      "epoch:2; batch 95360; train accuracy: 0.869624\n",
      "epoch 2; batch 95488; loss 0.192386\n",
      "epoch:2; batch 95488; train accuracy: 0.869662\n",
      "epoch 2; batch 95616; loss 0.255198\n",
      "epoch:2; batch 95616; train accuracy: 0.869700\n",
      "epoch 2; batch 95744; loss 0.225945\n",
      "epoch:2; batch 95744; train accuracy: 0.869729\n",
      "epoch 2; batch 95872; loss 0.257523\n",
      "epoch:2; batch 95872; train accuracy: 0.869742\n",
      "epoch 2; batch 96000; loss 0.191184\n",
      "epoch:2; batch 96000; train accuracy: 0.869790\n",
      "epoch 2; batch 96128; loss 0.213398\n",
      "epoch:2; batch 96128; train accuracy: 0.869823\n",
      "epoch 2; batch 96256; loss 0.154107\n",
      "epoch:2; batch 96256; train accuracy: 0.869861\n",
      "epoch 2; batch 96384; loss 0.168190\n",
      "epoch:2; batch 96384; train accuracy: 0.869899\n",
      "epoch 2; batch 96512; loss 0.192761\n",
      "epoch:2; batch 96512; train accuracy: 0.869927\n",
      "epoch 2; batch 96640; loss 0.179271\n",
      "epoch:2; batch 96640; train accuracy: 0.869960\n",
      "epoch 2; batch 96768; loss 0.191376\n",
      "epoch:2; batch 96768; train accuracy: 0.869993\n",
      "epoch 2; batch 96896; loss 0.305323\n",
      "epoch:2; batch 96896; train accuracy: 0.869991\n",
      "epoch 2; batch 97024; loss 0.220355\n",
      "epoch:2; batch 97024; train accuracy: 0.870005\n",
      "epoch 2; batch 97152; loss 0.160357\n",
      "epoch:2; batch 97152; train accuracy: 0.870052\n",
      "epoch 2; batch 97280; loss 0.197665\n",
      "epoch:2; batch 97280; train accuracy: 0.870090\n",
      "epoch 2; batch 97408; loss 0.209889\n",
      "epoch:2; batch 97408; train accuracy: 0.870143\n",
      "epoch 2; batch 97536; loss 0.166631\n",
      "epoch:2; batch 97536; train accuracy: 0.870195\n",
      "epoch 2; batch 97664; loss 0.252880\n",
      "epoch:2; batch 97664; train accuracy: 0.870242\n",
      "epoch 2; batch 97792; loss 0.204824\n",
      "epoch:2; batch 97792; train accuracy: 0.870275\n",
      "epoch 2; batch 97920; loss 0.228272\n",
      "epoch:2; batch 97920; train accuracy: 0.870308\n",
      "epoch 2; batch 98048; loss 0.174753\n",
      "epoch:2; batch 98048; train accuracy: 0.870360\n",
      "epoch 2; batch 98176; loss 0.176396\n",
      "epoch:2; batch 98176; train accuracy: 0.870402\n",
      "epoch 2; batch 98304; loss 0.214513\n",
      "epoch:2; batch 98304; train accuracy: 0.870435\n",
      "epoch 2; batch 98432; loss 0.155898\n",
      "epoch:2; batch 98432; train accuracy: 0.870472\n",
      "epoch 2; batch 98560; loss 0.208263\n",
      "epoch:2; batch 98560; train accuracy: 0.870514\n",
      "epoch 2; batch 98688; loss 0.180005\n",
      "epoch:2; batch 98688; train accuracy: 0.870561\n",
      "epoch 2; batch 98816; loss 0.292413\n",
      "epoch:2; batch 98816; train accuracy: 0.870598\n",
      "epoch 2; batch 98944; loss 0.179324\n",
      "epoch:2; batch 98944; train accuracy: 0.870645\n",
      "epoch 2; batch 99072; loss 0.180972\n",
      "epoch:2; batch 99072; train accuracy: 0.870682\n",
      "epoch 2; batch 99200; loss 0.197182\n",
      "epoch:2; batch 99200; train accuracy: 0.870719\n",
      "epoch 2; batch 99328; loss 0.188098\n",
      "epoch:2; batch 99328; train accuracy: 0.870751\n",
      "epoch 2; batch 99456; loss 0.259471\n",
      "epoch:2; batch 99456; train accuracy: 0.870764\n",
      "epoch 2; batch 99584; loss 0.225076\n",
      "epoch:2; batch 99584; train accuracy: 0.870791\n",
      "epoch 2; batch 99712; loss 0.233477\n",
      "epoch:2; batch 99712; train accuracy: 0.870818\n",
      "epoch 2; batch 99840; loss 0.167938\n",
      "epoch:2; batch 99840; train accuracy: 0.870874\n",
      "epoch 2; batch 99968; loss 0.315825\n",
      "epoch:2; batch 99968; train accuracy: 0.870891\n",
      "epoch 2; batch 100096; loss 0.168121\n",
      "epoch:2; batch 100096; train accuracy: 0.870928\n",
      "epoch 2; batch 100224; loss 0.165021\n",
      "epoch:2; batch 100224; train accuracy: 0.870950\n",
      "epoch 2; batch 100352; loss 0.259828\n",
      "epoch:2; batch 100352; train accuracy: 0.870987\n",
      "epoch 2; batch 100480; loss 0.246255\n",
      "epoch:2; batch 100480; train accuracy: 0.871023\n",
      "epoch 2; batch 100608; loss 0.276158\n",
      "epoch:2; batch 100608; train accuracy: 0.871035\n",
      "epoch 2; batch 100736; loss 0.217951\n",
      "epoch:2; batch 100736; train accuracy: 0.871062\n",
      "epoch 2; batch 100864; loss 0.241309\n",
      "epoch:2; batch 100864; train accuracy: 0.871103\n",
      "epoch 2; batch 100992; loss 0.161016\n",
      "epoch:2; batch 100992; train accuracy: 0.871159\n",
      "epoch 2; batch 101120; loss 0.197684\n",
      "epoch:2; batch 101120; train accuracy: 0.871191\n",
      "epoch 2; batch 101248; loss 0.198991\n",
      "epoch:2; batch 101248; train accuracy: 0.871237\n",
      "epoch 2; batch 101376; loss 0.144752\n",
      "epoch:2; batch 101376; train accuracy: 0.871288\n",
      "epoch 2; batch 101504; loss 0.297046\n",
      "epoch:2; batch 101504; train accuracy: 0.871300\n",
      "epoch 2; batch 101632; loss 0.199738\n",
      "epoch:2; batch 101632; train accuracy: 0.871336\n",
      "epoch 2; batch 101760; loss 0.175442\n",
      "epoch:2; batch 101760; train accuracy: 0.871377\n",
      "epoch 2; batch 101888; loss 0.196038\n",
      "epoch:2; batch 101888; train accuracy: 0.871413\n",
      "epoch 2; batch 102016; loss 0.222116\n",
      "epoch:2; batch 102016; train accuracy: 0.871444\n",
      "epoch 2; batch 102144; loss 0.245436\n",
      "epoch:2; batch 102144; train accuracy: 0.871475\n",
      "epoch 2; batch 102272; loss 0.183666\n",
      "epoch:2; batch 102272; train accuracy: 0.871506\n",
      "epoch 2; batch 102400; loss 0.175753\n",
      "epoch:2; batch 102400; train accuracy: 0.871542\n",
      "epoch 2; batch 102528; loss 0.100418\n",
      "epoch:2; batch 102528; train accuracy: 0.871602\n",
      "epoch 2; batch 102656; loss 0.250861\n",
      "epoch:2; batch 102656; train accuracy: 0.871633\n",
      "epoch 2; batch 102784; loss 0.123136\n",
      "epoch:2; batch 102784; train accuracy: 0.871688\n",
      "epoch 2; batch 102912; loss 0.163683\n",
      "epoch:2; batch 102912; train accuracy: 0.871724\n",
      "epoch 2; batch 103040; loss 0.163878\n",
      "epoch:2; batch 103040; train accuracy: 0.871769\n",
      "epoch 2; batch 103168; loss 0.168327\n",
      "epoch:2; batch 103168; train accuracy: 0.871805\n",
      "epoch 2; batch 103296; loss 0.144306\n",
      "epoch:2; batch 103296; train accuracy: 0.871850\n",
      "epoch 2; batch 103424; loss 0.221511\n",
      "epoch:2; batch 103424; train accuracy: 0.871876\n",
      "epoch 2; batch 103552; loss 0.308735\n",
      "epoch:2; batch 103552; train accuracy: 0.871897\n",
      "epoch 2; batch 103680; loss 0.205402\n",
      "epoch:2; batch 103680; train accuracy: 0.871933\n",
      "epoch 2; batch 103808; loss 0.204616\n",
      "epoch:2; batch 103808; train accuracy: 0.871968\n",
      "epoch 2; batch 103936; loss 0.189749\n",
      "epoch:2; batch 103936; train accuracy: 0.872003\n",
      "epoch 2; batch 104064; loss 0.167972\n",
      "epoch:2; batch 104064; train accuracy: 0.872043\n",
      "epoch 2; batch 104192; loss 0.154333\n",
      "epoch:2; batch 104192; train accuracy: 0.872083\n",
      "epoch 2; batch 104320; loss 0.227156\n",
      "epoch:2; batch 104320; train accuracy: 0.872119\n",
      "epoch 2; batch 104448; loss 0.187138\n",
      "epoch:2; batch 104448; train accuracy: 0.872159\n",
      "epoch 2; batch 104576; loss 0.238791\n",
      "epoch:2; batch 104576; train accuracy: 0.872184\n",
      "epoch 2; batch 104704; loss 0.110847\n",
      "epoch:2; batch 104704; train accuracy: 0.872238\n",
      "epoch 2; batch 104832; loss 0.174293\n",
      "epoch:2; batch 104832; train accuracy: 0.872264\n",
      "epoch 2; batch 104960; loss 0.154911\n",
      "epoch:2; batch 104960; train accuracy: 0.872308\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 59102 ; rate: 0.563091\n",
      "y_true_label_1_num: 3645 ; rate: 0.034728\n",
      "y_true_label_2_num: 3068 ; rate: 0.029230\n",
      "y_true_label_3_num: 39145 ; rate: 0.372952\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.968093\n",
      "valid avg_precision: 0.971426\n",
      "valid avg_recall: 0.964224\n",
      "valid avg_f1: 0.967386\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 8544 ; rate: 0.570513\n",
      "y_true_label_1_num: 497 ; rate: 0.033186\n",
      "y_true_label_2_num: 424 ; rate: 0.028312\n",
      "y_true_label_3_num: 5511 ; rate: 0.367989\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.849159\n",
      "valid avg_precision: 0.850868\n",
      "valid avg_recall: 0.842615\n",
      "valid avg_f1: 0.843617\n",
      "epoch 3\n",
      "epoch 3; batch 128; loss 0.132957\n",
      "epoch:3; batch 128; train accuracy: 0.872367\n",
      "epoch 3; batch 256; loss 0.092675\n",
      "epoch:3; batch 256; train accuracy: 0.872421\n",
      "epoch 3; batch 384; loss 0.077211\n",
      "epoch:3; batch 384; train accuracy: 0.872480\n",
      "epoch 3; batch 512; loss 0.376512\n",
      "epoch:3; batch 512; train accuracy: 0.872505\n",
      "epoch 3; batch 640; loss 0.186430\n",
      "epoch:3; batch 640; train accuracy: 0.872549\n",
      "epoch 3; batch 768; loss 0.132002\n",
      "epoch:3; batch 768; train accuracy: 0.872598\n",
      "epoch 3; batch 896; loss 0.263448\n",
      "epoch:3; batch 896; train accuracy: 0.872633\n",
      "epoch 3; batch 1024; loss 0.159965\n",
      "epoch:3; batch 1024; train accuracy: 0.872672\n",
      "epoch 3; batch 1152; loss 0.128631\n",
      "epoch:3; batch 1152; train accuracy: 0.872721\n",
      "epoch 3; batch 1280; loss 0.167198\n",
      "epoch:3; batch 1280; train accuracy: 0.872760\n",
      "epoch 3; batch 1408; loss 0.154635\n",
      "epoch:3; batch 1408; train accuracy: 0.872804\n",
      "epoch 3; batch 1536; loss 0.106419\n",
      "epoch:3; batch 1536; train accuracy: 0.872858\n",
      "epoch 3; batch 1664; loss 0.165556\n",
      "epoch:3; batch 1664; train accuracy: 0.872902\n",
      "epoch 3; batch 1792; loss 0.074511\n",
      "epoch:3; batch 1792; train accuracy: 0.872964\n",
      "epoch 3; batch 1920; loss 0.112842\n",
      "epoch:3; batch 1920; train accuracy: 0.873022\n",
      "epoch 3; batch 2048; loss 0.147210\n",
      "epoch:3; batch 2048; train accuracy: 0.873070\n",
      "epoch 3; batch 2176; loss 0.235410\n",
      "epoch:3; batch 2176; train accuracy: 0.873100\n",
      "epoch 3; batch 2304; loss 0.152298\n",
      "epoch:3; batch 2304; train accuracy: 0.873139\n",
      "epoch 3; batch 2432; loss 0.082370\n",
      "epoch:3; batch 2432; train accuracy: 0.873206\n",
      "epoch 3; batch 2560; loss 0.177869\n",
      "epoch:3; batch 2560; train accuracy: 0.873245\n",
      "epoch 3; batch 2688; loss 0.164833\n",
      "epoch:3; batch 2688; train accuracy: 0.873283\n",
      "epoch 3; batch 2816; loss 0.146075\n",
      "epoch:3; batch 2816; train accuracy: 0.873336\n",
      "epoch 3; batch 2944; loss 0.153773\n",
      "epoch:3; batch 2944; train accuracy: 0.873379\n",
      "epoch 3; batch 3072; loss 0.153232\n",
      "epoch:3; batch 3072; train accuracy: 0.873423\n",
      "epoch 3; batch 3200; loss 0.145749\n",
      "epoch:3; batch 3200; train accuracy: 0.873475\n",
      "epoch 3; batch 3328; loss 0.109341\n",
      "epoch:3; batch 3328; train accuracy: 0.873528\n",
      "epoch 3; batch 3456; loss 0.149102\n",
      "epoch:3; batch 3456; train accuracy: 0.873580\n",
      "epoch 3; batch 3584; loss 0.164780\n",
      "epoch:3; batch 3584; train accuracy: 0.873623\n",
      "epoch 3; batch 3712; loss 0.128575\n",
      "epoch:3; batch 3712; train accuracy: 0.873671\n",
      "epoch 3; batch 3840; loss 0.160288\n",
      "epoch:3; batch 3840; train accuracy: 0.873704\n",
      "epoch 3; batch 3968; loss 0.102057\n",
      "epoch:3; batch 3968; train accuracy: 0.873761\n",
      "epoch 3; batch 4096; loss 0.110494\n",
      "epoch:3; batch 4096; train accuracy: 0.873823\n",
      "epoch 3; batch 4224; loss 0.057931\n",
      "epoch:3; batch 4224; train accuracy: 0.873884\n",
      "epoch 3; batch 4352; loss 0.191516\n",
      "epoch:3; batch 4352; train accuracy: 0.873913\n",
      "epoch 3; batch 4480; loss 0.177782\n",
      "epoch:3; batch 4480; train accuracy: 0.873951\n",
      "epoch 3; batch 4608; loss 0.116626\n",
      "epoch:3; batch 4608; train accuracy: 0.874002\n",
      "epoch 3; batch 4736; loss 0.176736\n",
      "epoch:3; batch 4736; train accuracy: 0.874036\n",
      "epoch 3; batch 4864; loss 0.178761\n",
      "epoch:3; batch 4864; train accuracy: 0.874083\n",
      "epoch 3; batch 4992; loss 0.118420\n",
      "epoch:3; batch 4992; train accuracy: 0.874139\n",
      "epoch 3; batch 5120; loss 0.102768\n",
      "epoch:3; batch 5120; train accuracy: 0.874205\n",
      "epoch 3; batch 5248; loss 0.111681\n",
      "epoch:3; batch 5248; train accuracy: 0.874247\n",
      "epoch 3; batch 5376; loss 0.106744\n",
      "epoch:3; batch 5376; train accuracy: 0.874299\n",
      "epoch 3; batch 5504; loss 0.123439\n",
      "epoch:3; batch 5504; train accuracy: 0.874345\n",
      "epoch 3; batch 5632; loss 0.078019\n",
      "epoch:3; batch 5632; train accuracy: 0.874402\n",
      "epoch 3; batch 5760; loss 0.098238\n",
      "epoch:3; batch 5760; train accuracy: 0.874453\n",
      "epoch 3; batch 5888; loss 0.165536\n",
      "epoch:3; batch 5888; train accuracy: 0.874486\n",
      "epoch 3; batch 6016; loss 0.096826\n",
      "epoch:3; batch 6016; train accuracy: 0.874542\n",
      "epoch 3; batch 6144; loss 0.113601\n",
      "epoch:3; batch 6144; train accuracy: 0.874583\n",
      "epoch 3; batch 6272; loss 0.126163\n",
      "epoch:3; batch 6272; train accuracy: 0.874621\n",
      "epoch 3; batch 6400; loss 0.200120\n",
      "epoch:3; batch 6400; train accuracy: 0.874663\n",
      "epoch 3; batch 6528; loss 0.152370\n",
      "epoch:3; batch 6528; train accuracy: 0.874700\n",
      "epoch 3; batch 6656; loss 0.150173\n",
      "epoch:3; batch 6656; train accuracy: 0.874741\n",
      "epoch 3; batch 6784; loss 0.043121\n",
      "epoch:3; batch 6784; train accuracy: 0.874811\n",
      "epoch 3; batch 6912; loss 0.133776\n",
      "epoch:3; batch 6912; train accuracy: 0.874862\n",
      "epoch 3; batch 7040; loss 0.158320\n",
      "epoch:3; batch 7040; train accuracy: 0.874894\n",
      "epoch 3; batch 7168; loss 0.159640\n",
      "epoch:3; batch 7168; train accuracy: 0.874936\n",
      "epoch 3; batch 7296; loss 0.086269\n",
      "epoch:3; batch 7296; train accuracy: 0.874991\n",
      "epoch 3; batch 7424; loss 0.116759\n",
      "epoch:3; batch 7424; train accuracy: 0.875051\n",
      "epoch 3; batch 7552; loss 0.153984\n",
      "epoch:3; batch 7552; train accuracy: 0.875083\n",
      "epoch 3; batch 7680; loss 0.144430\n",
      "epoch:3; batch 7680; train accuracy: 0.875124\n",
      "epoch 3; batch 7808; loss 0.138074\n",
      "epoch:3; batch 7808; train accuracy: 0.875161\n",
      "epoch 3; batch 7936; loss 0.146145\n",
      "epoch:3; batch 7936; train accuracy: 0.875216\n",
      "epoch 3; batch 8064; loss 0.242827\n",
      "epoch:3; batch 8064; train accuracy: 0.875229\n",
      "epoch 3; batch 8192; loss 0.093825\n",
      "epoch:3; batch 8192; train accuracy: 0.875284\n",
      "epoch 3; batch 8320; loss 0.099122\n",
      "epoch:3; batch 8320; train accuracy: 0.875339\n",
      "epoch 3; batch 8448; loss 0.114078\n",
      "epoch:3; batch 8448; train accuracy: 0.875389\n",
      "epoch 3; batch 8576; loss 0.204393\n",
      "epoch:3; batch 8576; train accuracy: 0.875421\n",
      "epoch 3; batch 8704; loss 0.113372\n",
      "epoch:3; batch 8704; train accuracy: 0.875462\n",
      "epoch 3; batch 8832; loss 0.146851\n",
      "epoch:3; batch 8832; train accuracy: 0.875507\n",
      "epoch 3; batch 8960; loss 0.128882\n",
      "epoch:3; batch 8960; train accuracy: 0.875557\n",
      "epoch 3; batch 9088; loss 0.147561\n",
      "epoch:3; batch 9088; train accuracy: 0.875612\n",
      "epoch 3; batch 9216; loss 0.073398\n",
      "epoch:3; batch 9216; train accuracy: 0.875671\n",
      "epoch 3; batch 9344; loss 0.128339\n",
      "epoch:3; batch 9344; train accuracy: 0.875702\n",
      "epoch 3; batch 9472; loss 0.089474\n",
      "epoch:3; batch 9472; train accuracy: 0.875752\n",
      "epoch 3; batch 9600; loss 0.160503\n",
      "epoch:3; batch 9600; train accuracy: 0.875788\n",
      "epoch 3; batch 9728; loss 0.054683\n",
      "epoch:3; batch 9728; train accuracy: 0.875856\n",
      "epoch 3; batch 9856; loss 0.158542\n",
      "epoch:3; batch 9856; train accuracy: 0.875910\n",
      "epoch 3; batch 9984; loss 0.111685\n",
      "epoch:3; batch 9984; train accuracy: 0.875955\n",
      "epoch 3; batch 10112; loss 0.117266\n",
      "epoch:3; batch 10112; train accuracy: 0.876000\n",
      "epoch 3; batch 10240; loss 0.156697\n",
      "epoch:3; batch 10240; train accuracy: 0.876049\n",
      "epoch 3; batch 10368; loss 0.157312\n",
      "epoch:3; batch 10368; train accuracy: 0.876089\n",
      "epoch 3; batch 10496; loss 0.120529\n",
      "epoch:3; batch 10496; train accuracy: 0.876130\n",
      "epoch 3; batch 10624; loss 0.057808\n",
      "epoch:3; batch 10624; train accuracy: 0.876193\n",
      "epoch 3; batch 10752; loss 0.095730\n",
      "epoch:3; batch 10752; train accuracy: 0.876242\n",
      "epoch 3; batch 10880; loss 0.038492\n",
      "epoch:3; batch 10880; train accuracy: 0.876304\n",
      "epoch 3; batch 11008; loss 0.063997\n",
      "epoch:3; batch 11008; train accuracy: 0.876362\n",
      "epoch 3; batch 11136; loss 0.133082\n",
      "epoch:3; batch 11136; train accuracy: 0.876411\n",
      "epoch 3; batch 11264; loss 0.071686\n",
      "epoch:3; batch 11264; train accuracy: 0.876469\n",
      "epoch 3; batch 11392; loss 0.098661\n",
      "epoch:3; batch 11392; train accuracy: 0.876523\n",
      "epoch 3; batch 11520; loss 0.094443\n",
      "epoch:3; batch 11520; train accuracy: 0.876581\n",
      "epoch 3; batch 11648; loss 0.266885\n",
      "epoch:3; batch 11648; train accuracy: 0.876598\n",
      "epoch 3; batch 11776; loss 0.143485\n",
      "epoch:3; batch 11776; train accuracy: 0.876628\n",
      "epoch 3; batch 11904; loss 0.051913\n",
      "epoch:3; batch 11904; train accuracy: 0.876695\n",
      "epoch 3; batch 12032; loss 0.069896\n",
      "epoch:3; batch 12032; train accuracy: 0.876757\n",
      "epoch 3; batch 12160; loss 0.121533\n",
      "epoch:3; batch 12160; train accuracy: 0.876806\n",
      "epoch 3; batch 12288; loss 0.100795\n",
      "epoch:3; batch 12288; train accuracy: 0.876854\n",
      "epoch 3; batch 12416; loss 0.133620\n",
      "epoch:3; batch 12416; train accuracy: 0.876894\n",
      "epoch 3; batch 12544; loss 0.106136\n",
      "epoch:3; batch 12544; train accuracy: 0.876942\n",
      "epoch 3; batch 12672; loss 0.099228\n",
      "epoch:3; batch 12672; train accuracy: 0.876995\n",
      "epoch 3; batch 12800; loss 0.022664\n",
      "epoch:3; batch 12800; train accuracy: 0.877061\n",
      "epoch 3; batch 12928; loss 0.040717\n",
      "epoch:3; batch 12928; train accuracy: 0.877127\n",
      "epoch 3; batch 13056; loss 0.064860\n",
      "epoch:3; batch 13056; train accuracy: 0.877189\n",
      "epoch 3; batch 13184; loss 0.107252\n",
      "epoch:3; batch 13184; train accuracy: 0.877237\n",
      "epoch 3; batch 13312; loss 0.157721\n",
      "epoch:3; batch 13312; train accuracy: 0.877262\n",
      "epoch 3; batch 13440; loss 0.143815\n",
      "epoch:3; batch 13440; train accuracy: 0.877301\n",
      "epoch 3; batch 13568; loss 0.146032\n",
      "epoch:3; batch 13568; train accuracy: 0.877327\n",
      "epoch 3; batch 13696; loss 0.244344\n",
      "epoch:3; batch 13696; train accuracy: 0.877348\n",
      "epoch 3; batch 13824; loss 0.110803\n",
      "epoch:3; batch 13824; train accuracy: 0.877405\n",
      "epoch 3; batch 13952; loss 0.093888\n",
      "epoch:3; batch 13952; train accuracy: 0.877452\n",
      "epoch 3; batch 14080; loss 0.148526\n",
      "epoch:3; batch 14080; train accuracy: 0.877478\n",
      "epoch 3; batch 14208; loss 0.170809\n",
      "epoch:3; batch 14208; train accuracy: 0.877503\n",
      "epoch 3; batch 14336; loss 0.171333\n",
      "epoch:3; batch 14336; train accuracy: 0.877546\n",
      "epoch 3; batch 14464; loss 0.134165\n",
      "epoch:3; batch 14464; train accuracy: 0.877589\n",
      "epoch 3; batch 14592; loss 0.134362\n",
      "epoch:3; batch 14592; train accuracy: 0.877628\n",
      "epoch 3; batch 14720; loss 0.134469\n",
      "epoch:3; batch 14720; train accuracy: 0.877666\n",
      "epoch 3; batch 14848; loss 0.226034\n",
      "epoch:3; batch 14848; train accuracy: 0.877696\n",
      "epoch 3; batch 14976; loss 0.121231\n",
      "epoch:3; batch 14976; train accuracy: 0.877743\n",
      "epoch 3; batch 15104; loss 0.077856\n",
      "epoch:3; batch 15104; train accuracy: 0.877809\n",
      "epoch 3; batch 15232; loss 0.071188\n",
      "epoch:3; batch 15232; train accuracy: 0.877856\n",
      "epoch 3; batch 15360; loss 0.149855\n",
      "epoch:3; batch 15360; train accuracy: 0.877894\n",
      "epoch 3; batch 15488; loss 0.153288\n",
      "epoch:3; batch 15488; train accuracy: 0.877928\n",
      "epoch 3; batch 15616; loss 0.154660\n",
      "epoch:3; batch 15616; train accuracy: 0.877949\n",
      "epoch 3; batch 15744; loss 0.079664\n",
      "epoch:3; batch 15744; train accuracy: 0.878004\n",
      "epoch 3; batch 15872; loss 0.122062\n",
      "epoch:3; batch 15872; train accuracy: 0.878043\n",
      "epoch 3; batch 16000; loss 0.050073\n",
      "epoch:3; batch 16000; train accuracy: 0.878107\n",
      "epoch 3; batch 16128; loss 0.119027\n",
      "epoch:3; batch 16128; train accuracy: 0.878150\n",
      "epoch 3; batch 16256; loss 0.124421\n",
      "epoch:3; batch 16256; train accuracy: 0.878192\n",
      "epoch 3; batch 16384; loss 0.227574\n",
      "epoch:3; batch 16384; train accuracy: 0.878213\n",
      "epoch 3; batch 16512; loss 0.134692\n",
      "epoch:3; batch 16512; train accuracy: 0.878259\n",
      "epoch 3; batch 16640; loss 0.131021\n",
      "epoch:3; batch 16640; train accuracy: 0.878315\n",
      "epoch 3; batch 16768; loss 0.106436\n",
      "epoch:3; batch 16768; train accuracy: 0.878353\n",
      "epoch 3; batch 16896; loss 0.080324\n",
      "epoch:3; batch 16896; train accuracy: 0.878404\n",
      "epoch 3; batch 17024; loss 0.047392\n",
      "epoch:3; batch 17024; train accuracy: 0.878459\n",
      "epoch 3; batch 17152; loss 0.155451\n",
      "epoch:3; batch 17152; train accuracy: 0.878488\n",
      "epoch 3; batch 17280; loss 0.159378\n",
      "epoch:3; batch 17280; train accuracy: 0.878526\n",
      "epoch 3; batch 17408; loss 0.064758\n",
      "epoch:3; batch 17408; train accuracy: 0.878576\n",
      "epoch 3; batch 17536; loss 0.077801\n",
      "epoch:3; batch 17536; train accuracy: 0.878636\n",
      "epoch 3; batch 17664; loss 0.071310\n",
      "epoch:3; batch 17664; train accuracy: 0.878687\n",
      "epoch 3; batch 17792; loss 0.095555\n",
      "epoch:3; batch 17792; train accuracy: 0.878737\n",
      "epoch 3; batch 17920; loss 0.070514\n",
      "epoch:3; batch 17920; train accuracy: 0.878792\n",
      "epoch 3; batch 18048; loss 0.164921\n",
      "epoch:3; batch 18048; train accuracy: 0.878829\n",
      "epoch 3; batch 18176; loss 0.074579\n",
      "epoch:3; batch 18176; train accuracy: 0.878884\n",
      "epoch 3; batch 18304; loss 0.150901\n",
      "epoch:3; batch 18304; train accuracy: 0.878922\n",
      "epoch 3; batch 18432; loss 0.091902\n",
      "epoch:3; batch 18432; train accuracy: 0.878963\n",
      "epoch 3; batch 18560; loss 0.128902\n",
      "epoch:3; batch 18560; train accuracy: 0.878996\n",
      "epoch 3; batch 18688; loss 0.026203\n",
      "epoch:3; batch 18688; train accuracy: 0.879064\n",
      "epoch 3; batch 18816; loss 0.103252\n",
      "epoch:3; batch 18816; train accuracy: 0.879110\n",
      "epoch 3; batch 18944; loss 0.118803\n",
      "epoch:3; batch 18944; train accuracy: 0.879155\n",
      "epoch 3; batch 19072; loss 0.057237\n",
      "epoch:3; batch 19072; train accuracy: 0.879210\n",
      "epoch 3; batch 19200; loss 0.080911\n",
      "epoch:3; batch 19200; train accuracy: 0.879255\n",
      "epoch 3; batch 19328; loss 0.219723\n",
      "epoch:3; batch 19328; train accuracy: 0.879279\n",
      "epoch 3; batch 19456; loss 0.040183\n",
      "epoch:3; batch 19456; train accuracy: 0.879329\n",
      "epoch 3; batch 19584; loss 0.075470\n",
      "epoch:3; batch 19584; train accuracy: 0.879379\n",
      "epoch 3; batch 19712; loss 0.067460\n",
      "epoch:3; batch 19712; train accuracy: 0.879433\n",
      "epoch 3; batch 19840; loss 0.046228\n",
      "epoch:3; batch 19840; train accuracy: 0.879487\n",
      "epoch 3; batch 19968; loss 0.124359\n",
      "epoch:3; batch 19968; train accuracy: 0.879524\n",
      "epoch 3; batch 20096; loss 0.061090\n",
      "epoch:3; batch 20096; train accuracy: 0.879582\n",
      "epoch 3; batch 20224; loss 0.068246\n",
      "epoch:3; batch 20224; train accuracy: 0.879632\n",
      "epoch 3; batch 20352; loss 0.049883\n",
      "epoch:3; batch 20352; train accuracy: 0.879690\n",
      "epoch 3; batch 20480; loss 0.085862\n",
      "epoch:3; batch 20480; train accuracy: 0.879731\n",
      "epoch 3; batch 20608; loss 0.087246\n",
      "epoch:3; batch 20608; train accuracy: 0.879772\n",
      "epoch 3; batch 20736; loss 0.155531\n",
      "epoch:3; batch 20736; train accuracy: 0.879812\n",
      "epoch 3; batch 20864; loss 0.135123\n",
      "epoch:3; batch 20864; train accuracy: 0.879857\n",
      "epoch 3; batch 20992; loss 0.088448\n",
      "epoch:3; batch 20992; train accuracy: 0.879907\n",
      "epoch 3; batch 21120; loss 0.105014\n",
      "epoch:3; batch 21120; train accuracy: 0.879943\n",
      "epoch 3; batch 21248; loss 0.048040\n",
      "epoch:3; batch 21248; train accuracy: 0.880001\n",
      "epoch 3; batch 21376; loss 0.095607\n",
      "epoch:3; batch 21376; train accuracy: 0.880054\n",
      "epoch 3; batch 21504; loss 0.073456\n",
      "epoch:3; batch 21504; train accuracy: 0.880112\n",
      "epoch 3; batch 21632; loss 0.108907\n",
      "epoch:3; batch 21632; train accuracy: 0.880157\n",
      "epoch 3; batch 21760; loss 0.054139\n",
      "epoch:3; batch 21760; train accuracy: 0.880218\n",
      "epoch 3; batch 21888; loss 0.073703\n",
      "epoch:3; batch 21888; train accuracy: 0.880267\n",
      "epoch 3; batch 22016; loss 0.080834\n",
      "epoch:3; batch 22016; train accuracy: 0.880320\n",
      "epoch 3; batch 22144; loss 0.067327\n",
      "epoch:3; batch 22144; train accuracy: 0.880365\n",
      "epoch 3; batch 22272; loss 0.045120\n",
      "epoch:3; batch 22272; train accuracy: 0.880414\n",
      "epoch 3; batch 22400; loss 0.082298\n",
      "epoch:3; batch 22400; train accuracy: 0.880458\n",
      "epoch 3; batch 22528; loss 0.092405\n",
      "epoch:3; batch 22528; train accuracy: 0.880507\n",
      "epoch 3; batch 22656; loss 0.096889\n",
      "epoch:3; batch 22656; train accuracy: 0.880555\n",
      "epoch 3; batch 22784; loss 0.103454\n",
      "epoch:3; batch 22784; train accuracy: 0.880604\n",
      "epoch 3; batch 22912; loss 0.025006\n",
      "epoch:3; batch 22912; train accuracy: 0.880665\n",
      "epoch 3; batch 23040; loss 0.153558\n",
      "epoch:3; batch 23040; train accuracy: 0.880709\n",
      "epoch 3; batch 23168; loss 0.142940\n",
      "epoch:3; batch 23168; train accuracy: 0.880762\n",
      "epoch 3; batch 23296; loss 0.087354\n",
      "epoch:3; batch 23296; train accuracy: 0.880806\n",
      "epoch 3; batch 23424; loss 0.089575\n",
      "epoch:3; batch 23424; train accuracy: 0.880850\n",
      "epoch 3; batch 23552; loss 0.132053\n",
      "epoch:3; batch 23552; train accuracy: 0.880881\n",
      "epoch 3; batch 23680; loss 0.059287\n",
      "epoch:3; batch 23680; train accuracy: 0.880938\n",
      "epoch 3; batch 23808; loss 0.113800\n",
      "epoch:3; batch 23808; train accuracy: 0.880973\n",
      "epoch 3; batch 23936; loss 0.115887\n",
      "epoch:3; batch 23936; train accuracy: 0.881012\n",
      "epoch 3; batch 24064; loss 0.050243\n",
      "epoch:3; batch 24064; train accuracy: 0.881077\n",
      "epoch 3; batch 24192; loss 0.063844\n",
      "epoch:3; batch 24192; train accuracy: 0.881134\n",
      "epoch 3; batch 24320; loss 0.093631\n",
      "epoch:3; batch 24320; train accuracy: 0.881186\n",
      "epoch 3; batch 24448; loss 0.076400\n",
      "epoch:3; batch 24448; train accuracy: 0.881234\n",
      "epoch 3; batch 24576; loss 0.049194\n",
      "epoch:3; batch 24576; train accuracy: 0.881290\n",
      "epoch 3; batch 24704; loss 0.100552\n",
      "epoch:3; batch 24704; train accuracy: 0.881334\n",
      "epoch 3; batch 24832; loss 0.049325\n",
      "epoch:3; batch 24832; train accuracy: 0.881390\n",
      "epoch 3; batch 24960; loss 0.170790\n",
      "epoch:3; batch 24960; train accuracy: 0.881420\n",
      "epoch 3; batch 25088; loss 0.081481\n",
      "epoch:3; batch 25088; train accuracy: 0.881468\n",
      "epoch 3; batch 25216; loss 0.063934\n",
      "epoch:3; batch 25216; train accuracy: 0.881524\n",
      "epoch 3; batch 25344; loss 0.135038\n",
      "epoch:3; batch 25344; train accuracy: 0.881567\n",
      "epoch 3; batch 25472; loss 0.054105\n",
      "epoch:3; batch 25472; train accuracy: 0.881623\n",
      "epoch 3; batch 25600; loss 0.097725\n",
      "epoch:3; batch 25600; train accuracy: 0.881658\n",
      "epoch 3; batch 25728; loss 0.051050\n",
      "epoch:3; batch 25728; train accuracy: 0.881713\n",
      "epoch 3; batch 25856; loss 0.060377\n",
      "epoch:3; batch 25856; train accuracy: 0.881765\n",
      "epoch 3; batch 25984; loss 0.118185\n",
      "epoch:3; batch 25984; train accuracy: 0.881808\n",
      "epoch 3; batch 26112; loss 0.114130\n",
      "epoch:3; batch 26112; train accuracy: 0.881847\n",
      "epoch 3; batch 26240; loss 0.066269\n",
      "epoch:3; batch 26240; train accuracy: 0.881889\n",
      "epoch 3; batch 26368; loss 0.043480\n",
      "epoch:3; batch 26368; train accuracy: 0.881945\n",
      "epoch 3; batch 26496; loss 0.030445\n",
      "epoch:3; batch 26496; train accuracy: 0.882005\n",
      "epoch 3; batch 26624; loss 0.035978\n",
      "epoch:3; batch 26624; train accuracy: 0.882060\n",
      "epoch 3; batch 26752; loss 0.052505\n",
      "epoch:3; batch 26752; train accuracy: 0.882111\n",
      "epoch 3; batch 26880; loss 0.210878\n",
      "epoch:3; batch 26880; train accuracy: 0.882137\n",
      "epoch 3; batch 27008; loss 0.208701\n",
      "epoch:3; batch 27008; train accuracy: 0.882179\n",
      "epoch 3; batch 27136; loss 0.163915\n",
      "epoch:3; batch 27136; train accuracy: 0.882213\n",
      "epoch 3; batch 27264; loss 0.084442\n",
      "epoch:3; batch 27264; train accuracy: 0.882256\n",
      "epoch 3; batch 27392; loss 0.039871\n",
      "epoch:3; batch 27392; train accuracy: 0.882315\n",
      "epoch 3; batch 27520; loss 0.130876\n",
      "epoch:3; batch 27520; train accuracy: 0.882353\n",
      "epoch 3; batch 27648; loss 0.111768\n",
      "epoch:3; batch 27648; train accuracy: 0.882387\n",
      "epoch 3; batch 27776; loss 0.087707\n",
      "epoch:3; batch 27776; train accuracy: 0.882438\n",
      "epoch 3; batch 27904; loss 0.042154\n",
      "epoch:3; batch 27904; train accuracy: 0.882493\n",
      "epoch 3; batch 28032; loss 0.071379\n",
      "epoch:3; batch 28032; train accuracy: 0.882552\n",
      "epoch 3; batch 28160; loss 0.078643\n",
      "epoch:3; batch 28160; train accuracy: 0.882598\n",
      "epoch 3; batch 28288; loss 0.122200\n",
      "epoch:3; batch 28288; train accuracy: 0.882632\n",
      "epoch 3; batch 28416; loss 0.099995\n",
      "epoch:3; batch 28416; train accuracy: 0.882678\n",
      "epoch 3; batch 28544; loss 0.081184\n",
      "epoch:3; batch 28544; train accuracy: 0.882724\n",
      "epoch 3; batch 28672; loss 0.160249\n",
      "epoch:3; batch 28672; train accuracy: 0.882758\n",
      "epoch 3; batch 28800; loss 0.085454\n",
      "epoch:3; batch 28800; train accuracy: 0.882808\n",
      "epoch 3; batch 28928; loss 0.042542\n",
      "epoch:3; batch 28928; train accuracy: 0.882854\n",
      "epoch 3; batch 29056; loss 0.062729\n",
      "epoch:3; batch 29056; train accuracy: 0.882913\n",
      "epoch 3; batch 29184; loss 0.106031\n",
      "epoch:3; batch 29184; train accuracy: 0.882951\n",
      "epoch 3; batch 29312; loss 0.051080\n",
      "epoch:3; batch 29312; train accuracy: 0.883001\n",
      "epoch 3; batch 29440; loss 0.146948\n",
      "epoch:3; batch 29440; train accuracy: 0.883038\n",
      "epoch 3; batch 29568; loss 0.139623\n",
      "epoch:3; batch 29568; train accuracy: 0.883080\n",
      "epoch 3; batch 29696; loss 0.096590\n",
      "epoch:3; batch 29696; train accuracy: 0.883121\n",
      "epoch 3; batch 29824; loss 0.126738\n",
      "epoch:3; batch 29824; train accuracy: 0.883167\n",
      "epoch 3; batch 29952; loss 0.055775\n",
      "epoch:3; batch 29952; train accuracy: 0.883221\n",
      "epoch 3; batch 30080; loss 0.082774\n",
      "epoch:3; batch 30080; train accuracy: 0.883275\n",
      "epoch 3; batch 30208; loss 0.027447\n",
      "epoch:3; batch 30208; train accuracy: 0.883329\n",
      "epoch 3; batch 30336; loss 0.065716\n",
      "epoch:3; batch 30336; train accuracy: 0.883383\n",
      "epoch 3; batch 30464; loss 0.064121\n",
      "epoch:3; batch 30464; train accuracy: 0.883428\n",
      "epoch 3; batch 30592; loss 0.117227\n",
      "epoch:3; batch 30592; train accuracy: 0.883465\n",
      "epoch 3; batch 30720; loss 0.127718\n",
      "epoch:3; batch 30720; train accuracy: 0.883506\n",
      "epoch 3; batch 30848; loss 0.083060\n",
      "epoch:3; batch 30848; train accuracy: 0.883552\n",
      "epoch 3; batch 30976; loss 0.086593\n",
      "epoch:3; batch 30976; train accuracy: 0.883593\n",
      "epoch 3; batch 31104; loss 0.087275\n",
      "epoch:3; batch 31104; train accuracy: 0.883638\n",
      "epoch 3; batch 31232; loss 0.128866\n",
      "epoch:3; batch 31232; train accuracy: 0.883667\n",
      "epoch 3; batch 31360; loss 0.052602\n",
      "epoch:3; batch 31360; train accuracy: 0.883720\n",
      "epoch 3; batch 31488; loss 0.045632\n",
      "epoch:3; batch 31488; train accuracy: 0.883774\n",
      "epoch 3; batch 31616; loss 0.040964\n",
      "epoch:3; batch 31616; train accuracy: 0.883831\n",
      "epoch 3; batch 31744; loss 0.101126\n",
      "epoch:3; batch 31744; train accuracy: 0.883876\n",
      "epoch 3; batch 31872; loss 0.052405\n",
      "epoch:3; batch 31872; train accuracy: 0.883929\n",
      "epoch 3; batch 32000; loss 0.060398\n",
      "epoch:3; batch 32000; train accuracy: 0.883978\n",
      "epoch 3; batch 32128; loss 0.077120\n",
      "epoch:3; batch 32128; train accuracy: 0.884023\n",
      "epoch 3; batch 32256; loss 0.026665\n",
      "epoch:3; batch 32256; train accuracy: 0.884080\n",
      "epoch 3; batch 32384; loss 0.093625\n",
      "epoch:3; batch 32384; train accuracy: 0.884125\n",
      "epoch 3; batch 32512; loss 0.047397\n",
      "epoch:3; batch 32512; train accuracy: 0.884178\n",
      "epoch 3; batch 32640; loss 0.111171\n",
      "epoch:3; batch 32640; train accuracy: 0.884227\n",
      "epoch 3; batch 32768; loss 0.017428\n",
      "epoch:3; batch 32768; train accuracy: 0.884288\n",
      "epoch 3; batch 32896; loss 0.125994\n",
      "epoch:3; batch 32896; train accuracy: 0.884324\n",
      "epoch 3; batch 33024; loss 0.134191\n",
      "epoch:3; batch 33024; train accuracy: 0.884364\n",
      "epoch 3; batch 33152; loss 0.093004\n",
      "epoch:3; batch 33152; train accuracy: 0.884405\n",
      "epoch 3; batch 33280; loss 0.101586\n",
      "epoch:3; batch 33280; train accuracy: 0.884445\n",
      "epoch 3; batch 33408; loss 0.077456\n",
      "epoch:3; batch 33408; train accuracy: 0.884493\n",
      "epoch 3; batch 33536; loss 0.080844\n",
      "epoch:3; batch 33536; train accuracy: 0.884538\n",
      "epoch 3; batch 33664; loss 0.090707\n",
      "epoch:3; batch 33664; train accuracy: 0.884586\n",
      "epoch 3; batch 33792; loss 0.074898\n",
      "epoch:3; batch 33792; train accuracy: 0.884634\n",
      "epoch 3; batch 33920; loss 0.094350\n",
      "epoch:3; batch 33920; train accuracy: 0.884670\n",
      "epoch 3; batch 34048; loss 0.053593\n",
      "epoch:3; batch 34048; train accuracy: 0.884723\n",
      "epoch 3; batch 34176; loss 0.030903\n",
      "epoch:3; batch 34176; train accuracy: 0.884779\n",
      "epoch 3; batch 34304; loss 0.041935\n",
      "epoch:3; batch 34304; train accuracy: 0.884831\n",
      "epoch 3; batch 34432; loss 0.080338\n",
      "epoch:3; batch 34432; train accuracy: 0.884875\n",
      "epoch 3; batch 34560; loss 0.072246\n",
      "epoch:3; batch 34560; train accuracy: 0.884923\n",
      "epoch 3; batch 34688; loss 0.104751\n",
      "epoch:3; batch 34688; train accuracy: 0.884971\n",
      "epoch 3; batch 34816; loss 0.070659\n",
      "epoch:3; batch 34816; train accuracy: 0.885019\n",
      "epoch 3; batch 34944; loss 0.082808\n",
      "epoch:3; batch 34944; train accuracy: 0.885059\n",
      "epoch 3; batch 35072; loss 0.046192\n",
      "epoch:3; batch 35072; train accuracy: 0.885106\n",
      "epoch 3; batch 35200; loss 0.064313\n",
      "epoch:3; batch 35200; train accuracy: 0.885158\n",
      "epoch 3; batch 35328; loss 0.079014\n",
      "epoch:3; batch 35328; train accuracy: 0.885206\n",
      "epoch 3; batch 35456; loss 0.036625\n",
      "epoch:3; batch 35456; train accuracy: 0.885258\n",
      "epoch 3; batch 35584; loss 0.052890\n",
      "epoch:3; batch 35584; train accuracy: 0.885301\n",
      "epoch 3; batch 35712; loss 0.026249\n",
      "epoch:3; batch 35712; train accuracy: 0.885361\n",
      "epoch 3; batch 35840; loss 0.048186\n",
      "epoch:3; batch 35840; train accuracy: 0.885417\n",
      "epoch 3; batch 35968; loss 0.093319\n",
      "epoch:3; batch 35968; train accuracy: 0.885456\n",
      "epoch 3; batch 36096; loss 0.170584\n",
      "epoch:3; batch 36096; train accuracy: 0.885491\n",
      "epoch 3; batch 36224; loss 0.092640\n",
      "epoch:3; batch 36224; train accuracy: 0.885534\n",
      "epoch 3; batch 36352; loss 0.044766\n",
      "epoch:3; batch 36352; train accuracy: 0.885586\n",
      "epoch 3; batch 36480; loss 0.167618\n",
      "epoch:3; batch 36480; train accuracy: 0.885621\n",
      "epoch 3; batch 36608; loss 0.057438\n",
      "epoch:3; batch 36608; train accuracy: 0.885668\n",
      "epoch 3; batch 36736; loss 0.029996\n",
      "epoch:3; batch 36736; train accuracy: 0.885719\n",
      "epoch 3; batch 36864; loss 0.224093\n",
      "epoch:3; batch 36864; train accuracy: 0.885742\n",
      "epoch 3; batch 36992; loss 0.056364\n",
      "epoch:3; batch 36992; train accuracy: 0.885789\n",
      "epoch 3; batch 37120; loss 0.139618\n",
      "epoch:3; batch 37120; train accuracy: 0.885828\n",
      "epoch 3; batch 37248; loss 0.049112\n",
      "epoch:3; batch 37248; train accuracy: 0.885883\n",
      "epoch 3; batch 37376; loss 0.095187\n",
      "epoch:3; batch 37376; train accuracy: 0.885918\n",
      "epoch 3; batch 37504; loss 0.020779\n",
      "epoch:3; batch 37504; train accuracy: 0.885977\n",
      "epoch 3; batch 37632; loss 0.162192\n",
      "epoch:3; batch 37632; train accuracy: 0.886016\n",
      "epoch 3; batch 37760; loss 0.084905\n",
      "epoch:3; batch 37760; train accuracy: 0.886063\n",
      "epoch 3; batch 37888; loss 0.056239\n",
      "epoch:3; batch 37888; train accuracy: 0.886113\n",
      "epoch 3; batch 38016; loss 0.080370\n",
      "epoch:3; batch 38016; train accuracy: 0.886152\n",
      "epoch 3; batch 38144; loss 0.065162\n",
      "epoch:3; batch 38144; train accuracy: 0.886203\n",
      "epoch 3; batch 38272; loss 0.030662\n",
      "epoch:3; batch 38272; train accuracy: 0.886257\n",
      "epoch 3; batch 38400; loss 0.122503\n",
      "epoch:3; batch 38400; train accuracy: 0.886292\n",
      "epoch 3; batch 38528; loss 0.081998\n",
      "epoch:3; batch 38528; train accuracy: 0.886342\n",
      "epoch 3; batch 38656; loss 0.065446\n",
      "epoch:3; batch 38656; train accuracy: 0.886389\n",
      "epoch 3; batch 38784; loss 0.066809\n",
      "epoch:3; batch 38784; train accuracy: 0.886435\n",
      "epoch 3; batch 38912; loss 0.097290\n",
      "epoch:3; batch 38912; train accuracy: 0.886474\n",
      "epoch 3; batch 39040; loss 0.027478\n",
      "epoch:3; batch 39040; train accuracy: 0.886532\n",
      "epoch 3; batch 39168; loss 0.016461\n",
      "epoch:3; batch 39168; train accuracy: 0.886590\n",
      "epoch 3; batch 39296; loss 0.037061\n",
      "epoch:3; batch 39296; train accuracy: 0.886645\n",
      "epoch 3; batch 39424; loss 0.077473\n",
      "epoch:3; batch 39424; train accuracy: 0.886687\n",
      "epoch 3; batch 39552; loss 0.030579\n",
      "epoch:3; batch 39552; train accuracy: 0.886745\n",
      "epoch 3; batch 39680; loss 0.017857\n",
      "epoch:3; batch 39680; train accuracy: 0.886799\n",
      "epoch 3; batch 39808; loss 0.129874\n",
      "epoch:3; batch 39808; train accuracy: 0.886833\n",
      "epoch 3; batch 39936; loss 0.097004\n",
      "epoch:3; batch 39936; train accuracy: 0.886875\n",
      "epoch 3; batch 40064; loss 0.074440\n",
      "epoch:3; batch 40064; train accuracy: 0.886917\n",
      "epoch 3; batch 40192; loss 0.036866\n",
      "epoch:3; batch 40192; train accuracy: 0.886971\n",
      "epoch 3; batch 40320; loss 0.049192\n",
      "epoch:3; batch 40320; train accuracy: 0.887020\n",
      "epoch 3; batch 40448; loss 0.080392\n",
      "epoch:3; batch 40448; train accuracy: 0.887058\n",
      "epoch 3; batch 40576; loss 0.051256\n",
      "epoch:3; batch 40576; train accuracy: 0.887100\n",
      "epoch 3; batch 40704; loss 0.054824\n",
      "epoch:3; batch 40704; train accuracy: 0.887150\n",
      "epoch 3; batch 40832; loss 0.082925\n",
      "epoch:3; batch 40832; train accuracy: 0.887195\n",
      "epoch 3; batch 40960; loss 0.034366\n",
      "epoch:3; batch 40960; train accuracy: 0.887245\n",
      "epoch 3; batch 41088; loss 0.035507\n",
      "epoch:3; batch 41088; train accuracy: 0.887298\n",
      "epoch 3; batch 41216; loss 0.036203\n",
      "epoch:3; batch 41216; train accuracy: 0.887352\n",
      "epoch 3; batch 41344; loss 0.075881\n",
      "epoch:3; batch 41344; train accuracy: 0.887389\n",
      "epoch 3; batch 41472; loss 0.031179\n",
      "epoch:3; batch 41472; train accuracy: 0.887443\n",
      "epoch 3; batch 41600; loss 0.085644\n",
      "epoch:3; batch 41600; train accuracy: 0.887480\n",
      "epoch 3; batch 41728; loss 0.043978\n",
      "epoch:3; batch 41728; train accuracy: 0.887533\n",
      "epoch 3; batch 41856; loss 0.062653\n",
      "epoch:3; batch 41856; train accuracy: 0.887583\n",
      "epoch 3; batch 41984; loss 0.042675\n",
      "epoch:3; batch 41984; train accuracy: 0.887628\n",
      "epoch 3; batch 42112; loss 0.051705\n",
      "epoch:3; batch 42112; train accuracy: 0.887677\n",
      "epoch 3; batch 42240; loss 0.103694\n",
      "epoch:3; batch 42240; train accuracy: 0.887718\n",
      "epoch 3; batch 42368; loss 0.042623\n",
      "epoch:3; batch 42368; train accuracy: 0.887767\n",
      "epoch 3; batch 42496; loss 0.081616\n",
      "epoch:3; batch 42496; train accuracy: 0.887808\n",
      "epoch 3; batch 42624; loss 0.061757\n",
      "epoch:3; batch 42624; train accuracy: 0.887857\n",
      "epoch 3; batch 42752; loss 0.088502\n",
      "epoch:3; batch 42752; train accuracy: 0.887898\n",
      "epoch 3; batch 42880; loss 0.045726\n",
      "epoch:3; batch 42880; train accuracy: 0.887947\n",
      "epoch 3; batch 43008; loss 0.032416\n",
      "epoch:3; batch 43008; train accuracy: 0.888000\n",
      "epoch 3; batch 43136; loss 0.075553\n",
      "epoch:3; batch 43136; train accuracy: 0.888045\n",
      "epoch 3; batch 43264; loss 0.096474\n",
      "epoch:3; batch 43264; train accuracy: 0.888085\n",
      "epoch 3; batch 43392; loss 0.084703\n",
      "epoch:3; batch 43392; train accuracy: 0.888118\n",
      "epoch 3; batch 43520; loss 0.075936\n",
      "epoch:3; batch 43520; train accuracy: 0.888159\n",
      "epoch 3; batch 43648; loss 0.026256\n",
      "epoch:3; batch 43648; train accuracy: 0.888211\n",
      "epoch 3; batch 43776; loss 0.145417\n",
      "epoch:3; batch 43776; train accuracy: 0.888248\n",
      "epoch 3; batch 43904; loss 0.134004\n",
      "epoch:3; batch 43904; train accuracy: 0.888289\n",
      "epoch 3; batch 44032; loss 0.068915\n",
      "epoch:3; batch 44032; train accuracy: 0.888325\n",
      "epoch 3; batch 44160; loss 0.068272\n",
      "epoch:3; batch 44160; train accuracy: 0.888370\n",
      "epoch 3; batch 44288; loss 0.051435\n",
      "epoch:3; batch 44288; train accuracy: 0.888418\n",
      "epoch 3; batch 44416; loss 0.107058\n",
      "epoch:3; batch 44416; train accuracy: 0.888451\n",
      "epoch 3; batch 44544; loss 0.088734\n",
      "epoch:3; batch 44544; train accuracy: 0.888491\n",
      "epoch 3; batch 44672; loss 0.028775\n",
      "epoch:3; batch 44672; train accuracy: 0.888547\n",
      "epoch 3; batch 44800; loss 0.028738\n",
      "epoch:3; batch 44800; train accuracy: 0.888599\n",
      "epoch 3; batch 44928; loss 0.086911\n",
      "epoch:3; batch 44928; train accuracy: 0.888643\n",
      "epoch 3; batch 45056; loss 0.031522\n",
      "epoch:3; batch 45056; train accuracy: 0.888691\n",
      "epoch 3; batch 45184; loss 0.015645\n",
      "epoch:3; batch 45184; train accuracy: 0.888747\n",
      "epoch 3; batch 45312; loss 0.033660\n",
      "epoch:3; batch 45312; train accuracy: 0.888795\n",
      "epoch 3; batch 45440; loss 0.062646\n",
      "epoch:3; batch 45440; train accuracy: 0.888839\n",
      "epoch 3; batch 45568; loss 0.058310\n",
      "epoch:3; batch 45568; train accuracy: 0.888875\n",
      "epoch 3; batch 45696; loss 0.050415\n",
      "epoch:3; batch 45696; train accuracy: 0.888915\n",
      "epoch 3; batch 45824; loss 0.061196\n",
      "epoch:3; batch 45824; train accuracy: 0.888955\n",
      "epoch 3; batch 45952; loss 0.061369\n",
      "epoch:3; batch 45952; train accuracy: 0.888995\n",
      "epoch 3; batch 46080; loss 0.035782\n",
      "epoch:3; batch 46080; train accuracy: 0.889051\n",
      "epoch 3; batch 46208; loss 0.048565\n",
      "epoch:3; batch 46208; train accuracy: 0.889102\n",
      "epoch 3; batch 46336; loss 0.033990\n",
      "epoch:3; batch 46336; train accuracy: 0.889150\n",
      "epoch 3; batch 46464; loss 0.094758\n",
      "epoch:3; batch 46464; train accuracy: 0.889186\n",
      "epoch 3; batch 46592; loss 0.057401\n",
      "epoch:3; batch 46592; train accuracy: 0.889229\n",
      "epoch 3; batch 46720; loss 0.053980\n",
      "epoch:3; batch 46720; train accuracy: 0.889281\n",
      "epoch 3; batch 46848; loss 0.043981\n",
      "epoch:3; batch 46848; train accuracy: 0.889328\n",
      "epoch 3; batch 46976; loss 0.061260\n",
      "epoch:3; batch 46976; train accuracy: 0.889375\n",
      "epoch 3; batch 47104; loss 0.030455\n",
      "epoch:3; batch 47104; train accuracy: 0.889427\n",
      "epoch 3; batch 47232; loss 0.016754\n",
      "epoch:3; batch 47232; train accuracy: 0.889482\n",
      "epoch 3; batch 47360; loss 0.094981\n",
      "epoch:3; batch 47360; train accuracy: 0.889521\n",
      "epoch 3; batch 47488; loss 0.081070\n",
      "epoch:3; batch 47488; train accuracy: 0.889564\n",
      "epoch 3; batch 47616; loss 0.044912\n",
      "epoch:3; batch 47616; train accuracy: 0.889612\n",
      "epoch 3; batch 47744; loss 0.107482\n",
      "epoch:3; batch 47744; train accuracy: 0.889643\n",
      "epoch 3; batch 47872; loss 0.043595\n",
      "epoch:3; batch 47872; train accuracy: 0.889690\n",
      "epoch 3; batch 48000; loss 0.042648\n",
      "epoch:3; batch 48000; train accuracy: 0.889737\n",
      "epoch 3; batch 48128; loss 0.025149\n",
      "epoch:3; batch 48128; train accuracy: 0.889788\n",
      "epoch 3; batch 48256; loss 0.020475\n",
      "epoch:3; batch 48256; train accuracy: 0.889839\n",
      "epoch 3; batch 48384; loss 0.095116\n",
      "epoch:3; batch 48384; train accuracy: 0.889878\n",
      "epoch 3; batch 48512; loss 0.111866\n",
      "epoch:3; batch 48512; train accuracy: 0.889913\n",
      "epoch 3; batch 48640; loss 0.050996\n",
      "epoch:3; batch 48640; train accuracy: 0.889948\n",
      "epoch 3; batch 48768; loss 0.063813\n",
      "epoch:3; batch 48768; train accuracy: 0.889991\n",
      "epoch 3; batch 48896; loss 0.074248\n",
      "epoch:3; batch 48896; train accuracy: 0.890026\n",
      "epoch 3; batch 49024; loss 0.081828\n",
      "epoch:3; batch 49024; train accuracy: 0.890061\n",
      "epoch 3; batch 49152; loss 0.075077\n",
      "epoch:3; batch 49152; train accuracy: 0.890100\n",
      "epoch 3; batch 49280; loss 0.054442\n",
      "epoch:3; batch 49280; train accuracy: 0.890139\n",
      "epoch 3; batch 49408; loss 0.085797\n",
      "epoch:3; batch 49408; train accuracy: 0.890182\n",
      "epoch 3; batch 49536; loss 0.052122\n",
      "epoch:3; batch 49536; train accuracy: 0.890232\n",
      "epoch 3; batch 49664; loss 0.035523\n",
      "epoch:3; batch 49664; train accuracy: 0.890278\n",
      "epoch 3; batch 49792; loss 0.067616\n",
      "epoch:3; batch 49792; train accuracy: 0.890321\n",
      "epoch 3; batch 49920; loss 0.058174\n",
      "epoch:3; batch 49920; train accuracy: 0.890363\n",
      "epoch 3; batch 50048; loss 0.043659\n",
      "epoch:3; batch 50048; train accuracy: 0.890406\n",
      "epoch 3; batch 50176; loss 0.131613\n",
      "epoch:3; batch 50176; train accuracy: 0.890440\n",
      "epoch 3; batch 50304; loss 0.053240\n",
      "epoch:3; batch 50304; train accuracy: 0.890487\n",
      "epoch 3; batch 50432; loss 0.051071\n",
      "epoch:3; batch 50432; train accuracy: 0.890529\n",
      "epoch 3; batch 50560; loss 0.037080\n",
      "epoch:3; batch 50560; train accuracy: 0.890575\n",
      "epoch 3; batch 50688; loss 0.047097\n",
      "epoch:3; batch 50688; train accuracy: 0.890617\n",
      "epoch 3; batch 50816; loss 0.063593\n",
      "epoch:3; batch 50816; train accuracy: 0.890667\n",
      "epoch 3; batch 50944; loss 0.112834\n",
      "epoch:3; batch 50944; train accuracy: 0.890709\n",
      "epoch 3; batch 51072; loss 0.041877\n",
      "epoch:3; batch 51072; train accuracy: 0.890751\n",
      "epoch 3; batch 51200; loss 0.055310\n",
      "epoch:3; batch 51200; train accuracy: 0.890794\n",
      "epoch 3; batch 51328; loss 0.057530\n",
      "epoch:3; batch 51328; train accuracy: 0.890839\n",
      "epoch 3; batch 51456; loss 0.110687\n",
      "epoch:3; batch 51456; train accuracy: 0.890874\n",
      "epoch 3; batch 51584; loss 0.058785\n",
      "epoch:3; batch 51584; train accuracy: 0.890916\n",
      "epoch 3; batch 51712; loss 0.035795\n",
      "epoch:3; batch 51712; train accuracy: 0.890961\n",
      "epoch 3; batch 51840; loss 0.021005\n",
      "epoch:3; batch 51840; train accuracy: 0.891011\n",
      "epoch 3; batch 51968; loss 0.130280\n",
      "epoch:3; batch 51968; train accuracy: 0.891045\n",
      "epoch 3; batch 52096; loss 0.062113\n",
      "epoch:3; batch 52096; train accuracy: 0.891091\n",
      "epoch 3; batch 52224; loss 0.025677\n",
      "epoch:3; batch 52224; train accuracy: 0.891140\n",
      "epoch 3; batch 52352; loss 0.046118\n",
      "epoch:3; batch 52352; train accuracy: 0.891185\n",
      "epoch 3; batch 52480; loss 0.063071\n",
      "epoch:3; batch 52480; train accuracy: 0.891223\n",
      "epoch 3; batch 52608; loss 0.025971\n",
      "epoch:3; batch 52608; train accuracy: 0.891269\n",
      "epoch 3; batch 52736; loss 0.073525\n",
      "epoch:3; batch 52736; train accuracy: 0.891307\n",
      "epoch 3; batch 52864; loss 0.079513\n",
      "epoch:3; batch 52864; train accuracy: 0.891348\n",
      "epoch 3; batch 52992; loss 0.053990\n",
      "epoch:3; batch 52992; train accuracy: 0.891393\n",
      "epoch 3; batch 53120; loss 0.036103\n",
      "epoch:3; batch 53120; train accuracy: 0.891442\n",
      "epoch 3; batch 53248; loss 0.077105\n",
      "epoch:3; batch 53248; train accuracy: 0.891476\n",
      "epoch 3; batch 53376; loss 0.025453\n",
      "epoch:3; batch 53376; train accuracy: 0.891529\n",
      "epoch 3; batch 53504; loss 0.082009\n",
      "epoch:3; batch 53504; train accuracy: 0.891570\n",
      "epoch 3; batch 53632; loss 0.035799\n",
      "epoch:3; batch 53632; train accuracy: 0.891615\n",
      "epoch 3; batch 53760; loss 0.076134\n",
      "epoch:3; batch 53760; train accuracy: 0.891653\n",
      "epoch 3; batch 53888; loss 0.099704\n",
      "epoch:3; batch 53888; train accuracy: 0.891686\n",
      "epoch 3; batch 54016; loss 0.093333\n",
      "epoch:3; batch 54016; train accuracy: 0.891720\n",
      "epoch 3; batch 54144; loss 0.099695\n",
      "epoch:3; batch 54144; train accuracy: 0.891753\n",
      "epoch 3; batch 54272; loss 0.032310\n",
      "epoch:3; batch 54272; train accuracy: 0.891802\n",
      "epoch 3; batch 54400; loss 0.092250\n",
      "epoch:3; batch 54400; train accuracy: 0.891839\n",
      "epoch 3; batch 54528; loss 0.050646\n",
      "epoch:3; batch 54528; train accuracy: 0.891884\n",
      "epoch 3; batch 54656; loss 0.096246\n",
      "epoch:3; batch 54656; train accuracy: 0.891921\n",
      "epoch 3; batch 54784; loss 0.080787\n",
      "epoch:3; batch 54784; train accuracy: 0.891959\n",
      "epoch 3; batch 54912; loss 0.039397\n",
      "epoch:3; batch 54912; train accuracy: 0.892007\n",
      "epoch 3; batch 55040; loss 0.036393\n",
      "epoch:3; batch 55040; train accuracy: 0.892055\n",
      "epoch 3; batch 55168; loss 0.054885\n",
      "epoch:3; batch 55168; train accuracy: 0.892100\n",
      "epoch 3; batch 55296; loss 0.075964\n",
      "epoch:3; batch 55296; train accuracy: 0.892129\n",
      "epoch 3; batch 55424; loss 0.041573\n",
      "epoch:3; batch 55424; train accuracy: 0.892178\n",
      "epoch 3; batch 55552; loss 0.034014\n",
      "epoch:3; batch 55552; train accuracy: 0.892222\n",
      "epoch 3; batch 55680; loss 0.026268\n",
      "epoch:3; batch 55680; train accuracy: 0.892270\n",
      "epoch 3; batch 55808; loss 0.017555\n",
      "epoch:3; batch 55808; train accuracy: 0.892322\n",
      "epoch 3; batch 55936; loss 0.044235\n",
      "epoch:3; batch 55936; train accuracy: 0.892367\n",
      "epoch 3; batch 56064; loss 0.077787\n",
      "epoch:3; batch 56064; train accuracy: 0.892407\n",
      "epoch 3; batch 56192; loss 0.067554\n",
      "epoch:3; batch 56192; train accuracy: 0.892451\n",
      "epoch 3; batch 56320; loss 0.031187\n",
      "epoch:3; batch 56320; train accuracy: 0.892499\n",
      "epoch 3; batch 56448; loss 0.037350\n",
      "epoch:3; batch 56448; train accuracy: 0.892543\n",
      "epoch 3; batch 56576; loss 0.135744\n",
      "epoch:3; batch 56576; train accuracy: 0.892576\n",
      "epoch 3; batch 56704; loss 0.084323\n",
      "epoch:3; batch 56704; train accuracy: 0.892620\n",
      "epoch 3; batch 56832; loss 0.042595\n",
      "epoch:3; batch 56832; train accuracy: 0.892661\n",
      "epoch 3; batch 56960; loss 0.054290\n",
      "epoch:3; batch 56960; train accuracy: 0.892705\n",
      "epoch 3; batch 57088; loss 0.054569\n",
      "epoch:3; batch 57088; train accuracy: 0.892749\n",
      "epoch 3; batch 57216; loss 0.044303\n",
      "epoch:3; batch 57216; train accuracy: 0.892796\n",
      "epoch 3; batch 57344; loss 0.047074\n",
      "epoch:3; batch 57344; train accuracy: 0.892833\n",
      "epoch 3; batch 57472; loss 0.032370\n",
      "epoch:3; batch 57472; train accuracy: 0.892880\n",
      "epoch 3; batch 57600; loss 0.032599\n",
      "epoch:3; batch 57600; train accuracy: 0.892928\n",
      "epoch 3; batch 57728; loss 0.058733\n",
      "epoch:3; batch 57728; train accuracy: 0.892968\n",
      "epoch 3; batch 57856; loss 0.061448\n",
      "epoch:3; batch 57856; train accuracy: 0.893004\n",
      "epoch 3; batch 57984; loss 0.049785\n",
      "epoch:3; batch 57984; train accuracy: 0.893048\n",
      "epoch 3; batch 58112; loss 0.056400\n",
      "epoch:3; batch 58112; train accuracy: 0.893084\n",
      "epoch 3; batch 58240; loss 0.042043\n",
      "epoch:3; batch 58240; train accuracy: 0.893127\n",
      "epoch 3; batch 58368; loss 0.024476\n",
      "epoch:3; batch 58368; train accuracy: 0.893178\n",
      "epoch 3; batch 58496; loss 0.086993\n",
      "epoch:3; batch 58496; train accuracy: 0.893225\n",
      "epoch 3; batch 58624; loss 0.009278\n",
      "epoch:3; batch 58624; train accuracy: 0.893276\n",
      "epoch 3; batch 58752; loss 0.080832\n",
      "epoch:3; batch 58752; train accuracy: 0.893309\n",
      "epoch 3; batch 58880; loss 0.116812\n",
      "epoch:3; batch 58880; train accuracy: 0.893348\n",
      "epoch 3; batch 59008; loss 0.035233\n",
      "epoch:3; batch 59008; train accuracy: 0.893388\n",
      "epoch 3; batch 59136; loss 0.040483\n",
      "epoch:3; batch 59136; train accuracy: 0.893427\n",
      "epoch 3; batch 59264; loss 0.020298\n",
      "epoch:3; batch 59264; train accuracy: 0.893478\n",
      "epoch 3; batch 59392; loss 0.086610\n",
      "epoch:3; batch 59392; train accuracy: 0.893518\n",
      "epoch 3; batch 59520; loss 0.031811\n",
      "epoch:3; batch 59520; train accuracy: 0.893564\n",
      "epoch 3; batch 59648; loss 0.176070\n",
      "epoch:3; batch 59648; train accuracy: 0.893589\n",
      "epoch 3; batch 59776; loss 0.052988\n",
      "epoch:3; batch 59776; train accuracy: 0.893628\n",
      "epoch 3; batch 59904; loss 0.072257\n",
      "epoch:3; batch 59904; train accuracy: 0.893668\n",
      "epoch 3; batch 60032; loss 0.115875\n",
      "epoch:3; batch 60032; train accuracy: 0.893700\n",
      "epoch 3; batch 60160; loss 0.048858\n",
      "epoch:3; batch 60160; train accuracy: 0.893735\n",
      "epoch 3; batch 60288; loss 0.136290\n",
      "epoch:3; batch 60288; train accuracy: 0.893771\n",
      "epoch 3; batch 60416; loss 0.034263\n",
      "epoch:3; batch 60416; train accuracy: 0.893814\n",
      "epoch 3; batch 60544; loss 0.087664\n",
      "epoch:3; batch 60544; train accuracy: 0.893853\n",
      "epoch 3; batch 60672; loss 0.089762\n",
      "epoch:3; batch 60672; train accuracy: 0.893888\n",
      "epoch 3; batch 60800; loss 0.102849\n",
      "epoch:3; batch 60800; train accuracy: 0.893916\n",
      "epoch 3; batch 60928; loss 0.042408\n",
      "epoch:3; batch 60928; train accuracy: 0.893959\n",
      "epoch 3; batch 61056; loss 0.045530\n",
      "epoch:3; batch 61056; train accuracy: 0.894002\n",
      "epoch 3; batch 61184; loss 0.021631\n",
      "epoch:3; batch 61184; train accuracy: 0.894052\n",
      "epoch 3; batch 61312; loss 0.045899\n",
      "epoch:3; batch 61312; train accuracy: 0.894091\n",
      "epoch 3; batch 61440; loss 0.026761\n",
      "epoch:3; batch 61440; train accuracy: 0.894137\n",
      "epoch 3; batch 61568; loss 0.094273\n",
      "epoch:3; batch 61568; train accuracy: 0.894172\n",
      "epoch 3; batch 61696; loss 0.046088\n",
      "epoch:3; batch 61696; train accuracy: 0.894215\n",
      "epoch 3; batch 61824; loss 0.020422\n",
      "epoch:3; batch 61824; train accuracy: 0.894261\n",
      "epoch 3; batch 61952; loss 0.011816\n",
      "epoch:3; batch 61952; train accuracy: 0.894311\n",
      "epoch 3; batch 62080; loss 0.127749\n",
      "epoch:3; batch 62080; train accuracy: 0.894346\n",
      "epoch 3; batch 62208; loss 0.027270\n",
      "epoch:3; batch 62208; train accuracy: 0.894392\n",
      "epoch 3; batch 62336; loss 0.058308\n",
      "epoch:3; batch 62336; train accuracy: 0.894430\n",
      "epoch 3; batch 62464; loss 0.050369\n",
      "epoch:3; batch 62464; train accuracy: 0.894476\n",
      "epoch 3; batch 62592; loss 0.020277\n",
      "epoch:3; batch 62592; train accuracy: 0.894522\n",
      "epoch 3; batch 62720; loss 0.174026\n",
      "epoch:3; batch 62720; train accuracy: 0.894553\n",
      "epoch 3; batch 62848; loss 0.043346\n",
      "epoch:3; batch 62848; train accuracy: 0.894592\n",
      "epoch 3; batch 62976; loss 0.077844\n",
      "epoch:3; batch 62976; train accuracy: 0.894630\n",
      "epoch 3; batch 63104; loss 0.147661\n",
      "epoch:3; batch 63104; train accuracy: 0.894654\n",
      "epoch 3; batch 63232; loss 0.027573\n",
      "epoch:3; batch 63232; train accuracy: 0.894696\n",
      "epoch 3; batch 63360; loss 0.028603\n",
      "epoch:3; batch 63360; train accuracy: 0.894738\n",
      "epoch 3; batch 63488; loss 0.062889\n",
      "epoch:3; batch 63488; train accuracy: 0.894773\n",
      "epoch 3; batch 63616; loss 0.019083\n",
      "epoch:3; batch 63616; train accuracy: 0.894818\n",
      "epoch 3; batch 63744; loss 0.084830\n",
      "epoch:3; batch 63744; train accuracy: 0.894856\n",
      "epoch 3; batch 63872; loss 0.048656\n",
      "epoch:3; batch 63872; train accuracy: 0.894898\n",
      "epoch 3; batch 64000; loss 0.088423\n",
      "epoch:3; batch 64000; train accuracy: 0.894929\n",
      "epoch 3; batch 64128; loss 0.135772\n",
      "epoch:3; batch 64128; train accuracy: 0.894967\n",
      "epoch 3; batch 64256; loss 0.052733\n",
      "epoch:3; batch 64256; train accuracy: 0.895009\n",
      "epoch 3; batch 64384; loss 0.048970\n",
      "epoch:3; batch 64384; train accuracy: 0.895051\n",
      "epoch 3; batch 64512; loss 0.032349\n",
      "epoch:3; batch 64512; train accuracy: 0.895092\n",
      "epoch 3; batch 64640; loss 0.068379\n",
      "epoch:3; batch 64640; train accuracy: 0.895130\n",
      "epoch 3; batch 64768; loss 0.098634\n",
      "epoch:3; batch 64768; train accuracy: 0.895165\n",
      "epoch 3; batch 64896; loss 0.029194\n",
      "epoch:3; batch 64896; train accuracy: 0.895210\n",
      "epoch 3; batch 65024; loss 0.026494\n",
      "epoch:3; batch 65024; train accuracy: 0.895259\n",
      "epoch 3; batch 65152; loss 0.042094\n",
      "epoch:3; batch 65152; train accuracy: 0.895300\n",
      "epoch 3; batch 65280; loss 0.066733\n",
      "epoch:3; batch 65280; train accuracy: 0.895334\n",
      "epoch 3; batch 65408; loss 0.028396\n",
      "epoch:3; batch 65408; train accuracy: 0.895376\n",
      "epoch 3; batch 65536; loss 0.095283\n",
      "epoch:3; batch 65536; train accuracy: 0.895417\n",
      "epoch 3; batch 65664; loss 0.199125\n",
      "epoch:3; batch 65664; train accuracy: 0.895444\n",
      "epoch 3; batch 65792; loss 0.101496\n",
      "epoch:3; batch 65792; train accuracy: 0.895471\n",
      "epoch 3; batch 65920; loss 0.078514\n",
      "epoch:3; batch 65920; train accuracy: 0.895505\n",
      "epoch 3; batch 66048; loss 0.062035\n",
      "epoch:3; batch 66048; train accuracy: 0.895546\n",
      "epoch 3; batch 66176; loss 0.023457\n",
      "epoch:3; batch 66176; train accuracy: 0.895594\n",
      "epoch 3; batch 66304; loss 0.046589\n",
      "epoch:3; batch 66304; train accuracy: 0.895635\n",
      "epoch 3; batch 66432; loss 0.067703\n",
      "epoch:3; batch 66432; train accuracy: 0.895666\n",
      "epoch 3; batch 66560; loss 0.010425\n",
      "epoch:3; batch 66560; train accuracy: 0.895714\n",
      "epoch 3; batch 66688; loss 0.035481\n",
      "epoch:3; batch 66688; train accuracy: 0.895755\n",
      "epoch 3; batch 66816; loss 0.035322\n",
      "epoch:3; batch 66816; train accuracy: 0.895800\n",
      "epoch 3; batch 66944; loss 0.055077\n",
      "epoch:3; batch 66944; train accuracy: 0.895833\n",
      "epoch 3; batch 67072; loss 0.045292\n",
      "epoch:3; batch 67072; train accuracy: 0.895874\n",
      "epoch 3; batch 67200; loss 0.113428\n",
      "epoch:3; batch 67200; train accuracy: 0.895908\n",
      "epoch 3; batch 67328; loss 0.021903\n",
      "epoch:3; batch 67328; train accuracy: 0.895952\n",
      "epoch 3; batch 67456; loss 0.025901\n",
      "epoch:3; batch 67456; train accuracy: 0.895997\n",
      "epoch 3; batch 67584; loss 0.057195\n",
      "epoch:3; batch 67584; train accuracy: 0.896038\n",
      "epoch 3; batch 67712; loss 0.033851\n",
      "epoch:3; batch 67712; train accuracy: 0.896082\n",
      "epoch 3; batch 67840; loss 0.033585\n",
      "epoch:3; batch 67840; train accuracy: 0.896126\n",
      "epoch 3; batch 67968; loss 0.060742\n",
      "epoch:3; batch 67968; train accuracy: 0.896152\n",
      "epoch 3; batch 68096; loss 0.066065\n",
      "epoch:3; batch 68096; train accuracy: 0.896193\n",
      "epoch 3; batch 68224; loss 0.049064\n",
      "epoch:3; batch 68224; train accuracy: 0.896230\n",
      "epoch 3; batch 68352; loss 0.051059\n",
      "epoch:3; batch 68352; train accuracy: 0.896271\n",
      "epoch 3; batch 68480; loss 0.063036\n",
      "epoch:3; batch 68480; train accuracy: 0.896307\n",
      "epoch 3; batch 68608; loss 0.028001\n",
      "epoch:3; batch 68608; train accuracy: 0.896352\n",
      "epoch 3; batch 68736; loss 0.044986\n",
      "epoch:3; batch 68736; train accuracy: 0.896392\n",
      "epoch 3; batch 68864; loss 0.018182\n",
      "epoch:3; batch 68864; train accuracy: 0.896436\n",
      "epoch 3; batch 68992; loss 0.053088\n",
      "epoch:3; batch 68992; train accuracy: 0.896476\n",
      "epoch 3; batch 69120; loss 0.055837\n",
      "epoch:3; batch 69120; train accuracy: 0.896513\n",
      "epoch 3; batch 69248; loss 0.033971\n",
      "epoch:3; batch 69248; train accuracy: 0.896553\n",
      "epoch 3; batch 69376; loss 0.039983\n",
      "epoch:3; batch 69376; train accuracy: 0.896594\n",
      "epoch 3; batch 69504; loss 0.043990\n",
      "epoch:3; batch 69504; train accuracy: 0.896634\n",
      "epoch 3; batch 69632; loss 0.179229\n",
      "epoch:3; batch 69632; train accuracy: 0.896667\n",
      "epoch 3; batch 69760; loss 0.172094\n",
      "epoch:3; batch 69760; train accuracy: 0.896685\n",
      "epoch 3; batch 69888; loss 0.071131\n",
      "epoch:3; batch 69888; train accuracy: 0.896722\n",
      "epoch 3; batch 70016; loss 0.059939\n",
      "epoch:3; batch 70016; train accuracy: 0.896759\n",
      "epoch 3; batch 70144; loss 0.030321\n",
      "epoch:3; batch 70144; train accuracy: 0.896802\n",
      "epoch 3; batch 70272; loss 0.031542\n",
      "epoch:3; batch 70272; train accuracy: 0.896842\n",
      "epoch 3; batch 70400; loss 0.079256\n",
      "epoch:3; batch 70400; train accuracy: 0.896871\n",
      "epoch 3; batch 70528; loss 0.055482\n",
      "epoch:3; batch 70528; train accuracy: 0.896908\n",
      "epoch 3; batch 70656; loss 0.020681\n",
      "epoch:3; batch 70656; train accuracy: 0.896951\n",
      "epoch 3; batch 70784; loss 0.029054\n",
      "epoch:3; batch 70784; train accuracy: 0.896995\n",
      "epoch 3; batch 70912; loss 0.029421\n",
      "epoch:3; batch 70912; train accuracy: 0.897035\n",
      "epoch 3; batch 71040; loss 0.073389\n",
      "epoch:3; batch 71040; train accuracy: 0.897067\n",
      "epoch 3; batch 71168; loss 0.026749\n",
      "epoch:3; batch 71168; train accuracy: 0.897107\n",
      "epoch 3; batch 71296; loss 0.047976\n",
      "epoch:3; batch 71296; train accuracy: 0.897143\n",
      "epoch 3; batch 71424; loss 0.023078\n",
      "epoch:3; batch 71424; train accuracy: 0.897186\n",
      "epoch 3; batch 71552; loss 0.068829\n",
      "epoch:3; batch 71552; train accuracy: 0.897222\n",
      "epoch 3; batch 71680; loss 0.099299\n",
      "epoch:3; batch 71680; train accuracy: 0.897251\n",
      "epoch 3; batch 71808; loss 0.041680\n",
      "epoch:3; batch 71808; train accuracy: 0.897295\n",
      "epoch 3; batch 71936; loss 0.012177\n",
      "epoch:3; batch 71936; train accuracy: 0.897341\n",
      "epoch 3; batch 72064; loss 0.124206\n",
      "epoch:3; batch 72064; train accuracy: 0.897377\n",
      "epoch 3; batch 72192; loss 0.051396\n",
      "epoch:3; batch 72192; train accuracy: 0.897413\n",
      "epoch 3; batch 72320; loss 0.056772\n",
      "epoch:3; batch 72320; train accuracy: 0.897449\n",
      "epoch 3; batch 72448; loss 0.019548\n",
      "epoch:3; batch 72448; train accuracy: 0.897492\n",
      "epoch 3; batch 72576; loss 0.040336\n",
      "epoch:3; batch 72576; train accuracy: 0.897528\n",
      "epoch 3; batch 72704; loss 0.080892\n",
      "epoch:3; batch 72704; train accuracy: 0.897560\n",
      "epoch 3; batch 72832; loss 0.140512\n",
      "epoch:3; batch 72832; train accuracy: 0.897585\n",
      "epoch 3; batch 72960; loss 0.148322\n",
      "epoch:3; batch 72960; train accuracy: 0.897603\n",
      "epoch 3; batch 73088; loss 0.052214\n",
      "epoch:3; batch 73088; train accuracy: 0.897642\n",
      "epoch 3; batch 73216; loss 0.040085\n",
      "epoch:3; batch 73216; train accuracy: 0.897682\n",
      "epoch 3; batch 73344; loss 0.038461\n",
      "epoch:3; batch 73344; train accuracy: 0.897717\n",
      "epoch 3; batch 73472; loss 0.040003\n",
      "epoch:3; batch 73472; train accuracy: 0.897760\n",
      "epoch 3; batch 73600; loss 0.095783\n",
      "epoch:3; batch 73600; train accuracy: 0.897789\n",
      "epoch 3; batch 73728; loss 0.015638\n",
      "epoch:3; batch 73728; train accuracy: 0.897835\n",
      "epoch 3; batch 73856; loss 0.112422\n",
      "epoch:3; batch 73856; train accuracy: 0.897867\n",
      "epoch 3; batch 73984; loss 0.035393\n",
      "epoch:3; batch 73984; train accuracy: 0.897906\n",
      "epoch 3; batch 74112; loss 0.052489\n",
      "epoch:3; batch 74112; train accuracy: 0.897941\n",
      "epoch 3; batch 74240; loss 0.043039\n",
      "epoch:3; batch 74240; train accuracy: 0.897980\n",
      "epoch 3; batch 74368; loss 0.089776\n",
      "epoch:3; batch 74368; train accuracy: 0.898012\n",
      "epoch 3; batch 74496; loss 0.079679\n",
      "epoch:3; batch 74496; train accuracy: 0.898040\n",
      "epoch 3; batch 74624; loss 0.067355\n",
      "epoch:3; batch 74624; train accuracy: 0.898076\n",
      "epoch 3; batch 74752; loss 0.139517\n",
      "epoch:3; batch 74752; train accuracy: 0.898100\n",
      "epoch 3; batch 74880; loss 0.022104\n",
      "epoch:3; batch 74880; train accuracy: 0.898143\n",
      "epoch 3; batch 75008; loss 0.017453\n",
      "epoch:3; batch 75008; train accuracy: 0.898188\n",
      "epoch 3; batch 75136; loss 0.060638\n",
      "epoch:3; batch 75136; train accuracy: 0.898227\n",
      "epoch 3; batch 75264; loss 0.068288\n",
      "epoch:3; batch 75264; train accuracy: 0.898266\n",
      "epoch 3; batch 75392; loss 0.017715\n",
      "epoch:3; batch 75392; train accuracy: 0.898311\n",
      "epoch 3; batch 75520; loss 0.026056\n",
      "epoch:3; batch 75520; train accuracy: 0.898353\n",
      "epoch 3; batch 75648; loss 0.106684\n",
      "epoch:3; batch 75648; train accuracy: 0.898385\n",
      "epoch 3; batch 75776; loss 0.061021\n",
      "epoch:3; batch 75776; train accuracy: 0.898417\n",
      "epoch 3; batch 75904; loss 0.026220\n",
      "epoch:3; batch 75904; train accuracy: 0.898455\n",
      "epoch 3; batch 76032; loss 0.036325\n",
      "epoch:3; batch 76032; train accuracy: 0.898497\n",
      "epoch 3; batch 76160; loss 0.024117\n",
      "epoch:3; batch 76160; train accuracy: 0.898539\n",
      "epoch 3; batch 76288; loss 0.051944\n",
      "epoch:3; batch 76288; train accuracy: 0.898577\n",
      "epoch 3; batch 76416; loss 0.053827\n",
      "epoch:3; batch 76416; train accuracy: 0.898619\n",
      "epoch 3; batch 76544; loss 0.020536\n",
      "epoch:3; batch 76544; train accuracy: 0.898661\n",
      "epoch 3; batch 76672; loss 0.019775\n",
      "epoch:3; batch 76672; train accuracy: 0.898706\n",
      "epoch 3; batch 76800; loss 0.013709\n",
      "epoch:3; batch 76800; train accuracy: 0.898751\n",
      "epoch 3; batch 76928; loss 0.050302\n",
      "epoch:3; batch 76928; train accuracy: 0.898786\n",
      "epoch 3; batch 77056; loss 0.064495\n",
      "epoch:3; batch 77056; train accuracy: 0.898824\n",
      "epoch 3; batch 77184; loss 0.036592\n",
      "epoch:3; batch 77184; train accuracy: 0.898859\n",
      "epoch 3; batch 77312; loss 0.057320\n",
      "epoch:3; batch 77312; train accuracy: 0.898897\n",
      "epoch 3; batch 77440; loss 0.020242\n",
      "epoch:3; batch 77440; train accuracy: 0.898939\n",
      "epoch 3; batch 77568; loss 0.016465\n",
      "epoch:3; batch 77568; train accuracy: 0.898980\n",
      "epoch 3; batch 77696; loss 0.052997\n",
      "epoch:3; batch 77696; train accuracy: 0.899018\n",
      "epoch 3; batch 77824; loss 0.010323\n",
      "epoch:3; batch 77824; train accuracy: 0.899063\n",
      "epoch 3; batch 77952; loss 0.132407\n",
      "epoch:3; batch 77952; train accuracy: 0.899087\n",
      "epoch 3; batch 78080; loss 0.066772\n",
      "epoch:3; batch 78080; train accuracy: 0.899118\n",
      "epoch 3; batch 78208; loss 0.035662\n",
      "epoch:3; batch 78208; train accuracy: 0.899156\n",
      "epoch 3; batch 78336; loss 0.021949\n",
      "epoch:3; batch 78336; train accuracy: 0.899194\n",
      "epoch 3; batch 78464; loss 0.026683\n",
      "epoch:3; batch 78464; train accuracy: 0.899232\n",
      "epoch 3; batch 78592; loss 0.072018\n",
      "epoch:3; batch 78592; train accuracy: 0.899259\n",
      "epoch 3; batch 78720; loss 0.027197\n",
      "epoch:3; batch 78720; train accuracy: 0.899300\n",
      "epoch 3; batch 78848; loss 0.043859\n",
      "epoch:3; batch 78848; train accuracy: 0.899334\n",
      "epoch 3; batch 78976; loss 0.054529\n",
      "epoch:3; batch 78976; train accuracy: 0.899369\n",
      "epoch 3; batch 79104; loss 0.018142\n",
      "epoch:3; batch 79104; train accuracy: 0.899410\n",
      "epoch 3; batch 79232; loss 0.051781\n",
      "epoch:3; batch 79232; train accuracy: 0.899444\n",
      "epoch 3; batch 79360; loss 0.041155\n",
      "epoch:3; batch 79360; train accuracy: 0.899475\n",
      "epoch 3; batch 79488; loss 0.030191\n",
      "epoch:3; batch 79488; train accuracy: 0.899512\n",
      "epoch 3; batch 79616; loss 0.048694\n",
      "epoch:3; batch 79616; train accuracy: 0.899550\n",
      "epoch 3; batch 79744; loss 0.061757\n",
      "epoch:3; batch 79744; train accuracy: 0.899591\n",
      "epoch 3; batch 79872; loss 0.098805\n",
      "epoch:3; batch 79872; train accuracy: 0.899625\n",
      "epoch 3; batch 80000; loss 0.012127\n",
      "epoch:3; batch 80000; train accuracy: 0.899669\n",
      "epoch 3; batch 80128; loss 0.018639\n",
      "epoch:3; batch 80128; train accuracy: 0.899713\n",
      "epoch 3; batch 80256; loss 0.057631\n",
      "epoch:3; batch 80256; train accuracy: 0.899744\n",
      "epoch 3; batch 80384; loss 0.025328\n",
      "epoch:3; batch 80384; train accuracy: 0.899781\n",
      "epoch 3; batch 80512; loss 0.049684\n",
      "epoch:3; batch 80512; train accuracy: 0.899815\n",
      "epoch 3; batch 80640; loss 0.051611\n",
      "epoch:3; batch 80640; train accuracy: 0.899849\n",
      "epoch 3; batch 80768; loss 0.022145\n",
      "epoch:3; batch 80768; train accuracy: 0.899886\n",
      "epoch 3; batch 80896; loss 0.043038\n",
      "epoch:3; batch 80896; train accuracy: 0.899923\n",
      "epoch 3; batch 81024; loss 0.064410\n",
      "epoch:3; batch 81024; train accuracy: 0.899957\n",
      "epoch 3; batch 81152; loss 0.027184\n",
      "epoch:3; batch 81152; train accuracy: 0.899994\n",
      "epoch 3; batch 81280; loss 0.069794\n",
      "epoch:3; batch 81280; train accuracy: 0.900031\n",
      "epoch 3; batch 81408; loss 0.025339\n",
      "epoch:3; batch 81408; train accuracy: 0.900071\n",
      "epoch 3; batch 81536; loss 0.041321\n",
      "epoch:3; batch 81536; train accuracy: 0.900105\n",
      "epoch 3; batch 81664; loss 0.031222\n",
      "epoch:3; batch 81664; train accuracy: 0.900145\n",
      "epoch 3; batch 81792; loss 0.005159\n",
      "epoch:3; batch 81792; train accuracy: 0.900189\n",
      "epoch 3; batch 81920; loss 0.017275\n",
      "epoch:3; batch 81920; train accuracy: 0.900233\n",
      "epoch 3; batch 82048; loss 0.006677\n",
      "epoch:3; batch 82048; train accuracy: 0.900277\n",
      "epoch 3; batch 82176; loss 0.010006\n",
      "epoch:3; batch 82176; train accuracy: 0.900320\n",
      "epoch 3; batch 82304; loss 0.043723\n",
      "epoch:3; batch 82304; train accuracy: 0.900357\n",
      "epoch 3; batch 82432; loss 0.070727\n",
      "epoch:3; batch 82432; train accuracy: 0.900391\n",
      "epoch 3; batch 82560; loss 0.085330\n",
      "epoch:3; batch 82560; train accuracy: 0.900421\n",
      "epoch 3; batch 82688; loss 0.028814\n",
      "epoch:3; batch 82688; train accuracy: 0.900461\n",
      "epoch 3; batch 82816; loss 0.064504\n",
      "epoch:3; batch 82816; train accuracy: 0.900494\n",
      "epoch 3; batch 82944; loss 0.041134\n",
      "epoch:3; batch 82944; train accuracy: 0.900531\n",
      "epoch 3; batch 83072; loss 0.026446\n",
      "epoch:3; batch 83072; train accuracy: 0.900567\n",
      "epoch 3; batch 83200; loss 0.047805\n",
      "epoch:3; batch 83200; train accuracy: 0.900600\n",
      "epoch 3; batch 83328; loss 0.017387\n",
      "epoch:3; batch 83328; train accuracy: 0.900640\n",
      "epoch 3; batch 83456; loss 0.061838\n",
      "epoch:3; batch 83456; train accuracy: 0.900674\n",
      "epoch 3; batch 83584; loss 0.021198\n",
      "epoch:3; batch 83584; train accuracy: 0.900710\n",
      "epoch 3; batch 83712; loss 0.055002\n",
      "epoch:3; batch 83712; train accuracy: 0.900747\n",
      "epoch 3; batch 83840; loss 0.032988\n",
      "epoch:3; batch 83840; train accuracy: 0.900783\n",
      "epoch 3; batch 83968; loss 0.037143\n",
      "epoch:3; batch 83968; train accuracy: 0.900819\n",
      "epoch 3; batch 84096; loss 0.027315\n",
      "epoch:3; batch 84096; train accuracy: 0.900856\n",
      "epoch 3; batch 84224; loss 0.013831\n",
      "epoch:3; batch 84224; train accuracy: 0.900899\n",
      "epoch 3; batch 84352; loss 0.019828\n",
      "epoch:3; batch 84352; train accuracy: 0.900935\n",
      "epoch 3; batch 84480; loss 0.049827\n",
      "epoch:3; batch 84480; train accuracy: 0.900968\n",
      "epoch 3; batch 84608; loss 0.022216\n",
      "epoch:3; batch 84608; train accuracy: 0.901008\n",
      "epoch 3; batch 84736; loss 0.010952\n",
      "epoch:3; batch 84736; train accuracy: 0.901047\n",
      "epoch 3; batch 84864; loss 0.077478\n",
      "epoch:3; batch 84864; train accuracy: 0.901087\n",
      "epoch 3; batch 84992; loss 0.076466\n",
      "epoch:3; batch 84992; train accuracy: 0.901123\n",
      "epoch 3; batch 85120; loss 0.010812\n",
      "epoch:3; batch 85120; train accuracy: 0.901163\n",
      "epoch 3; batch 85248; loss 0.051762\n",
      "epoch:3; batch 85248; train accuracy: 0.901195\n",
      "epoch 3; batch 85376; loss 0.056940\n",
      "epoch:3; batch 85376; train accuracy: 0.901225\n",
      "epoch 3; batch 85504; loss 0.124337\n",
      "epoch:3; batch 85504; train accuracy: 0.901261\n",
      "epoch 3; batch 85632; loss 0.020523\n",
      "epoch:3; batch 85632; train accuracy: 0.901300\n",
      "epoch 3; batch 85760; loss 0.029358\n",
      "epoch:3; batch 85760; train accuracy: 0.901336\n",
      "epoch 3; batch 85888; loss 0.081778\n",
      "epoch:3; batch 85888; train accuracy: 0.901362\n",
      "epoch 3; batch 86016; loss 0.027901\n",
      "epoch:3; batch 86016; train accuracy: 0.901401\n",
      "epoch 3; batch 86144; loss 0.022653\n",
      "epoch:3; batch 86144; train accuracy: 0.901440\n",
      "epoch 3; batch 86272; loss 0.041253\n",
      "epoch:3; batch 86272; train accuracy: 0.901479\n",
      "epoch 3; batch 86400; loss 0.071717\n",
      "epoch:3; batch 86400; train accuracy: 0.901515\n",
      "epoch 3; batch 86528; loss 0.043921\n",
      "epoch:3; batch 86528; train accuracy: 0.901554\n",
      "epoch 3; batch 86656; loss 0.025956\n",
      "epoch:3; batch 86656; train accuracy: 0.901594\n",
      "epoch 3; batch 86784; loss 0.031134\n",
      "epoch:3; batch 86784; train accuracy: 0.901629\n",
      "epoch 3; batch 86912; loss 0.020453\n",
      "epoch:3; batch 86912; train accuracy: 0.901668\n",
      "epoch 3; batch 87040; loss 0.041864\n",
      "epoch:3; batch 87040; train accuracy: 0.901704\n",
      "epoch 3; batch 87168; loss 0.035924\n",
      "epoch:3; batch 87168; train accuracy: 0.901740\n",
      "epoch 3; batch 87296; loss 0.055478\n",
      "epoch:3; batch 87296; train accuracy: 0.901775\n",
      "epoch 3; batch 87424; loss 0.031643\n",
      "epoch:3; batch 87424; train accuracy: 0.901814\n",
      "epoch 3; batch 87552; loss 0.013330\n",
      "epoch:3; batch 87552; train accuracy: 0.901856\n",
      "epoch 3; batch 87680; loss 0.067398\n",
      "epoch:3; batch 87680; train accuracy: 0.901895\n",
      "epoch 3; batch 87808; loss 0.106048\n",
      "epoch:3; batch 87808; train accuracy: 0.901931\n",
      "epoch 3; batch 87936; loss 0.030143\n",
      "epoch:3; batch 87936; train accuracy: 0.901969\n",
      "epoch 3; batch 88064; loss 0.031187\n",
      "epoch:3; batch 88064; train accuracy: 0.902005\n",
      "epoch 3; batch 88192; loss 0.026337\n",
      "epoch:3; batch 88192; train accuracy: 0.902040\n",
      "epoch 3; batch 88320; loss 0.055262\n",
      "epoch:3; batch 88320; train accuracy: 0.902079\n",
      "epoch 3; batch 88448; loss 0.040014\n",
      "epoch:3; batch 88448; train accuracy: 0.902111\n",
      "epoch 3; batch 88576; loss 0.105428\n",
      "epoch:3; batch 88576; train accuracy: 0.902139\n",
      "epoch 3; batch 88704; loss 0.011699\n",
      "epoch:3; batch 88704; train accuracy: 0.902181\n",
      "epoch 3; batch 88832; loss 0.034525\n",
      "epoch:3; batch 88832; train accuracy: 0.902217\n",
      "epoch 3; batch 88960; loss 0.014724\n",
      "epoch:3; batch 88960; train accuracy: 0.902255\n",
      "epoch 3; batch 89088; loss 0.021573\n",
      "epoch:3; batch 89088; train accuracy: 0.902294\n",
      "epoch 3; batch 89216; loss 0.023460\n",
      "epoch:3; batch 89216; train accuracy: 0.902329\n",
      "epoch 3; batch 89344; loss 0.026164\n",
      "epoch:3; batch 89344; train accuracy: 0.902367\n",
      "epoch 3; batch 89472; loss 0.048287\n",
      "epoch:3; batch 89472; train accuracy: 0.902399\n",
      "epoch 3; batch 89600; loss 0.071766\n",
      "epoch:3; batch 89600; train accuracy: 0.902431\n",
      "epoch 3; batch 89728; loss 0.004149\n",
      "epoch:3; batch 89728; train accuracy: 0.902472\n",
      "epoch 3; batch 89856; loss 0.066928\n",
      "epoch:3; batch 89856; train accuracy: 0.902507\n",
      "epoch 3; batch 89984; loss 0.032619\n",
      "epoch:3; batch 89984; train accuracy: 0.902545\n",
      "epoch 3; batch 90112; loss 0.050492\n",
      "epoch:3; batch 90112; train accuracy: 0.902580\n",
      "epoch 3; batch 90240; loss 0.071383\n",
      "epoch:3; batch 90240; train accuracy: 0.902609\n",
      "epoch 3; batch 90368; loss 0.023244\n",
      "epoch:3; batch 90368; train accuracy: 0.902647\n",
      "epoch 3; batch 90496; loss 0.029471\n",
      "epoch:3; batch 90496; train accuracy: 0.902685\n",
      "epoch 3; batch 90624; loss 0.012940\n",
      "epoch:3; batch 90624; train accuracy: 0.902726\n",
      "epoch 3; batch 90752; loss 0.043668\n",
      "epoch:3; batch 90752; train accuracy: 0.902761\n",
      "epoch 3; batch 90880; loss 0.008081\n",
      "epoch:3; batch 90880; train accuracy: 0.902803\n",
      "epoch 3; batch 91008; loss 0.008400\n",
      "epoch:3; batch 91008; train accuracy: 0.902844\n",
      "epoch 3; batch 91136; loss 0.018402\n",
      "epoch:3; batch 91136; train accuracy: 0.902882\n",
      "epoch 3; batch 91264; loss 0.028044\n",
      "epoch:3; batch 91264; train accuracy: 0.902920\n",
      "epoch 3; batch 91392; loss 0.061940\n",
      "epoch:3; batch 91392; train accuracy: 0.902944\n",
      "epoch 3; batch 91520; loss 0.009610\n",
      "epoch:3; batch 91520; train accuracy: 0.902986\n",
      "epoch 3; batch 91648; loss 0.004863\n",
      "epoch:3; batch 91648; train accuracy: 0.903027\n",
      "epoch 3; batch 91776; loss 0.023578\n",
      "epoch:3; batch 91776; train accuracy: 0.903061\n",
      "epoch 3; batch 91904; loss 0.023993\n",
      "epoch:3; batch 91904; train accuracy: 0.903099\n",
      "epoch 3; batch 92032; loss 0.051042\n",
      "epoch:3; batch 92032; train accuracy: 0.903130\n",
      "epoch 3; batch 92160; loss 0.054280\n",
      "epoch:3; batch 92160; train accuracy: 0.903161\n",
      "epoch 3; batch 92288; loss 0.023467\n",
      "epoch:3; batch 92288; train accuracy: 0.903199\n",
      "epoch 3; batch 92416; loss 0.022047\n",
      "epoch:3; batch 92416; train accuracy: 0.903233\n",
      "epoch 3; batch 92544; loss 0.022654\n",
      "epoch:3; batch 92544; train accuracy: 0.903271\n",
      "epoch 3; batch 92672; loss 0.029035\n",
      "epoch:3; batch 92672; train accuracy: 0.903309\n",
      "epoch 3; batch 92800; loss 0.012705\n",
      "epoch:3; batch 92800; train accuracy: 0.903346\n",
      "epoch 3; batch 92928; loss 0.045895\n",
      "epoch:3; batch 92928; train accuracy: 0.903381\n",
      "epoch 3; batch 93056; loss 0.015537\n",
      "epoch:3; batch 93056; train accuracy: 0.903421\n",
      "epoch 3; batch 93184; loss 0.088420\n",
      "epoch:3; batch 93184; train accuracy: 0.903449\n",
      "epoch 3; batch 93312; loss 0.030417\n",
      "epoch:3; batch 93312; train accuracy: 0.903483\n",
      "epoch 3; batch 93440; loss 0.011554\n",
      "epoch:3; batch 93440; train accuracy: 0.903524\n",
      "epoch 3; batch 93568; loss 0.046923\n",
      "epoch:3; batch 93568; train accuracy: 0.903558\n",
      "epoch 3; batch 93696; loss 0.019633\n",
      "epoch:3; batch 93696; train accuracy: 0.903595\n",
      "epoch 3; batch 93824; loss 0.172740\n",
      "epoch:3; batch 93824; train accuracy: 0.903629\n",
      "epoch 3; batch 93952; loss 0.024199\n",
      "epoch:3; batch 93952; train accuracy: 0.903667\n",
      "epoch 3; batch 94080; loss 0.029792\n",
      "epoch:3; batch 94080; train accuracy: 0.903704\n",
      "epoch 3; batch 94208; loss 0.004083\n",
      "epoch:3; batch 94208; train accuracy: 0.903744\n",
      "epoch 3; batch 94336; loss 0.021203\n",
      "epoch:3; batch 94336; train accuracy: 0.903782\n",
      "epoch 3; batch 94464; loss 0.050245\n",
      "epoch:3; batch 94464; train accuracy: 0.903812\n",
      "epoch 3; batch 94592; loss 0.012143\n",
      "epoch:3; batch 94592; train accuracy: 0.903849\n",
      "epoch 3; batch 94720; loss 0.062570\n",
      "epoch:3; batch 94720; train accuracy: 0.903887\n",
      "epoch 3; batch 94848; loss 0.144136\n",
      "epoch:3; batch 94848; train accuracy: 0.903917\n",
      "epoch 3; batch 94976; loss 0.072269\n",
      "epoch:3; batch 94976; train accuracy: 0.903948\n",
      "epoch 3; batch 95104; loss 0.020455\n",
      "epoch:3; batch 95104; train accuracy: 0.903985\n",
      "epoch 3; batch 95232; loss 0.012802\n",
      "epoch:3; batch 95232; train accuracy: 0.904022\n",
      "epoch 3; batch 95360; loss 0.014651\n",
      "epoch:3; batch 95360; train accuracy: 0.904062\n",
      "epoch 3; batch 95488; loss 0.008146\n",
      "epoch:3; batch 95488; train accuracy: 0.904102\n",
      "epoch 3; batch 95616; loss 0.028157\n",
      "epoch:3; batch 95616; train accuracy: 0.904136\n",
      "epoch 3; batch 95744; loss 0.018510\n",
      "epoch:3; batch 95744; train accuracy: 0.904176\n",
      "epoch 3; batch 95872; loss 0.021126\n",
      "epoch:3; batch 95872; train accuracy: 0.904213\n",
      "epoch 3; batch 96000; loss 0.014478\n",
      "epoch:3; batch 96000; train accuracy: 0.904253\n",
      "epoch 3; batch 96128; loss 0.051373\n",
      "epoch:3; batch 96128; train accuracy: 0.904283\n",
      "epoch 3; batch 96256; loss 0.012701\n",
      "epoch:3; batch 96256; train accuracy: 0.904323\n",
      "epoch 3; batch 96384; loss 0.014792\n",
      "epoch:3; batch 96384; train accuracy: 0.904360\n",
      "epoch 3; batch 96512; loss 0.045283\n",
      "epoch:3; batch 96512; train accuracy: 0.904393\n",
      "epoch 3; batch 96640; loss 0.072818\n",
      "epoch:3; batch 96640; train accuracy: 0.904427\n",
      "epoch 3; batch 96768; loss 0.073421\n",
      "epoch:3; batch 96768; train accuracy: 0.904453\n",
      "epoch 3; batch 96896; loss 0.013249\n",
      "epoch:3; batch 96896; train accuracy: 0.904490\n",
      "epoch 3; batch 97024; loss 0.006132\n",
      "epoch:3; batch 97024; train accuracy: 0.904530\n",
      "epoch 3; batch 97152; loss 0.035453\n",
      "epoch:3; batch 97152; train accuracy: 0.904566\n",
      "epoch 3; batch 97280; loss 0.010141\n",
      "epoch:3; batch 97280; train accuracy: 0.904606\n",
      "epoch 3; batch 97408; loss 0.034966\n",
      "epoch:3; batch 97408; train accuracy: 0.904639\n",
      "epoch 3; batch 97536; loss 0.013874\n",
      "epoch:3; batch 97536; train accuracy: 0.904676\n",
      "epoch 3; batch 97664; loss 0.025895\n",
      "epoch:3; batch 97664; train accuracy: 0.904712\n",
      "epoch 3; batch 97792; loss 0.038903\n",
      "epoch:3; batch 97792; train accuracy: 0.904742\n",
      "epoch 3; batch 97920; loss 0.006559\n",
      "epoch:3; batch 97920; train accuracy: 0.904782\n",
      "epoch 3; batch 98048; loss 0.058007\n",
      "epoch:3; batch 98048; train accuracy: 0.904815\n",
      "epoch 3; batch 98176; loss 0.005636\n",
      "epoch:3; batch 98176; train accuracy: 0.904854\n",
      "epoch 3; batch 98304; loss 0.027016\n",
      "epoch:3; batch 98304; train accuracy: 0.904891\n",
      "epoch 3; batch 98432; loss 0.059642\n",
      "epoch:3; batch 98432; train accuracy: 0.904927\n",
      "epoch 3; batch 98560; loss 0.016631\n",
      "epoch:3; batch 98560; train accuracy: 0.904963\n",
      "epoch 3; batch 98688; loss 0.011435\n",
      "epoch:3; batch 98688; train accuracy: 0.905002\n",
      "epoch 3; batch 98816; loss 0.057142\n",
      "epoch:3; batch 98816; train accuracy: 0.905032\n",
      "epoch 3; batch 98944; loss 0.041429\n",
      "epoch:3; batch 98944; train accuracy: 0.905065\n",
      "epoch 3; batch 99072; loss 0.027624\n",
      "epoch:3; batch 99072; train accuracy: 0.905098\n",
      "epoch 3; batch 99200; loss 0.014825\n",
      "epoch:3; batch 99200; train accuracy: 0.905134\n",
      "epoch 3; batch 99328; loss 0.025005\n",
      "epoch:3; batch 99328; train accuracy: 0.905170\n",
      "epoch 3; batch 99456; loss 0.224073\n",
      "epoch:3; batch 99456; train accuracy: 0.905190\n",
      "epoch 3; batch 99584; loss 0.057450\n",
      "epoch:3; batch 99584; train accuracy: 0.905226\n",
      "epoch 3; batch 99712; loss 0.017121\n",
      "epoch:3; batch 99712; train accuracy: 0.905262\n",
      "epoch 3; batch 99840; loss 0.048122\n",
      "epoch:3; batch 99840; train accuracy: 0.905291\n",
      "epoch 3; batch 99968; loss 0.093835\n",
      "epoch:3; batch 99968; train accuracy: 0.905324\n",
      "epoch 3; batch 100096; loss 0.024541\n",
      "epoch:3; batch 100096; train accuracy: 0.905360\n",
      "epoch 3; batch 100224; loss 0.079415\n",
      "epoch:3; batch 100224; train accuracy: 0.905392\n",
      "epoch 3; batch 100352; loss 0.017735\n",
      "epoch:3; batch 100352; train accuracy: 0.905428\n",
      "epoch 3; batch 100480; loss 0.007647\n",
      "epoch:3; batch 100480; train accuracy: 0.905467\n",
      "epoch 3; batch 100608; loss 0.046078\n",
      "epoch:3; batch 100608; train accuracy: 0.905496\n",
      "epoch 3; batch 100736; loss 0.060130\n",
      "epoch:3; batch 100736; train accuracy: 0.905523\n",
      "epoch 3; batch 100864; loss 0.060682\n",
      "epoch:3; batch 100864; train accuracy: 0.905558\n",
      "epoch 3; batch 100992; loss 0.007663\n",
      "epoch:3; batch 100992; train accuracy: 0.905597\n",
      "epoch 3; batch 101120; loss 0.028278\n",
      "epoch:3; batch 101120; train accuracy: 0.905633\n",
      "epoch 3; batch 101248; loss 0.034854\n",
      "epoch:3; batch 101248; train accuracy: 0.905665\n",
      "epoch 3; batch 101376; loss 0.094773\n",
      "epoch:3; batch 101376; train accuracy: 0.905691\n",
      "epoch 3; batch 101504; loss 0.070518\n",
      "epoch:3; batch 101504; train accuracy: 0.905727\n",
      "epoch 3; batch 101632; loss 0.058070\n",
      "epoch:3; batch 101632; train accuracy: 0.905756\n",
      "epoch 3; batch 101760; loss 0.035546\n",
      "epoch:3; batch 101760; train accuracy: 0.905788\n",
      "epoch 3; batch 101888; loss 0.012672\n",
      "epoch:3; batch 101888; train accuracy: 0.905827\n",
      "epoch 3; batch 102016; loss 0.022459\n",
      "epoch:3; batch 102016; train accuracy: 0.905865\n",
      "epoch 3; batch 102144; loss 0.056371\n",
      "epoch:3; batch 102144; train accuracy: 0.905894\n",
      "epoch 3; batch 102272; loss 0.071955\n",
      "epoch:3; batch 102272; train accuracy: 0.905920\n",
      "epoch 3; batch 102400; loss 0.056399\n",
      "epoch:3; batch 102400; train accuracy: 0.905949\n",
      "epoch 3; batch 102528; loss 0.012310\n",
      "epoch:3; batch 102528; train accuracy: 0.905988\n",
      "epoch 3; batch 102656; loss 0.078797\n",
      "epoch:3; batch 102656; train accuracy: 0.906016\n",
      "epoch 3; batch 102784; loss 0.060697\n",
      "epoch:3; batch 102784; train accuracy: 0.906045\n",
      "epoch 3; batch 102912; loss 0.015104\n",
      "epoch:3; batch 102912; train accuracy: 0.906084\n",
      "epoch 3; batch 103040; loss 0.007479\n",
      "epoch:3; batch 103040; train accuracy: 0.906122\n",
      "epoch 3; batch 103168; loss 0.014597\n",
      "epoch:3; batch 103168; train accuracy: 0.906157\n",
      "epoch 3; batch 103296; loss 0.042381\n",
      "epoch:3; batch 103296; train accuracy: 0.906186\n",
      "epoch 3; batch 103424; loss 0.007342\n",
      "epoch:3; batch 103424; train accuracy: 0.906224\n",
      "epoch 3; batch 103552; loss 0.029523\n",
      "epoch:3; batch 103552; train accuracy: 0.906256\n",
      "epoch 3; batch 103680; loss 0.033140\n",
      "epoch:3; batch 103680; train accuracy: 0.906288\n",
      "epoch 3; batch 103808; loss 0.031942\n",
      "epoch:3; batch 103808; train accuracy: 0.906320\n",
      "epoch 3; batch 103936; loss 0.043923\n",
      "epoch:3; batch 103936; train accuracy: 0.906355\n",
      "epoch 3; batch 104064; loss 0.021104\n",
      "epoch:3; batch 104064; train accuracy: 0.906390\n",
      "epoch 3; batch 104192; loss 0.047327\n",
      "epoch:3; batch 104192; train accuracy: 0.906422\n",
      "epoch 3; batch 104320; loss 0.008717\n",
      "epoch:3; batch 104320; train accuracy: 0.906460\n",
      "epoch 3; batch 104448; loss 0.013738\n",
      "epoch:3; batch 104448; train accuracy: 0.906498\n",
      "epoch 3; batch 104576; loss 0.037564\n",
      "epoch:3; batch 104576; train accuracy: 0.906527\n",
      "epoch 3; batch 104704; loss 0.028332\n",
      "epoch:3; batch 104704; train accuracy: 0.906558\n",
      "epoch 3; batch 104832; loss 0.061370\n",
      "epoch:3; batch 104832; train accuracy: 0.906587\n",
      "epoch 3; batch 104960; loss 0.015308\n",
      "epoch:3; batch 104960; train accuracy: 0.906625\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 59104 ; rate: 0.563110\n",
      "y_true_label_1_num: 3642 ; rate: 0.034699\n",
      "y_true_label_2_num: 3067 ; rate: 0.029221\n",
      "y_true_label_3_num: 39147 ; rate: 0.372971\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.989891\n",
      "valid avg_precision: 0.990347\n",
      "valid avg_recall: 0.989539\n",
      "valid avg_f1: 0.989907\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 8549 ; rate: 0.570847\n",
      "y_true_label_1_num: 497 ; rate: 0.033186\n",
      "y_true_label_2_num: 425 ; rate: 0.028379\n",
      "y_true_label_3_num: 5505 ; rate: 0.367588\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.850227\n",
      "valid avg_precision: 0.848001\n",
      "valid avg_recall: 0.848892\n",
      "valid avg_f1: 0.847943\n",
      "epoch 4\n",
      "epoch 4; batch 128; loss 0.065768\n",
      "epoch:4; batch 128; train accuracy: 0.906644\n",
      "epoch 4; batch 256; loss 0.018189\n",
      "epoch:4; batch 256; train accuracy: 0.906678\n",
      "epoch 4; batch 384; loss 0.019841\n",
      "epoch:4; batch 384; train accuracy: 0.906716\n",
      "epoch 4; batch 512; loss 0.034138\n",
      "epoch:4; batch 512; train accuracy: 0.906748\n",
      "epoch 4; batch 640; loss 0.014295\n",
      "epoch:4; batch 640; train accuracy: 0.906782\n",
      "epoch 4; batch 768; loss 0.028408\n",
      "epoch:4; batch 768; train accuracy: 0.906814\n",
      "epoch 4; batch 896; loss 0.085551\n",
      "epoch:4; batch 896; train accuracy: 0.906839\n",
      "epoch 4; batch 1024; loss 0.027953\n",
      "epoch:4; batch 1024; train accuracy: 0.906874\n",
      "epoch 4; batch 1152; loss 0.030967\n",
      "epoch:4; batch 1152; train accuracy: 0.906905\n",
      "epoch 4; batch 1280; loss 0.035041\n",
      "epoch:4; batch 1280; train accuracy: 0.906936\n",
      "epoch 4; batch 1408; loss 0.019386\n",
      "epoch:4; batch 1408; train accuracy: 0.906971\n",
      "epoch 4; batch 1536; loss 0.047121\n",
      "epoch:4; batch 1536; train accuracy: 0.907002\n",
      "epoch 4; batch 1664; loss 0.034458\n",
      "epoch:4; batch 1664; train accuracy: 0.907037\n",
      "epoch 4; batch 1792; loss 0.069407\n",
      "epoch:4; batch 1792; train accuracy: 0.907065\n",
      "epoch 4; batch 1920; loss 0.081171\n",
      "epoch:4; batch 1920; train accuracy: 0.907096\n",
      "epoch 4; batch 2048; loss 0.146009\n",
      "epoch:4; batch 2048; train accuracy: 0.907124\n",
      "epoch 4; batch 2176; loss 0.031732\n",
      "epoch:4; batch 2176; train accuracy: 0.907155\n",
      "epoch 4; batch 2304; loss 0.020776\n",
      "epoch:4; batch 2304; train accuracy: 0.907189\n",
      "epoch 4; batch 2432; loss 0.012836\n",
      "epoch:4; batch 2432; train accuracy: 0.907227\n",
      "epoch 4; batch 2560; loss 0.009655\n",
      "epoch:4; batch 2560; train accuracy: 0.907264\n",
      "epoch 4; batch 2688; loss 0.024093\n",
      "epoch:4; batch 2688; train accuracy: 0.907295\n",
      "epoch 4; batch 2816; loss 0.010090\n",
      "epoch:4; batch 2816; train accuracy: 0.907333\n",
      "epoch 4; batch 2944; loss 0.019056\n",
      "epoch:4; batch 2944; train accuracy: 0.907367\n",
      "epoch 4; batch 3072; loss 0.009323\n",
      "epoch:4; batch 3072; train accuracy: 0.907404\n",
      "epoch 4; batch 3200; loss 0.033649\n",
      "epoch:4; batch 3200; train accuracy: 0.907438\n",
      "epoch 4; batch 3328; loss 0.018343\n",
      "epoch:4; batch 3328; train accuracy: 0.907472\n",
      "epoch 4; batch 3456; loss 0.025894\n",
      "epoch:4; batch 3456; train accuracy: 0.907503\n",
      "epoch 4; batch 3584; loss 0.013218\n",
      "epoch:4; batch 3584; train accuracy: 0.907541\n",
      "epoch 4; batch 3712; loss 0.053100\n",
      "epoch:4; batch 3712; train accuracy: 0.907568\n",
      "epoch 4; batch 3840; loss 0.008781\n",
      "epoch:4; batch 3840; train accuracy: 0.907605\n",
      "epoch 4; batch 3968; loss 0.034178\n",
      "epoch:4; batch 3968; train accuracy: 0.907636\n",
      "epoch 4; batch 4096; loss 0.019638\n",
      "epoch:4; batch 4096; train accuracy: 0.907670\n",
      "epoch 4; batch 4224; loss 0.018063\n",
      "epoch:4; batch 4224; train accuracy: 0.907704\n",
      "epoch 4; batch 4352; loss 0.014916\n",
      "epoch:4; batch 4352; train accuracy: 0.907741\n",
      "epoch 4; batch 4480; loss 0.019379\n",
      "epoch:4; batch 4480; train accuracy: 0.907775\n",
      "epoch 4; batch 4608; loss 0.033218\n",
      "epoch:4; batch 4608; train accuracy: 0.907806\n",
      "epoch 4; batch 4736; loss 0.008359\n",
      "epoch:4; batch 4736; train accuracy: 0.907843\n",
      "epoch 4; batch 4864; loss 0.012370\n",
      "epoch:4; batch 4864; train accuracy: 0.907876\n",
      "epoch 4; batch 4992; loss 0.022653\n",
      "epoch:4; batch 4992; train accuracy: 0.907910\n",
      "epoch 4; batch 5120; loss 0.011548\n",
      "epoch:4; batch 5120; train accuracy: 0.907947\n",
      "epoch 4; batch 5248; loss 0.005728\n",
      "epoch:4; batch 5248; train accuracy: 0.907984\n",
      "epoch 4; batch 5376; loss 0.004385\n",
      "epoch:4; batch 5376; train accuracy: 0.908020\n",
      "epoch 4; batch 5504; loss 0.010231\n",
      "epoch:4; batch 5504; train accuracy: 0.908054\n",
      "epoch 4; batch 5632; loss 0.008350\n",
      "epoch:4; batch 5632; train accuracy: 0.908091\n",
      "epoch 4; batch 5760; loss 0.074148\n",
      "epoch:4; batch 5760; train accuracy: 0.908118\n",
      "epoch 4; batch 5888; loss 0.009712\n",
      "epoch:4; batch 5888; train accuracy: 0.908155\n",
      "epoch 4; batch 6016; loss 0.044449\n",
      "epoch:4; batch 6016; train accuracy: 0.908185\n",
      "epoch 4; batch 6144; loss 0.003082\n",
      "epoch:4; batch 6144; train accuracy: 0.908222\n",
      "epoch 4; batch 6272; loss 0.019475\n",
      "epoch:4; batch 6272; train accuracy: 0.908255\n",
      "epoch 4; batch 6400; loss 0.022113\n",
      "epoch:4; batch 6400; train accuracy: 0.908289\n",
      "epoch 4; batch 6528; loss 0.065539\n",
      "epoch:4; batch 6528; train accuracy: 0.908316\n",
      "epoch 4; batch 6656; loss 0.009935\n",
      "epoch:4; batch 6656; train accuracy: 0.908352\n",
      "epoch 4; batch 6784; loss 0.138018\n",
      "epoch:4; batch 6784; train accuracy: 0.908373\n",
      "epoch 4; batch 6912; loss 0.025323\n",
      "epoch:4; batch 6912; train accuracy: 0.908404\n",
      "epoch 4; batch 7040; loss 0.021259\n",
      "epoch:4; batch 7040; train accuracy: 0.908437\n",
      "epoch 4; batch 7168; loss 0.028653\n",
      "epoch:4; batch 7168; train accuracy: 0.908470\n",
      "epoch 4; batch 7296; loss 0.017823\n",
      "epoch:4; batch 7296; train accuracy: 0.908507\n",
      "epoch 4; batch 7424; loss 0.005629\n",
      "epoch:4; batch 7424; train accuracy: 0.908543\n",
      "epoch 4; batch 7552; loss 0.058924\n",
      "epoch:4; batch 7552; train accuracy: 0.908576\n",
      "epoch 4; batch 7680; loss 0.055119\n",
      "epoch:4; batch 7680; train accuracy: 0.908603\n",
      "epoch 4; batch 7808; loss 0.019442\n",
      "epoch:4; batch 7808; train accuracy: 0.908636\n",
      "epoch 4; batch 7936; loss 0.008619\n",
      "epoch:4; batch 7936; train accuracy: 0.908672\n",
      "epoch 4; batch 8064; loss 0.081466\n",
      "epoch:4; batch 8064; train accuracy: 0.908699\n",
      "epoch 4; batch 8192; loss 0.012894\n",
      "epoch:4; batch 8192; train accuracy: 0.908736\n",
      "epoch 4; batch 8320; loss 0.056994\n",
      "epoch:4; batch 8320; train accuracy: 0.908762\n",
      "epoch 4; batch 8448; loss 0.035416\n",
      "epoch:4; batch 8448; train accuracy: 0.908789\n",
      "epoch 4; batch 8576; loss 0.015893\n",
      "epoch:4; batch 8576; train accuracy: 0.908822\n",
      "epoch 4; batch 8704; loss 0.020931\n",
      "epoch:4; batch 8704; train accuracy: 0.908855\n",
      "epoch 4; batch 8832; loss 0.012913\n",
      "epoch:4; batch 8832; train accuracy: 0.908888\n",
      "epoch 4; batch 8960; loss 0.054118\n",
      "epoch:4; batch 8960; train accuracy: 0.908915\n",
      "epoch 4; batch 9088; loss 0.004313\n",
      "epoch:4; batch 9088; train accuracy: 0.908951\n",
      "epoch 4; batch 9216; loss 0.026041\n",
      "epoch:4; batch 9216; train accuracy: 0.908984\n",
      "epoch 4; batch 9344; loss 0.032169\n",
      "epoch:4; batch 9344; train accuracy: 0.909017\n",
      "epoch 4; batch 9472; loss 0.032214\n",
      "epoch:4; batch 9472; train accuracy: 0.909049\n",
      "epoch 4; batch 9600; loss 0.019201\n",
      "epoch:4; batch 9600; train accuracy: 0.909082\n",
      "epoch 4; batch 9728; loss 0.035422\n",
      "epoch:4; batch 9728; train accuracy: 0.909112\n",
      "epoch 4; batch 9856; loss 0.007526\n",
      "epoch:4; batch 9856; train accuracy: 0.909148\n",
      "epoch 4; batch 9984; loss 0.003295\n",
      "epoch:4; batch 9984; train accuracy: 0.909184\n",
      "epoch 4; batch 10112; loss 0.002989\n",
      "epoch:4; batch 10112; train accuracy: 0.909219\n",
      "epoch 4; batch 10240; loss 0.034156\n",
      "epoch:4; batch 10240; train accuracy: 0.909252\n",
      "epoch 4; batch 10368; loss 0.005612\n",
      "epoch:4; batch 10368; train accuracy: 0.909288\n",
      "epoch 4; batch 10496; loss 0.004329\n",
      "epoch:4; batch 10496; train accuracy: 0.909323\n",
      "epoch 4; batch 10624; loss 0.002846\n",
      "epoch:4; batch 10624; train accuracy: 0.909359\n",
      "epoch 4; batch 10752; loss 0.009926\n",
      "epoch:4; batch 10752; train accuracy: 0.909392\n",
      "epoch 4; batch 10880; loss 0.005482\n",
      "epoch:4; batch 10880; train accuracy: 0.909427\n",
      "epoch 4; batch 11008; loss 0.024649\n",
      "epoch:4; batch 11008; train accuracy: 0.909457\n",
      "epoch 4; batch 11136; loss 0.076501\n",
      "epoch:4; batch 11136; train accuracy: 0.909486\n",
      "epoch 4; batch 11264; loss 0.047371\n",
      "epoch:4; batch 11264; train accuracy: 0.909518\n",
      "epoch 4; batch 11392; loss 0.038378\n",
      "epoch:4; batch 11392; train accuracy: 0.909551\n",
      "epoch 4; batch 11520; loss 0.002548\n",
      "epoch:4; batch 11520; train accuracy: 0.909586\n",
      "epoch 4; batch 11648; loss 0.017304\n",
      "epoch:4; batch 11648; train accuracy: 0.909619\n",
      "epoch 4; batch 11776; loss 0.005495\n",
      "epoch:4; batch 11776; train accuracy: 0.909654\n",
      "epoch 4; batch 11904; loss 0.020870\n",
      "epoch:4; batch 11904; train accuracy: 0.909687\n",
      "epoch 4; batch 12032; loss 0.012757\n",
      "epoch:4; batch 12032; train accuracy: 0.909719\n",
      "epoch 4; batch 12160; loss 0.042765\n",
      "epoch:4; batch 12160; train accuracy: 0.909745\n",
      "epoch 4; batch 12288; loss 0.055447\n",
      "epoch:4; batch 12288; train accuracy: 0.909774\n",
      "epoch 4; batch 12416; loss 0.023956\n",
      "epoch:4; batch 12416; train accuracy: 0.909806\n",
      "epoch 4; batch 12544; loss 0.023853\n",
      "epoch:4; batch 12544; train accuracy: 0.909839\n",
      "epoch 4; batch 12672; loss 0.006386\n",
      "epoch:4; batch 12672; train accuracy: 0.909874\n",
      "epoch 4; batch 12800; loss 0.021388\n",
      "epoch:4; batch 12800; train accuracy: 0.909906\n",
      "epoch 4; batch 12928; loss 0.023031\n",
      "epoch:4; batch 12928; train accuracy: 0.909935\n",
      "epoch 4; batch 13056; loss 0.031667\n",
      "epoch:4; batch 13056; train accuracy: 0.909964\n",
      "epoch 4; batch 13184; loss 0.046050\n",
      "epoch:4; batch 13184; train accuracy: 0.909990\n",
      "epoch 4; batch 13312; loss 0.031667\n",
      "epoch:4; batch 13312; train accuracy: 0.910019\n",
      "epoch 4; batch 13440; loss 0.008487\n",
      "epoch:4; batch 13440; train accuracy: 0.910054\n",
      "epoch 4; batch 13568; loss 0.039907\n",
      "epoch:4; batch 13568; train accuracy: 0.910086\n",
      "epoch 4; batch 13696; loss 0.018241\n",
      "epoch:4; batch 13696; train accuracy: 0.910121\n",
      "epoch 4; batch 13824; loss 0.028207\n",
      "epoch:4; batch 13824; train accuracy: 0.910150\n",
      "epoch 4; batch 13952; loss 0.036606\n",
      "epoch:4; batch 13952; train accuracy: 0.910179\n",
      "epoch 4; batch 14080; loss 0.049359\n",
      "epoch:4; batch 14080; train accuracy: 0.910208\n",
      "epoch 4; batch 14208; loss 0.003958\n",
      "epoch:4; batch 14208; train accuracy: 0.910243\n",
      "epoch 4; batch 14336; loss 0.005469\n",
      "epoch:4; batch 14336; train accuracy: 0.910278\n",
      "epoch 4; batch 14464; loss 0.032196\n",
      "epoch:4; batch 14464; train accuracy: 0.910307\n",
      "epoch 4; batch 14592; loss 0.028551\n",
      "epoch:4; batch 14592; train accuracy: 0.910335\n",
      "epoch 4; batch 14720; loss 0.014706\n",
      "epoch:4; batch 14720; train accuracy: 0.910367\n",
      "epoch 4; batch 14848; loss 0.022085\n",
      "epoch:4; batch 14848; train accuracy: 0.910396\n",
      "epoch 4; batch 14976; loss 0.048277\n",
      "epoch:4; batch 14976; train accuracy: 0.910428\n",
      "epoch 4; batch 15104; loss 0.018115\n",
      "epoch:4; batch 15104; train accuracy: 0.910459\n",
      "epoch 4; batch 15232; loss 0.031704\n",
      "epoch:4; batch 15232; train accuracy: 0.910488\n",
      "epoch 4; batch 15360; loss 0.004668\n",
      "epoch:4; batch 15360; train accuracy: 0.910523\n",
      "epoch 4; batch 15488; loss 0.015831\n",
      "epoch:4; batch 15488; train accuracy: 0.910554\n",
      "epoch 4; batch 15616; loss 0.006810\n",
      "epoch:4; batch 15616; train accuracy: 0.910589\n",
      "epoch 4; batch 15744; loss 0.006973\n",
      "epoch:4; batch 15744; train accuracy: 0.910624\n",
      "epoch 4; batch 15872; loss 0.032406\n",
      "epoch:4; batch 15872; train accuracy: 0.910655\n",
      "epoch 4; batch 16000; loss 0.028512\n",
      "epoch:4; batch 16000; train accuracy: 0.910687\n",
      "epoch 4; batch 16128; loss 0.008606\n",
      "epoch:4; batch 16128; train accuracy: 0.910718\n",
      "epoch 4; batch 16256; loss 0.029219\n",
      "epoch:4; batch 16256; train accuracy: 0.910747\n",
      "epoch 4; batch 16384; loss 0.002799\n",
      "epoch:4; batch 16384; train accuracy: 0.910781\n",
      "epoch 4; batch 16512; loss 0.036485\n",
      "epoch:4; batch 16512; train accuracy: 0.910810\n",
      "epoch 4; batch 16640; loss 0.104777\n",
      "epoch:4; batch 16640; train accuracy: 0.910838\n",
      "epoch 4; batch 16768; loss 0.039877\n",
      "epoch:4; batch 16768; train accuracy: 0.910866\n",
      "epoch 4; batch 16896; loss 0.004793\n",
      "epoch:4; batch 16896; train accuracy: 0.910901\n",
      "epoch 4; batch 17024; loss 0.003735\n",
      "epoch:4; batch 17024; train accuracy: 0.910935\n",
      "epoch 4; batch 17152; loss 0.008322\n",
      "epoch:4; batch 17152; train accuracy: 0.910969\n",
      "epoch 4; batch 17280; loss 0.010040\n",
      "epoch:4; batch 17280; train accuracy: 0.911004\n",
      "epoch 4; batch 17408; loss 0.029438\n",
      "epoch:4; batch 17408; train accuracy: 0.911032\n",
      "epoch 4; batch 17536; loss 0.004452\n",
      "epoch:4; batch 17536; train accuracy: 0.911066\n",
      "epoch 4; batch 17664; loss 0.033684\n",
      "epoch:4; batch 17664; train accuracy: 0.911094\n",
      "epoch 4; batch 17792; loss 0.045385\n",
      "epoch:4; batch 17792; train accuracy: 0.911117\n",
      "epoch 4; batch 17920; loss 0.021083\n",
      "epoch:4; batch 17920; train accuracy: 0.911148\n",
      "epoch 4; batch 18048; loss 0.001535\n",
      "epoch:4; batch 18048; train accuracy: 0.911182\n",
      "epoch 4; batch 18176; loss 0.006508\n",
      "epoch:4; batch 18176; train accuracy: 0.911216\n",
      "epoch 4; batch 18304; loss 0.009002\n",
      "epoch:4; batch 18304; train accuracy: 0.911250\n",
      "epoch 4; batch 18432; loss 0.019043\n",
      "epoch:4; batch 18432; train accuracy: 0.911281\n",
      "epoch 4; batch 18560; loss 0.065213\n",
      "epoch:4; batch 18560; train accuracy: 0.911303\n",
      "epoch 4; batch 18688; loss 0.043692\n",
      "epoch:4; batch 18688; train accuracy: 0.911328\n",
      "epoch 4; batch 18816; loss 0.013476\n",
      "epoch:4; batch 18816; train accuracy: 0.911362\n",
      "epoch 4; batch 18944; loss 0.010932\n",
      "epoch:4; batch 18944; train accuracy: 0.911396\n",
      "epoch 4; batch 19072; loss 0.030555\n",
      "epoch:4; batch 19072; train accuracy: 0.911427\n",
      "epoch 4; batch 19200; loss 0.013480\n",
      "epoch:4; batch 19200; train accuracy: 0.911461\n",
      "epoch 4; batch 19328; loss 0.056621\n",
      "epoch:4; batch 19328; train accuracy: 0.911489\n",
      "epoch 4; batch 19456; loss 0.045136\n",
      "epoch:4; batch 19456; train accuracy: 0.911517\n",
      "epoch 4; batch 19584; loss 0.018177\n",
      "epoch:4; batch 19584; train accuracy: 0.911548\n",
      "epoch 4; batch 19712; loss 0.026958\n",
      "epoch:4; batch 19712; train accuracy: 0.911579\n",
      "epoch 4; batch 19840; loss 0.029898\n",
      "epoch:4; batch 19840; train accuracy: 0.911607\n",
      "epoch 4; batch 19968; loss 0.040418\n",
      "epoch:4; batch 19968; train accuracy: 0.911638\n",
      "epoch 4; batch 20096; loss 0.012605\n",
      "epoch:4; batch 20096; train accuracy: 0.911671\n",
      "epoch 4; batch 20224; loss 0.021479\n",
      "epoch:4; batch 20224; train accuracy: 0.911702\n",
      "epoch 4; batch 20352; loss 0.028831\n",
      "epoch:4; batch 20352; train accuracy: 0.911733\n",
      "epoch 4; batch 20480; loss 0.042651\n",
      "epoch:4; batch 20480; train accuracy: 0.911763\n",
      "epoch 4; batch 20608; loss 0.003694\n",
      "epoch:4; batch 20608; train accuracy: 0.911797\n",
      "epoch 4; batch 20736; loss 0.033572\n",
      "epoch:4; batch 20736; train accuracy: 0.911828\n",
      "epoch 4; batch 20864; loss 0.012057\n",
      "epoch:4; batch 20864; train accuracy: 0.911858\n",
      "epoch 4; batch 20992; loss 0.002808\n",
      "epoch:4; batch 20992; train accuracy: 0.911892\n",
      "epoch 4; batch 21120; loss 0.034879\n",
      "epoch:4; batch 21120; train accuracy: 0.911923\n",
      "epoch 4; batch 21248; loss 0.039638\n",
      "epoch:4; batch 21248; train accuracy: 0.911950\n",
      "epoch 4; batch 21376; loss 0.017662\n",
      "epoch:4; batch 21376; train accuracy: 0.911981\n",
      "epoch 4; batch 21504; loss 0.014115\n",
      "epoch:4; batch 21504; train accuracy: 0.912011\n",
      "epoch 4; batch 21632; loss 0.007155\n",
      "epoch:4; batch 21632; train accuracy: 0.912045\n",
      "epoch 4; batch 21760; loss 0.029674\n",
      "epoch:4; batch 21760; train accuracy: 0.912072\n",
      "epoch 4; batch 21888; loss 0.007316\n",
      "epoch:4; batch 21888; train accuracy: 0.912106\n",
      "epoch 4; batch 22016; loss 0.009649\n",
      "epoch:4; batch 22016; train accuracy: 0.912139\n",
      "epoch 4; batch 22144; loss 0.058532\n",
      "epoch:4; batch 22144; train accuracy: 0.912163\n",
      "epoch 4; batch 22272; loss 0.032687\n",
      "epoch:4; batch 22272; train accuracy: 0.912194\n",
      "epoch 4; batch 22400; loss 0.004365\n",
      "epoch:4; batch 22400; train accuracy: 0.912227\n",
      "epoch 4; batch 22528; loss 0.007129\n",
      "epoch:4; batch 22528; train accuracy: 0.912261\n",
      "epoch 4; batch 22656; loss 0.025888\n",
      "epoch:4; batch 22656; train accuracy: 0.912291\n",
      "epoch 4; batch 22784; loss 0.010570\n",
      "epoch:4; batch 22784; train accuracy: 0.912324\n",
      "epoch 4; batch 22912; loss 0.029069\n",
      "epoch:4; batch 22912; train accuracy: 0.912354\n",
      "epoch 4; batch 23040; loss 0.019258\n",
      "epoch:4; batch 23040; train accuracy: 0.912385\n",
      "epoch 4; batch 23168; loss 0.001068\n",
      "epoch:4; batch 23168; train accuracy: 0.912418\n",
      "epoch 4; batch 23296; loss 0.005586\n",
      "epoch:4; batch 23296; train accuracy: 0.912451\n",
      "epoch 4; batch 23424; loss 0.015529\n",
      "epoch:4; batch 23424; train accuracy: 0.912484\n",
      "epoch 4; batch 23552; loss 0.001722\n",
      "epoch:4; batch 23552; train accuracy: 0.912517\n",
      "epoch 4; batch 23680; loss 0.011088\n",
      "epoch:4; batch 23680; train accuracy: 0.912547\n",
      "epoch 4; batch 23808; loss 0.036572\n",
      "epoch:4; batch 23808; train accuracy: 0.912577\n",
      "epoch 4; batch 23936; loss 0.014994\n",
      "epoch:4; batch 23936; train accuracy: 0.912607\n",
      "epoch 4; batch 24064; loss 0.003367\n",
      "epoch:4; batch 24064; train accuracy: 0.912640\n",
      "epoch 4; batch 24192; loss 0.007319\n",
      "epoch:4; batch 24192; train accuracy: 0.912670\n",
      "epoch 4; batch 24320; loss 0.007339\n",
      "epoch:4; batch 24320; train accuracy: 0.912703\n",
      "epoch 4; batch 24448; loss 0.049749\n",
      "epoch:4; batch 24448; train accuracy: 0.912728\n",
      "epoch 4; batch 24576; loss 0.045052\n",
      "epoch:4; batch 24576; train accuracy: 0.912757\n",
      "epoch 4; batch 24704; loss 0.023709\n",
      "epoch:4; batch 24704; train accuracy: 0.912787\n",
      "epoch 4; batch 24832; loss 0.004274\n",
      "epoch:4; batch 24832; train accuracy: 0.912820\n",
      "epoch 4; batch 24960; loss 0.007149\n",
      "epoch:4; batch 24960; train accuracy: 0.912853\n",
      "epoch 4; batch 25088; loss 0.009102\n",
      "epoch:4; batch 25088; train accuracy: 0.912886\n",
      "epoch 4; batch 25216; loss 0.037134\n",
      "epoch:4; batch 25216; train accuracy: 0.912916\n",
      "epoch 4; batch 25344; loss 0.001908\n",
      "epoch:4; batch 25344; train accuracy: 0.912949\n",
      "epoch 4; batch 25472; loss 0.020162\n",
      "epoch:4; batch 25472; train accuracy: 0.912975\n",
      "epoch 4; batch 25600; loss 0.066182\n",
      "epoch:4; batch 25600; train accuracy: 0.913005\n",
      "epoch 4; batch 25728; loss 0.027090\n",
      "epoch:4; batch 25728; train accuracy: 0.913035\n",
      "epoch 4; batch 25856; loss 0.005471\n",
      "epoch:4; batch 25856; train accuracy: 0.913068\n",
      "epoch 4; batch 25984; loss 0.003177\n",
      "epoch:4; batch 25984; train accuracy: 0.913100\n",
      "epoch 4; batch 26112; loss 0.021021\n",
      "epoch:4; batch 26112; train accuracy: 0.913133\n",
      "epoch 4; batch 26240; loss 0.005354\n",
      "epoch:4; batch 26240; train accuracy: 0.913165\n",
      "epoch 4; batch 26368; loss 0.045223\n",
      "epoch:4; batch 26368; train accuracy: 0.913195\n",
      "epoch 4; batch 26496; loss 0.001737\n",
      "epoch:4; batch 26496; train accuracy: 0.913228\n",
      "epoch 4; batch 26624; loss 0.002388\n",
      "epoch:4; batch 26624; train accuracy: 0.913260\n",
      "epoch 4; batch 26752; loss 0.018233\n",
      "epoch:4; batch 26752; train accuracy: 0.913290\n",
      "epoch 4; batch 26880; loss 0.022768\n",
      "epoch:4; batch 26880; train accuracy: 0.913316\n",
      "epoch 4; batch 27008; loss 0.013378\n",
      "epoch:4; batch 27008; train accuracy: 0.913349\n",
      "epoch 4; batch 27136; loss 0.003039\n",
      "epoch:4; batch 27136; train accuracy: 0.913381\n",
      "epoch 4; batch 27264; loss 0.009605\n",
      "epoch:4; batch 27264; train accuracy: 0.913414\n",
      "epoch 4; batch 27392; loss 0.052505\n",
      "epoch:4; batch 27392; train accuracy: 0.913440\n",
      "epoch 4; batch 27520; loss 0.028355\n",
      "epoch:4; batch 27520; train accuracy: 0.913470\n",
      "epoch 4; batch 27648; loss 0.002302\n",
      "epoch:4; batch 27648; train accuracy: 0.913502\n",
      "epoch 4; batch 27776; loss 0.026099\n",
      "epoch:4; batch 27776; train accuracy: 0.913528\n",
      "epoch 4; batch 27904; loss 0.002776\n",
      "epoch:4; batch 27904; train accuracy: 0.913561\n",
      "epoch 4; batch 28032; loss 0.087374\n",
      "epoch:4; batch 28032; train accuracy: 0.913581\n",
      "epoch 4; batch 28160; loss 0.013999\n",
      "epoch:4; batch 28160; train accuracy: 0.913611\n",
      "epoch 4; batch 28288; loss 0.022720\n",
      "epoch:4; batch 28288; train accuracy: 0.913637\n",
      "epoch 4; batch 28416; loss 0.047376\n",
      "epoch:4; batch 28416; train accuracy: 0.913663\n",
      "epoch 4; batch 28544; loss 0.011794\n",
      "epoch:4; batch 28544; train accuracy: 0.913693\n",
      "epoch 4; batch 28672; loss 0.011856\n",
      "epoch:4; batch 28672; train accuracy: 0.913722\n",
      "epoch 4; batch 28800; loss 0.013119\n",
      "epoch:4; batch 28800; train accuracy: 0.913751\n",
      "epoch 4; batch 28928; loss 0.040564\n",
      "epoch:4; batch 28928; train accuracy: 0.913777\n",
      "epoch 4; batch 29056; loss 0.009553\n",
      "epoch:4; batch 29056; train accuracy: 0.913810\n",
      "epoch 4; batch 29184; loss 0.071972\n",
      "epoch:4; batch 29184; train accuracy: 0.913836\n",
      "epoch 4; batch 29312; loss 0.008441\n",
      "epoch:4; batch 29312; train accuracy: 0.913865\n",
      "epoch 4; batch 29440; loss 0.022855\n",
      "epoch:4; batch 29440; train accuracy: 0.913894\n",
      "epoch 4; batch 29568; loss 0.007295\n",
      "epoch:4; batch 29568; train accuracy: 0.913926\n",
      "epoch 4; batch 29696; loss 0.007480\n",
      "epoch:4; batch 29696; train accuracy: 0.913958\n",
      "epoch 4; batch 29824; loss 0.024410\n",
      "epoch:4; batch 29824; train accuracy: 0.913987\n",
      "epoch 4; batch 29952; loss 0.009917\n",
      "epoch:4; batch 29952; train accuracy: 0.914019\n",
      "epoch 4; batch 30080; loss 0.018710\n",
      "epoch:4; batch 30080; train accuracy: 0.914048\n",
      "epoch 4; batch 30208; loss 0.060679\n",
      "epoch:4; batch 30208; train accuracy: 0.914077\n",
      "epoch 4; batch 30336; loss 0.003318\n",
      "epoch:4; batch 30336; train accuracy: 0.914109\n",
      "epoch 4; batch 30464; loss 0.011330\n",
      "epoch:4; batch 30464; train accuracy: 0.914141\n",
      "epoch 4; batch 30592; loss 0.030253\n",
      "epoch:4; batch 30592; train accuracy: 0.914170\n",
      "epoch 4; batch 30720; loss 0.082016\n",
      "epoch:4; batch 30720; train accuracy: 0.914196\n",
      "epoch 4; batch 30848; loss 0.012998\n",
      "epoch:4; batch 30848; train accuracy: 0.914227\n",
      "epoch 4; batch 30976; loss 0.046466\n",
      "epoch:4; batch 30976; train accuracy: 0.914256\n",
      "epoch 4; batch 31104; loss 0.022747\n",
      "epoch:4; batch 31104; train accuracy: 0.914285\n",
      "epoch 4; batch 31232; loss 0.008161\n",
      "epoch:4; batch 31232; train accuracy: 0.914314\n",
      "epoch 4; batch 31360; loss 0.016689\n",
      "epoch:4; batch 31360; train accuracy: 0.914340\n",
      "epoch 4; batch 31488; loss 0.007699\n",
      "epoch:4; batch 31488; train accuracy: 0.914371\n",
      "epoch 4; batch 31616; loss 0.051688\n",
      "epoch:4; batch 31616; train accuracy: 0.914397\n",
      "epoch 4; batch 31744; loss 0.004424\n",
      "epoch:4; batch 31744; train accuracy: 0.914429\n",
      "epoch 4; batch 31872; loss 0.005300\n",
      "epoch:4; batch 31872; train accuracy: 0.914460\n",
      "epoch 4; batch 32000; loss 0.001637\n",
      "epoch:4; batch 32000; train accuracy: 0.914492\n",
      "epoch 4; batch 32128; loss 0.010221\n",
      "epoch:4; batch 32128; train accuracy: 0.914524\n",
      "epoch 4; batch 32256; loss 0.024969\n",
      "epoch:4; batch 32256; train accuracy: 0.914552\n",
      "epoch 4; batch 32384; loss 0.003609\n",
      "epoch:4; batch 32384; train accuracy: 0.914584\n",
      "epoch 4; batch 32512; loss 0.008208\n",
      "epoch:4; batch 32512; train accuracy: 0.914615\n",
      "epoch 4; batch 32640; loss 0.008925\n",
      "epoch:4; batch 32640; train accuracy: 0.914647\n",
      "epoch 4; batch 32768; loss 0.001679\n",
      "epoch:4; batch 32768; train accuracy: 0.914678\n",
      "epoch 4; batch 32896; loss 0.070442\n",
      "epoch:4; batch 32896; train accuracy: 0.914704\n",
      "epoch 4; batch 33024; loss 0.042989\n",
      "epoch:4; batch 33024; train accuracy: 0.914726\n",
      "epoch 4; batch 33152; loss 0.065624\n",
      "epoch:4; batch 33152; train accuracy: 0.914755\n",
      "epoch 4; batch 33280; loss 0.014141\n",
      "epoch:4; batch 33280; train accuracy: 0.914786\n",
      "epoch 4; batch 33408; loss 0.006126\n",
      "epoch:4; batch 33408; train accuracy: 0.914818\n",
      "epoch 4; batch 33536; loss 0.049190\n",
      "epoch:4; batch 33536; train accuracy: 0.914843\n",
      "epoch 4; batch 33664; loss 0.048132\n",
      "epoch:4; batch 33664; train accuracy: 0.914866\n",
      "epoch 4; batch 33792; loss 0.009567\n",
      "epoch:4; batch 33792; train accuracy: 0.914894\n",
      "epoch 4; batch 33920; loss 0.011196\n",
      "epoch:4; batch 33920; train accuracy: 0.914925\n",
      "epoch 4; batch 34048; loss 0.013109\n",
      "epoch:4; batch 34048; train accuracy: 0.914954\n",
      "epoch 4; batch 34176; loss 0.008517\n",
      "epoch:4; batch 34176; train accuracy: 0.914982\n",
      "epoch 4; batch 34304; loss 0.005198\n",
      "epoch:4; batch 34304; train accuracy: 0.915013\n",
      "epoch 4; batch 34432; loss 0.038070\n",
      "epoch:4; batch 34432; train accuracy: 0.915039\n",
      "epoch 4; batch 34560; loss 0.005333\n",
      "epoch:4; batch 34560; train accuracy: 0.915070\n",
      "epoch 4; batch 34688; loss 0.030067\n",
      "epoch:4; batch 34688; train accuracy: 0.915095\n",
      "epoch 4; batch 34816; loss 0.019147\n",
      "epoch:4; batch 34816; train accuracy: 0.915123\n",
      "epoch 4; batch 34944; loss 0.002754\n",
      "epoch:4; batch 34944; train accuracy: 0.915154\n",
      "epoch 4; batch 35072; loss 0.004212\n",
      "epoch:4; batch 35072; train accuracy: 0.915186\n",
      "epoch 4; batch 35200; loss 0.018768\n",
      "epoch:4; batch 35200; train accuracy: 0.915214\n",
      "epoch 4; batch 35328; loss 0.037457\n",
      "epoch:4; batch 35328; train accuracy: 0.915242\n",
      "epoch 4; batch 35456; loss 0.006167\n",
      "epoch:4; batch 35456; train accuracy: 0.915273\n",
      "epoch 4; batch 35584; loss 0.062880\n",
      "epoch:4; batch 35584; train accuracy: 0.915298\n",
      "epoch 4; batch 35712; loss 0.017684\n",
      "epoch:4; batch 35712; train accuracy: 0.915326\n",
      "epoch 4; batch 35840; loss 0.004041\n",
      "epoch:4; batch 35840; train accuracy: 0.915357\n",
      "epoch 4; batch 35968; loss 0.013025\n",
      "epoch:4; batch 35968; train accuracy: 0.915385\n",
      "epoch 4; batch 36096; loss 0.005009\n",
      "epoch:4; batch 36096; train accuracy: 0.915416\n",
      "epoch 4; batch 36224; loss 0.006090\n",
      "epoch:4; batch 36224; train accuracy: 0.915447\n",
      "epoch 4; batch 36352; loss 0.003352\n",
      "epoch:4; batch 36352; train accuracy: 0.915478\n",
      "epoch 4; batch 36480; loss 0.003196\n",
      "epoch:4; batch 36480; train accuracy: 0.915508\n",
      "epoch 4; batch 36608; loss 0.005756\n",
      "epoch:4; batch 36608; train accuracy: 0.915539\n",
      "epoch 4; batch 36736; loss 0.009858\n",
      "epoch:4; batch 36736; train accuracy: 0.915570\n",
      "epoch 4; batch 36864; loss 0.012025\n",
      "epoch:4; batch 36864; train accuracy: 0.915601\n",
      "epoch 4; batch 36992; loss 0.008619\n",
      "epoch:4; batch 36992; train accuracy: 0.915628\n",
      "epoch 4; batch 37120; loss 0.002312\n",
      "epoch:4; batch 37120; train accuracy: 0.915659\n",
      "epoch 4; batch 37248; loss 0.001833\n",
      "epoch:4; batch 37248; train accuracy: 0.915690\n",
      "epoch 4; batch 37376; loss 0.018639\n",
      "epoch:4; batch 37376; train accuracy: 0.915718\n",
      "epoch 4; batch 37504; loss 0.006156\n",
      "epoch:4; batch 37504; train accuracy: 0.915748\n",
      "epoch 4; batch 37632; loss 0.015095\n",
      "epoch:4; batch 37632; train accuracy: 0.915779\n",
      "epoch 4; batch 37760; loss 0.021118\n",
      "epoch:4; batch 37760; train accuracy: 0.915806\n",
      "epoch 4; batch 37888; loss 0.042580\n",
      "epoch:4; batch 37888; train accuracy: 0.915834\n",
      "epoch 4; batch 38016; loss 0.057117\n",
      "epoch:4; batch 38016; train accuracy: 0.915862\n",
      "epoch 4; batch 38144; loss 0.001768\n",
      "epoch:4; batch 38144; train accuracy: 0.915892\n",
      "epoch 4; batch 38272; loss 0.015196\n",
      "epoch:4; batch 38272; train accuracy: 0.915920\n",
      "epoch 4; batch 38400; loss 0.010473\n",
      "epoch:4; batch 38400; train accuracy: 0.915951\n",
      "epoch 4; batch 38528; loss 0.025699\n",
      "epoch:4; batch 38528; train accuracy: 0.915978\n",
      "epoch 4; batch 38656; loss 0.008997\n",
      "epoch:4; batch 38656; train accuracy: 0.916009\n",
      "epoch 4; batch 38784; loss 0.004233\n",
      "epoch:4; batch 38784; train accuracy: 0.916039\n",
      "epoch 4; batch 38912; loss 0.003604\n",
      "epoch:4; batch 38912; train accuracy: 0.916069\n",
      "epoch 4; batch 39040; loss 0.003500\n",
      "epoch:4; batch 39040; train accuracy: 0.916100\n",
      "epoch 4; batch 39168; loss 0.017827\n",
      "epoch:4; batch 39168; train accuracy: 0.916127\n",
      "epoch 4; batch 39296; loss 0.001190\n",
      "epoch:4; batch 39296; train accuracy: 0.916157\n",
      "epoch 4; batch 39424; loss 0.034485\n",
      "epoch:4; batch 39424; train accuracy: 0.916182\n",
      "epoch 4; batch 39552; loss 0.025158\n",
      "epoch:4; batch 39552; train accuracy: 0.916210\n",
      "epoch 4; batch 39680; loss 0.011337\n",
      "epoch:4; batch 39680; train accuracy: 0.916240\n",
      "epoch 4; batch 39808; loss 0.040088\n",
      "epoch:4; batch 39808; train accuracy: 0.916262\n",
      "epoch 4; batch 39936; loss 0.004457\n",
      "epoch:4; batch 39936; train accuracy: 0.916292\n",
      "epoch 4; batch 40064; loss 0.006921\n",
      "epoch:4; batch 40064; train accuracy: 0.916322\n",
      "epoch 4; batch 40192; loss 0.011292\n",
      "epoch:4; batch 40192; train accuracy: 0.916349\n",
      "epoch 4; batch 40320; loss 0.023758\n",
      "epoch:4; batch 40320; train accuracy: 0.916377\n",
      "epoch 4; batch 40448; loss 0.005413\n",
      "epoch:4; batch 40448; train accuracy: 0.916407\n",
      "epoch 4; batch 40576; loss 0.003232\n",
      "epoch:4; batch 40576; train accuracy: 0.916437\n",
      "epoch 4; batch 40704; loss 0.009432\n",
      "epoch:4; batch 40704; train accuracy: 0.916464\n",
      "epoch 4; batch 40832; loss 0.002225\n",
      "epoch:4; batch 40832; train accuracy: 0.916494\n",
      "epoch 4; batch 40960; loss 0.001157\n",
      "epoch:4; batch 40960; train accuracy: 0.916524\n",
      "epoch 4; batch 41088; loss 0.091660\n",
      "epoch:4; batch 41088; train accuracy: 0.916549\n",
      "epoch 4; batch 41216; loss 0.011195\n",
      "epoch:4; batch 41216; train accuracy: 0.916576\n",
      "epoch 4; batch 41344; loss 0.004457\n",
      "epoch:4; batch 41344; train accuracy: 0.916606\n",
      "epoch 4; batch 41472; loss 0.004733\n",
      "epoch:4; batch 41472; train accuracy: 0.916636\n",
      "epoch 4; batch 41600; loss 0.002299\n",
      "epoch:4; batch 41600; train accuracy: 0.916666\n",
      "epoch 4; batch 41728; loss 0.007031\n",
      "epoch:4; batch 41728; train accuracy: 0.916696\n",
      "epoch 4; batch 41856; loss 0.006607\n",
      "epoch:4; batch 41856; train accuracy: 0.916726\n",
      "epoch 4; batch 41984; loss 0.033525\n",
      "epoch:4; batch 41984; train accuracy: 0.916750\n",
      "epoch 4; batch 42112; loss 0.002514\n",
      "epoch:4; batch 42112; train accuracy: 0.916780\n",
      "epoch 4; batch 42240; loss 0.001149\n",
      "epoch:4; batch 42240; train accuracy: 0.916809\n",
      "epoch 4; batch 42368; loss 0.005148\n",
      "epoch:4; batch 42368; train accuracy: 0.916839\n",
      "epoch 4; batch 42496; loss 0.004097\n",
      "epoch:4; batch 42496; train accuracy: 0.916869\n",
      "epoch 4; batch 42624; loss 0.001248\n",
      "epoch:4; batch 42624; train accuracy: 0.916899\n",
      "epoch 4; batch 42752; loss 0.003413\n",
      "epoch:4; batch 42752; train accuracy: 0.916929\n",
      "epoch 4; batch 42880; loss 0.003554\n",
      "epoch:4; batch 42880; train accuracy: 0.916958\n",
      "epoch 4; batch 43008; loss 0.000666\n",
      "epoch:4; batch 43008; train accuracy: 0.916988\n",
      "epoch 4; batch 43136; loss 0.009583\n",
      "epoch:4; batch 43136; train accuracy: 0.917018\n",
      "epoch 4; batch 43264; loss 0.002998\n",
      "epoch:4; batch 43264; train accuracy: 0.917047\n",
      "epoch 4; batch 43392; loss 0.009097\n",
      "epoch:4; batch 43392; train accuracy: 0.917074\n",
      "epoch 4; batch 43520; loss 0.093608\n",
      "epoch:4; batch 43520; train accuracy: 0.917101\n",
      "epoch 4; batch 43648; loss 0.006437\n",
      "epoch:4; batch 43648; train accuracy: 0.917131\n",
      "epoch 4; batch 43776; loss 0.003706\n",
      "epoch:4; batch 43776; train accuracy: 0.917160\n",
      "epoch 4; batch 43904; loss 0.007451\n",
      "epoch:4; batch 43904; train accuracy: 0.917190\n",
      "epoch 4; batch 44032; loss 0.005207\n",
      "epoch:4; batch 44032; train accuracy: 0.917219\n",
      "epoch 4; batch 44160; loss 0.011421\n",
      "epoch:4; batch 44160; train accuracy: 0.917246\n",
      "epoch 4; batch 44288; loss 0.105173\n",
      "epoch:4; batch 44288; train accuracy: 0.917270\n",
      "epoch 4; batch 44416; loss 0.082308\n",
      "epoch:4; batch 44416; train accuracy: 0.917291\n",
      "epoch 4; batch 44544; loss 0.011769\n",
      "epoch:4; batch 44544; train accuracy: 0.917320\n",
      "epoch 4; batch 44672; loss 0.006013\n",
      "epoch:4; batch 44672; train accuracy: 0.917350\n",
      "epoch 4; batch 44800; loss 0.007786\n",
      "epoch:4; batch 44800; train accuracy: 0.917379\n",
      "epoch 4; batch 44928; loss 0.008674\n",
      "epoch:4; batch 44928; train accuracy: 0.917409\n",
      "epoch 4; batch 45056; loss 0.005894\n",
      "epoch:4; batch 45056; train accuracy: 0.917438\n",
      "epoch 4; batch 45184; loss 0.080933\n",
      "epoch:4; batch 45184; train accuracy: 0.917462\n",
      "epoch 4; batch 45312; loss 0.004885\n",
      "epoch:4; batch 45312; train accuracy: 0.917491\n",
      "epoch 4; batch 45440; loss 0.017597\n",
      "epoch:4; batch 45440; train accuracy: 0.917518\n",
      "epoch 4; batch 45568; loss 0.001889\n",
      "epoch:4; batch 45568; train accuracy: 0.917547\n",
      "epoch 4; batch 45696; loss 0.018426\n",
      "epoch:4; batch 45696; train accuracy: 0.917576\n",
      "epoch 4; batch 45824; loss 0.005264\n",
      "epoch:4; batch 45824; train accuracy: 0.917606\n",
      "epoch 4; batch 45952; loss 0.028570\n",
      "epoch:4; batch 45952; train accuracy: 0.917629\n",
      "epoch 4; batch 46080; loss 0.019379\n",
      "epoch:4; batch 46080; train accuracy: 0.917656\n",
      "epoch 4; batch 46208; loss 0.005333\n",
      "epoch:4; batch 46208; train accuracy: 0.917685\n",
      "epoch 4; batch 46336; loss 0.002865\n",
      "epoch:4; batch 46336; train accuracy: 0.917714\n",
      "epoch 4; batch 46464; loss 0.030608\n",
      "epoch:4; batch 46464; train accuracy: 0.917738\n",
      "epoch 4; batch 46592; loss 0.009239\n",
      "epoch:4; batch 46592; train accuracy: 0.917767\n",
      "epoch 4; batch 46720; loss 0.001899\n",
      "epoch:4; batch 46720; train accuracy: 0.917796\n",
      "epoch 4; batch 46848; loss 0.010762\n",
      "epoch:4; batch 46848; train accuracy: 0.917825\n",
      "epoch 4; batch 46976; loss 0.063109\n",
      "epoch:4; batch 46976; train accuracy: 0.917851\n",
      "epoch 4; batch 47104; loss 0.008369\n",
      "epoch:4; batch 47104; train accuracy: 0.917880\n",
      "epoch 4; batch 47232; loss 0.002053\n",
      "epoch:4; batch 47232; train accuracy: 0.917909\n",
      "epoch 4; batch 47360; loss 0.007852\n",
      "epoch:4; batch 47360; train accuracy: 0.917938\n",
      "epoch 4; batch 47488; loss 0.037154\n",
      "epoch:4; batch 47488; train accuracy: 0.917962\n",
      "epoch 4; batch 47616; loss 0.001660\n",
      "epoch:4; batch 47616; train accuracy: 0.917991\n",
      "epoch 4; batch 47744; loss 0.001565\n",
      "epoch:4; batch 47744; train accuracy: 0.918020\n",
      "epoch 4; batch 47872; loss 0.040126\n",
      "epoch:4; batch 47872; train accuracy: 0.918046\n",
      "epoch 4; batch 48000; loss 0.078022\n",
      "epoch:4; batch 48000; train accuracy: 0.918069\n",
      "epoch 4; batch 48128; loss 0.004247\n",
      "epoch:4; batch 48128; train accuracy: 0.918098\n",
      "epoch 4; batch 48256; loss 0.007488\n",
      "epoch:4; batch 48256; train accuracy: 0.918127\n",
      "epoch 4; batch 48384; loss 0.011523\n",
      "epoch:4; batch 48384; train accuracy: 0.918153\n",
      "epoch 4; batch 48512; loss 0.001340\n",
      "epoch:4; batch 48512; train accuracy: 0.918182\n",
      "epoch 4; batch 48640; loss 0.001215\n",
      "epoch:4; batch 48640; train accuracy: 0.918211\n",
      "epoch 4; batch 48768; loss 0.024364\n",
      "epoch:4; batch 48768; train accuracy: 0.918237\n",
      "epoch 4; batch 48896; loss 0.015580\n",
      "epoch:4; batch 48896; train accuracy: 0.918263\n",
      "epoch 4; batch 49024; loss 0.000492\n",
      "epoch:4; batch 49024; train accuracy: 0.918292\n",
      "epoch 4; batch 49152; loss 0.012452\n",
      "epoch:4; batch 49152; train accuracy: 0.918320\n",
      "epoch 4; batch 49280; loss 0.030365\n",
      "epoch:4; batch 49280; train accuracy: 0.918344\n",
      "epoch 4; batch 49408; loss 0.001439\n",
      "epoch:4; batch 49408; train accuracy: 0.918372\n",
      "epoch 4; batch 49536; loss 0.009048\n",
      "epoch:4; batch 49536; train accuracy: 0.918401\n",
      "epoch 4; batch 49664; loss 0.022803\n",
      "epoch:4; batch 49664; train accuracy: 0.918427\n",
      "epoch 4; batch 49792; loss 0.001601\n",
      "epoch:4; batch 49792; train accuracy: 0.918455\n",
      "epoch 4; batch 49920; loss 0.012402\n",
      "epoch:4; batch 49920; train accuracy: 0.918481\n",
      "epoch 4; batch 50048; loss 0.001035\n",
      "epoch:4; batch 50048; train accuracy: 0.918510\n",
      "epoch 4; batch 50176; loss 0.002113\n",
      "epoch:4; batch 50176; train accuracy: 0.918539\n",
      "epoch 4; batch 50304; loss 0.004124\n",
      "epoch:4; batch 50304; train accuracy: 0.918567\n",
      "epoch 4; batch 50432; loss 0.002830\n",
      "epoch:4; batch 50432; train accuracy: 0.918596\n",
      "epoch 4; batch 50560; loss 0.008522\n",
      "epoch:4; batch 50560; train accuracy: 0.918624\n",
      "epoch 4; batch 50688; loss 0.004145\n",
      "epoch:4; batch 50688; train accuracy: 0.918653\n",
      "epoch 4; batch 50816; loss 0.008594\n",
      "epoch:4; batch 50816; train accuracy: 0.918678\n",
      "epoch 4; batch 50944; loss 0.019358\n",
      "epoch:4; batch 50944; train accuracy: 0.918704\n",
      "epoch 4; batch 51072; loss 0.004024\n",
      "epoch:4; batch 51072; train accuracy: 0.918733\n",
      "epoch 4; batch 51200; loss 0.011215\n",
      "epoch:4; batch 51200; train accuracy: 0.918758\n",
      "epoch 4; batch 51328; loss 0.013639\n",
      "epoch:4; batch 51328; train accuracy: 0.918784\n",
      "epoch 4; batch 51456; loss 0.001818\n",
      "epoch:4; batch 51456; train accuracy: 0.918812\n",
      "epoch 4; batch 51584; loss 0.028663\n",
      "epoch:4; batch 51584; train accuracy: 0.918835\n",
      "epoch 4; batch 51712; loss 0.015021\n",
      "epoch:4; batch 51712; train accuracy: 0.918861\n",
      "epoch 4; batch 51840; loss 0.010839\n",
      "epoch:4; batch 51840; train accuracy: 0.918889\n",
      "epoch 4; batch 51968; loss 0.001064\n",
      "epoch:4; batch 51968; train accuracy: 0.918917\n",
      "epoch 4; batch 52096; loss 0.001761\n",
      "epoch:4; batch 52096; train accuracy: 0.918946\n",
      "epoch 4; batch 52224; loss 0.007180\n",
      "epoch:4; batch 52224; train accuracy: 0.918971\n",
      "epoch 4; batch 52352; loss 0.031829\n",
      "epoch:4; batch 52352; train accuracy: 0.918997\n",
      "epoch 4; batch 52480; loss 0.013624\n",
      "epoch:4; batch 52480; train accuracy: 0.919022\n",
      "epoch 4; batch 52608; loss 0.001603\n",
      "epoch:4; batch 52608; train accuracy: 0.919050\n",
      "epoch 4; batch 52736; loss 0.001887\n",
      "epoch:4; batch 52736; train accuracy: 0.919079\n",
      "epoch 4; batch 52864; loss 0.021798\n",
      "epoch:4; batch 52864; train accuracy: 0.919101\n",
      "epoch 4; batch 52992; loss 0.045625\n",
      "epoch:4; batch 52992; train accuracy: 0.919124\n",
      "epoch 4; batch 53120; loss 0.029532\n",
      "epoch:4; batch 53120; train accuracy: 0.919147\n",
      "epoch 4; batch 53248; loss 0.003443\n",
      "epoch:4; batch 53248; train accuracy: 0.919175\n",
      "epoch 4; batch 53376; loss 0.013896\n",
      "epoch:4; batch 53376; train accuracy: 0.919200\n",
      "epoch 4; batch 53504; loss 0.010733\n",
      "epoch:4; batch 53504; train accuracy: 0.919228\n",
      "epoch 4; batch 53632; loss 0.018696\n",
      "epoch:4; batch 53632; train accuracy: 0.919254\n",
      "epoch 4; batch 53760; loss 0.008317\n",
      "epoch:4; batch 53760; train accuracy: 0.919282\n",
      "epoch 4; batch 53888; loss 0.001286\n",
      "epoch:4; batch 53888; train accuracy: 0.919310\n",
      "epoch 4; batch 54016; loss 0.044328\n",
      "epoch:4; batch 54016; train accuracy: 0.919332\n",
      "epoch 4; batch 54144; loss 0.003079\n",
      "epoch:4; batch 54144; train accuracy: 0.919360\n",
      "epoch 4; batch 54272; loss 0.004920\n",
      "epoch:4; batch 54272; train accuracy: 0.919388\n",
      "epoch 4; batch 54400; loss 0.012275\n",
      "epoch:4; batch 54400; train accuracy: 0.919413\n",
      "epoch 4; batch 54528; loss 0.006345\n",
      "epoch:4; batch 54528; train accuracy: 0.919441\n",
      "epoch 4; batch 54656; loss 0.000688\n",
      "epoch:4; batch 54656; train accuracy: 0.919469\n",
      "epoch 4; batch 54784; loss 0.005113\n",
      "epoch:4; batch 54784; train accuracy: 0.919497\n",
      "epoch 4; batch 54912; loss 0.004322\n",
      "epoch:4; batch 54912; train accuracy: 0.919525\n",
      "epoch 4; batch 55040; loss 0.002446\n",
      "epoch:4; batch 55040; train accuracy: 0.919553\n",
      "epoch 4; batch 55168; loss 0.038341\n",
      "epoch:4; batch 55168; train accuracy: 0.919578\n",
      "epoch 4; batch 55296; loss 0.003148\n",
      "epoch:4; batch 55296; train accuracy: 0.919606\n",
      "epoch 4; batch 55424; loss 0.007748\n",
      "epoch:4; batch 55424; train accuracy: 0.919631\n",
      "epoch 4; batch 55552; loss 0.007971\n",
      "epoch:4; batch 55552; train accuracy: 0.919656\n",
      "epoch 4; batch 55680; loss 0.034880\n",
      "epoch:4; batch 55680; train accuracy: 0.919681\n",
      "epoch 4; batch 55808; loss 0.027805\n",
      "epoch:4; batch 55808; train accuracy: 0.919706\n",
      "epoch 4; batch 55936; loss 0.019193\n",
      "epoch:4; batch 55936; train accuracy: 0.919728\n",
      "epoch 4; batch 56064; loss 0.000749\n",
      "epoch:4; batch 56064; train accuracy: 0.919756\n",
      "epoch 4; batch 56192; loss 0.057960\n",
      "epoch:4; batch 56192; train accuracy: 0.919781\n",
      "epoch 4; batch 56320; loss 0.026370\n",
      "epoch:4; batch 56320; train accuracy: 0.919803\n",
      "epoch 4; batch 56448; loss 0.009499\n",
      "epoch:4; batch 56448; train accuracy: 0.919828\n",
      "epoch 4; batch 56576; loss 0.008513\n",
      "epoch:4; batch 56576; train accuracy: 0.919853\n",
      "epoch 4; batch 56704; loss 0.004645\n",
      "epoch:4; batch 56704; train accuracy: 0.919881\n",
      "epoch 4; batch 56832; loss 0.002732\n",
      "epoch:4; batch 56832; train accuracy: 0.919908\n",
      "epoch 4; batch 56960; loss 0.039462\n",
      "epoch:4; batch 56960; train accuracy: 0.919933\n",
      "epoch 4; batch 57088; loss 0.016967\n",
      "epoch:4; batch 57088; train accuracy: 0.919958\n",
      "epoch 4; batch 57216; loss 0.008356\n",
      "epoch:4; batch 57216; train accuracy: 0.919983\n",
      "epoch 4; batch 57344; loss 0.014189\n",
      "epoch:4; batch 57344; train accuracy: 0.920008\n",
      "epoch 4; batch 57472; loss 0.000650\n",
      "epoch:4; batch 57472; train accuracy: 0.920035\n",
      "epoch 4; batch 57600; loss 0.000960\n",
      "epoch:4; batch 57600; train accuracy: 0.920063\n",
      "epoch 4; batch 57728; loss 0.010358\n",
      "epoch:4; batch 57728; train accuracy: 0.920090\n",
      "epoch 4; batch 57856; loss 0.001150\n",
      "epoch:4; batch 57856; train accuracy: 0.920118\n",
      "epoch 4; batch 57984; loss 0.001405\n",
      "epoch:4; batch 57984; train accuracy: 0.920145\n",
      "epoch 4; batch 58112; loss 0.024046\n",
      "epoch:4; batch 58112; train accuracy: 0.920170\n",
      "epoch 4; batch 58240; loss 0.006890\n",
      "epoch:4; batch 58240; train accuracy: 0.920197\n",
      "epoch 4; batch 58368; loss 0.004171\n",
      "epoch:4; batch 58368; train accuracy: 0.920225\n",
      "epoch 4; batch 58496; loss 0.001376\n",
      "epoch:4; batch 58496; train accuracy: 0.920252\n",
      "epoch 4; batch 58624; loss 0.049812\n",
      "epoch:4; batch 58624; train accuracy: 0.920274\n",
      "epoch 4; batch 58752; loss 0.010645\n",
      "epoch:4; batch 58752; train accuracy: 0.920299\n",
      "epoch 4; batch 58880; loss 0.012808\n",
      "epoch:4; batch 58880; train accuracy: 0.920323\n",
      "epoch 4; batch 59008; loss 0.018861\n",
      "epoch:4; batch 59008; train accuracy: 0.920345\n",
      "epoch 4; batch 59136; loss 0.023992\n",
      "epoch:4; batch 59136; train accuracy: 0.920370\n",
      "epoch 4; batch 59264; loss 0.008141\n",
      "epoch:4; batch 59264; train accuracy: 0.920397\n",
      "epoch 4; batch 59392; loss 0.027721\n",
      "epoch:4; batch 59392; train accuracy: 0.920419\n",
      "epoch 4; batch 59520; loss 0.034617\n",
      "epoch:4; batch 59520; train accuracy: 0.920443\n",
      "epoch 4; batch 59648; loss 0.003575\n",
      "epoch:4; batch 59648; train accuracy: 0.920471\n",
      "epoch 4; batch 59776; loss 0.015639\n",
      "epoch:4; batch 59776; train accuracy: 0.920495\n",
      "epoch 4; batch 59904; loss 0.014619\n",
      "epoch:4; batch 59904; train accuracy: 0.920520\n",
      "epoch 4; batch 60032; loss 0.002332\n",
      "epoch:4; batch 60032; train accuracy: 0.920547\n",
      "epoch 4; batch 60160; loss 0.019120\n",
      "epoch:4; batch 60160; train accuracy: 0.920571\n",
      "epoch 4; batch 60288; loss 0.031948\n",
      "epoch:4; batch 60288; train accuracy: 0.920596\n",
      "epoch 4; batch 60416; loss 0.005514\n",
      "epoch:4; batch 60416; train accuracy: 0.920623\n",
      "epoch 4; batch 60544; loss 0.005915\n",
      "epoch:4; batch 60544; train accuracy: 0.920650\n",
      "epoch 4; batch 60672; loss 0.068333\n",
      "epoch:4; batch 60672; train accuracy: 0.920671\n",
      "epoch 4; batch 60800; loss 0.008253\n",
      "epoch:4; batch 60800; train accuracy: 0.920698\n",
      "epoch 4; batch 60928; loss 0.014379\n",
      "epoch:4; batch 60928; train accuracy: 0.920723\n",
      "epoch 4; batch 61056; loss 0.034503\n",
      "epoch:4; batch 61056; train accuracy: 0.920747\n",
      "epoch 4; batch 61184; loss 0.005246\n",
      "epoch:4; batch 61184; train accuracy: 0.920774\n",
      "epoch 4; batch 61312; loss 0.008968\n",
      "epoch:4; batch 61312; train accuracy: 0.920798\n",
      "epoch 4; batch 61440; loss 0.021897\n",
      "epoch:4; batch 61440; train accuracy: 0.920823\n",
      "epoch 4; batch 61568; loss 0.012988\n",
      "epoch:4; batch 61568; train accuracy: 0.920847\n",
      "epoch 4; batch 61696; loss 0.014018\n",
      "epoch:4; batch 61696; train accuracy: 0.920874\n",
      "epoch 4; batch 61824; loss 0.030017\n",
      "epoch:4; batch 61824; train accuracy: 0.920898\n",
      "epoch 4; batch 61952; loss 0.001640\n",
      "epoch:4; batch 61952; train accuracy: 0.920925\n",
      "epoch 4; batch 62080; loss 0.007952\n",
      "epoch:4; batch 62080; train accuracy: 0.920952\n",
      "epoch 4; batch 62208; loss 0.001044\n",
      "epoch:4; batch 62208; train accuracy: 0.920979\n",
      "epoch 4; batch 62336; loss 0.004208\n",
      "epoch:4; batch 62336; train accuracy: 0.921005\n",
      "epoch 4; batch 62464; loss 0.003196\n",
      "epoch:4; batch 62464; train accuracy: 0.921032\n",
      "epoch 4; batch 62592; loss 0.000954\n",
      "epoch:4; batch 62592; train accuracy: 0.921059\n",
      "epoch 4; batch 62720; loss 0.005578\n",
      "epoch:4; batch 62720; train accuracy: 0.921086\n",
      "epoch 4; batch 62848; loss 0.001767\n",
      "epoch:4; batch 62848; train accuracy: 0.921113\n",
      "epoch 4; batch 62976; loss 0.002017\n",
      "epoch:4; batch 62976; train accuracy: 0.921139\n",
      "epoch 4; batch 63104; loss 0.117206\n",
      "epoch:4; batch 63104; train accuracy: 0.921161\n",
      "epoch 4; batch 63232; loss 0.001034\n",
      "epoch:4; batch 63232; train accuracy: 0.921187\n",
      "epoch 4; batch 63360; loss 0.001547\n",
      "epoch:4; batch 63360; train accuracy: 0.921214\n",
      "epoch 4; batch 63488; loss 0.002972\n",
      "epoch:4; batch 63488; train accuracy: 0.921241\n",
      "epoch 4; batch 63616; loss 0.032865\n",
      "epoch:4; batch 63616; train accuracy: 0.921262\n",
      "epoch 4; batch 63744; loss 0.003579\n",
      "epoch:4; batch 63744; train accuracy: 0.921289\n",
      "epoch 4; batch 63872; loss 0.022207\n",
      "epoch:4; batch 63872; train accuracy: 0.921313\n",
      "epoch 4; batch 64000; loss 0.014867\n",
      "epoch:4; batch 64000; train accuracy: 0.921339\n",
      "epoch 4; batch 64128; loss 0.003811\n",
      "epoch:4; batch 64128; train accuracy: 0.921366\n",
      "epoch 4; batch 64256; loss 0.004279\n",
      "epoch:4; batch 64256; train accuracy: 0.921392\n",
      "epoch 4; batch 64384; loss 0.001132\n",
      "epoch:4; batch 64384; train accuracy: 0.921419\n",
      "epoch 4; batch 64512; loss 0.003526\n",
      "epoch:4; batch 64512; train accuracy: 0.921445\n",
      "epoch 4; batch 64640; loss 0.054558\n",
      "epoch:4; batch 64640; train accuracy: 0.921469\n",
      "epoch 4; batch 64768; loss 0.024216\n",
      "epoch:4; batch 64768; train accuracy: 0.921490\n",
      "epoch 4; batch 64896; loss 0.016506\n",
      "epoch:4; batch 64896; train accuracy: 0.921514\n",
      "epoch 4; batch 65024; loss 0.020091\n",
      "epoch:4; batch 65024; train accuracy: 0.921538\n",
      "epoch 4; batch 65152; loss 0.036918\n",
      "epoch:4; batch 65152; train accuracy: 0.921559\n",
      "epoch 4; batch 65280; loss 0.005667\n",
      "epoch:4; batch 65280; train accuracy: 0.921586\n",
      "epoch 4; batch 65408; loss 0.021792\n",
      "epoch:4; batch 65408; train accuracy: 0.921607\n",
      "epoch 4; batch 65536; loss 0.007096\n",
      "epoch:4; batch 65536; train accuracy: 0.921633\n",
      "epoch 4; batch 65664; loss 0.048838\n",
      "epoch:4; batch 65664; train accuracy: 0.921654\n",
      "epoch 4; batch 65792; loss 0.016043\n",
      "epoch:4; batch 65792; train accuracy: 0.921678\n",
      "epoch 4; batch 65920; loss 0.052847\n",
      "epoch:4; batch 65920; train accuracy: 0.921702\n",
      "epoch 4; batch 66048; loss 0.058485\n",
      "epoch:4; batch 66048; train accuracy: 0.921723\n",
      "epoch 4; batch 66176; loss 0.012580\n",
      "epoch:4; batch 66176; train accuracy: 0.921746\n",
      "epoch 4; batch 66304; loss 0.007626\n",
      "epoch:4; batch 66304; train accuracy: 0.921773\n",
      "epoch 4; batch 66432; loss 0.010109\n",
      "epoch:4; batch 66432; train accuracy: 0.921799\n",
      "epoch 4; batch 66560; loss 0.024151\n",
      "epoch:4; batch 66560; train accuracy: 0.921820\n",
      "epoch 4; batch 66688; loss 0.021756\n",
      "epoch:4; batch 66688; train accuracy: 0.921844\n",
      "epoch 4; batch 66816; loss 0.022214\n",
      "epoch:4; batch 66816; train accuracy: 0.921867\n",
      "epoch 4; batch 66944; loss 0.011618\n",
      "epoch:4; batch 66944; train accuracy: 0.921893\n",
      "epoch 4; batch 67072; loss 0.027972\n",
      "epoch:4; batch 67072; train accuracy: 0.921914\n",
      "epoch 4; batch 67200; loss 0.011429\n",
      "epoch:4; batch 67200; train accuracy: 0.921940\n",
      "epoch 4; batch 67328; loss 0.003154\n",
      "epoch:4; batch 67328; train accuracy: 0.921967\n",
      "epoch 4; batch 67456; loss 0.006526\n",
      "epoch:4; batch 67456; train accuracy: 0.921993\n",
      "epoch 4; batch 67584; loss 0.007249\n",
      "epoch:4; batch 67584; train accuracy: 0.922019\n",
      "epoch 4; batch 67712; loss 0.048974\n",
      "epoch:4; batch 67712; train accuracy: 0.922040\n",
      "epoch 4; batch 67840; loss 0.006926\n",
      "epoch:4; batch 67840; train accuracy: 0.922066\n",
      "epoch 4; batch 67968; loss 0.004454\n",
      "epoch:4; batch 67968; train accuracy: 0.922092\n",
      "epoch 4; batch 68096; loss 0.002239\n",
      "epoch:4; batch 68096; train accuracy: 0.922118\n",
      "epoch 4; batch 68224; loss 0.017966\n",
      "epoch:4; batch 68224; train accuracy: 0.922139\n",
      "epoch 4; batch 68352; loss 0.026259\n",
      "epoch:4; batch 68352; train accuracy: 0.922159\n",
      "epoch 4; batch 68480; loss 0.004615\n",
      "epoch:4; batch 68480; train accuracy: 0.922185\n",
      "epoch 4; batch 68608; loss 0.004107\n",
      "epoch:4; batch 68608; train accuracy: 0.922211\n",
      "epoch 4; batch 68736; loss 0.003624\n",
      "epoch:4; batch 68736; train accuracy: 0.922237\n",
      "epoch 4; batch 68864; loss 0.009007\n",
      "epoch:4; batch 68864; train accuracy: 0.922261\n",
      "epoch 4; batch 68992; loss 0.031956\n",
      "epoch:4; batch 68992; train accuracy: 0.922284\n",
      "epoch 4; batch 69120; loss 0.002019\n",
      "epoch:4; batch 69120; train accuracy: 0.922310\n",
      "epoch 4; batch 69248; loss 0.000459\n",
      "epoch:4; batch 69248; train accuracy: 0.922336\n",
      "epoch 4; batch 69376; loss 0.003360\n",
      "epoch:4; batch 69376; train accuracy: 0.922362\n",
      "epoch 4; batch 69504; loss 0.006564\n",
      "epoch:4; batch 69504; train accuracy: 0.922385\n",
      "epoch 4; batch 69632; loss 0.032899\n",
      "epoch:4; batch 69632; train accuracy: 0.922408\n",
      "epoch 4; batch 69760; loss 0.109687\n",
      "epoch:4; batch 69760; train accuracy: 0.922431\n",
      "epoch 4; batch 69888; loss 0.007003\n",
      "epoch:4; batch 69888; train accuracy: 0.922457\n",
      "epoch 4; batch 70016; loss 0.003855\n",
      "epoch:4; batch 70016; train accuracy: 0.922483\n",
      "epoch 4; batch 70144; loss 0.027162\n",
      "epoch:4; batch 70144; train accuracy: 0.922506\n",
      "epoch 4; batch 70272; loss 0.002956\n",
      "epoch:4; batch 70272; train accuracy: 0.922532\n",
      "epoch 4; batch 70400; loss 0.001337\n",
      "epoch:4; batch 70400; train accuracy: 0.922558\n",
      "epoch 4; batch 70528; loss 0.009199\n",
      "epoch:4; batch 70528; train accuracy: 0.922583\n",
      "epoch 4; batch 70656; loss 0.034811\n",
      "epoch:4; batch 70656; train accuracy: 0.922606\n",
      "epoch 4; batch 70784; loss 0.111702\n",
      "epoch:4; batch 70784; train accuracy: 0.922630\n",
      "epoch 4; batch 70912; loss 0.010548\n",
      "epoch:4; batch 70912; train accuracy: 0.922653\n",
      "epoch 4; batch 71040; loss 0.013277\n",
      "epoch:4; batch 71040; train accuracy: 0.922676\n",
      "epoch 4; batch 71168; loss 0.010419\n",
      "epoch:4; batch 71168; train accuracy: 0.922699\n",
      "epoch 4; batch 71296; loss 0.030137\n",
      "epoch:4; batch 71296; train accuracy: 0.922717\n",
      "epoch 4; batch 71424; loss 0.005732\n",
      "epoch:4; batch 71424; train accuracy: 0.922742\n",
      "epoch 4; batch 71552; loss 0.001802\n",
      "epoch:4; batch 71552; train accuracy: 0.922768\n",
      "epoch 4; batch 71680; loss 0.023102\n",
      "epoch:4; batch 71680; train accuracy: 0.922791\n",
      "epoch 4; batch 71808; loss 0.001728\n",
      "epoch:4; batch 71808; train accuracy: 0.922816\n",
      "epoch 4; batch 71936; loss 0.036108\n",
      "epoch:4; batch 71936; train accuracy: 0.922839\n",
      "epoch 4; batch 72064; loss 0.008645\n",
      "epoch:4; batch 72064; train accuracy: 0.922862\n",
      "epoch 4; batch 72192; loss 0.001210\n",
      "epoch:4; batch 72192; train accuracy: 0.922888\n",
      "epoch 4; batch 72320; loss 0.014757\n",
      "epoch:4; batch 72320; train accuracy: 0.922911\n",
      "epoch 4; batch 72448; loss 0.002286\n",
      "epoch:4; batch 72448; train accuracy: 0.922936\n",
      "epoch 4; batch 72576; loss 0.004127\n",
      "epoch:4; batch 72576; train accuracy: 0.922962\n",
      "epoch 4; batch 72704; loss 0.006071\n",
      "epoch:4; batch 72704; train accuracy: 0.922987\n",
      "epoch 4; batch 72832; loss 0.028368\n",
      "epoch:4; batch 72832; train accuracy: 0.923010\n",
      "epoch 4; batch 72960; loss 0.003684\n",
      "epoch:4; batch 72960; train accuracy: 0.923035\n",
      "epoch 4; batch 73088; loss 0.022354\n",
      "epoch:4; batch 73088; train accuracy: 0.923056\n",
      "epoch 4; batch 73216; loss 0.011963\n",
      "epoch:4; batch 73216; train accuracy: 0.923081\n",
      "epoch 4; batch 73344; loss 0.008204\n",
      "epoch:4; batch 73344; train accuracy: 0.923106\n",
      "epoch 4; batch 73472; loss 0.002976\n",
      "epoch:4; batch 73472; train accuracy: 0.923132\n",
      "epoch 4; batch 73600; loss 0.006858\n",
      "epoch:4; batch 73600; train accuracy: 0.923154\n",
      "epoch 4; batch 73728; loss 0.001589\n",
      "epoch:4; batch 73728; train accuracy: 0.923180\n",
      "epoch 4; batch 73856; loss 0.008125\n",
      "epoch:4; batch 73856; train accuracy: 0.923202\n",
      "epoch 4; batch 73984; loss 0.016245\n",
      "epoch:4; batch 73984; train accuracy: 0.923225\n",
      "epoch 4; batch 74112; loss 0.014337\n",
      "epoch:4; batch 74112; train accuracy: 0.923248\n",
      "epoch 4; batch 74240; loss 0.002952\n",
      "epoch:4; batch 74240; train accuracy: 0.923273\n",
      "epoch 4; batch 74368; loss 0.012719\n",
      "epoch:4; batch 74368; train accuracy: 0.923296\n",
      "epoch 4; batch 74496; loss 0.008418\n",
      "epoch:4; batch 74496; train accuracy: 0.923321\n",
      "epoch 4; batch 74624; loss 0.003672\n",
      "epoch:4; batch 74624; train accuracy: 0.923346\n",
      "epoch 4; batch 74752; loss 0.013871\n",
      "epoch:4; batch 74752; train accuracy: 0.923369\n",
      "epoch 4; batch 74880; loss 0.010048\n",
      "epoch:4; batch 74880; train accuracy: 0.923394\n",
      "epoch 4; batch 75008; loss 0.002086\n",
      "epoch:4; batch 75008; train accuracy: 0.923419\n",
      "epoch 4; batch 75136; loss 0.006353\n",
      "epoch:4; batch 75136; train accuracy: 0.923444\n",
      "epoch 4; batch 75264; loss 0.009923\n",
      "epoch:4; batch 75264; train accuracy: 0.923467\n",
      "epoch 4; batch 75392; loss 0.001860\n",
      "epoch:4; batch 75392; train accuracy: 0.923492\n",
      "epoch 4; batch 75520; loss 0.003707\n",
      "epoch:4; batch 75520; train accuracy: 0.923517\n",
      "epoch 4; batch 75648; loss 0.112425\n",
      "epoch:4; batch 75648; train accuracy: 0.923532\n",
      "epoch 4; batch 75776; loss 0.000673\n",
      "epoch:4; batch 75776; train accuracy: 0.923557\n",
      "epoch 4; batch 75904; loss 0.006152\n",
      "epoch:4; batch 75904; train accuracy: 0.923582\n",
      "epoch 4; batch 76032; loss 0.007944\n",
      "epoch:4; batch 76032; train accuracy: 0.923607\n",
      "epoch 4; batch 76160; loss 0.007042\n",
      "epoch:4; batch 76160; train accuracy: 0.923632\n",
      "epoch 4; batch 76288; loss 0.013745\n",
      "epoch:4; batch 76288; train accuracy: 0.923654\n",
      "epoch 4; batch 76416; loss 0.012495\n",
      "epoch:4; batch 76416; train accuracy: 0.923677\n",
      "epoch 4; batch 76544; loss 0.013761\n",
      "epoch:4; batch 76544; train accuracy: 0.923699\n",
      "epoch 4; batch 76672; loss 0.000609\n",
      "epoch:4; batch 76672; train accuracy: 0.923724\n",
      "epoch 4; batch 76800; loss 0.000951\n",
      "epoch:4; batch 76800; train accuracy: 0.923749\n",
      "epoch 4; batch 76928; loss 0.003020\n",
      "epoch:4; batch 76928; train accuracy: 0.923774\n",
      "epoch 4; batch 77056; loss 0.016731\n",
      "epoch:4; batch 77056; train accuracy: 0.923796\n",
      "epoch 4; batch 77184; loss 0.009971\n",
      "epoch:4; batch 77184; train accuracy: 0.923819\n",
      "epoch 4; batch 77312; loss 0.002403\n",
      "epoch:4; batch 77312; train accuracy: 0.923843\n",
      "epoch 4; batch 77440; loss 0.014576\n",
      "epoch:4; batch 77440; train accuracy: 0.923868\n",
      "epoch 4; batch 77568; loss 0.002187\n",
      "epoch:4; batch 77568; train accuracy: 0.923893\n",
      "epoch 4; batch 77696; loss 0.005894\n",
      "epoch:4; batch 77696; train accuracy: 0.923918\n",
      "epoch 4; batch 77824; loss 0.006975\n",
      "epoch:4; batch 77824; train accuracy: 0.923943\n",
      "epoch 4; batch 77952; loss 0.001391\n",
      "epoch:4; batch 77952; train accuracy: 0.923967\n",
      "epoch 4; batch 78080; loss 0.000731\n",
      "epoch:4; batch 78080; train accuracy: 0.923992\n",
      "epoch 4; batch 78208; loss 0.018248\n",
      "epoch:4; batch 78208; train accuracy: 0.924012\n",
      "epoch 4; batch 78336; loss 0.005108\n",
      "epoch:4; batch 78336; train accuracy: 0.924037\n",
      "epoch 4; batch 78464; loss 0.001737\n",
      "epoch:4; batch 78464; train accuracy: 0.924061\n",
      "epoch 4; batch 78592; loss 0.006922\n",
      "epoch:4; batch 78592; train accuracy: 0.924086\n",
      "epoch 4; batch 78720; loss 0.001939\n",
      "epoch:4; batch 78720; train accuracy: 0.924111\n",
      "epoch 4; batch 78848; loss 0.014881\n",
      "epoch:4; batch 78848; train accuracy: 0.924133\n",
      "epoch 4; batch 78976; loss 0.002175\n",
      "epoch:4; batch 78976; train accuracy: 0.924158\n",
      "epoch 4; batch 79104; loss 0.006219\n",
      "epoch:4; batch 79104; train accuracy: 0.924182\n",
      "epoch 4; batch 79232; loss 0.033722\n",
      "epoch:4; batch 79232; train accuracy: 0.924204\n",
      "epoch 4; batch 79360; loss 0.003938\n",
      "epoch:4; batch 79360; train accuracy: 0.924229\n",
      "epoch 4; batch 79488; loss 0.000704\n",
      "epoch:4; batch 79488; train accuracy: 0.924253\n",
      "epoch 4; batch 79616; loss 0.025230\n",
      "epoch:4; batch 79616; train accuracy: 0.924276\n",
      "epoch 4; batch 79744; loss 0.008147\n",
      "epoch:4; batch 79744; train accuracy: 0.924298\n",
      "epoch 4; batch 79872; loss 0.018946\n",
      "epoch:4; batch 79872; train accuracy: 0.924320\n",
      "epoch 4; batch 80000; loss 0.013726\n",
      "epoch:4; batch 80000; train accuracy: 0.924342\n",
      "epoch 4; batch 80128; loss 0.001825\n",
      "epoch:4; batch 80128; train accuracy: 0.924366\n",
      "epoch 4; batch 80256; loss 0.002704\n",
      "epoch:4; batch 80256; train accuracy: 0.924391\n",
      "epoch 4; batch 80384; loss 0.003908\n",
      "epoch:4; batch 80384; train accuracy: 0.924415\n",
      "epoch 4; batch 80512; loss 0.001494\n",
      "epoch:4; batch 80512; train accuracy: 0.924440\n",
      "epoch 4; batch 80640; loss 0.001241\n",
      "epoch:4; batch 80640; train accuracy: 0.924464\n",
      "epoch 4; batch 80768; loss 0.001379\n",
      "epoch:4; batch 80768; train accuracy: 0.924488\n",
      "epoch 4; batch 80896; loss 0.002410\n",
      "epoch:4; batch 80896; train accuracy: 0.924513\n",
      "epoch 4; batch 81024; loss 0.011118\n",
      "epoch:4; batch 81024; train accuracy: 0.924535\n",
      "epoch 4; batch 81152; loss 0.001130\n",
      "epoch:4; batch 81152; train accuracy: 0.924559\n",
      "epoch 4; batch 81280; loss 0.000753\n",
      "epoch:4; batch 81280; train accuracy: 0.924583\n",
      "epoch 4; batch 81408; loss 0.004020\n",
      "epoch:4; batch 81408; train accuracy: 0.924608\n",
      "epoch 4; batch 81536; loss 0.015698\n",
      "epoch:4; batch 81536; train accuracy: 0.924630\n",
      "epoch 4; batch 81664; loss 0.000973\n",
      "epoch:4; batch 81664; train accuracy: 0.924654\n",
      "epoch 4; batch 81792; loss 0.001379\n",
      "epoch:4; batch 81792; train accuracy: 0.924678\n",
      "epoch 4; batch 81920; loss 0.011331\n",
      "epoch:4; batch 81920; train accuracy: 0.924700\n",
      "epoch 4; batch 82048; loss 0.007622\n",
      "epoch:4; batch 82048; train accuracy: 0.924722\n",
      "epoch 4; batch 82176; loss 0.001241\n",
      "epoch:4; batch 82176; train accuracy: 0.924746\n",
      "epoch 4; batch 82304; loss 0.001829\n",
      "epoch:4; batch 82304; train accuracy: 0.924770\n",
      "epoch 4; batch 82432; loss 0.132013\n",
      "epoch:4; batch 82432; train accuracy: 0.924792\n",
      "epoch 4; batch 82560; loss 0.012708\n",
      "epoch:4; batch 82560; train accuracy: 0.924814\n",
      "epoch 4; batch 82688; loss 0.023759\n",
      "epoch:4; batch 82688; train accuracy: 0.924836\n",
      "epoch 4; batch 82816; loss 0.000543\n",
      "epoch:4; batch 82816; train accuracy: 0.924860\n",
      "epoch 4; batch 82944; loss 0.016181\n",
      "epoch:4; batch 82944; train accuracy: 0.924881\n",
      "epoch 4; batch 83072; loss 0.003911\n",
      "epoch:4; batch 83072; train accuracy: 0.924906\n",
      "epoch 4; batch 83200; loss 0.057166\n",
      "epoch:4; batch 83200; train accuracy: 0.924927\n",
      "epoch 4; batch 83328; loss 0.004989\n",
      "epoch:4; batch 83328; train accuracy: 0.924951\n",
      "epoch 4; batch 83456; loss 0.044949\n",
      "epoch:4; batch 83456; train accuracy: 0.924973\n",
      "epoch 4; batch 83584; loss 0.004603\n",
      "epoch:4; batch 83584; train accuracy: 0.924997\n",
      "epoch 4; batch 83712; loss 0.002613\n",
      "epoch:4; batch 83712; train accuracy: 0.925021\n",
      "epoch 4; batch 83840; loss 0.001696\n",
      "epoch:4; batch 83840; train accuracy: 0.925045\n",
      "epoch 4; batch 83968; loss 0.000554\n",
      "epoch:4; batch 83968; train accuracy: 0.925069\n",
      "epoch 4; batch 84096; loss 0.026801\n",
      "epoch:4; batch 84096; train accuracy: 0.925091\n",
      "epoch 4; batch 84224; loss 0.000695\n",
      "epoch:4; batch 84224; train accuracy: 0.925115\n",
      "epoch 4; batch 84352; loss 0.002337\n",
      "epoch:4; batch 84352; train accuracy: 0.925139\n",
      "epoch 4; batch 84480; loss 0.011709\n",
      "epoch:4; batch 84480; train accuracy: 0.925160\n",
      "epoch 4; batch 84608; loss 0.099023\n",
      "epoch:4; batch 84608; train accuracy: 0.925182\n",
      "epoch 4; batch 84736; loss 0.019728\n",
      "epoch:4; batch 84736; train accuracy: 0.925201\n",
      "epoch 4; batch 84864; loss 0.003246\n",
      "epoch:4; batch 84864; train accuracy: 0.925225\n",
      "epoch 4; batch 84992; loss 0.005803\n",
      "epoch:4; batch 84992; train accuracy: 0.925249\n",
      "epoch 4; batch 85120; loss 0.006376\n",
      "epoch:4; batch 85120; train accuracy: 0.925273\n",
      "epoch 4; batch 85248; loss 0.000944\n",
      "epoch:4; batch 85248; train accuracy: 0.925296\n",
      "epoch 4; batch 85376; loss 0.007291\n",
      "epoch:4; batch 85376; train accuracy: 0.925320\n",
      "epoch 4; batch 85504; loss 0.001244\n",
      "epoch:4; batch 85504; train accuracy: 0.925344\n",
      "epoch 4; batch 85632; loss 0.002979\n",
      "epoch:4; batch 85632; train accuracy: 0.925368\n",
      "epoch 4; batch 85760; loss 0.148620\n",
      "epoch:4; batch 85760; train accuracy: 0.925387\n",
      "epoch 4; batch 85888; loss 0.024729\n",
      "epoch:4; batch 85888; train accuracy: 0.925408\n",
      "epoch 4; batch 86016; loss 0.011898\n",
      "epoch:4; batch 86016; train accuracy: 0.925432\n",
      "epoch 4; batch 86144; loss 0.010045\n",
      "epoch:4; batch 86144; train accuracy: 0.925453\n",
      "epoch 4; batch 86272; loss 0.003091\n",
      "epoch:4; batch 86272; train accuracy: 0.925477\n",
      "epoch 4; batch 86400; loss 0.012363\n",
      "epoch:4; batch 86400; train accuracy: 0.925501\n",
      "epoch 4; batch 86528; loss 0.002500\n",
      "epoch:4; batch 86528; train accuracy: 0.925525\n",
      "epoch 4; batch 86656; loss 0.005633\n",
      "epoch:4; batch 86656; train accuracy: 0.925548\n",
      "epoch 4; batch 86784; loss 0.128721\n",
      "epoch:4; batch 86784; train accuracy: 0.925570\n",
      "epoch 4; batch 86912; loss 0.003903\n",
      "epoch:4; batch 86912; train accuracy: 0.925593\n",
      "epoch 4; batch 87040; loss 0.025724\n",
      "epoch:4; batch 87040; train accuracy: 0.925615\n",
      "epoch 4; batch 87168; loss 0.022136\n",
      "epoch:4; batch 87168; train accuracy: 0.925636\n",
      "epoch 4; batch 87296; loss 0.003236\n",
      "epoch:4; batch 87296; train accuracy: 0.925659\n",
      "epoch 4; batch 87424; loss 0.015359\n",
      "epoch:4; batch 87424; train accuracy: 0.925681\n",
      "epoch 4; batch 87552; loss 0.000505\n",
      "epoch:4; batch 87552; train accuracy: 0.925704\n",
      "epoch 4; batch 87680; loss 0.004837\n",
      "epoch:4; batch 87680; train accuracy: 0.925728\n",
      "epoch 4; batch 87808; loss 0.004586\n",
      "epoch:4; batch 87808; train accuracy: 0.925751\n",
      "epoch 4; batch 87936; loss 0.134428\n",
      "epoch:4; batch 87936; train accuracy: 0.925773\n",
      "epoch 4; batch 88064; loss 0.048030\n",
      "epoch:4; batch 88064; train accuracy: 0.925791\n",
      "epoch 4; batch 88192; loss 0.008524\n",
      "epoch:4; batch 88192; train accuracy: 0.925815\n",
      "epoch 4; batch 88320; loss 0.012779\n",
      "epoch:4; batch 88320; train accuracy: 0.925836\n",
      "epoch 4; batch 88448; loss 0.001990\n",
      "epoch:4; batch 88448; train accuracy: 0.925859\n",
      "epoch 4; batch 88576; loss 0.000950\n",
      "epoch:4; batch 88576; train accuracy: 0.925883\n",
      "epoch 4; batch 88704; loss 0.019103\n",
      "epoch:4; batch 88704; train accuracy: 0.925904\n",
      "epoch 4; batch 88832; loss 0.007407\n",
      "epoch:4; batch 88832; train accuracy: 0.925927\n",
      "epoch 4; batch 88960; loss 0.002148\n",
      "epoch:4; batch 88960; train accuracy: 0.925951\n",
      "epoch 4; batch 89088; loss 0.006560\n",
      "epoch:4; batch 89088; train accuracy: 0.925974\n",
      "epoch 4; batch 89216; loss 0.048584\n",
      "epoch:4; batch 89216; train accuracy: 0.925995\n",
      "epoch 4; batch 89344; loss 0.003432\n",
      "epoch:4; batch 89344; train accuracy: 0.926019\n",
      "epoch 4; batch 89472; loss 0.001452\n",
      "epoch:4; batch 89472; train accuracy: 0.926042\n",
      "epoch 4; batch 89600; loss 0.000507\n",
      "epoch:4; batch 89600; train accuracy: 0.926066\n",
      "epoch 4; batch 89728; loss 0.004893\n",
      "epoch:4; batch 89728; train accuracy: 0.926089\n",
      "epoch 4; batch 89856; loss 0.025371\n",
      "epoch:4; batch 89856; train accuracy: 0.926110\n",
      "epoch 4; batch 89984; loss 0.015875\n",
      "epoch:4; batch 89984; train accuracy: 0.926131\n",
      "epoch 4; batch 90112; loss 0.003359\n",
      "epoch:4; batch 90112; train accuracy: 0.926154\n",
      "epoch 4; batch 90240; loss 0.048848\n",
      "epoch:4; batch 90240; train accuracy: 0.926175\n",
      "epoch 4; batch 90368; loss 0.011813\n",
      "epoch:4; batch 90368; train accuracy: 0.926196\n",
      "epoch 4; batch 90496; loss 0.031202\n",
      "epoch:4; batch 90496; train accuracy: 0.926217\n",
      "epoch 4; batch 90624; loss 0.019548\n",
      "epoch:4; batch 90624; train accuracy: 0.926237\n",
      "epoch 4; batch 90752; loss 0.052236\n",
      "epoch:4; batch 90752; train accuracy: 0.926256\n",
      "epoch 4; batch 90880; loss 0.012714\n",
      "epoch:4; batch 90880; train accuracy: 0.926279\n",
      "epoch 4; batch 91008; loss 0.006229\n",
      "epoch:4; batch 91008; train accuracy: 0.926302\n",
      "epoch 4; batch 91136; loss 0.007901\n",
      "epoch:4; batch 91136; train accuracy: 0.926326\n",
      "epoch 4; batch 91264; loss 0.003949\n",
      "epoch:4; batch 91264; train accuracy: 0.926349\n",
      "epoch 4; batch 91392; loss 0.000666\n",
      "epoch:4; batch 91392; train accuracy: 0.926372\n",
      "epoch 4; batch 91520; loss 0.084649\n",
      "epoch:4; batch 91520; train accuracy: 0.926390\n",
      "epoch 4; batch 91648; loss 0.003957\n",
      "epoch:4; batch 91648; train accuracy: 0.926413\n",
      "epoch 4; batch 91776; loss 0.015863\n",
      "epoch:4; batch 91776; train accuracy: 0.926434\n",
      "epoch 4; batch 91904; loss 0.006731\n",
      "epoch:4; batch 91904; train accuracy: 0.926455\n",
      "epoch 4; batch 92032; loss 0.005046\n",
      "epoch:4; batch 92032; train accuracy: 0.926478\n",
      "epoch 4; batch 92160; loss 0.080372\n",
      "epoch:4; batch 92160; train accuracy: 0.926499\n",
      "epoch 4; batch 92288; loss 0.002598\n",
      "epoch:4; batch 92288; train accuracy: 0.926522\n",
      "epoch 4; batch 92416; loss 0.003054\n",
      "epoch:4; batch 92416; train accuracy: 0.926545\n",
      "epoch 4; batch 92544; loss 0.002208\n",
      "epoch:4; batch 92544; train accuracy: 0.926568\n",
      "epoch 4; batch 92672; loss 0.002353\n",
      "epoch:4; batch 92672; train accuracy: 0.926591\n",
      "epoch 4; batch 92800; loss 0.002285\n",
      "epoch:4; batch 92800; train accuracy: 0.926614\n",
      "epoch 4; batch 92928; loss 0.008388\n",
      "epoch:4; batch 92928; train accuracy: 0.926635\n",
      "epoch 4; batch 93056; loss 0.000868\n",
      "epoch:4; batch 93056; train accuracy: 0.926658\n",
      "epoch 4; batch 93184; loss 0.019801\n",
      "epoch:4; batch 93184; train accuracy: 0.926678\n",
      "epoch 4; batch 93312; loss 0.041970\n",
      "epoch:4; batch 93312; train accuracy: 0.926696\n",
      "epoch 4; batch 93440; loss 0.021250\n",
      "epoch:4; batch 93440; train accuracy: 0.926714\n",
      "epoch 4; batch 93568; loss 0.006056\n",
      "epoch:4; batch 93568; train accuracy: 0.926737\n",
      "epoch 4; batch 93696; loss 0.007867\n",
      "epoch:4; batch 93696; train accuracy: 0.926760\n",
      "epoch 4; batch 93824; loss 0.004587\n",
      "epoch:4; batch 93824; train accuracy: 0.926783\n",
      "epoch 4; batch 93952; loss 0.039200\n",
      "epoch:4; batch 93952; train accuracy: 0.926801\n",
      "epoch 4; batch 94080; loss 0.003514\n",
      "epoch:4; batch 94080; train accuracy: 0.926824\n",
      "epoch 4; batch 94208; loss 0.021094\n",
      "epoch:4; batch 94208; train accuracy: 0.926845\n",
      "epoch 4; batch 94336; loss 0.001573\n",
      "epoch:4; batch 94336; train accuracy: 0.926867\n",
      "epoch 4; batch 94464; loss 0.000733\n",
      "epoch:4; batch 94464; train accuracy: 0.926890\n",
      "epoch 4; batch 94592; loss 0.070017\n",
      "epoch:4; batch 94592; train accuracy: 0.926911\n",
      "epoch 4; batch 94720; loss 0.001266\n",
      "epoch:4; batch 94720; train accuracy: 0.926934\n",
      "epoch 4; batch 94848; loss 0.009361\n",
      "epoch:4; batch 94848; train accuracy: 0.926954\n",
      "epoch 4; batch 94976; loss 0.087761\n",
      "epoch:4; batch 94976; train accuracy: 0.926972\n",
      "epoch 4; batch 95104; loss 0.004319\n",
      "epoch:4; batch 95104; train accuracy: 0.926995\n",
      "epoch 4; batch 95232; loss 0.022672\n",
      "epoch:4; batch 95232; train accuracy: 0.927015\n",
      "epoch 4; batch 95360; loss 0.001267\n",
      "epoch:4; batch 95360; train accuracy: 0.927038\n",
      "epoch 4; batch 95488; loss 0.003176\n",
      "epoch:4; batch 95488; train accuracy: 0.927061\n",
      "epoch 4; batch 95616; loss 0.002126\n",
      "epoch:4; batch 95616; train accuracy: 0.927083\n",
      "epoch 4; batch 95744; loss 0.007216\n",
      "epoch:4; batch 95744; train accuracy: 0.927104\n",
      "epoch 4; batch 95872; loss 0.000363\n",
      "epoch:4; batch 95872; train accuracy: 0.927126\n",
      "epoch 4; batch 96000; loss 0.003111\n",
      "epoch:4; batch 96000; train accuracy: 0.927149\n",
      "epoch 4; batch 96128; loss 0.011564\n",
      "epoch:4; batch 96128; train accuracy: 0.927169\n",
      "epoch 4; batch 96256; loss 0.003271\n",
      "epoch:4; batch 96256; train accuracy: 0.927192\n",
      "epoch 4; batch 96384; loss 0.000749\n",
      "epoch:4; batch 96384; train accuracy: 0.927215\n",
      "epoch 4; batch 96512; loss 0.003627\n",
      "epoch:4; batch 96512; train accuracy: 0.927237\n",
      "epoch 4; batch 96640; loss 0.000539\n",
      "epoch:4; batch 96640; train accuracy: 0.927260\n",
      "epoch 4; batch 96768; loss 0.070283\n",
      "epoch:4; batch 96768; train accuracy: 0.927278\n",
      "epoch 4; batch 96896; loss 0.007751\n",
      "epoch:4; batch 96896; train accuracy: 0.927300\n",
      "epoch 4; batch 97024; loss 0.001358\n",
      "epoch:4; batch 97024; train accuracy: 0.927323\n",
      "epoch 4; batch 97152; loss 0.035848\n",
      "epoch:4; batch 97152; train accuracy: 0.927343\n",
      "epoch 4; batch 97280; loss 0.000727\n",
      "epoch:4; batch 97280; train accuracy: 0.927366\n",
      "epoch 4; batch 97408; loss 0.002145\n",
      "epoch:4; batch 97408; train accuracy: 0.927388\n",
      "epoch 4; batch 97536; loss 0.004445\n",
      "epoch:4; batch 97536; train accuracy: 0.927411\n",
      "epoch 4; batch 97664; loss 0.000817\n",
      "epoch:4; batch 97664; train accuracy: 0.927433\n",
      "epoch 4; batch 97792; loss 0.002290\n",
      "epoch:4; batch 97792; train accuracy: 0.927456\n",
      "epoch 4; batch 97920; loss 0.019715\n",
      "epoch:4; batch 97920; train accuracy: 0.927476\n",
      "epoch 4; batch 98048; loss 0.018962\n",
      "epoch:4; batch 98048; train accuracy: 0.927496\n",
      "epoch 4; batch 98176; loss 0.004497\n",
      "epoch:4; batch 98176; train accuracy: 0.927518\n",
      "epoch 4; batch 98304; loss 0.002289\n",
      "epoch:4; batch 98304; train accuracy: 0.927541\n",
      "epoch 4; batch 98432; loss 0.005364\n",
      "epoch:4; batch 98432; train accuracy: 0.927563\n",
      "epoch 4; batch 98560; loss 0.003477\n",
      "epoch:4; batch 98560; train accuracy: 0.927586\n",
      "epoch 4; batch 98688; loss 0.014584\n",
      "epoch:4; batch 98688; train accuracy: 0.927606\n",
      "epoch 4; batch 98816; loss 0.052511\n",
      "epoch:4; batch 98816; train accuracy: 0.927626\n",
      "epoch 4; batch 98944; loss 0.005671\n",
      "epoch:4; batch 98944; train accuracy: 0.927648\n",
      "epoch 4; batch 99072; loss 0.031835\n",
      "epoch:4; batch 99072; train accuracy: 0.927666\n",
      "epoch 4; batch 99200; loss 0.006215\n",
      "epoch:4; batch 99200; train accuracy: 0.927688\n",
      "epoch 4; batch 99328; loss 0.002342\n",
      "epoch:4; batch 99328; train accuracy: 0.927710\n",
      "epoch 4; batch 99456; loss 0.001988\n",
      "epoch:4; batch 99456; train accuracy: 0.927733\n",
      "epoch 4; batch 99584; loss 0.016854\n",
      "epoch:4; batch 99584; train accuracy: 0.927752\n",
      "epoch 4; batch 99712; loss 0.130033\n",
      "epoch:4; batch 99712; train accuracy: 0.927770\n",
      "epoch 4; batch 99840; loss 0.002309\n",
      "epoch:4; batch 99840; train accuracy: 0.927792\n",
      "epoch 4; batch 99968; loss 0.028835\n",
      "epoch:4; batch 99968; train accuracy: 0.927810\n",
      "epoch 4; batch 100096; loss 0.005108\n",
      "epoch:4; batch 100096; train accuracy: 0.927832\n",
      "epoch 4; batch 100224; loss 0.010075\n",
      "epoch:4; batch 100224; train accuracy: 0.927852\n",
      "epoch 4; batch 100352; loss 0.000881\n",
      "epoch:4; batch 100352; train accuracy: 0.927874\n",
      "epoch 4; batch 100480; loss 0.005463\n",
      "epoch:4; batch 100480; train accuracy: 0.927896\n",
      "epoch 4; batch 100608; loss 0.001702\n",
      "epoch:4; batch 100608; train accuracy: 0.927918\n",
      "epoch 4; batch 100736; loss 0.000925\n",
      "epoch:4; batch 100736; train accuracy: 0.927941\n",
      "epoch 4; batch 100864; loss 0.002894\n",
      "epoch:4; batch 100864; train accuracy: 0.927963\n",
      "epoch 4; batch 100992; loss 0.007944\n",
      "epoch:4; batch 100992; train accuracy: 0.927985\n",
      "epoch 4; batch 101120; loss 0.001807\n",
      "epoch:4; batch 101120; train accuracy: 0.928007\n",
      "epoch 4; batch 101248; loss 0.002591\n",
      "epoch:4; batch 101248; train accuracy: 0.928029\n",
      "epoch 4; batch 101376; loss 0.005174\n",
      "epoch:4; batch 101376; train accuracy: 0.928051\n",
      "epoch 4; batch 101504; loss 0.003029\n",
      "epoch:4; batch 101504; train accuracy: 0.928074\n",
      "epoch 4; batch 101632; loss 0.013605\n",
      "epoch:4; batch 101632; train accuracy: 0.928093\n",
      "epoch 4; batch 101760; loss 0.019846\n",
      "epoch:4; batch 101760; train accuracy: 0.928113\n",
      "epoch 4; batch 101888; loss 0.001168\n",
      "epoch:4; batch 101888; train accuracy: 0.928135\n",
      "epoch 4; batch 102016; loss 0.005655\n",
      "epoch:4; batch 102016; train accuracy: 0.928157\n",
      "epoch 4; batch 102144; loss 0.008991\n",
      "epoch:4; batch 102144; train accuracy: 0.928177\n",
      "epoch 4; batch 102272; loss 0.009148\n",
      "epoch:4; batch 102272; train accuracy: 0.928196\n",
      "epoch 4; batch 102400; loss 0.029322\n",
      "epoch:4; batch 102400; train accuracy: 0.928216\n",
      "epoch 4; batch 102528; loss 0.009713\n",
      "epoch:4; batch 102528; train accuracy: 0.928236\n",
      "epoch 4; batch 102656; loss 0.014434\n",
      "epoch:4; batch 102656; train accuracy: 0.928255\n",
      "epoch 4; batch 102784; loss 0.013357\n",
      "epoch:4; batch 102784; train accuracy: 0.928277\n",
      "epoch 4; batch 102912; loss 0.003247\n",
      "epoch:4; batch 102912; train accuracy: 0.928299\n",
      "epoch 4; batch 103040; loss 0.012186\n",
      "epoch:4; batch 103040; train accuracy: 0.928319\n",
      "epoch 4; batch 103168; loss 0.052068\n",
      "epoch:4; batch 103168; train accuracy: 0.928338\n",
      "epoch 4; batch 103296; loss 0.001347\n",
      "epoch:4; batch 103296; train accuracy: 0.928360\n",
      "epoch 4; batch 103424; loss 0.011786\n",
      "epoch:4; batch 103424; train accuracy: 0.928380\n",
      "epoch 4; batch 103552; loss 0.012574\n",
      "epoch:4; batch 103552; train accuracy: 0.928399\n",
      "epoch 4; batch 103680; loss 0.027329\n",
      "epoch:4; batch 103680; train accuracy: 0.928416\n",
      "epoch 4; batch 103808; loss 0.034341\n",
      "epoch:4; batch 103808; train accuracy: 0.928436\n",
      "epoch 4; batch 103936; loss 0.001112\n",
      "epoch:4; batch 103936; train accuracy: 0.928458\n",
      "epoch 4; batch 104064; loss 0.008859\n",
      "epoch:4; batch 104064; train accuracy: 0.928480\n",
      "epoch 4; batch 104192; loss 0.032916\n",
      "epoch:4; batch 104192; train accuracy: 0.928499\n",
      "epoch 4; batch 104320; loss 0.019563\n",
      "epoch:4; batch 104320; train accuracy: 0.928519\n",
      "epoch 4; batch 104448; loss 0.093275\n",
      "epoch:4; batch 104448; train accuracy: 0.928533\n",
      "epoch 4; batch 104576; loss 0.005313\n",
      "epoch:4; batch 104576; train accuracy: 0.928555\n",
      "epoch 4; batch 104704; loss 0.026400\n",
      "epoch:4; batch 104704; train accuracy: 0.928575\n",
      "epoch 4; batch 104832; loss 0.004240\n",
      "epoch:4; batch 104832; train accuracy: 0.928596\n",
      "epoch 4; batch 104960; loss 0.003954\n",
      "epoch:4; batch 104960; train accuracy: 0.928618\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 59106 ; rate: 0.563129\n",
      "y_true_label_1_num: 3644 ; rate: 0.034718\n",
      "y_true_label_2_num: 3067 ; rate: 0.029221\n",
      "y_true_label_3_num: 39143 ; rate: 0.372933\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.995989\n",
      "valid avg_precision: 0.996037\n",
      "valid avg_recall: 0.995875\n",
      "valid avg_f1: 0.995955\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 8540 ; rate: 0.570246\n",
      "y_true_label_1_num: 496 ; rate: 0.033120\n",
      "y_true_label_2_num: 425 ; rate: 0.028379\n",
      "y_true_label_3_num: 5515 ; rate: 0.368256\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.842748\n",
      "valid avg_precision: 0.841921\n",
      "valid avg_recall: 0.841346\n",
      "valid avg_f1: 0.839529\n"
     ]
    }
   ],
   "source": [
    "#训练过程\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "for num in range(epochs):\n",
    "    print('epoch %d' % (num+1))\n",
    "    train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=10000).batch(batch_size,drop_remainder=True)\n",
    "    index = 0\n",
    "    for x,y in train:\n",
    "        index +=1\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(x)\n",
    "            #print(y.shape)\n",
    "            #print(labels_pred.shape)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            sparse_categorical_accuracy.update_state(y_true = y, y_pred= labels_pred)\n",
    "            #loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=labels_pred)\n",
    "            print(\"epoch %d; batch %d; loss %f\" % (num+1, index*batch_size, loss.numpy()))\n",
    "            print(\"epoch:%d; batch %d; train accuracy: %f\" % (num+1, index*batch_size, sparse_categorical_accuracy.result()))\n",
    "            grads = tape.gradient(loss, model.trainable_variables) #梯度\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables)) #梯度和变量\n",
    "    \n",
    "    # 模型在训练集上 的评估参数,batch_size 128\n",
    "    print('模型在训练集上的评价指标：')\n",
    "    eval_train = eval_of_experiment((train_x,train_y), batch_size ,model)\n",
    "    eval_train.eval_of_valid()\n",
    "    eval_train.result()\n",
    "    #  模型在验证集上 的评估参数，batch_size 128\n",
    "    print('模型在验证集上的评价指标：')\n",
    "    eval_valid = eval_of_experiment((valid_x,valid_y), batch_size ,model)\n",
    "    eval_valid.eval_of_valid()\n",
    "    eval_valid.result()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5  service 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "embedding_matrix = load_obj('./data2/train_sales_embedding_matrix.pkl')\n",
    "\n",
    "model = single_attention_aspect(\n",
    "                maxlen=maxlen, \n",
    "                embedding_matrix=embedding_matrix, \n",
    "                # aspect=loc,\n",
    "                aspect=ser,\n",
    "                embedding_dim=200,\n",
    "                aspect_len=20,\n",
    "                hidden_size=100,\n",
    "                activation=None,\n",
    "                output_size=4\n",
    "            )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "epoch 1; batch 128; loss 1.396342\n",
      "epoch:1; batch 128; train accuracy: 0.140625\n",
      "epoch 1; batch 256; loss 1.352156\n",
      "epoch:1; batch 256; train accuracy: 0.234375\n",
      "epoch 1; batch 384; loss 1.313836\n",
      "epoch:1; batch 384; train accuracy: 0.242188\n",
      "epoch 1; batch 512; loss 1.293488\n",
      "epoch:1; batch 512; train accuracy: 0.283203\n",
      "epoch 1; batch 640; loss 1.253166\n",
      "epoch:1; batch 640; train accuracy: 0.320312\n",
      "epoch 1; batch 768; loss 1.334800\n",
      "epoch:1; batch 768; train accuracy: 0.341146\n",
      "epoch 1; batch 896; loss 1.336356\n",
      "epoch:1; batch 896; train accuracy: 0.343750\n",
      "epoch 1; batch 1024; loss 1.251834\n",
      "epoch:1; batch 1024; train accuracy: 0.353516\n",
      "epoch 1; batch 1152; loss 1.262757\n",
      "epoch:1; batch 1152; train accuracy: 0.358507\n",
      "epoch 1; batch 1280; loss 1.246750\n",
      "epoch:1; batch 1280; train accuracy: 0.367969\n",
      "epoch 1; batch 1408; loss 1.216053\n",
      "epoch:1; batch 1408; train accuracy: 0.377131\n",
      "epoch 1; batch 1536; loss 1.211154\n",
      "epoch:1; batch 1536; train accuracy: 0.373698\n",
      "epoch 1; batch 1664; loss 1.274697\n",
      "epoch:1; batch 1664; train accuracy: 0.371394\n",
      "epoch 1; batch 1792; loss 1.270094\n",
      "epoch:1; batch 1792; train accuracy: 0.377790\n",
      "epoch 1; batch 1920; loss 1.234470\n",
      "epoch:1; batch 1920; train accuracy: 0.382812\n",
      "epoch 1; batch 2048; loss 1.273186\n",
      "epoch:1; batch 2048; train accuracy: 0.385254\n",
      "epoch 1; batch 2176; loss 1.224622\n",
      "epoch:1; batch 2176; train accuracy: 0.389246\n",
      "epoch 1; batch 2304; loss 1.243538\n",
      "epoch:1; batch 2304; train accuracy: 0.391493\n",
      "epoch 1; batch 2432; loss 1.260769\n",
      "epoch:1; batch 2432; train accuracy: 0.391859\n",
      "epoch 1; batch 2560; loss 1.217290\n",
      "epoch:1; batch 2560; train accuracy: 0.396094\n",
      "epoch 1; batch 2688; loss 1.275322\n",
      "epoch:1; batch 2688; train accuracy: 0.397693\n",
      "epoch 1; batch 2816; loss 1.263745\n",
      "epoch:1; batch 2816; train accuracy: 0.397727\n",
      "epoch 1; batch 2944; loss 1.213348\n",
      "epoch:1; batch 2944; train accuracy: 0.397418\n",
      "epoch 1; batch 3072; loss 1.271008\n",
      "epoch:1; batch 3072; train accuracy: 0.400065\n",
      "epoch 1; batch 3200; loss 1.189251\n",
      "epoch:1; batch 3200; train accuracy: 0.400313\n",
      "epoch 1; batch 3328; loss 1.137143\n",
      "epoch:1; batch 3328; train accuracy: 0.402644\n",
      "epoch 1; batch 3456; loss 1.164133\n",
      "epoch:1; batch 3456; train accuracy: 0.405961\n",
      "epoch 1; batch 3584; loss 1.216162\n",
      "epoch:1; batch 3584; train accuracy: 0.406808\n",
      "epoch 1; batch 3712; loss 1.254987\n",
      "epoch:1; batch 3712; train accuracy: 0.407597\n",
      "epoch 1; batch 3840; loss 1.207734\n",
      "epoch:1; batch 3840; train accuracy: 0.407552\n",
      "epoch 1; batch 3968; loss 1.187694\n",
      "epoch:1; batch 3968; train accuracy: 0.410282\n",
      "epoch 1; batch 4096; loss 1.204453\n",
      "epoch:1; batch 4096; train accuracy: 0.411133\n",
      "epoch 1; batch 4224; loss 1.226302\n",
      "epoch:1; batch 4224; train accuracy: 0.414536\n",
      "epoch 1; batch 4352; loss 1.194583\n",
      "epoch:1; batch 4352; train accuracy: 0.417969\n",
      "epoch 1; batch 4480; loss 1.138905\n",
      "epoch:1; batch 4480; train accuracy: 0.421652\n",
      "epoch 1; batch 4608; loss 1.127848\n",
      "epoch:1; batch 4608; train accuracy: 0.424045\n",
      "epoch 1; batch 4736; loss 1.122788\n",
      "epoch:1; batch 4736; train accuracy: 0.426943\n",
      "epoch 1; batch 4864; loss 1.087219\n",
      "epoch:1; batch 4864; train accuracy: 0.428043\n",
      "epoch 1; batch 4992; loss 1.167554\n",
      "epoch:1; batch 4992; train accuracy: 0.429688\n",
      "epoch 1; batch 5120; loss 1.202408\n",
      "epoch:1; batch 5120; train accuracy: 0.430273\n",
      "epoch 1; batch 5248; loss 1.115878\n",
      "epoch:1; batch 5248; train accuracy: 0.431974\n",
      "epoch 1; batch 5376; loss 1.126815\n",
      "epoch:1; batch 5376; train accuracy: 0.433036\n",
      "epoch 1; batch 5504; loss 1.199302\n",
      "epoch:1; batch 5504; train accuracy: 0.432049\n",
      "epoch 1; batch 5632; loss 1.156148\n",
      "epoch:1; batch 5632; train accuracy: 0.433239\n",
      "epoch 1; batch 5760; loss 1.109350\n",
      "epoch:1; batch 5760; train accuracy: 0.433854\n",
      "epoch 1; batch 5888; loss 1.112183\n",
      "epoch:1; batch 5888; train accuracy: 0.435292\n",
      "epoch 1; batch 6016; loss 1.176448\n",
      "epoch:1; batch 6016; train accuracy: 0.436170\n",
      "epoch 1; batch 6144; loss 1.194074\n",
      "epoch:1; batch 6144; train accuracy: 0.436523\n",
      "epoch 1; batch 6272; loss 1.138778\n",
      "epoch:1; batch 6272; train accuracy: 0.436862\n",
      "epoch 1; batch 6400; loss 1.051952\n",
      "epoch:1; batch 6400; train accuracy: 0.437344\n",
      "epoch 1; batch 6528; loss 1.147665\n",
      "epoch:1; batch 6528; train accuracy: 0.438726\n",
      "epoch 1; batch 6656; loss 1.109630\n",
      "epoch:1; batch 6656; train accuracy: 0.439303\n",
      "epoch 1; batch 6784; loss 1.085093\n",
      "epoch:1; batch 6784; train accuracy: 0.439711\n",
      "epoch 1; batch 6912; loss 1.196184\n",
      "epoch:1; batch 6912; train accuracy: 0.440104\n",
      "epoch 1; batch 7040; loss 1.166734\n",
      "epoch:1; batch 7040; train accuracy: 0.440625\n",
      "epoch 1; batch 7168; loss 1.052926\n",
      "epoch:1; batch 7168; train accuracy: 0.441685\n",
      "epoch 1; batch 7296; loss 1.092332\n",
      "epoch:1; batch 7296; train accuracy: 0.442708\n",
      "epoch 1; batch 7424; loss 1.123818\n",
      "epoch:1; batch 7424; train accuracy: 0.443561\n",
      "epoch 1; batch 7552; loss 1.296401\n",
      "epoch:1; batch 7552; train accuracy: 0.442532\n",
      "epoch 1; batch 7680; loss 1.045128\n",
      "epoch:1; batch 7680; train accuracy: 0.444531\n",
      "epoch 1; batch 7808; loss 1.104034\n",
      "epoch:1; batch 7808; train accuracy: 0.445569\n",
      "epoch 1; batch 7936; loss 1.107223\n",
      "epoch:1; batch 7936; train accuracy: 0.446573\n",
      "epoch 1; batch 8064; loss 1.096724\n",
      "epoch:1; batch 8064; train accuracy: 0.448413\n",
      "epoch 1; batch 8192; loss 1.067755\n",
      "epoch:1; batch 8192; train accuracy: 0.449951\n",
      "epoch 1; batch 8320; loss 1.067498\n",
      "epoch:1; batch 8320; train accuracy: 0.450962\n",
      "epoch 1; batch 8448; loss 1.078177\n",
      "epoch:1; batch 8448; train accuracy: 0.453362\n",
      "epoch 1; batch 8576; loss 0.987780\n",
      "epoch:1; batch 8576; train accuracy: 0.456273\n",
      "epoch 1; batch 8704; loss 1.077804\n",
      "epoch:1; batch 8704; train accuracy: 0.457950\n",
      "epoch 1; batch 8832; loss 1.118010\n",
      "epoch:1; batch 8832; train accuracy: 0.458447\n",
      "epoch 1; batch 8960; loss 1.027524\n",
      "epoch:1; batch 8960; train accuracy: 0.459933\n",
      "epoch 1; batch 9088; loss 1.134595\n",
      "epoch:1; batch 9088; train accuracy: 0.460827\n",
      "epoch 1; batch 9216; loss 1.053142\n",
      "epoch:1; batch 9216; train accuracy: 0.462023\n",
      "epoch 1; batch 9344; loss 1.086585\n",
      "epoch:1; batch 9344; train accuracy: 0.462115\n",
      "epoch 1; batch 9472; loss 1.090563\n",
      "epoch:1; batch 9472; train accuracy: 0.462838\n",
      "epoch 1; batch 9600; loss 1.180113\n",
      "epoch:1; batch 9600; train accuracy: 0.463958\n",
      "epoch 1; batch 9728; loss 0.985975\n",
      "epoch:1; batch 9728; train accuracy: 0.465666\n",
      "epoch 1; batch 9856; loss 1.002035\n",
      "epoch:1; batch 9856; train accuracy: 0.467228\n",
      "epoch 1; batch 9984; loss 1.077849\n",
      "epoch:1; batch 9984; train accuracy: 0.468149\n",
      "epoch 1; batch 10112; loss 1.041757\n",
      "epoch:1; batch 10112; train accuracy: 0.468948\n",
      "epoch 1; batch 10240; loss 1.034420\n",
      "epoch:1; batch 10240; train accuracy: 0.470508\n",
      "epoch 1; batch 10368; loss 1.044510\n",
      "epoch:1; batch 10368; train accuracy: 0.470679\n",
      "epoch 1; batch 10496; loss 0.914982\n",
      "epoch:1; batch 10496; train accuracy: 0.472561\n",
      "epoch 1; batch 10624; loss 0.883180\n",
      "epoch:1; batch 10624; train accuracy: 0.474303\n",
      "epoch 1; batch 10752; loss 1.046537\n",
      "epoch:1; batch 10752; train accuracy: 0.474981\n",
      "epoch 1; batch 10880; loss 0.965425\n",
      "epoch:1; batch 10880; train accuracy: 0.475919\n",
      "epoch 1; batch 11008; loss 0.862324\n",
      "epoch:1; batch 11008; train accuracy: 0.477834\n",
      "epoch 1; batch 11136; loss 0.795490\n",
      "epoch:1; batch 11136; train accuracy: 0.480154\n",
      "epoch 1; batch 11264; loss 1.032673\n",
      "epoch:1; batch 11264; train accuracy: 0.481445\n",
      "epoch 1; batch 11392; loss 0.954446\n",
      "epoch:1; batch 11392; train accuracy: 0.482883\n",
      "epoch 1; batch 11520; loss 0.933125\n",
      "epoch:1; batch 11520; train accuracy: 0.484462\n",
      "epoch 1; batch 11648; loss 0.870714\n",
      "epoch:1; batch 11648; train accuracy: 0.486006\n",
      "epoch 1; batch 11776; loss 1.046613\n",
      "epoch:1; batch 11776; train accuracy: 0.487092\n",
      "epoch 1; batch 11904; loss 0.857368\n",
      "epoch:1; batch 11904; train accuracy: 0.488659\n",
      "epoch 1; batch 12032; loss 0.946136\n",
      "epoch:1; batch 12032; train accuracy: 0.490193\n",
      "epoch 1; batch 12160; loss 1.081287\n",
      "epoch:1; batch 12160; train accuracy: 0.490378\n",
      "epoch 1; batch 12288; loss 0.918685\n",
      "epoch:1; batch 12288; train accuracy: 0.491943\n",
      "epoch 1; batch 12416; loss 0.967450\n",
      "epoch:1; batch 12416; train accuracy: 0.492751\n",
      "epoch 1; batch 12544; loss 0.977851\n",
      "epoch:1; batch 12544; train accuracy: 0.493862\n",
      "epoch 1; batch 12672; loss 0.927981\n",
      "epoch:1; batch 12672; train accuracy: 0.494871\n",
      "epoch 1; batch 12800; loss 0.930406\n",
      "epoch:1; batch 12800; train accuracy: 0.496250\n",
      "epoch 1; batch 12928; loss 0.935849\n",
      "epoch:1; batch 12928; train accuracy: 0.497293\n",
      "epoch 1; batch 13056; loss 0.984047\n",
      "epoch:1; batch 13056; train accuracy: 0.498238\n",
      "epoch 1; batch 13184; loss 0.977652\n",
      "epoch:1; batch 13184; train accuracy: 0.499848\n",
      "epoch 1; batch 13312; loss 0.901268\n",
      "epoch:1; batch 13312; train accuracy: 0.501202\n",
      "epoch 1; batch 13440; loss 1.014754\n",
      "epoch:1; batch 13440; train accuracy: 0.502679\n",
      "epoch 1; batch 13568; loss 0.812954\n",
      "epoch:1; batch 13568; train accuracy: 0.504717\n",
      "epoch 1; batch 13696; loss 0.796970\n",
      "epoch:1; batch 13696; train accuracy: 0.506352\n",
      "epoch 1; batch 13824; loss 0.857867\n",
      "epoch:1; batch 13824; train accuracy: 0.507668\n",
      "epoch 1; batch 13952; loss 0.726161\n",
      "epoch:1; batch 13952; train accuracy: 0.509891\n",
      "epoch 1; batch 14080; loss 0.853301\n",
      "epoch:1; batch 14080; train accuracy: 0.511435\n",
      "epoch 1; batch 14208; loss 0.869795\n",
      "epoch:1; batch 14208; train accuracy: 0.513162\n",
      "epoch 1; batch 14336; loss 0.874677\n",
      "epoch:1; batch 14336; train accuracy: 0.514718\n",
      "epoch 1; batch 14464; loss 0.786365\n",
      "epoch:1; batch 14464; train accuracy: 0.516040\n",
      "epoch 1; batch 14592; loss 0.856093\n",
      "epoch:1; batch 14592; train accuracy: 0.517338\n",
      "epoch 1; batch 14720; loss 0.870085\n",
      "epoch:1; batch 14720; train accuracy: 0.518886\n",
      "epoch 1; batch 14848; loss 0.962932\n",
      "epoch:1; batch 14848; train accuracy: 0.520205\n",
      "epoch 1; batch 14976; loss 0.951828\n",
      "epoch:1; batch 14976; train accuracy: 0.520967\n",
      "epoch 1; batch 15104; loss 0.806887\n",
      "epoch:1; batch 15104; train accuracy: 0.522643\n",
      "epoch 1; batch 15232; loss 0.841834\n",
      "epoch:1; batch 15232; train accuracy: 0.523569\n",
      "epoch 1; batch 15360; loss 0.863850\n",
      "epoch:1; batch 15360; train accuracy: 0.524805\n",
      "epoch 1; batch 15488; loss 0.694837\n",
      "epoch:1; batch 15488; train accuracy: 0.526278\n",
      "epoch 1; batch 15616; loss 0.763075\n",
      "epoch:1; batch 15616; train accuracy: 0.527728\n",
      "epoch 1; batch 15744; loss 0.851645\n",
      "epoch:1; batch 15744; train accuracy: 0.528836\n",
      "epoch 1; batch 15872; loss 0.932197\n",
      "epoch:1; batch 15872; train accuracy: 0.529549\n",
      "epoch 1; batch 16000; loss 0.902307\n",
      "epoch:1; batch 16000; train accuracy: 0.530125\n",
      "epoch 1; batch 16128; loss 0.834643\n",
      "epoch:1; batch 16128; train accuracy: 0.531188\n",
      "epoch 1; batch 16256; loss 0.842082\n",
      "epoch:1; batch 16256; train accuracy: 0.532419\n",
      "epoch 1; batch 16384; loss 0.817270\n",
      "epoch:1; batch 16384; train accuracy: 0.533752\n",
      "epoch 1; batch 16512; loss 0.769855\n",
      "epoch:1; batch 16512; train accuracy: 0.535368\n",
      "epoch 1; batch 16640; loss 0.782767\n",
      "epoch:1; batch 16640; train accuracy: 0.536779\n",
      "epoch 1; batch 16768; loss 0.760679\n",
      "epoch:1; batch 16768; train accuracy: 0.537691\n",
      "epoch 1; batch 16896; loss 0.776802\n",
      "epoch:1; batch 16896; train accuracy: 0.538589\n",
      "epoch 1; batch 17024; loss 0.796112\n",
      "epoch:1; batch 17024; train accuracy: 0.539532\n",
      "epoch 1; batch 17152; loss 0.779791\n",
      "epoch:1; batch 17152; train accuracy: 0.540870\n",
      "epoch 1; batch 17280; loss 0.817260\n",
      "epoch:1; batch 17280; train accuracy: 0.541840\n",
      "epoch 1; batch 17408; loss 0.649954\n",
      "epoch:1; batch 17408; train accuracy: 0.543773\n",
      "epoch 1; batch 17536; loss 0.851068\n",
      "epoch:1; batch 17536; train accuracy: 0.545335\n",
      "epoch 1; batch 17664; loss 0.836501\n",
      "epoch:1; batch 17664; train accuracy: 0.546309\n",
      "epoch 1; batch 17792; loss 0.804694\n",
      "epoch:1; batch 17792; train accuracy: 0.547437\n",
      "epoch 1; batch 17920; loss 0.706014\n",
      "epoch:1; batch 17920; train accuracy: 0.548717\n",
      "epoch 1; batch 18048; loss 0.787568\n",
      "epoch:1; batch 18048; train accuracy: 0.549368\n",
      "epoch 1; batch 18176; loss 0.730422\n",
      "epoch:1; batch 18176; train accuracy: 0.550726\n",
      "epoch 1; batch 18304; loss 0.797640\n",
      "epoch:1; batch 18304; train accuracy: 0.551628\n",
      "epoch 1; batch 18432; loss 0.847845\n",
      "epoch:1; batch 18432; train accuracy: 0.552463\n",
      "epoch 1; batch 18560; loss 0.759863\n",
      "epoch:1; batch 18560; train accuracy: 0.553394\n",
      "epoch 1; batch 18688; loss 0.994565\n",
      "epoch:1; batch 18688; train accuracy: 0.553617\n",
      "epoch 1; batch 18816; loss 0.761151\n",
      "epoch:1; batch 18816; train accuracy: 0.555006\n",
      "epoch 1; batch 18944; loss 0.642336\n",
      "epoch:1; batch 18944; train accuracy: 0.556482\n",
      "epoch 1; batch 19072; loss 0.721021\n",
      "epoch:1; batch 19072; train accuracy: 0.557676\n",
      "epoch 1; batch 19200; loss 0.713475\n",
      "epoch:1; batch 19200; train accuracy: 0.558698\n",
      "epoch 1; batch 19328; loss 0.851284\n",
      "epoch:1; batch 19328; train accuracy: 0.559499\n",
      "epoch 1; batch 19456; loss 0.759131\n",
      "epoch:1; batch 19456; train accuracy: 0.560290\n",
      "epoch 1; batch 19584; loss 0.741492\n",
      "epoch:1; batch 19584; train accuracy: 0.561172\n",
      "epoch 1; batch 19712; loss 0.819317\n",
      "epoch:1; batch 19712; train accuracy: 0.561942\n",
      "epoch 1; batch 19840; loss 0.735498\n",
      "epoch:1; batch 19840; train accuracy: 0.563054\n",
      "epoch 1; batch 19968; loss 0.980400\n",
      "epoch:1; batch 19968; train accuracy: 0.563902\n",
      "epoch 1; batch 20096; loss 0.590146\n",
      "epoch:1; batch 20096; train accuracy: 0.565237\n",
      "epoch 1; batch 20224; loss 0.625947\n",
      "epoch:1; batch 20224; train accuracy: 0.566555\n",
      "epoch 1; batch 20352; loss 0.748098\n",
      "epoch:1; batch 20352; train accuracy: 0.567463\n",
      "epoch 1; batch 20480; loss 0.851938\n",
      "epoch:1; batch 20480; train accuracy: 0.568359\n",
      "epoch 1; batch 20608; loss 0.700957\n",
      "epoch:1; batch 20608; train accuracy: 0.569391\n",
      "epoch 1; batch 20736; loss 0.711118\n",
      "epoch:1; batch 20736; train accuracy: 0.570409\n",
      "epoch 1; batch 20864; loss 0.743813\n",
      "epoch:1; batch 20864; train accuracy: 0.571463\n",
      "epoch 1; batch 20992; loss 0.809822\n",
      "epoch:1; batch 20992; train accuracy: 0.572170\n",
      "epoch 1; batch 21120; loss 0.719510\n",
      "epoch:1; batch 21120; train accuracy: 0.573059\n",
      "epoch 1; batch 21248; loss 0.782202\n",
      "epoch:1; batch 21248; train accuracy: 0.573936\n",
      "epoch 1; batch 21376; loss 0.716879\n",
      "epoch:1; batch 21376; train accuracy: 0.574616\n",
      "epoch 1; batch 21504; loss 0.719678\n",
      "epoch:1; batch 21504; train accuracy: 0.575428\n",
      "epoch 1; batch 21632; loss 0.823693\n",
      "epoch:1; batch 21632; train accuracy: 0.576183\n",
      "epoch 1; batch 21760; loss 0.740269\n",
      "epoch:1; batch 21760; train accuracy: 0.576654\n",
      "epoch 1; batch 21888; loss 0.681191\n",
      "epoch:1; batch 21888; train accuracy: 0.577348\n",
      "epoch 1; batch 22016; loss 0.879968\n",
      "epoch:1; batch 22016; train accuracy: 0.577807\n",
      "epoch 1; batch 22144; loss 0.732097\n",
      "epoch:1; batch 22144; train accuracy: 0.578712\n",
      "epoch 1; batch 22272; loss 0.811741\n",
      "epoch:1; batch 22272; train accuracy: 0.579427\n",
      "epoch 1; batch 22400; loss 0.809521\n",
      "epoch:1; batch 22400; train accuracy: 0.580045\n",
      "epoch 1; batch 22528; loss 0.688395\n",
      "epoch:1; batch 22528; train accuracy: 0.580788\n",
      "epoch 1; batch 22656; loss 0.676569\n",
      "epoch:1; batch 22656; train accuracy: 0.581921\n",
      "epoch 1; batch 22784; loss 0.762325\n",
      "epoch:1; batch 22784; train accuracy: 0.582558\n",
      "epoch 1; batch 22912; loss 0.696106\n",
      "epoch:1; batch 22912; train accuracy: 0.583493\n",
      "epoch 1; batch 23040; loss 0.669595\n",
      "epoch:1; batch 23040; train accuracy: 0.584462\n",
      "epoch 1; batch 23168; loss 0.678818\n",
      "epoch:1; batch 23168; train accuracy: 0.585117\n",
      "epoch 1; batch 23296; loss 0.870128\n",
      "epoch:1; batch 23296; train accuracy: 0.585637\n",
      "epoch 1; batch 23424; loss 0.597782\n",
      "epoch:1; batch 23424; train accuracy: 0.586578\n",
      "epoch 1; batch 23552; loss 0.621392\n",
      "epoch:1; batch 23552; train accuracy: 0.587466\n",
      "epoch 1; batch 23680; loss 0.680293\n",
      "epoch:1; batch 23680; train accuracy: 0.588302\n",
      "epoch 1; batch 23808; loss 0.674429\n",
      "epoch:1; batch 23808; train accuracy: 0.589088\n",
      "epoch 1; batch 23936; loss 0.838817\n",
      "epoch:1; batch 23936; train accuracy: 0.589739\n",
      "epoch 1; batch 24064; loss 0.795809\n",
      "epoch:1; batch 24064; train accuracy: 0.590384\n",
      "epoch 1; batch 24192; loss 0.593879\n",
      "epoch:1; batch 24192; train accuracy: 0.591228\n",
      "epoch 1; batch 24320; loss 0.868688\n",
      "epoch:1; batch 24320; train accuracy: 0.591694\n",
      "epoch 1; batch 24448; loss 0.743014\n",
      "epoch:1; batch 24448; train accuracy: 0.592237\n",
      "epoch 1; batch 24576; loss 0.714921\n",
      "epoch:1; batch 24576; train accuracy: 0.592896\n",
      "epoch 1; batch 24704; loss 0.774692\n",
      "epoch:1; batch 24704; train accuracy: 0.593588\n",
      "epoch 1; batch 24832; loss 0.731727\n",
      "epoch:1; batch 24832; train accuracy: 0.594314\n",
      "epoch 1; batch 24960; loss 0.695404\n",
      "epoch:1; batch 24960; train accuracy: 0.595072\n",
      "epoch 1; batch 25088; loss 0.850319\n",
      "epoch:1; batch 25088; train accuracy: 0.595344\n",
      "epoch 1; batch 25216; loss 0.854140\n",
      "epoch:1; batch 25216; train accuracy: 0.595614\n",
      "epoch 1; batch 25344; loss 0.698074\n",
      "epoch:1; batch 25344; train accuracy: 0.596433\n",
      "epoch 1; batch 25472; loss 0.776886\n",
      "epoch:1; batch 25472; train accuracy: 0.596851\n",
      "epoch 1; batch 25600; loss 0.729055\n",
      "epoch:1; batch 25600; train accuracy: 0.597344\n",
      "epoch 1; batch 25728; loss 0.677925\n",
      "epoch:1; batch 25728; train accuracy: 0.598142\n",
      "epoch 1; batch 25856; loss 0.601901\n",
      "epoch:1; batch 25856; train accuracy: 0.599049\n",
      "epoch 1; batch 25984; loss 0.871463\n",
      "epoch:1; batch 25984; train accuracy: 0.599446\n",
      "epoch 1; batch 26112; loss 0.920180\n",
      "epoch:1; batch 26112; train accuracy: 0.599763\n",
      "epoch 1; batch 26240; loss 0.861269\n",
      "epoch:1; batch 26240; train accuracy: 0.600000\n",
      "epoch 1; batch 26368; loss 0.712971\n",
      "epoch:1; batch 26368; train accuracy: 0.600576\n",
      "epoch 1; batch 26496; loss 0.760821\n",
      "epoch:1; batch 26496; train accuracy: 0.601034\n",
      "epoch 1; batch 26624; loss 0.903358\n",
      "epoch:1; batch 26624; train accuracy: 0.601450\n",
      "epoch 1; batch 26752; loss 0.739120\n",
      "epoch:1; batch 26752; train accuracy: 0.601936\n",
      "epoch 1; batch 26880; loss 0.761088\n",
      "epoch:1; batch 26880; train accuracy: 0.602493\n",
      "epoch 1; batch 27008; loss 0.784447\n",
      "epoch:1; batch 27008; train accuracy: 0.603081\n",
      "epoch 1; batch 27136; loss 0.623814\n",
      "epoch:1; batch 27136; train accuracy: 0.603884\n",
      "epoch 1; batch 27264; loss 0.615930\n",
      "epoch:1; batch 27264; train accuracy: 0.604643\n",
      "epoch 1; batch 27392; loss 0.704171\n",
      "epoch:1; batch 27392; train accuracy: 0.605177\n",
      "epoch 1; batch 27520; loss 0.696761\n",
      "epoch:1; batch 27520; train accuracy: 0.605596\n",
      "epoch 1; batch 27648; loss 0.760037\n",
      "epoch:1; batch 27648; train accuracy: 0.605867\n",
      "epoch 1; batch 27776; loss 0.661496\n",
      "epoch:1; batch 27776; train accuracy: 0.606315\n",
      "epoch 1; batch 27904; loss 0.709222\n",
      "epoch:1; batch 27904; train accuracy: 0.606831\n",
      "epoch 1; batch 28032; loss 0.706402\n",
      "epoch:1; batch 28032; train accuracy: 0.607449\n",
      "epoch 1; batch 28160; loss 0.572688\n",
      "epoch:1; batch 28160; train accuracy: 0.608203\n",
      "epoch 1; batch 28288; loss 0.634836\n",
      "epoch:1; batch 28288; train accuracy: 0.608774\n",
      "epoch 1; batch 28416; loss 0.750472\n",
      "epoch:1; batch 28416; train accuracy: 0.609199\n",
      "epoch 1; batch 28544; loss 0.777071\n",
      "epoch:1; batch 28544; train accuracy: 0.609550\n",
      "epoch 1; batch 28672; loss 0.766197\n",
      "epoch:1; batch 28672; train accuracy: 0.609863\n",
      "epoch 1; batch 28800; loss 0.591536\n",
      "epoch:1; batch 28800; train accuracy: 0.610590\n",
      "epoch 1; batch 28928; loss 0.743534\n",
      "epoch:1; batch 28928; train accuracy: 0.610965\n",
      "epoch 1; batch 29056; loss 0.609013\n",
      "epoch:1; batch 29056; train accuracy: 0.611543\n",
      "epoch 1; batch 29184; loss 0.781740\n",
      "epoch:1; batch 29184; train accuracy: 0.611911\n",
      "epoch 1; batch 29312; loss 0.801485\n",
      "epoch:1; batch 29312; train accuracy: 0.612036\n",
      "epoch 1; batch 29440; loss 0.663776\n",
      "epoch:1; batch 29440; train accuracy: 0.612636\n",
      "epoch 1; batch 29568; loss 0.629267\n",
      "epoch:1; batch 29568; train accuracy: 0.613366\n",
      "epoch 1; batch 29696; loss 0.650317\n",
      "epoch:1; batch 29696; train accuracy: 0.613988\n",
      "epoch 1; batch 29824; loss 0.694614\n",
      "epoch:1; batch 29824; train accuracy: 0.614371\n",
      "epoch 1; batch 29952; loss 0.708505\n",
      "epoch:1; batch 29952; train accuracy: 0.614817\n",
      "epoch 1; batch 30080; loss 0.619783\n",
      "epoch:1; batch 30080; train accuracy: 0.615259\n",
      "epoch 1; batch 30208; loss 0.608325\n",
      "epoch:1; batch 30208; train accuracy: 0.615863\n",
      "epoch 1; batch 30336; loss 0.533410\n",
      "epoch:1; batch 30336; train accuracy: 0.616726\n",
      "epoch 1; batch 30464; loss 0.582988\n",
      "epoch:1; batch 30464; train accuracy: 0.617352\n",
      "epoch 1; batch 30592; loss 0.837135\n",
      "epoch:1; batch 30592; train accuracy: 0.617711\n",
      "epoch 1; batch 30720; loss 0.778452\n",
      "epoch:1; batch 30720; train accuracy: 0.618359\n",
      "epoch 1; batch 30848; loss 0.564325\n",
      "epoch:1; batch 30848; train accuracy: 0.619035\n",
      "epoch 1; batch 30976; loss 0.691457\n",
      "epoch:1; batch 30976; train accuracy: 0.619609\n",
      "epoch 1; batch 31104; loss 0.554073\n",
      "epoch:1; batch 31104; train accuracy: 0.620210\n",
      "epoch 1; batch 31232; loss 0.623880\n",
      "epoch:1; batch 31232; train accuracy: 0.620806\n",
      "epoch 1; batch 31360; loss 0.817955\n",
      "epoch:1; batch 31360; train accuracy: 0.621142\n",
      "epoch 1; batch 31488; loss 0.661993\n",
      "epoch:1; batch 31488; train accuracy: 0.621570\n",
      "epoch 1; batch 31616; loss 0.700437\n",
      "epoch:1; batch 31616; train accuracy: 0.622027\n",
      "epoch 1; batch 31744; loss 0.713641\n",
      "epoch:1; batch 31744; train accuracy: 0.622480\n",
      "epoch 1; batch 31872; loss 0.636710\n",
      "epoch:1; batch 31872; train accuracy: 0.623023\n",
      "epoch 1; batch 32000; loss 0.727473\n",
      "epoch:1; batch 32000; train accuracy: 0.623281\n",
      "epoch 1; batch 32128; loss 0.794086\n",
      "epoch:1; batch 32128; train accuracy: 0.623662\n",
      "epoch 1; batch 32256; loss 0.706869\n",
      "epoch:1; batch 32256; train accuracy: 0.624256\n",
      "epoch 1; batch 32384; loss 0.810418\n",
      "epoch:1; batch 32384; train accuracy: 0.624568\n",
      "epoch 1; batch 32512; loss 0.669404\n",
      "epoch:1; batch 32512; train accuracy: 0.625000\n",
      "epoch 1; batch 32640; loss 0.663483\n",
      "epoch:1; batch 32640; train accuracy: 0.625306\n",
      "epoch 1; batch 32768; loss 0.742687\n",
      "epoch:1; batch 32768; train accuracy: 0.625763\n",
      "epoch 1; batch 32896; loss 0.547019\n",
      "epoch:1; batch 32896; train accuracy: 0.626429\n",
      "epoch 1; batch 33024; loss 0.679753\n",
      "epoch:1; batch 33024; train accuracy: 0.626817\n",
      "epoch 1; batch 33152; loss 0.626243\n",
      "epoch:1; batch 33152; train accuracy: 0.627353\n",
      "epoch 1; batch 33280; loss 0.715065\n",
      "epoch:1; batch 33280; train accuracy: 0.627825\n",
      "epoch 1; batch 33408; loss 0.649894\n",
      "epoch:1; batch 33408; train accuracy: 0.628323\n",
      "epoch 1; batch 33536; loss 0.610512\n",
      "epoch:1; batch 33536; train accuracy: 0.629055\n",
      "epoch 1; batch 33664; loss 0.861393\n",
      "epoch:1; batch 33664; train accuracy: 0.629159\n",
      "epoch 1; batch 33792; loss 0.621771\n",
      "epoch:1; batch 33792; train accuracy: 0.629557\n",
      "epoch 1; batch 33920; loss 0.777948\n",
      "epoch:1; batch 33920; train accuracy: 0.629864\n",
      "epoch 1; batch 34048; loss 0.793090\n",
      "epoch:1; batch 34048; train accuracy: 0.630169\n",
      "epoch 1; batch 34176; loss 0.688779\n",
      "epoch:1; batch 34176; train accuracy: 0.630618\n",
      "epoch 1; batch 34304; loss 0.645987\n",
      "epoch:1; batch 34304; train accuracy: 0.631238\n",
      "epoch 1; batch 34432; loss 0.678763\n",
      "epoch:1; batch 34432; train accuracy: 0.631535\n",
      "epoch 1; batch 34560; loss 0.644747\n",
      "epoch:1; batch 34560; train accuracy: 0.631944\n",
      "epoch 1; batch 34688; loss 0.704619\n",
      "epoch:1; batch 34688; train accuracy: 0.632409\n",
      "epoch 1; batch 34816; loss 0.656855\n",
      "epoch:1; batch 34816; train accuracy: 0.632784\n",
      "epoch 1; batch 34944; loss 0.650674\n",
      "epoch:1; batch 34944; train accuracy: 0.633242\n",
      "epoch 1; batch 35072; loss 0.734481\n",
      "epoch:1; batch 35072; train accuracy: 0.633525\n",
      "epoch 1; batch 35200; loss 0.639469\n",
      "epoch:1; batch 35200; train accuracy: 0.633977\n",
      "epoch 1; batch 35328; loss 0.613222\n",
      "epoch:1; batch 35328; train accuracy: 0.634539\n",
      "epoch 1; batch 35456; loss 0.832041\n",
      "epoch:1; batch 35456; train accuracy: 0.634730\n",
      "epoch 1; batch 35584; loss 0.685424\n",
      "epoch:1; batch 35584; train accuracy: 0.635005\n",
      "epoch 1; batch 35712; loss 0.788638\n",
      "epoch:1; batch 35712; train accuracy: 0.635305\n",
      "epoch 1; batch 35840; loss 0.736092\n",
      "epoch:1; batch 35840; train accuracy: 0.635575\n",
      "epoch 1; batch 35968; loss 0.726838\n",
      "epoch:1; batch 35968; train accuracy: 0.635843\n",
      "epoch 1; batch 36096; loss 0.749604\n",
      "epoch:1; batch 36096; train accuracy: 0.636026\n",
      "epoch 1; batch 36224; loss 0.562854\n",
      "epoch:1; batch 36224; train accuracy: 0.636429\n",
      "epoch 1; batch 36352; loss 0.741157\n",
      "epoch:1; batch 36352; train accuracy: 0.636691\n",
      "epoch 1; batch 36480; loss 0.762954\n",
      "epoch:1; batch 36480; train accuracy: 0.636924\n",
      "epoch 1; batch 36608; loss 0.726971\n",
      "epoch:1; batch 36608; train accuracy: 0.637210\n",
      "epoch 1; batch 36736; loss 0.581989\n",
      "epoch:1; batch 36736; train accuracy: 0.637876\n",
      "epoch 1; batch 36864; loss 0.623745\n",
      "epoch:1; batch 36864; train accuracy: 0.638292\n",
      "epoch 1; batch 36992; loss 0.663213\n",
      "epoch:1; batch 36992; train accuracy: 0.638625\n",
      "epoch 1; batch 37120; loss 0.794547\n",
      "epoch:1; batch 37120; train accuracy: 0.638874\n",
      "epoch 1; batch 37248; loss 0.728298\n",
      "epoch:1; batch 37248; train accuracy: 0.639202\n",
      "epoch 1; batch 37376; loss 0.635644\n",
      "epoch:1; batch 37376; train accuracy: 0.639528\n",
      "epoch 1; batch 37504; loss 0.650216\n",
      "epoch:1; batch 37504; train accuracy: 0.639958\n",
      "epoch 1; batch 37632; loss 0.810220\n",
      "epoch:1; batch 37632; train accuracy: 0.640173\n",
      "epoch 1; batch 37760; loss 0.743677\n",
      "epoch:1; batch 37760; train accuracy: 0.640440\n",
      "epoch 1; batch 37888; loss 0.754364\n",
      "epoch:1; batch 37888; train accuracy: 0.640546\n",
      "epoch 1; batch 38016; loss 0.533897\n",
      "epoch:1; batch 38016; train accuracy: 0.641046\n",
      "epoch 1; batch 38144; loss 0.699450\n",
      "epoch:1; batch 38144; train accuracy: 0.641280\n",
      "epoch 1; batch 38272; loss 0.728626\n",
      "epoch:1; batch 38272; train accuracy: 0.641618\n",
      "epoch 1; batch 38400; loss 0.712428\n",
      "epoch:1; batch 38400; train accuracy: 0.641849\n",
      "epoch 1; batch 38528; loss 0.569843\n",
      "epoch:1; batch 38528; train accuracy: 0.642416\n",
      "epoch 1; batch 38656; loss 0.750194\n",
      "epoch:1; batch 38656; train accuracy: 0.642539\n",
      "epoch 1; batch 38784; loss 0.494807\n",
      "epoch:1; batch 38784; train accuracy: 0.643178\n",
      "epoch 1; batch 38912; loss 0.634122\n",
      "epoch:1; batch 38912; train accuracy: 0.643529\n",
      "epoch 1; batch 39040; loss 0.662490\n",
      "epoch:1; batch 39040; train accuracy: 0.643801\n",
      "epoch 1; batch 39168; loss 0.779372\n",
      "epoch:1; batch 39168; train accuracy: 0.644046\n",
      "epoch 1; batch 39296; loss 0.640905\n",
      "epoch:1; batch 39296; train accuracy: 0.644340\n",
      "epoch 1; batch 39424; loss 0.649299\n",
      "epoch:1; batch 39424; train accuracy: 0.644683\n",
      "epoch 1; batch 39552; loss 0.654566\n",
      "epoch:1; batch 39552; train accuracy: 0.644948\n",
      "epoch 1; batch 39680; loss 0.657004\n",
      "epoch:1; batch 39680; train accuracy: 0.645287\n",
      "epoch 1; batch 39808; loss 0.708195\n",
      "epoch:1; batch 39808; train accuracy: 0.645523\n",
      "epoch 1; batch 39936; loss 0.694876\n",
      "epoch:1; batch 39936; train accuracy: 0.645858\n",
      "epoch 1; batch 40064; loss 0.638318\n",
      "epoch:1; batch 40064; train accuracy: 0.646166\n",
      "epoch 1; batch 40192; loss 0.701072\n",
      "epoch:1; batch 40192; train accuracy: 0.646547\n",
      "epoch 1; batch 40320; loss 0.718076\n",
      "epoch:1; batch 40320; train accuracy: 0.646825\n",
      "epoch 1; batch 40448; loss 0.694827\n",
      "epoch:1; batch 40448; train accuracy: 0.646979\n",
      "epoch 1; batch 40576; loss 0.718847\n",
      "epoch:1; batch 40576; train accuracy: 0.647156\n",
      "epoch 1; batch 40704; loss 0.747894\n",
      "epoch:1; batch 40704; train accuracy: 0.647332\n",
      "epoch 1; batch 40832; loss 0.851650\n",
      "epoch:1; batch 40832; train accuracy: 0.647409\n",
      "epoch 1; batch 40960; loss 0.679667\n",
      "epoch:1; batch 40960; train accuracy: 0.647730\n",
      "epoch 1; batch 41088; loss 0.686982\n",
      "epoch:1; batch 41088; train accuracy: 0.647975\n",
      "epoch 1; batch 41216; loss 0.641877\n",
      "epoch:1; batch 41216; train accuracy: 0.648413\n",
      "epoch 1; batch 41344; loss 0.620730\n",
      "epoch:1; batch 41344; train accuracy: 0.648849\n",
      "epoch 1; batch 41472; loss 0.627252\n",
      "epoch:1; batch 41472; train accuracy: 0.649257\n",
      "epoch 1; batch 41600; loss 0.676372\n",
      "epoch:1; batch 41600; train accuracy: 0.649543\n",
      "epoch 1; batch 41728; loss 0.695951\n",
      "epoch:1; batch 41728; train accuracy: 0.649827\n",
      "epoch 1; batch 41856; loss 0.673335\n",
      "epoch:1; batch 41856; train accuracy: 0.650134\n",
      "epoch 1; batch 41984; loss 0.560137\n",
      "epoch:1; batch 41984; train accuracy: 0.650557\n",
      "epoch 1; batch 42112; loss 0.651516\n",
      "epoch:1; batch 42112; train accuracy: 0.650812\n",
      "epoch 1; batch 42240; loss 0.617118\n",
      "epoch:1; batch 42240; train accuracy: 0.651231\n",
      "epoch 1; batch 42368; loss 0.688867\n",
      "epoch:1; batch 42368; train accuracy: 0.651577\n",
      "epoch 1; batch 42496; loss 0.829113\n",
      "epoch:1; batch 42496; train accuracy: 0.651661\n",
      "epoch 1; batch 42624; loss 0.647872\n",
      "epoch:1; batch 42624; train accuracy: 0.651980\n",
      "epoch 1; batch 42752; loss 0.748773\n",
      "epoch:1; batch 42752; train accuracy: 0.652133\n",
      "epoch 1; batch 42880; loss 0.714266\n",
      "epoch:1; batch 42880; train accuracy: 0.652332\n",
      "epoch 1; batch 43008; loss 0.749016\n",
      "epoch:1; batch 43008; train accuracy: 0.652600\n",
      "epoch 1; batch 43136; loss 0.676602\n",
      "epoch:1; batch 43136; train accuracy: 0.652912\n",
      "epoch 1; batch 43264; loss 0.646323\n",
      "epoch:1; batch 43264; train accuracy: 0.653083\n",
      "epoch 1; batch 43392; loss 0.677943\n",
      "epoch:1; batch 43392; train accuracy: 0.653415\n",
      "epoch 1; batch 43520; loss 0.538686\n",
      "epoch:1; batch 43520; train accuracy: 0.653952\n",
      "epoch 1; batch 43648; loss 0.565450\n",
      "epoch:1; batch 43648; train accuracy: 0.654325\n",
      "epoch 1; batch 43776; loss 0.697239\n",
      "epoch:1; batch 43776; train accuracy: 0.654537\n",
      "epoch 1; batch 43904; loss 0.651942\n",
      "epoch:1; batch 43904; train accuracy: 0.654861\n",
      "epoch 1; batch 44032; loss 0.671764\n",
      "epoch:1; batch 44032; train accuracy: 0.655183\n",
      "epoch 1; batch 44160; loss 0.695805\n",
      "epoch:1; batch 44160; train accuracy: 0.655367\n",
      "epoch 1; batch 44288; loss 0.680557\n",
      "epoch:1; batch 44288; train accuracy: 0.655618\n",
      "epoch 1; batch 44416; loss 0.543939\n",
      "epoch:1; batch 44416; train accuracy: 0.656092\n",
      "epoch 1; batch 44544; loss 0.583085\n",
      "epoch:1; batch 44544; train accuracy: 0.656385\n",
      "epoch 1; batch 44672; loss 0.613521\n",
      "epoch:1; batch 44672; train accuracy: 0.656742\n",
      "epoch 1; batch 44800; loss 0.727589\n",
      "epoch:1; batch 44800; train accuracy: 0.656897\n",
      "epoch 1; batch 44928; loss 0.577459\n",
      "epoch:1; batch 44928; train accuracy: 0.657252\n",
      "epoch 1; batch 45056; loss 0.567628\n",
      "epoch:1; batch 45056; train accuracy: 0.657648\n",
      "epoch 1; batch 45184; loss 0.940835\n",
      "epoch:1; batch 45184; train accuracy: 0.657534\n",
      "epoch 1; batch 45312; loss 0.761153\n",
      "epoch:1; batch 45312; train accuracy: 0.657707\n",
      "epoch 1; batch 45440; loss 0.591067\n",
      "epoch:1; batch 45440; train accuracy: 0.657989\n",
      "epoch 1; batch 45568; loss 0.704604\n",
      "epoch:1; batch 45568; train accuracy: 0.658247\n",
      "epoch 1; batch 45696; loss 0.772437\n",
      "epoch:1; batch 45696; train accuracy: 0.658373\n",
      "epoch 1; batch 45824; loss 0.601616\n",
      "epoch:1; batch 45824; train accuracy: 0.658694\n",
      "epoch 1; batch 45952; loss 0.651280\n",
      "epoch:1; batch 45952; train accuracy: 0.659014\n",
      "epoch 1; batch 46080; loss 0.525878\n",
      "epoch:1; batch 46080; train accuracy: 0.659462\n",
      "epoch 1; batch 46208; loss 0.794091\n",
      "epoch:1; batch 46208; train accuracy: 0.659583\n",
      "epoch 1; batch 46336; loss 0.554994\n",
      "epoch:1; batch 46336; train accuracy: 0.659897\n",
      "epoch 1; batch 46464; loss 0.655307\n",
      "epoch:1; batch 46464; train accuracy: 0.660167\n",
      "epoch 1; batch 46592; loss 0.699719\n",
      "epoch:1; batch 46592; train accuracy: 0.660349\n",
      "epoch 1; batch 46720; loss 0.512057\n",
      "epoch:1; batch 46720; train accuracy: 0.660788\n",
      "epoch 1; batch 46848; loss 0.748982\n",
      "epoch:1; batch 46848; train accuracy: 0.660882\n",
      "epoch 1; batch 46976; loss 0.684321\n",
      "epoch:1; batch 46976; train accuracy: 0.661061\n",
      "epoch 1; batch 47104; loss 0.681846\n",
      "epoch:1; batch 47104; train accuracy: 0.661218\n",
      "epoch 1; batch 47232; loss 0.750053\n",
      "epoch:1; batch 47232; train accuracy: 0.661395\n",
      "epoch 1; batch 47360; loss 0.657207\n",
      "epoch:1; batch 47360; train accuracy: 0.661508\n",
      "epoch 1; batch 47488; loss 0.744562\n",
      "epoch:1; batch 47488; train accuracy: 0.661662\n",
      "epoch 1; batch 47616; loss 0.664252\n",
      "epoch:1; batch 47616; train accuracy: 0.661899\n",
      "epoch 1; batch 47744; loss 0.678041\n",
      "epoch:1; batch 47744; train accuracy: 0.662177\n",
      "epoch 1; batch 47872; loss 0.723996\n",
      "epoch:1; batch 47872; train accuracy: 0.662287\n",
      "epoch 1; batch 48000; loss 0.611149\n",
      "epoch:1; batch 48000; train accuracy: 0.662625\n",
      "epoch 1; batch 48128; loss 0.664096\n",
      "epoch:1; batch 48128; train accuracy: 0.662878\n",
      "epoch 1; batch 48256; loss 0.729260\n",
      "epoch:1; batch 48256; train accuracy: 0.663026\n",
      "epoch 1; batch 48384; loss 0.637247\n",
      "epoch:1; batch 48384; train accuracy: 0.663360\n",
      "epoch 1; batch 48512; loss 0.636117\n",
      "epoch:1; batch 48512; train accuracy: 0.663588\n",
      "epoch 1; batch 48640; loss 0.623838\n",
      "epoch:1; batch 48640; train accuracy: 0.663754\n",
      "epoch 1; batch 48768; loss 0.569770\n",
      "epoch:1; batch 48768; train accuracy: 0.664042\n",
      "epoch 1; batch 48896; loss 0.678788\n",
      "epoch:1; batch 48896; train accuracy: 0.664267\n",
      "epoch 1; batch 49024; loss 0.660667\n",
      "epoch:1; batch 49024; train accuracy: 0.664470\n",
      "epoch 1; batch 49152; loss 0.450594\n",
      "epoch:1; batch 49152; train accuracy: 0.664978\n",
      "epoch 1; batch 49280; loss 0.772399\n",
      "epoch:1; batch 49280; train accuracy: 0.665138\n",
      "epoch 1; batch 49408; loss 0.535393\n",
      "epoch:1; batch 49408; train accuracy: 0.665479\n",
      "epoch 1; batch 49536; loss 0.543265\n",
      "epoch:1; batch 49536; train accuracy: 0.665799\n",
      "epoch 1; batch 49664; loss 0.603820\n",
      "epoch:1; batch 49664; train accuracy: 0.666096\n",
      "epoch 1; batch 49792; loss 0.594836\n",
      "epoch:1; batch 49792; train accuracy: 0.666372\n",
      "epoch 1; batch 49920; loss 0.740885\n",
      "epoch:1; batch 49920; train accuracy: 0.666526\n",
      "epoch 1; batch 50048; loss 0.789848\n",
      "epoch:1; batch 50048; train accuracy: 0.666720\n",
      "epoch 1; batch 50176; loss 0.671205\n",
      "epoch:1; batch 50176; train accuracy: 0.666793\n",
      "epoch 1; batch 50304; loss 0.655776\n",
      "epoch:1; batch 50304; train accuracy: 0.666985\n",
      "epoch 1; batch 50432; loss 0.515209\n",
      "epoch:1; batch 50432; train accuracy: 0.667394\n",
      "epoch 1; batch 50560; loss 0.543625\n",
      "epoch:1; batch 50560; train accuracy: 0.667682\n",
      "epoch 1; batch 50688; loss 0.528134\n",
      "epoch:1; batch 50688; train accuracy: 0.668107\n",
      "epoch 1; batch 50816; loss 0.651268\n",
      "epoch:1; batch 50816; train accuracy: 0.668274\n",
      "epoch 1; batch 50944; loss 0.674304\n",
      "epoch:1; batch 50944; train accuracy: 0.668420\n",
      "epoch 1; batch 51072; loss 0.675643\n",
      "epoch:1; batch 51072; train accuracy: 0.668664\n",
      "epoch 1; batch 51200; loss 0.694175\n",
      "epoch:1; batch 51200; train accuracy: 0.668867\n",
      "epoch 1; batch 51328; loss 0.648531\n",
      "epoch:1; batch 51328; train accuracy: 0.669089\n",
      "epoch 1; batch 51456; loss 0.528196\n",
      "epoch:1; batch 51456; train accuracy: 0.669485\n",
      "epoch 1; batch 51584; loss 0.693499\n",
      "epoch:1; batch 51584; train accuracy: 0.669607\n",
      "epoch 1; batch 51712; loss 0.581551\n",
      "epoch:1; batch 51712; train accuracy: 0.669941\n",
      "epoch 1; batch 51840; loss 0.613870\n",
      "epoch:1; batch 51840; train accuracy: 0.670274\n",
      "epoch 1; batch 51968; loss 0.664646\n",
      "epoch:1; batch 51968; train accuracy: 0.670432\n",
      "epoch 1; batch 52096; loss 0.661708\n",
      "epoch:1; batch 52096; train accuracy: 0.670685\n",
      "epoch 1; batch 52224; loss 0.559519\n",
      "epoch:1; batch 52224; train accuracy: 0.670994\n",
      "epoch 1; batch 52352; loss 0.619731\n",
      "epoch:1; batch 52352; train accuracy: 0.671206\n",
      "epoch 1; batch 52480; loss 0.666867\n",
      "epoch:1; batch 52480; train accuracy: 0.671360\n",
      "epoch 1; batch 52608; loss 0.739877\n",
      "epoch:1; batch 52608; train accuracy: 0.671533\n",
      "epoch 1; batch 52736; loss 0.566029\n",
      "epoch:1; batch 52736; train accuracy: 0.671894\n",
      "epoch 1; batch 52864; loss 0.640814\n",
      "epoch:1; batch 52864; train accuracy: 0.671988\n",
      "epoch 1; batch 52992; loss 0.619551\n",
      "epoch:1; batch 52992; train accuracy: 0.672120\n",
      "epoch 1; batch 53120; loss 0.628290\n",
      "epoch:1; batch 53120; train accuracy: 0.672364\n",
      "epoch 1; batch 53248; loss 0.672363\n",
      "epoch:1; batch 53248; train accuracy: 0.672476\n",
      "epoch 1; batch 53376; loss 0.543079\n",
      "epoch:1; batch 53376; train accuracy: 0.672737\n",
      "epoch 1; batch 53504; loss 0.647606\n",
      "epoch:1; batch 53504; train accuracy: 0.672978\n",
      "epoch 1; batch 53632; loss 0.518546\n",
      "epoch:1; batch 53632; train accuracy: 0.673367\n",
      "epoch 1; batch 53760; loss 0.684717\n",
      "epoch:1; batch 53760; train accuracy: 0.673531\n",
      "epoch 1; batch 53888; loss 0.706313\n",
      "epoch:1; batch 53888; train accuracy: 0.673656\n",
      "epoch 1; batch 54016; loss 0.533697\n",
      "epoch:1; batch 54016; train accuracy: 0.674004\n",
      "epoch 1; batch 54144; loss 0.642814\n",
      "epoch:1; batch 54144; train accuracy: 0.674017\n",
      "epoch 1; batch 54272; loss 0.628439\n",
      "epoch:1; batch 54272; train accuracy: 0.674270\n",
      "epoch 1; batch 54400; loss 0.584587\n",
      "epoch:1; batch 54400; train accuracy: 0.674540\n",
      "epoch 1; batch 54528; loss 0.707926\n",
      "epoch:1; batch 54528; train accuracy: 0.674589\n",
      "epoch 1; batch 54656; loss 0.708071\n",
      "epoch:1; batch 54656; train accuracy: 0.674693\n",
      "epoch 1; batch 54784; loss 0.756510\n",
      "epoch:1; batch 54784; train accuracy: 0.674704\n",
      "epoch 1; batch 54912; loss 0.707393\n",
      "epoch:1; batch 54912; train accuracy: 0.674880\n",
      "epoch 1; batch 55040; loss 0.702623\n",
      "epoch:1; batch 55040; train accuracy: 0.675054\n",
      "epoch 1; batch 55168; loss 0.602184\n",
      "epoch:1; batch 55168; train accuracy: 0.675319\n",
      "epoch 1; batch 55296; loss 0.665864\n",
      "epoch:1; batch 55296; train accuracy: 0.675420\n",
      "epoch 1; batch 55424; loss 0.772966\n",
      "epoch:1; batch 55424; train accuracy: 0.675592\n",
      "epoch 1; batch 55552; loss 0.550381\n",
      "epoch:1; batch 55552; train accuracy: 0.675889\n",
      "epoch 1; batch 55680; loss 0.695734\n",
      "epoch:1; batch 55680; train accuracy: 0.676024\n",
      "epoch 1; batch 55808; loss 0.592116\n",
      "epoch:1; batch 55808; train accuracy: 0.676247\n",
      "epoch 1; batch 55936; loss 0.674710\n",
      "epoch:1; batch 55936; train accuracy: 0.676362\n",
      "epoch 1; batch 56064; loss 0.618772\n",
      "epoch:1; batch 56064; train accuracy: 0.676637\n",
      "epoch 1; batch 56192; loss 0.537289\n",
      "epoch:1; batch 56192; train accuracy: 0.676947\n",
      "epoch 1; batch 56320; loss 0.585918\n",
      "epoch:1; batch 56320; train accuracy: 0.677184\n",
      "epoch 1; batch 56448; loss 0.679268\n",
      "epoch:1; batch 56448; train accuracy: 0.677349\n",
      "epoch 1; batch 56576; loss 0.834228\n",
      "epoch:1; batch 56576; train accuracy: 0.677354\n",
      "epoch 1; batch 56704; loss 0.641440\n",
      "epoch:1; batch 56704; train accuracy: 0.677554\n",
      "epoch 1; batch 56832; loss 0.609309\n",
      "epoch:1; batch 56832; train accuracy: 0.677787\n",
      "epoch 1; batch 56960; loss 0.676901\n",
      "epoch:1; batch 56960; train accuracy: 0.677932\n",
      "epoch 1; batch 57088; loss 0.558024\n",
      "epoch:1; batch 57088; train accuracy: 0.678111\n",
      "epoch 1; batch 57216; loss 0.698831\n",
      "epoch:1; batch 57216; train accuracy: 0.678149\n",
      "epoch 1; batch 57344; loss 0.569111\n",
      "epoch:1; batch 57344; train accuracy: 0.678449\n",
      "epoch 1; batch 57472; loss 0.610669\n",
      "epoch:1; batch 57472; train accuracy: 0.678574\n",
      "epoch 1; batch 57600; loss 0.582918\n",
      "epoch:1; batch 57600; train accuracy: 0.678802\n",
      "epoch 1; batch 57728; loss 0.679802\n",
      "epoch:1; batch 57728; train accuracy: 0.678943\n",
      "epoch 1; batch 57856; loss 0.652523\n",
      "epoch:1; batch 57856; train accuracy: 0.679065\n",
      "epoch 1; batch 57984; loss 0.853249\n",
      "epoch:1; batch 57984; train accuracy: 0.679067\n",
      "epoch 1; batch 58112; loss 0.770612\n",
      "epoch:1; batch 58112; train accuracy: 0.679102\n",
      "epoch 1; batch 58240; loss 0.711607\n",
      "epoch:1; batch 58240; train accuracy: 0.679190\n",
      "epoch 1; batch 58368; loss 0.651190\n",
      "epoch:1; batch 58368; train accuracy: 0.679311\n",
      "epoch 1; batch 58496; loss 0.654886\n",
      "epoch:1; batch 58496; train accuracy: 0.679414\n",
      "epoch 1; batch 58624; loss 0.635446\n",
      "epoch:1; batch 58624; train accuracy: 0.679551\n",
      "epoch 1; batch 58752; loss 0.592699\n",
      "epoch:1; batch 58752; train accuracy: 0.679722\n",
      "epoch 1; batch 58880; loss 0.711884\n",
      "epoch:1; batch 58880; train accuracy: 0.679840\n",
      "epoch 1; batch 59008; loss 0.683838\n",
      "epoch:1; batch 59008; train accuracy: 0.679959\n",
      "epoch 1; batch 59136; loss 0.610795\n",
      "epoch:1; batch 59136; train accuracy: 0.680043\n",
      "epoch 1; batch 59264; loss 0.606494\n",
      "epoch:1; batch 59264; train accuracy: 0.680278\n",
      "epoch 1; batch 59392; loss 0.638309\n",
      "epoch:1; batch 59392; train accuracy: 0.680428\n",
      "epoch 1; batch 59520; loss 0.644256\n",
      "epoch:1; batch 59520; train accuracy: 0.680528\n",
      "epoch 1; batch 59648; loss 0.615446\n",
      "epoch:1; batch 59648; train accuracy: 0.680727\n",
      "epoch 1; batch 59776; loss 0.680468\n",
      "epoch:1; batch 59776; train accuracy: 0.680825\n",
      "epoch 1; batch 59904; loss 0.803752\n",
      "epoch:1; batch 59904; train accuracy: 0.680839\n",
      "epoch 1; batch 60032; loss 0.662400\n",
      "epoch:1; batch 60032; train accuracy: 0.681037\n",
      "epoch 1; batch 60160; loss 0.511005\n",
      "epoch:1; batch 60160; train accuracy: 0.681333\n",
      "epoch 1; batch 60288; loss 0.781452\n",
      "epoch:1; batch 60288; train accuracy: 0.681363\n",
      "epoch 1; batch 60416; loss 0.508599\n",
      "epoch:1; batch 60416; train accuracy: 0.681707\n",
      "epoch 1; batch 60544; loss 0.603784\n",
      "epoch:1; batch 60544; train accuracy: 0.681884\n",
      "epoch 1; batch 60672; loss 0.582148\n",
      "epoch:1; batch 60672; train accuracy: 0.682077\n",
      "epoch 1; batch 60800; loss 0.593641\n",
      "epoch:1; batch 60800; train accuracy: 0.682303\n",
      "epoch 1; batch 60928; loss 0.538426\n",
      "epoch:1; batch 60928; train accuracy: 0.682527\n",
      "epoch 1; batch 61056; loss 0.676710\n",
      "epoch:1; batch 61056; train accuracy: 0.682636\n",
      "epoch 1; batch 61184; loss 0.642194\n",
      "epoch:1; batch 61184; train accuracy: 0.682826\n",
      "epoch 1; batch 61312; loss 0.631649\n",
      "epoch:1; batch 61312; train accuracy: 0.683015\n",
      "epoch 1; batch 61440; loss 0.564724\n",
      "epoch:1; batch 61440; train accuracy: 0.683236\n",
      "epoch 1; batch 61568; loss 0.589391\n",
      "epoch:1; batch 61568; train accuracy: 0.683326\n",
      "epoch 1; batch 61696; loss 0.708909\n",
      "epoch:1; batch 61696; train accuracy: 0.683415\n",
      "epoch 1; batch 61824; loss 0.712811\n",
      "epoch:1; batch 61824; train accuracy: 0.683569\n",
      "epoch 1; batch 61952; loss 0.652619\n",
      "epoch:1; batch 61952; train accuracy: 0.683707\n",
      "epoch 1; batch 62080; loss 0.687238\n",
      "epoch:1; batch 62080; train accuracy: 0.683715\n",
      "epoch 1; batch 62208; loss 0.765838\n",
      "epoch:1; batch 62208; train accuracy: 0.683835\n",
      "epoch 1; batch 62336; loss 0.564263\n",
      "epoch:1; batch 62336; train accuracy: 0.684035\n",
      "epoch 1; batch 62464; loss 0.542899\n",
      "epoch:1; batch 62464; train accuracy: 0.684282\n",
      "epoch 1; batch 62592; loss 0.763599\n",
      "epoch:1; batch 62592; train accuracy: 0.684305\n",
      "epoch 1; batch 62720; loss 0.583338\n",
      "epoch:1; batch 62720; train accuracy: 0.684407\n",
      "epoch 1; batch 62848; loss 0.738160\n",
      "epoch:1; batch 62848; train accuracy: 0.684509\n",
      "epoch 1; batch 62976; loss 0.681542\n",
      "epoch:1; batch 62976; train accuracy: 0.684594\n",
      "epoch 1; batch 63104; loss 0.784677\n",
      "epoch:1; batch 63104; train accuracy: 0.684537\n",
      "epoch 1; batch 63232; loss 0.813725\n",
      "epoch:1; batch 63232; train accuracy: 0.684558\n",
      "epoch 1; batch 63360; loss 0.747283\n",
      "epoch:1; batch 63360; train accuracy: 0.684533\n",
      "epoch 1; batch 63488; loss 0.627178\n",
      "epoch:1; batch 63488; train accuracy: 0.684649\n",
      "epoch 1; batch 63616; loss 0.534844\n",
      "epoch:1; batch 63616; train accuracy: 0.684875\n",
      "epoch 1; batch 63744; loss 0.590250\n",
      "epoch:1; batch 63744; train accuracy: 0.685068\n",
      "epoch 1; batch 63872; loss 0.619891\n",
      "epoch:1; batch 63872; train accuracy: 0.685246\n",
      "epoch 1; batch 64000; loss 0.621182\n",
      "epoch:1; batch 64000; train accuracy: 0.685438\n",
      "epoch 1; batch 64128; loss 0.636445\n",
      "epoch:1; batch 64128; train accuracy: 0.685520\n",
      "epoch 1; batch 64256; loss 0.757774\n",
      "epoch:1; batch 64256; train accuracy: 0.685586\n",
      "epoch 1; batch 64384; loss 0.626385\n",
      "epoch:1; batch 64384; train accuracy: 0.685729\n",
      "epoch 1; batch 64512; loss 0.694530\n",
      "epoch:1; batch 64512; train accuracy: 0.685810\n",
      "epoch 1; batch 64640; loss 0.744699\n",
      "epoch:1; batch 64640; train accuracy: 0.685798\n",
      "epoch 1; batch 64768; loss 0.621360\n",
      "epoch:1; batch 64768; train accuracy: 0.685971\n",
      "epoch 1; batch 64896; loss 0.725172\n",
      "epoch:1; batch 64896; train accuracy: 0.686036\n",
      "epoch 1; batch 65024; loss 0.708297\n",
      "epoch:1; batch 65024; train accuracy: 0.686039\n",
      "epoch 1; batch 65152; loss 0.672023\n",
      "epoch:1; batch 65152; train accuracy: 0.686180\n",
      "epoch 1; batch 65280; loss 0.670467\n",
      "epoch:1; batch 65280; train accuracy: 0.686366\n",
      "epoch 1; batch 65408; loss 0.717646\n",
      "epoch:1; batch 65408; train accuracy: 0.686384\n",
      "epoch 1; batch 65536; loss 0.819829\n",
      "epoch:1; batch 65536; train accuracy: 0.686371\n",
      "epoch 1; batch 65664; loss 0.742553\n",
      "epoch:1; batch 65664; train accuracy: 0.686419\n",
      "epoch 1; batch 65792; loss 0.673976\n",
      "epoch:1; batch 65792; train accuracy: 0.686634\n",
      "epoch 1; batch 65920; loss 0.651414\n",
      "epoch:1; batch 65920; train accuracy: 0.686772\n",
      "epoch 1; batch 66048; loss 0.655680\n",
      "epoch:1; batch 66048; train accuracy: 0.686940\n",
      "epoch 1; batch 66176; loss 0.533993\n",
      "epoch:1; batch 66176; train accuracy: 0.687183\n",
      "epoch 1; batch 66304; loss 0.510240\n",
      "epoch:1; batch 66304; train accuracy: 0.687394\n",
      "epoch 1; batch 66432; loss 0.602817\n",
      "epoch:1; batch 66432; train accuracy: 0.687530\n",
      "epoch 1; batch 66560; loss 0.585636\n",
      "epoch:1; batch 66560; train accuracy: 0.687695\n",
      "epoch 1; batch 66688; loss 0.698618\n",
      "epoch:1; batch 66688; train accuracy: 0.687755\n",
      "epoch 1; batch 66816; loss 0.585274\n",
      "epoch:1; batch 66816; train accuracy: 0.687949\n",
      "epoch 1; batch 66944; loss 0.645221\n",
      "epoch:1; batch 66944; train accuracy: 0.688068\n",
      "epoch 1; batch 67072; loss 0.720360\n",
      "epoch:1; batch 67072; train accuracy: 0.688171\n",
      "epoch 1; batch 67200; loss 0.630019\n",
      "epoch:1; batch 67200; train accuracy: 0.688304\n",
      "epoch 1; batch 67328; loss 0.629351\n",
      "epoch:1; batch 67328; train accuracy: 0.688376\n",
      "epoch 1; batch 67456; loss 0.649475\n",
      "epoch:1; batch 67456; train accuracy: 0.688464\n",
      "epoch 1; batch 67584; loss 0.798855\n",
      "epoch:1; batch 67584; train accuracy: 0.688477\n",
      "epoch 1; batch 67712; loss 0.681530\n",
      "epoch:1; batch 67712; train accuracy: 0.688563\n",
      "epoch 1; batch 67840; loss 0.637529\n",
      "epoch:1; batch 67840; train accuracy: 0.688709\n",
      "epoch 1; batch 67968; loss 0.645602\n",
      "epoch:1; batch 67968; train accuracy: 0.688824\n",
      "epoch 1; batch 68096; loss 0.595957\n",
      "epoch:1; batch 68096; train accuracy: 0.688954\n",
      "epoch 1; batch 68224; loss 0.635924\n",
      "epoch:1; batch 68224; train accuracy: 0.689054\n",
      "epoch 1; batch 68352; loss 0.611528\n",
      "epoch:1; batch 68352; train accuracy: 0.689197\n",
      "epoch 1; batch 68480; loss 0.658899\n",
      "epoch:1; batch 68480; train accuracy: 0.689311\n",
      "epoch 1; batch 68608; loss 0.680846\n",
      "epoch:1; batch 68608; train accuracy: 0.689351\n",
      "epoch 1; batch 68736; loss 0.571535\n",
      "epoch:1; batch 68736; train accuracy: 0.689580\n",
      "epoch 1; batch 68864; loss 0.558119\n",
      "epoch:1; batch 68864; train accuracy: 0.689809\n",
      "epoch 1; batch 68992; loss 0.683995\n",
      "epoch:1; batch 68992; train accuracy: 0.689950\n",
      "epoch 1; batch 69120; loss 0.500615\n",
      "epoch:1; batch 69120; train accuracy: 0.690191\n",
      "epoch 1; batch 69248; loss 0.653930\n",
      "epoch:1; batch 69248; train accuracy: 0.690258\n",
      "epoch 1; batch 69376; loss 0.648175\n",
      "epoch:1; batch 69376; train accuracy: 0.690340\n",
      "epoch 1; batch 69504; loss 0.623409\n",
      "epoch:1; batch 69504; train accuracy: 0.690493\n",
      "epoch 1; batch 69632; loss 0.633493\n",
      "epoch:1; batch 69632; train accuracy: 0.690616\n",
      "epoch 1; batch 69760; loss 0.700485\n",
      "epoch:1; batch 69760; train accuracy: 0.690582\n",
      "epoch 1; batch 69888; loss 0.729836\n",
      "epoch:1; batch 69888; train accuracy: 0.690705\n",
      "epoch 1; batch 70016; loss 0.614052\n",
      "epoch:1; batch 70016; train accuracy: 0.690842\n",
      "epoch 1; batch 70144; loss 0.668850\n",
      "epoch:1; batch 70144; train accuracy: 0.690850\n",
      "epoch 1; batch 70272; loss 0.619129\n",
      "epoch:1; batch 70272; train accuracy: 0.690972\n",
      "epoch 1; batch 70400; loss 0.730684\n",
      "epoch:1; batch 70400; train accuracy: 0.691009\n",
      "epoch 1; batch 70528; loss 0.644523\n",
      "epoch:1; batch 70528; train accuracy: 0.691144\n",
      "epoch 1; batch 70656; loss 0.594651\n",
      "epoch:1; batch 70656; train accuracy: 0.691222\n",
      "epoch 1; batch 70784; loss 0.719262\n",
      "epoch:1; batch 70784; train accuracy: 0.691272\n",
      "epoch 1; batch 70912; loss 0.563624\n",
      "epoch:1; batch 70912; train accuracy: 0.691449\n",
      "epoch 1; batch 71040; loss 0.737224\n",
      "epoch:1; batch 71040; train accuracy: 0.691512\n",
      "epoch 1; batch 71168; loss 0.612293\n",
      "epoch:1; batch 71168; train accuracy: 0.691687\n",
      "epoch 1; batch 71296; loss 0.662221\n",
      "epoch:1; batch 71296; train accuracy: 0.691792\n",
      "epoch 1; batch 71424; loss 0.549004\n",
      "epoch:1; batch 71424; train accuracy: 0.692008\n",
      "epoch 1; batch 71552; loss 0.644840\n",
      "epoch:1; batch 71552; train accuracy: 0.692126\n",
      "epoch 1; batch 71680; loss 0.648169\n",
      "epoch:1; batch 71680; train accuracy: 0.692243\n",
      "epoch 1; batch 71808; loss 0.436031\n",
      "epoch:1; batch 71808; train accuracy: 0.692486\n",
      "epoch 1; batch 71936; loss 0.692554\n",
      "epoch:1; batch 71936; train accuracy: 0.692602\n",
      "epoch 1; batch 72064; loss 0.564303\n",
      "epoch:1; batch 72064; train accuracy: 0.692759\n",
      "epoch 1; batch 72192; loss 0.777093\n",
      "epoch:1; batch 72192; train accuracy: 0.692750\n",
      "epoch 1; batch 72320; loss 0.560067\n",
      "epoch:1; batch 72320; train accuracy: 0.692934\n",
      "epoch 1; batch 72448; loss 0.665323\n",
      "epoch:1; batch 72448; train accuracy: 0.693063\n",
      "epoch 1; batch 72576; loss 0.780469\n",
      "epoch:1; batch 72576; train accuracy: 0.693039\n",
      "epoch 1; batch 72704; loss 0.615746\n",
      "epoch:1; batch 72704; train accuracy: 0.693236\n",
      "epoch 1; batch 72832; loss 0.592479\n",
      "epoch:1; batch 72832; train accuracy: 0.693322\n",
      "epoch 1; batch 72960; loss 0.632072\n",
      "epoch:1; batch 72960; train accuracy: 0.693435\n",
      "epoch 1; batch 73088; loss 0.808789\n",
      "epoch:1; batch 73088; train accuracy: 0.693356\n",
      "epoch 1; batch 73216; loss 0.702509\n",
      "epoch:1; batch 73216; train accuracy: 0.693387\n",
      "epoch 1; batch 73344; loss 0.705707\n",
      "epoch:1; batch 73344; train accuracy: 0.693513\n",
      "epoch 1; batch 73472; loss 0.583618\n",
      "epoch:1; batch 73472; train accuracy: 0.693666\n",
      "epoch 1; batch 73600; loss 0.698970\n",
      "epoch:1; batch 73600; train accuracy: 0.693696\n",
      "epoch 1; batch 73728; loss 0.583789\n",
      "epoch:1; batch 73728; train accuracy: 0.693875\n",
      "epoch 1; batch 73856; loss 0.630095\n",
      "epoch:1; batch 73856; train accuracy: 0.694013\n",
      "epoch 1; batch 73984; loss 0.652648\n",
      "epoch:1; batch 73984; train accuracy: 0.694055\n",
      "epoch 1; batch 74112; loss 0.464712\n",
      "epoch:1; batch 74112; train accuracy: 0.694274\n",
      "epoch 1; batch 74240; loss 0.561035\n",
      "epoch:1; batch 74240; train accuracy: 0.694383\n",
      "epoch 1; batch 74368; loss 0.649752\n",
      "epoch:1; batch 74368; train accuracy: 0.694425\n",
      "epoch 1; batch 74496; loss 0.584673\n",
      "epoch:1; batch 74496; train accuracy: 0.694534\n",
      "epoch 1; batch 74624; loss 0.519365\n",
      "epoch:1; batch 74624; train accuracy: 0.694723\n",
      "epoch 1; batch 74752; loss 0.645058\n",
      "epoch:1; batch 74752; train accuracy: 0.694884\n",
      "epoch 1; batch 74880; loss 0.904966\n",
      "epoch:1; batch 74880; train accuracy: 0.694818\n",
      "epoch 1; batch 75008; loss 0.622201\n",
      "epoch:1; batch 75008; train accuracy: 0.694939\n",
      "epoch 1; batch 75136; loss 0.540207\n",
      "epoch:1; batch 75136; train accuracy: 0.695126\n",
      "epoch 1; batch 75264; loss 0.547583\n",
      "epoch:1; batch 75264; train accuracy: 0.695312\n",
      "epoch 1; batch 75392; loss 0.641209\n",
      "epoch:1; batch 75392; train accuracy: 0.695419\n",
      "epoch 1; batch 75520; loss 0.510332\n",
      "epoch:1; batch 75520; train accuracy: 0.695564\n",
      "epoch 1; batch 75648; loss 0.724436\n",
      "epoch:1; batch 75648; train accuracy: 0.695590\n",
      "epoch 1; batch 75776; loss 0.697661\n",
      "epoch:1; batch 75776; train accuracy: 0.695563\n",
      "epoch 1; batch 75904; loss 0.618942\n",
      "epoch:1; batch 75904; train accuracy: 0.695695\n",
      "epoch 1; batch 76032; loss 0.683705\n",
      "epoch:1; batch 76032; train accuracy: 0.695799\n",
      "epoch 1; batch 76160; loss 0.620950\n",
      "epoch:1; batch 76160; train accuracy: 0.695969\n",
      "epoch 1; batch 76288; loss 0.692619\n",
      "epoch:1; batch 76288; train accuracy: 0.695994\n",
      "epoch 1; batch 76416; loss 0.767404\n",
      "epoch:1; batch 76416; train accuracy: 0.696019\n",
      "epoch 1; batch 76544; loss 0.796707\n",
      "epoch:1; batch 76544; train accuracy: 0.696031\n",
      "epoch 1; batch 76672; loss 0.542055\n",
      "epoch:1; batch 76672; train accuracy: 0.696186\n",
      "epoch 1; batch 76800; loss 0.613872\n",
      "epoch:1; batch 76800; train accuracy: 0.696276\n",
      "epoch 1; batch 76928; loss 0.614878\n",
      "epoch:1; batch 76928; train accuracy: 0.696430\n",
      "epoch 1; batch 77056; loss 0.571820\n",
      "epoch:1; batch 77056; train accuracy: 0.696610\n",
      "epoch 1; batch 77184; loss 0.704442\n",
      "epoch:1; batch 77184; train accuracy: 0.696686\n",
      "epoch 1; batch 77312; loss 0.669435\n",
      "epoch:1; batch 77312; train accuracy: 0.696774\n",
      "epoch 1; batch 77440; loss 0.633266\n",
      "epoch:1; batch 77440; train accuracy: 0.696798\n",
      "epoch 1; batch 77568; loss 0.730957\n",
      "epoch:1; batch 77568; train accuracy: 0.696808\n",
      "epoch 1; batch 77696; loss 0.745851\n",
      "epoch:1; batch 77696; train accuracy: 0.696831\n",
      "epoch 1; batch 77824; loss 0.567798\n",
      "epoch:1; batch 77824; train accuracy: 0.697009\n",
      "epoch 1; batch 77952; loss 0.521424\n",
      "epoch:1; batch 77952; train accuracy: 0.697185\n",
      "epoch 1; batch 78080; loss 0.692314\n",
      "epoch:1; batch 78080; train accuracy: 0.697272\n",
      "epoch 1; batch 78208; loss 0.611487\n",
      "epoch:1; batch 78208; train accuracy: 0.697409\n",
      "epoch 1; batch 78336; loss 0.675033\n",
      "epoch:1; batch 78336; train accuracy: 0.697432\n",
      "epoch 1; batch 78464; loss 0.631298\n",
      "epoch:1; batch 78464; train accuracy: 0.697543\n",
      "epoch 1; batch 78592; loss 0.482830\n",
      "epoch:1; batch 78592; train accuracy: 0.697730\n",
      "epoch 1; batch 78720; loss 0.712927\n",
      "epoch:1; batch 78720; train accuracy: 0.697777\n",
      "epoch 1; batch 78848; loss 0.637637\n",
      "epoch:1; batch 78848; train accuracy: 0.697849\n",
      "epoch 1; batch 78976; loss 0.694379\n",
      "epoch:1; batch 78976; train accuracy: 0.697908\n",
      "epoch 1; batch 79104; loss 0.581771\n",
      "epoch:1; batch 79104; train accuracy: 0.698056\n",
      "epoch 1; batch 79232; loss 0.678620\n",
      "epoch:1; batch 79232; train accuracy: 0.698152\n",
      "epoch 1; batch 79360; loss 0.673017\n",
      "epoch:1; batch 79360; train accuracy: 0.698211\n",
      "epoch 1; batch 79488; loss 0.581256\n",
      "epoch:1; batch 79488; train accuracy: 0.698344\n",
      "epoch 1; batch 79616; loss 0.642710\n",
      "epoch:1; batch 79616; train accuracy: 0.698427\n",
      "epoch 1; batch 79744; loss 0.723766\n",
      "epoch:1; batch 79744; train accuracy: 0.698435\n",
      "epoch 1; batch 79872; loss 0.665268\n",
      "epoch:1; batch 79872; train accuracy: 0.698505\n",
      "epoch 1; batch 80000; loss 0.637549\n",
      "epoch:1; batch 80000; train accuracy: 0.698575\n",
      "epoch 1; batch 80128; loss 0.512908\n",
      "epoch:1; batch 80128; train accuracy: 0.698757\n",
      "epoch 1; batch 80256; loss 0.614947\n",
      "epoch:1; batch 80256; train accuracy: 0.698826\n",
      "epoch 1; batch 80384; loss 0.670937\n",
      "epoch:1; batch 80384; train accuracy: 0.698821\n",
      "epoch 1; batch 80512; loss 0.693324\n",
      "epoch:1; batch 80512; train accuracy: 0.698877\n",
      "epoch 1; batch 80640; loss 0.711023\n",
      "epoch:1; batch 80640; train accuracy: 0.698921\n",
      "epoch 1; batch 80768; loss 0.723313\n",
      "epoch:1; batch 80768; train accuracy: 0.698977\n",
      "epoch 1; batch 80896; loss 0.647020\n",
      "epoch:1; batch 80896; train accuracy: 0.698996\n",
      "epoch 1; batch 81024; loss 0.727525\n",
      "epoch:1; batch 81024; train accuracy: 0.698990\n",
      "epoch 1; batch 81152; loss 0.631113\n",
      "epoch:1; batch 81152; train accuracy: 0.699108\n",
      "epoch 1; batch 81280; loss 0.571093\n",
      "epoch:1; batch 81280; train accuracy: 0.699225\n",
      "epoch 1; batch 81408; loss 0.622097\n",
      "epoch:1; batch 81408; train accuracy: 0.699268\n",
      "epoch 1; batch 81536; loss 0.665341\n",
      "epoch:1; batch 81536; train accuracy: 0.699335\n",
      "epoch 1; batch 81664; loss 0.604771\n",
      "epoch:1; batch 81664; train accuracy: 0.699476\n",
      "epoch 1; batch 81792; loss 0.602907\n",
      "epoch:1; batch 81792; train accuracy: 0.699531\n",
      "epoch 1; batch 81920; loss 0.643621\n",
      "epoch:1; batch 81920; train accuracy: 0.699646\n",
      "epoch 1; batch 82048; loss 0.703202\n",
      "epoch:1; batch 82048; train accuracy: 0.699700\n",
      "epoch 1; batch 82176; loss 0.652149\n",
      "epoch:1; batch 82176; train accuracy: 0.699754\n",
      "epoch 1; batch 82304; loss 0.771671\n",
      "epoch:1; batch 82304; train accuracy: 0.699772\n",
      "epoch 1; batch 82432; loss 0.636132\n",
      "epoch:1; batch 82432; train accuracy: 0.699850\n",
      "epoch 1; batch 82560; loss 0.648978\n",
      "epoch:1; batch 82560; train accuracy: 0.699879\n",
      "epoch 1; batch 82688; loss 0.754681\n",
      "epoch:1; batch 82688; train accuracy: 0.699896\n",
      "epoch 1; batch 82816; loss 0.485842\n",
      "epoch:1; batch 82816; train accuracy: 0.700058\n",
      "epoch 1; batch 82944; loss 0.524252\n",
      "epoch:1; batch 82944; train accuracy: 0.700244\n",
      "epoch 1; batch 83072; loss 0.581112\n",
      "epoch:1; batch 83072; train accuracy: 0.700344\n",
      "epoch 1; batch 83200; loss 0.661725\n",
      "epoch:1; batch 83200; train accuracy: 0.700421\n",
      "epoch 1; batch 83328; loss 0.675458\n",
      "epoch:1; batch 83328; train accuracy: 0.700461\n",
      "epoch 1; batch 83456; loss 0.539219\n",
      "epoch:1; batch 83456; train accuracy: 0.700573\n",
      "epoch 1; batch 83584; loss 0.534538\n",
      "epoch:1; batch 83584; train accuracy: 0.700744\n",
      "epoch 1; batch 83712; loss 0.688622\n",
      "epoch:1; batch 83712; train accuracy: 0.700736\n",
      "epoch 1; batch 83840; loss 0.563719\n",
      "epoch:1; batch 83840; train accuracy: 0.700835\n",
      "epoch 1; batch 83968; loss 0.641550\n",
      "epoch:1; batch 83968; train accuracy: 0.700934\n",
      "epoch 1; batch 84096; loss 0.627933\n",
      "epoch:1; batch 84096; train accuracy: 0.700949\n",
      "epoch 1; batch 84224; loss 0.640017\n",
      "epoch:1; batch 84224; train accuracy: 0.701023\n",
      "epoch 1; batch 84352; loss 0.729736\n",
      "epoch:1; batch 84352; train accuracy: 0.701074\n",
      "epoch 1; batch 84480; loss 0.616267\n",
      "epoch:1; batch 84480; train accuracy: 0.701125\n",
      "epoch 1; batch 84608; loss 0.430587\n",
      "epoch:1; batch 84608; train accuracy: 0.701399\n",
      "epoch 1; batch 84736; loss 0.623454\n",
      "epoch:1; batch 84736; train accuracy: 0.701496\n",
      "epoch 1; batch 84864; loss 0.667811\n",
      "epoch:1; batch 84864; train accuracy: 0.701570\n",
      "epoch 1; batch 84992; loss 0.548182\n",
      "epoch:1; batch 84992; train accuracy: 0.701725\n",
      "epoch 1; batch 85120; loss 0.688085\n",
      "epoch:1; batch 85120; train accuracy: 0.701739\n",
      "epoch 1; batch 85248; loss 0.610714\n",
      "epoch:1; batch 85248; train accuracy: 0.701870\n",
      "epoch 1; batch 85376; loss 0.732589\n",
      "epoch:1; batch 85376; train accuracy: 0.701907\n",
      "epoch 1; batch 85504; loss 0.675681\n",
      "epoch:1; batch 85504; train accuracy: 0.701932\n",
      "epoch 1; batch 85632; loss 0.719815\n",
      "epoch:1; batch 85632; train accuracy: 0.701969\n",
      "epoch 1; batch 85760; loss 0.553396\n",
      "epoch:1; batch 85760; train accuracy: 0.702111\n",
      "epoch 1; batch 85888; loss 0.706627\n",
      "epoch:1; batch 85888; train accuracy: 0.702182\n",
      "epoch 1; batch 86016; loss 0.700158\n",
      "epoch:1; batch 86016; train accuracy: 0.702183\n",
      "epoch 1; batch 86144; loss 0.609841\n",
      "epoch:1; batch 86144; train accuracy: 0.702278\n",
      "epoch 1; batch 86272; loss 0.807875\n",
      "epoch:1; batch 86272; train accuracy: 0.702232\n",
      "epoch 1; batch 86400; loss 0.603619\n",
      "epoch:1; batch 86400; train accuracy: 0.702361\n",
      "epoch 1; batch 86528; loss 0.604396\n",
      "epoch:1; batch 86528; train accuracy: 0.702443\n",
      "epoch 1; batch 86656; loss 0.655407\n",
      "epoch:1; batch 86656; train accuracy: 0.702536\n",
      "epoch 1; batch 86784; loss 0.585661\n",
      "epoch:1; batch 86784; train accuracy: 0.702630\n",
      "epoch 1; batch 86912; loss 0.645410\n",
      "epoch:1; batch 86912; train accuracy: 0.702734\n",
      "epoch 1; batch 87040; loss 0.646478\n",
      "epoch:1; batch 87040; train accuracy: 0.702734\n",
      "epoch 1; batch 87168; loss 0.594318\n",
      "epoch:1; batch 87168; train accuracy: 0.702827\n",
      "epoch 1; batch 87296; loss 0.520896\n",
      "epoch:1; batch 87296; train accuracy: 0.702965\n",
      "epoch 1; batch 87424; loss 0.632318\n",
      "epoch:1; batch 87424; train accuracy: 0.702988\n",
      "epoch 1; batch 87552; loss 0.520490\n",
      "epoch:1; batch 87552; train accuracy: 0.703125\n",
      "epoch 1; batch 87680; loss 0.615376\n",
      "epoch:1; batch 87680; train accuracy: 0.703182\n",
      "epoch 1; batch 87808; loss 0.798101\n",
      "epoch:1; batch 87808; train accuracy: 0.703216\n",
      "epoch 1; batch 87936; loss 0.684928\n",
      "epoch:1; batch 87936; train accuracy: 0.703250\n",
      "epoch 1; batch 88064; loss 0.502290\n",
      "epoch:1; batch 88064; train accuracy: 0.703420\n",
      "epoch 1; batch 88192; loss 0.619787\n",
      "epoch:1; batch 88192; train accuracy: 0.703499\n",
      "epoch 1; batch 88320; loss 0.640642\n",
      "epoch:1; batch 88320; train accuracy: 0.703555\n",
      "epoch 1; batch 88448; loss 0.577555\n",
      "epoch:1; batch 88448; train accuracy: 0.703668\n",
      "epoch 1; batch 88576; loss 0.658392\n",
      "epoch:1; batch 88576; train accuracy: 0.703701\n",
      "epoch 1; batch 88704; loss 0.677221\n",
      "epoch:1; batch 88704; train accuracy: 0.703711\n",
      "epoch 1; batch 88832; loss 0.548694\n",
      "epoch:1; batch 88832; train accuracy: 0.703857\n",
      "epoch 1; batch 88960; loss 0.593146\n",
      "epoch:1; batch 88960; train accuracy: 0.703968\n",
      "epoch 1; batch 89088; loss 0.646969\n",
      "epoch:1; batch 89088; train accuracy: 0.703989\n",
      "epoch 1; batch 89216; loss 0.579135\n",
      "epoch:1; batch 89216; train accuracy: 0.704100\n",
      "epoch 1; batch 89344; loss 0.786188\n",
      "epoch:1; batch 89344; train accuracy: 0.704076\n",
      "epoch 1; batch 89472; loss 0.540977\n",
      "epoch:1; batch 89472; train accuracy: 0.704187\n",
      "epoch 1; batch 89600; loss 0.551058\n",
      "epoch:1; batch 89600; train accuracy: 0.704342\n",
      "epoch 1; batch 89728; loss 0.783892\n",
      "epoch:1; batch 89728; train accuracy: 0.704306\n",
      "epoch 1; batch 89856; loss 0.582314\n",
      "epoch:1; batch 89856; train accuracy: 0.704405\n",
      "epoch 1; batch 89984; loss 0.599045\n",
      "epoch:1; batch 89984; train accuracy: 0.704514\n",
      "epoch 1; batch 90112; loss 0.563659\n",
      "epoch:1; batch 90112; train accuracy: 0.704679\n",
      "epoch 1; batch 90240; loss 0.673476\n",
      "epoch:1; batch 90240; train accuracy: 0.704754\n",
      "epoch 1; batch 90368; loss 0.590888\n",
      "epoch:1; batch 90368; train accuracy: 0.704873\n",
      "epoch 1; batch 90496; loss 0.597367\n",
      "epoch:1; batch 90496; train accuracy: 0.704959\n",
      "epoch 1; batch 90624; loss 0.461340\n",
      "epoch:1; batch 90624; train accuracy: 0.705144\n",
      "epoch 1; batch 90752; loss 0.645283\n",
      "epoch:1; batch 90752; train accuracy: 0.705186\n",
      "epoch 1; batch 90880; loss 0.558059\n",
      "epoch:1; batch 90880; train accuracy: 0.705348\n",
      "epoch 1; batch 91008; loss 0.583171\n",
      "epoch:1; batch 91008; train accuracy: 0.705520\n",
      "epoch 1; batch 91136; loss 0.727345\n",
      "epoch:1; batch 91136; train accuracy: 0.705594\n",
      "epoch 1; batch 91264; loss 0.609627\n",
      "epoch:1; batch 91264; train accuracy: 0.705645\n",
      "epoch 1; batch 91392; loss 0.723028\n",
      "epoch:1; batch 91392; train accuracy: 0.705718\n",
      "epoch 1; batch 91520; loss 0.596286\n",
      "epoch:1; batch 91520; train accuracy: 0.705802\n",
      "epoch 1; batch 91648; loss 0.754197\n",
      "epoch:1; batch 91648; train accuracy: 0.705886\n",
      "epoch 1; batch 91776; loss 0.594291\n",
      "epoch:1; batch 91776; train accuracy: 0.706002\n",
      "epoch 1; batch 91904; loss 0.652785\n",
      "epoch:1; batch 91904; train accuracy: 0.706041\n",
      "epoch 1; batch 92032; loss 0.704090\n",
      "epoch:1; batch 92032; train accuracy: 0.706091\n",
      "epoch 1; batch 92160; loss 0.661209\n",
      "epoch:1; batch 92160; train accuracy: 0.706163\n",
      "epoch 1; batch 92288; loss 0.636350\n",
      "epoch:1; batch 92288; train accuracy: 0.706224\n",
      "epoch 1; batch 92416; loss 0.538421\n",
      "epoch:1; batch 92416; train accuracy: 0.706371\n",
      "epoch 1; batch 92544; loss 0.690660\n",
      "epoch:1; batch 92544; train accuracy: 0.706475\n",
      "epoch 1; batch 92672; loss 0.688488\n",
      "epoch:1; batch 92672; train accuracy: 0.706556\n",
      "epoch 1; batch 92800; loss 0.543093\n",
      "epoch:1; batch 92800; train accuracy: 0.706638\n",
      "epoch 1; batch 92928; loss 0.658524\n",
      "epoch:1; batch 92928; train accuracy: 0.706698\n",
      "epoch 1; batch 93056; loss 0.581323\n",
      "epoch:1; batch 93056; train accuracy: 0.706757\n",
      "epoch 1; batch 93184; loss 0.579446\n",
      "epoch:1; batch 93184; train accuracy: 0.706827\n",
      "epoch 1; batch 93312; loss 0.634107\n",
      "epoch:1; batch 93312; train accuracy: 0.706887\n",
      "epoch 1; batch 93440; loss 0.739690\n",
      "epoch:1; batch 93440; train accuracy: 0.706924\n",
      "epoch 1; batch 93568; loss 0.593816\n",
      "epoch:1; batch 93568; train accuracy: 0.707047\n",
      "epoch 1; batch 93696; loss 0.600472\n",
      "epoch:1; batch 93696; train accuracy: 0.707181\n",
      "epoch 1; batch 93824; loss 0.588597\n",
      "epoch:1; batch 93824; train accuracy: 0.707314\n",
      "epoch 1; batch 93952; loss 0.564987\n",
      "epoch:1; batch 93952; train accuracy: 0.707436\n",
      "epoch 1; batch 94080; loss 0.541755\n",
      "epoch:1; batch 94080; train accuracy: 0.707536\n",
      "epoch 1; batch 94208; loss 0.531584\n",
      "epoch:1; batch 94208; train accuracy: 0.707668\n",
      "epoch 1; batch 94336; loss 0.607585\n",
      "epoch:1; batch 94336; train accuracy: 0.707747\n",
      "epoch 1; batch 94464; loss 0.588439\n",
      "epoch:1; batch 94464; train accuracy: 0.707825\n",
      "epoch 1; batch 94592; loss 0.721861\n",
      "epoch:1; batch 94592; train accuracy: 0.707829\n",
      "epoch 1; batch 94720; loss 0.618760\n",
      "epoch:1; batch 94720; train accuracy: 0.707855\n",
      "epoch 1; batch 94848; loss 0.724073\n",
      "epoch:1; batch 94848; train accuracy: 0.707880\n",
      "epoch 1; batch 94976; loss 0.638472\n",
      "epoch:1; batch 94976; train accuracy: 0.707916\n",
      "epoch 1; batch 95104; loss 0.538089\n",
      "epoch:1; batch 95104; train accuracy: 0.708014\n",
      "epoch 1; batch 95232; loss 0.600302\n",
      "epoch:1; batch 95232; train accuracy: 0.708155\n",
      "epoch 1; batch 95360; loss 0.628552\n",
      "epoch:1; batch 95360; train accuracy: 0.708295\n",
      "epoch 1; batch 95488; loss 0.670607\n",
      "epoch:1; batch 95488; train accuracy: 0.708340\n",
      "epoch 1; batch 95616; loss 0.650566\n",
      "epoch:1; batch 95616; train accuracy: 0.708375\n",
      "epoch 1; batch 95744; loss 0.547651\n",
      "epoch:1; batch 95744; train accuracy: 0.708514\n",
      "epoch 1; batch 95872; loss 0.585904\n",
      "epoch:1; batch 95872; train accuracy: 0.708622\n",
      "epoch 1; batch 96000; loss 0.512174\n",
      "epoch:1; batch 96000; train accuracy: 0.708760\n",
      "epoch 1; batch 96128; loss 0.695122\n",
      "epoch:1; batch 96128; train accuracy: 0.708795\n",
      "epoch 1; batch 96256; loss 0.656337\n",
      "epoch:1; batch 96256; train accuracy: 0.708829\n",
      "epoch 1; batch 96384; loss 0.666012\n",
      "epoch:1; batch 96384; train accuracy: 0.708831\n",
      "epoch 1; batch 96512; loss 0.701765\n",
      "epoch:1; batch 96512; train accuracy: 0.708855\n",
      "epoch 1; batch 96640; loss 0.638437\n",
      "epoch:1; batch 96640; train accuracy: 0.708889\n",
      "epoch 1; batch 96768; loss 0.537842\n",
      "epoch:1; batch 96768; train accuracy: 0.709005\n",
      "epoch 1; batch 96896; loss 0.642477\n",
      "epoch:1; batch 96896; train accuracy: 0.709018\n",
      "epoch 1; batch 97024; loss 0.671609\n",
      "epoch:1; batch 97024; train accuracy: 0.709072\n",
      "epoch 1; batch 97152; loss 0.682999\n",
      "epoch:1; batch 97152; train accuracy: 0.709085\n",
      "epoch 1; batch 97280; loss 0.578243\n",
      "epoch:1; batch 97280; train accuracy: 0.709180\n",
      "epoch 1; batch 97408; loss 0.564868\n",
      "epoch:1; batch 97408; train accuracy: 0.709295\n",
      "epoch 1; batch 97536; loss 0.627184\n",
      "epoch:1; batch 97536; train accuracy: 0.709348\n",
      "epoch 1; batch 97664; loss 0.599437\n",
      "epoch:1; batch 97664; train accuracy: 0.709443\n",
      "epoch 1; batch 97792; loss 0.604057\n",
      "epoch:1; batch 97792; train accuracy: 0.709537\n",
      "epoch 1; batch 97920; loss 0.663306\n",
      "epoch:1; batch 97920; train accuracy: 0.709620\n",
      "epoch 1; batch 98048; loss 0.640333\n",
      "epoch:1; batch 98048; train accuracy: 0.709703\n",
      "epoch 1; batch 98176; loss 0.654034\n",
      "epoch:1; batch 98176; train accuracy: 0.709715\n",
      "epoch 1; batch 98304; loss 0.539562\n",
      "epoch:1; batch 98304; train accuracy: 0.709839\n",
      "epoch 1; batch 98432; loss 0.633243\n",
      "epoch:1; batch 98432; train accuracy: 0.709901\n",
      "epoch 1; batch 98560; loss 0.591580\n",
      "epoch:1; batch 98560; train accuracy: 0.709974\n",
      "epoch 1; batch 98688; loss 0.652947\n",
      "epoch:1; batch 98688; train accuracy: 0.710005\n",
      "epoch 1; batch 98816; loss 0.500942\n",
      "epoch:1; batch 98816; train accuracy: 0.710118\n",
      "epoch 1; batch 98944; loss 0.549283\n",
      "epoch:1; batch 98944; train accuracy: 0.710200\n",
      "epoch 1; batch 99072; loss 0.625439\n",
      "epoch:1; batch 99072; train accuracy: 0.710241\n",
      "epoch 1; batch 99200; loss 0.617062\n",
      "epoch:1; batch 99200; train accuracy: 0.710242\n",
      "epoch 1; batch 99328; loss 0.499408\n",
      "epoch:1; batch 99328; train accuracy: 0.710414\n",
      "epoch 1; batch 99456; loss 0.638379\n",
      "epoch:1; batch 99456; train accuracy: 0.710465\n",
      "epoch 1; batch 99584; loss 0.576616\n",
      "epoch:1; batch 99584; train accuracy: 0.710516\n",
      "epoch 1; batch 99712; loss 0.925865\n",
      "epoch:1; batch 99712; train accuracy: 0.710426\n",
      "epoch 1; batch 99840; loss 0.610993\n",
      "epoch:1; batch 99840; train accuracy: 0.710497\n",
      "epoch 1; batch 99968; loss 0.654272\n",
      "epoch:1; batch 99968; train accuracy: 0.710527\n",
      "epoch 1; batch 100096; loss 0.517998\n",
      "epoch:1; batch 100096; train accuracy: 0.710648\n",
      "epoch 1; batch 100224; loss 0.652306\n",
      "epoch:1; batch 100224; train accuracy: 0.710658\n",
      "epoch 1; batch 100352; loss 0.521341\n",
      "epoch:1; batch 100352; train accuracy: 0.710788\n",
      "epoch 1; batch 100480; loss 0.720277\n",
      "epoch:1; batch 100480; train accuracy: 0.710808\n",
      "epoch 1; batch 100608; loss 0.544291\n",
      "epoch:1; batch 100608; train accuracy: 0.710967\n",
      "epoch 1; batch 100736; loss 0.599866\n",
      "epoch:1; batch 100736; train accuracy: 0.711047\n",
      "epoch 1; batch 100864; loss 0.577146\n",
      "epoch:1; batch 100864; train accuracy: 0.711106\n",
      "epoch 1; batch 100992; loss 0.589756\n",
      "epoch:1; batch 100992; train accuracy: 0.711254\n",
      "epoch 1; batch 101120; loss 0.497364\n",
      "epoch:1; batch 101120; train accuracy: 0.711363\n",
      "epoch 1; batch 101248; loss 0.560724\n",
      "epoch:1; batch 101248; train accuracy: 0.711441\n",
      "epoch 1; batch 101376; loss 0.648139\n",
      "epoch:1; batch 101376; train accuracy: 0.711529\n",
      "epoch 1; batch 101504; loss 0.688874\n",
      "epoch:1; batch 101504; train accuracy: 0.711568\n",
      "epoch 1; batch 101632; loss 0.653349\n",
      "epoch:1; batch 101632; train accuracy: 0.711607\n",
      "epoch 1; batch 101760; loss 0.662472\n",
      "epoch:1; batch 101760; train accuracy: 0.711655\n",
      "epoch 1; batch 101888; loss 0.590529\n",
      "epoch:1; batch 101888; train accuracy: 0.711703\n",
      "epoch 1; batch 102016; loss 0.571764\n",
      "epoch:1; batch 102016; train accuracy: 0.711800\n",
      "epoch 1; batch 102144; loss 0.718074\n",
      "epoch:1; batch 102144; train accuracy: 0.711789\n",
      "epoch 1; batch 102272; loss 0.577938\n",
      "epoch:1; batch 102272; train accuracy: 0.711905\n",
      "epoch 1; batch 102400; loss 0.532254\n",
      "epoch:1; batch 102400; train accuracy: 0.712051\n",
      "epoch 1; batch 102528; loss 0.651379\n",
      "epoch:1; batch 102528; train accuracy: 0.712020\n",
      "epoch 1; batch 102656; loss 0.589546\n",
      "epoch:1; batch 102656; train accuracy: 0.712067\n",
      "epoch 1; batch 102784; loss 0.558787\n",
      "epoch:1; batch 102784; train accuracy: 0.712115\n",
      "epoch 1; batch 102912; loss 0.651231\n",
      "epoch:1; batch 102912; train accuracy: 0.712152\n",
      "epoch 1; batch 103040; loss 0.517794\n",
      "epoch:1; batch 103040; train accuracy: 0.712267\n",
      "epoch 1; batch 103168; loss 0.546113\n",
      "epoch:1; batch 103168; train accuracy: 0.712372\n",
      "epoch 1; batch 103296; loss 0.622565\n",
      "epoch:1; batch 103296; train accuracy: 0.712409\n",
      "epoch 1; batch 103424; loss 0.792724\n",
      "epoch:1; batch 103424; train accuracy: 0.712427\n",
      "epoch 1; batch 103552; loss 0.610505\n",
      "epoch:1; batch 103552; train accuracy: 0.712473\n",
      "epoch 1; batch 103680; loss 0.692135\n",
      "epoch:1; batch 103680; train accuracy: 0.712481\n",
      "epoch 1; batch 103808; loss 0.682065\n",
      "epoch:1; batch 103808; train accuracy: 0.712469\n",
      "epoch 1; batch 103936; loss 0.684954\n",
      "epoch:1; batch 103936; train accuracy: 0.712515\n",
      "epoch 1; batch 104064; loss 0.650039\n",
      "epoch:1; batch 104064; train accuracy: 0.712571\n",
      "epoch 1; batch 104192; loss 0.475030\n",
      "epoch:1; batch 104192; train accuracy: 0.712713\n",
      "epoch 1; batch 104320; loss 0.697405\n",
      "epoch:1; batch 104320; train accuracy: 0.712692\n",
      "epoch 1; batch 104448; loss 0.560678\n",
      "epoch:1; batch 104448; train accuracy: 0.712747\n",
      "epoch 1; batch 104576; loss 0.685611\n",
      "epoch:1; batch 104576; train accuracy: 0.712735\n",
      "epoch 1; batch 104704; loss 0.571476\n",
      "epoch:1; batch 104704; train accuracy: 0.712838\n",
      "epoch 1; batch 104832; loss 0.629387\n",
      "epoch:1; batch 104832; train accuracy: 0.712883\n",
      "epoch 1; batch 104960; loss 0.640929\n",
      "epoch:1; batch 104960; train accuracy: 0.712891\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 31773 ; rate: 0.302715\n",
      "y_true_label_1_num: 12638 ; rate: 0.120408\n",
      "y_true_label_2_num: 14790 ; rate: 0.140911\n",
      "y_true_label_3_num: 45759 ; rate: 0.435966\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.817559\n",
      "valid avg_precision: 0.884733\n",
      "valid avg_recall: 0.756288\n",
      "valid avg_f1: 0.758207\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 4475 ; rate: 0.298811\n",
      "y_true_label_1_num: 1790 ; rate: 0.119525\n",
      "y_true_label_2_num: 2172 ; rate: 0.145032\n",
      "y_true_label_3_num: 6539 ; rate: 0.436632\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.760216\n",
      "valid avg_precision: 0.705949\n",
      "valid avg_recall: 0.698785\n",
      "valid avg_f1: 0.701999\n",
      "epoch 2\n",
      "epoch 2; batch 128; loss 0.520695\n",
      "epoch:2; batch 128; train accuracy: 0.712983\n",
      "epoch 2; batch 256; loss 0.573033\n",
      "epoch:2; batch 256; train accuracy: 0.713047\n",
      "epoch 2; batch 384; loss 0.476051\n",
      "epoch:2; batch 384; train accuracy: 0.713159\n",
      "epoch 2; batch 512; loss 0.570830\n",
      "epoch:2; batch 512; train accuracy: 0.713222\n",
      "epoch 2; batch 640; loss 0.458461\n",
      "epoch:2; batch 640; train accuracy: 0.713286\n",
      "epoch 2; batch 768; loss 0.594340\n",
      "epoch:2; batch 768; train accuracy: 0.713321\n",
      "epoch 2; batch 896; loss 0.650023\n",
      "epoch:2; batch 896; train accuracy: 0.713375\n",
      "epoch 2; batch 1024; loss 0.523966\n",
      "epoch:2; batch 1024; train accuracy: 0.713485\n",
      "epoch 2; batch 1152; loss 0.686827\n",
      "epoch:2; batch 1152; train accuracy: 0.713539\n",
      "epoch 2; batch 1280; loss 0.545169\n",
      "epoch:2; batch 1280; train accuracy: 0.713611\n",
      "epoch 2; batch 1408; loss 0.563153\n",
      "epoch:2; batch 1408; train accuracy: 0.713739\n",
      "epoch 2; batch 1536; loss 0.583525\n",
      "epoch:2; batch 1536; train accuracy: 0.713820\n",
      "epoch 2; batch 1664; loss 0.569750\n",
      "epoch:2; batch 1664; train accuracy: 0.713948\n",
      "epoch 2; batch 1792; loss 0.615408\n",
      "epoch:2; batch 1792; train accuracy: 0.714010\n",
      "epoch 2; batch 1920; loss 0.759132\n",
      "epoch:2; batch 1920; train accuracy: 0.713988\n",
      "epoch 2; batch 2048; loss 0.432472\n",
      "epoch:2; batch 2048; train accuracy: 0.714171\n",
      "epoch 2; batch 2176; loss 0.450248\n",
      "epoch:2; batch 2176; train accuracy: 0.714307\n",
      "epoch 2; batch 2304; loss 0.517340\n",
      "epoch:2; batch 2304; train accuracy: 0.714359\n",
      "epoch 2; batch 2432; loss 0.503620\n",
      "epoch:2; batch 2432; train accuracy: 0.714439\n",
      "epoch 2; batch 2560; loss 0.489120\n",
      "epoch:2; batch 2560; train accuracy: 0.714528\n",
      "epoch 2; batch 2688; loss 0.497395\n",
      "epoch:2; batch 2688; train accuracy: 0.714616\n",
      "epoch 2; batch 2816; loss 0.620024\n",
      "epoch:2; batch 2816; train accuracy: 0.714695\n",
      "epoch 2; batch 2944; loss 0.678012\n",
      "epoch:2; batch 2944; train accuracy: 0.714765\n",
      "epoch 2; batch 3072; loss 0.606081\n",
      "epoch:2; batch 3072; train accuracy: 0.714844\n",
      "epoch 2; batch 3200; loss 0.617802\n",
      "epoch:2; batch 3200; train accuracy: 0.714904\n",
      "epoch 2; batch 3328; loss 0.466321\n",
      "epoch:2; batch 3328; train accuracy: 0.715047\n",
      "epoch 2; batch 3456; loss 0.554719\n",
      "epoch:2; batch 3456; train accuracy: 0.715153\n",
      "epoch 2; batch 3584; loss 0.642258\n",
      "epoch:2; batch 3584; train accuracy: 0.715240\n",
      "epoch 2; batch 3712; loss 0.595123\n",
      "epoch:2; batch 3712; train accuracy: 0.715272\n",
      "epoch 2; batch 3840; loss 0.510232\n",
      "epoch:2; batch 3840; train accuracy: 0.715395\n",
      "epoch 2; batch 3968; loss 0.528159\n",
      "epoch:2; batch 3968; train accuracy: 0.715519\n",
      "epoch 2; batch 4096; loss 0.504549\n",
      "epoch:2; batch 4096; train accuracy: 0.715641\n",
      "epoch 2; batch 4224; loss 0.487964\n",
      "epoch:2; batch 4224; train accuracy: 0.715755\n",
      "epoch 2; batch 4352; loss 0.521458\n",
      "epoch:2; batch 4352; train accuracy: 0.715859\n",
      "epoch 2; batch 4480; loss 0.668539\n",
      "epoch:2; batch 4480; train accuracy: 0.715927\n",
      "epoch 2; batch 4608; loss 0.563993\n",
      "epoch:2; batch 4608; train accuracy: 0.716030\n",
      "epoch 2; batch 4736; loss 0.538086\n",
      "epoch:2; batch 4736; train accuracy: 0.716152\n",
      "epoch 2; batch 4864; loss 0.630316\n",
      "epoch:2; batch 4864; train accuracy: 0.716173\n",
      "epoch 2; batch 4992; loss 0.618646\n",
      "epoch:2; batch 4992; train accuracy: 0.716258\n",
      "epoch 2; batch 5120; loss 0.478797\n",
      "epoch:2; batch 5120; train accuracy: 0.716361\n",
      "epoch 2; batch 5248; loss 0.543299\n",
      "epoch:2; batch 5248; train accuracy: 0.716445\n",
      "epoch 2; batch 5376; loss 0.586067\n",
      "epoch:2; batch 5376; train accuracy: 0.716484\n",
      "epoch 2; batch 5504; loss 0.400937\n",
      "epoch:2; batch 5504; train accuracy: 0.716641\n",
      "epoch 2; batch 5632; loss 0.383270\n",
      "epoch:2; batch 5632; train accuracy: 0.716833\n",
      "epoch 2; batch 5760; loss 0.414927\n",
      "epoch:2; batch 5760; train accuracy: 0.716989\n",
      "epoch 2; batch 5888; loss 0.541855\n",
      "epoch:2; batch 5888; train accuracy: 0.717099\n",
      "epoch 2; batch 6016; loss 0.528451\n",
      "epoch:2; batch 6016; train accuracy: 0.717209\n",
      "epoch 2; batch 6144; loss 0.443484\n",
      "epoch:2; batch 6144; train accuracy: 0.717337\n",
      "epoch 2; batch 6272; loss 0.431929\n",
      "epoch:2; batch 6272; train accuracy: 0.717482\n",
      "epoch 2; batch 6400; loss 0.505221\n",
      "epoch:2; batch 6400; train accuracy: 0.717565\n",
      "epoch 2; batch 6528; loss 0.567274\n",
      "epoch:2; batch 6528; train accuracy: 0.717656\n",
      "epoch 2; batch 6656; loss 0.455021\n",
      "epoch:2; batch 6656; train accuracy: 0.717773\n",
      "epoch 2; batch 6784; loss 0.478243\n",
      "epoch:2; batch 6784; train accuracy: 0.717855\n",
      "epoch 2; batch 6912; loss 0.523734\n",
      "epoch:2; batch 6912; train accuracy: 0.717954\n",
      "epoch 2; batch 7040; loss 0.451223\n",
      "epoch:2; batch 7040; train accuracy: 0.718089\n",
      "epoch 2; batch 7168; loss 0.564621\n",
      "epoch:2; batch 7168; train accuracy: 0.718135\n",
      "epoch 2; batch 7296; loss 0.602876\n",
      "epoch:2; batch 7296; train accuracy: 0.718216\n",
      "epoch 2; batch 7424; loss 0.544110\n",
      "epoch:2; batch 7424; train accuracy: 0.718261\n",
      "epoch 2; batch 7552; loss 0.553126\n",
      "epoch:2; batch 7552; train accuracy: 0.718368\n",
      "epoch 2; batch 7680; loss 0.480086\n",
      "epoch:2; batch 7680; train accuracy: 0.718457\n",
      "epoch 2; batch 7808; loss 0.443913\n",
      "epoch:2; batch 7808; train accuracy: 0.718555\n",
      "epoch 2; batch 7936; loss 0.376671\n",
      "epoch:2; batch 7936; train accuracy: 0.718715\n",
      "epoch 2; batch 8064; loss 0.448049\n",
      "epoch:2; batch 8064; train accuracy: 0.718838\n",
      "epoch 2; batch 8192; loss 0.434709\n",
      "epoch:2; batch 8192; train accuracy: 0.718980\n",
      "epoch 2; batch 8320; loss 0.602264\n",
      "epoch:2; batch 8320; train accuracy: 0.719112\n",
      "epoch 2; batch 8448; loss 0.534312\n",
      "epoch:2; batch 8448; train accuracy: 0.719173\n",
      "epoch 2; batch 8576; loss 0.439949\n",
      "epoch:2; batch 8576; train accuracy: 0.719305\n",
      "epoch 2; batch 8704; loss 0.552506\n",
      "epoch:2; batch 8704; train accuracy: 0.719366\n",
      "epoch 2; batch 8832; loss 0.559276\n",
      "epoch:2; batch 8832; train accuracy: 0.719471\n",
      "epoch 2; batch 8960; loss 0.440372\n",
      "epoch:2; batch 8960; train accuracy: 0.719628\n",
      "epoch 2; batch 9088; loss 0.434663\n",
      "epoch:2; batch 9088; train accuracy: 0.719750\n",
      "epoch 2; batch 9216; loss 0.573145\n",
      "epoch:2; batch 9216; train accuracy: 0.719775\n",
      "epoch 2; batch 9344; loss 0.552441\n",
      "epoch:2; batch 9344; train accuracy: 0.719852\n",
      "epoch 2; batch 9472; loss 0.451111\n",
      "epoch:2; batch 9472; train accuracy: 0.719982\n",
      "epoch 2; batch 9600; loss 0.407160\n",
      "epoch:2; batch 9600; train accuracy: 0.720120\n",
      "epoch 2; batch 9728; loss 0.587542\n",
      "epoch:2; batch 9728; train accuracy: 0.720206\n",
      "epoch 2; batch 9856; loss 0.491360\n",
      "epoch:2; batch 9856; train accuracy: 0.720292\n",
      "epoch 2; batch 9984; loss 0.504872\n",
      "epoch:2; batch 9984; train accuracy: 0.720412\n",
      "epoch 2; batch 10112; loss 0.434380\n",
      "epoch:2; batch 10112; train accuracy: 0.720558\n",
      "epoch 2; batch 10240; loss 0.436784\n",
      "epoch:2; batch 10240; train accuracy: 0.720720\n",
      "epoch 2; batch 10368; loss 0.419589\n",
      "epoch:2; batch 10368; train accuracy: 0.720874\n",
      "epoch 2; batch 10496; loss 0.493343\n",
      "epoch:2; batch 10496; train accuracy: 0.720941\n",
      "epoch 2; batch 10624; loss 0.524394\n",
      "epoch:2; batch 10624; train accuracy: 0.720982\n",
      "epoch 2; batch 10752; loss 0.539044\n",
      "epoch:2; batch 10752; train accuracy: 0.721083\n",
      "epoch 2; batch 10880; loss 0.469695\n",
      "epoch:2; batch 10880; train accuracy: 0.721184\n",
      "epoch 2; batch 11008; loss 0.566541\n",
      "epoch:2; batch 11008; train accuracy: 0.721242\n",
      "epoch 2; batch 11136; loss 0.416483\n",
      "epoch:2; batch 11136; train accuracy: 0.721420\n",
      "epoch 2; batch 11264; loss 0.567259\n",
      "epoch:2; batch 11264; train accuracy: 0.721520\n",
      "epoch 2; batch 11392; loss 0.525823\n",
      "epoch:2; batch 11392; train accuracy: 0.721569\n",
      "epoch 2; batch 11520; loss 0.467172\n",
      "epoch:2; batch 11520; train accuracy: 0.721660\n",
      "epoch 2; batch 11648; loss 0.469548\n",
      "epoch:2; batch 11648; train accuracy: 0.721777\n",
      "epoch 2; batch 11776; loss 0.376138\n",
      "epoch:2; batch 11776; train accuracy: 0.721894\n",
      "epoch 2; batch 11904; loss 0.318817\n",
      "epoch:2; batch 11904; train accuracy: 0.722096\n",
      "epoch 2; batch 12032; loss 0.555803\n",
      "epoch:2; batch 12032; train accuracy: 0.722169\n",
      "epoch 2; batch 12160; loss 0.540230\n",
      "epoch:2; batch 12160; train accuracy: 0.722268\n",
      "epoch 2; batch 12288; loss 0.412706\n",
      "epoch:2; batch 12288; train accuracy: 0.722409\n",
      "epoch 2; batch 12416; loss 0.555450\n",
      "epoch:2; batch 12416; train accuracy: 0.722473\n",
      "epoch 2; batch 12544; loss 0.413994\n",
      "epoch:2; batch 12544; train accuracy: 0.722631\n",
      "epoch 2; batch 12672; loss 0.502561\n",
      "epoch:2; batch 12672; train accuracy: 0.722728\n",
      "epoch 2; batch 12800; loss 0.651802\n",
      "epoch:2; batch 12800; train accuracy: 0.722758\n",
      "epoch 2; batch 12928; loss 0.446054\n",
      "epoch:2; batch 12928; train accuracy: 0.722890\n",
      "epoch 2; batch 13056; loss 0.585745\n",
      "epoch:2; batch 13056; train accuracy: 0.722944\n",
      "epoch 2; batch 13184; loss 0.516865\n",
      "epoch:2; batch 13184; train accuracy: 0.722965\n",
      "epoch 2; batch 13312; loss 0.466105\n",
      "epoch:2; batch 13312; train accuracy: 0.723079\n",
      "epoch 2; batch 13440; loss 0.333245\n",
      "epoch:2; batch 13440; train accuracy: 0.723226\n",
      "epoch 2; batch 13568; loss 0.537745\n",
      "epoch:2; batch 13568; train accuracy: 0.723281\n",
      "epoch 2; batch 13696; loss 0.370276\n",
      "epoch:2; batch 13696; train accuracy: 0.723453\n",
      "epoch 2; batch 13824; loss 0.424165\n",
      "epoch:2; batch 13824; train accuracy: 0.723574\n",
      "epoch 2; batch 13952; loss 0.566578\n",
      "epoch:2; batch 13952; train accuracy: 0.723636\n",
      "epoch 2; batch 14080; loss 0.436511\n",
      "epoch:2; batch 14080; train accuracy: 0.723748\n",
      "epoch 2; batch 14208; loss 0.524091\n",
      "epoch:2; batch 14208; train accuracy: 0.723802\n",
      "epoch 2; batch 14336; loss 0.400778\n",
      "epoch:2; batch 14336; train accuracy: 0.723947\n",
      "epoch 2; batch 14464; loss 0.433640\n",
      "epoch:2; batch 14464; train accuracy: 0.724050\n",
      "epoch 2; batch 14592; loss 0.356745\n",
      "epoch:2; batch 14592; train accuracy: 0.724212\n",
      "epoch 2; batch 14720; loss 0.380221\n",
      "epoch:2; batch 14720; train accuracy: 0.724357\n",
      "epoch 2; batch 14848; loss 0.507755\n",
      "epoch:2; batch 14848; train accuracy: 0.724467\n",
      "epoch 2; batch 14976; loss 0.579986\n",
      "epoch:2; batch 14976; train accuracy: 0.724478\n",
      "epoch 2; batch 15104; loss 0.435811\n",
      "epoch:2; batch 15104; train accuracy: 0.724589\n",
      "epoch 2; batch 15232; loss 0.449965\n",
      "epoch:2; batch 15232; train accuracy: 0.724715\n",
      "epoch 2; batch 15360; loss 0.408648\n",
      "epoch:2; batch 15360; train accuracy: 0.724867\n",
      "epoch 2; batch 15488; loss 0.550093\n",
      "epoch:2; batch 15488; train accuracy: 0.724885\n",
      "epoch 2; batch 15616; loss 0.409757\n",
      "epoch:2; batch 15616; train accuracy: 0.725020\n",
      "epoch 2; batch 15744; loss 0.497527\n",
      "epoch:2; batch 15744; train accuracy: 0.725104\n",
      "epoch 2; batch 15872; loss 0.511660\n",
      "epoch:2; batch 15872; train accuracy: 0.725172\n",
      "epoch 2; batch 16000; loss 0.571480\n",
      "epoch:2; batch 16000; train accuracy: 0.725207\n",
      "epoch 2; batch 16128; loss 0.431723\n",
      "epoch:2; batch 16128; train accuracy: 0.725299\n",
      "epoch 2; batch 16256; loss 0.434982\n",
      "epoch:2; batch 16256; train accuracy: 0.725457\n",
      "epoch 2; batch 16384; loss 0.608776\n",
      "epoch:2; batch 16384; train accuracy: 0.725508\n",
      "epoch 2; batch 16512; loss 0.437570\n",
      "epoch:2; batch 16512; train accuracy: 0.725608\n",
      "epoch 2; batch 16640; loss 0.482450\n",
      "epoch:2; batch 16640; train accuracy: 0.725691\n",
      "epoch 2; batch 16768; loss 0.395417\n",
      "epoch:2; batch 16768; train accuracy: 0.725815\n",
      "epoch 2; batch 16896; loss 0.551125\n",
      "epoch:2; batch 16896; train accuracy: 0.725840\n",
      "epoch 2; batch 17024; loss 0.507470\n",
      "epoch:2; batch 17024; train accuracy: 0.725923\n",
      "epoch 2; batch 17152; loss 0.455300\n",
      "epoch:2; batch 17152; train accuracy: 0.726038\n",
      "epoch 2; batch 17280; loss 0.417586\n",
      "epoch:2; batch 17280; train accuracy: 0.726170\n",
      "epoch 2; batch 17408; loss 0.382314\n",
      "epoch:2; batch 17408; train accuracy: 0.726309\n",
      "epoch 2; batch 17536; loss 0.549353\n",
      "epoch:2; batch 17536; train accuracy: 0.726416\n",
      "epoch 2; batch 17664; loss 0.355390\n",
      "epoch:2; batch 17664; train accuracy: 0.726571\n",
      "epoch 2; batch 17792; loss 0.575789\n",
      "epoch:2; batch 17792; train accuracy: 0.726668\n",
      "epoch 2; batch 17920; loss 0.385552\n",
      "epoch:2; batch 17920; train accuracy: 0.726774\n",
      "epoch 2; batch 18048; loss 0.439435\n",
      "epoch:2; batch 18048; train accuracy: 0.726839\n",
      "epoch 2; batch 18176; loss 0.660663\n",
      "epoch:2; batch 18176; train accuracy: 0.726839\n",
      "epoch 2; batch 18304; loss 0.385552\n",
      "epoch:2; batch 18304; train accuracy: 0.726944\n",
      "epoch 2; batch 18432; loss 0.561847\n",
      "epoch:2; batch 18432; train accuracy: 0.727008\n",
      "epoch 2; batch 18560; loss 0.468388\n",
      "epoch:2; batch 18560; train accuracy: 0.727097\n",
      "epoch 2; batch 18688; loss 0.385473\n",
      "epoch:2; batch 18688; train accuracy: 0.727218\n",
      "epoch 2; batch 18816; loss 0.538417\n",
      "epoch:2; batch 18816; train accuracy: 0.727306\n",
      "epoch 2; batch 18944; loss 0.336739\n",
      "epoch:2; batch 18944; train accuracy: 0.727458\n",
      "epoch 2; batch 19072; loss 0.518465\n",
      "epoch:2; batch 19072; train accuracy: 0.727570\n",
      "epoch 2; batch 19200; loss 0.526663\n",
      "epoch:2; batch 19200; train accuracy: 0.727650\n",
      "epoch 2; batch 19328; loss 0.485982\n",
      "epoch:2; batch 19328; train accuracy: 0.727769\n",
      "epoch 2; batch 19456; loss 0.520163\n",
      "epoch:2; batch 19456; train accuracy: 0.727808\n",
      "epoch 2; batch 19584; loss 0.365394\n",
      "epoch:2; batch 19584; train accuracy: 0.727968\n",
      "epoch 2; batch 19712; loss 0.421870\n",
      "epoch:2; batch 19712; train accuracy: 0.728159\n",
      "epoch 2; batch 19840; loss 0.548845\n",
      "epoch:2; batch 19840; train accuracy: 0.728253\n",
      "epoch 2; batch 19968; loss 0.437867\n",
      "epoch:2; batch 19968; train accuracy: 0.728380\n",
      "epoch 2; batch 20096; loss 0.510155\n",
      "epoch:2; batch 20096; train accuracy: 0.728474\n",
      "epoch 2; batch 20224; loss 0.360122\n",
      "epoch:2; batch 20224; train accuracy: 0.728584\n",
      "epoch 2; batch 20352; loss 0.352529\n",
      "epoch:2; batch 20352; train accuracy: 0.728717\n",
      "epoch 2; batch 20480; loss 0.463202\n",
      "epoch:2; batch 20480; train accuracy: 0.728827\n",
      "epoch 2; batch 20608; loss 0.434530\n",
      "epoch:2; batch 20608; train accuracy: 0.728896\n",
      "epoch 2; batch 20736; loss 0.241524\n",
      "epoch:2; batch 20736; train accuracy: 0.729084\n",
      "epoch 2; batch 20864; loss 0.440575\n",
      "epoch:2; batch 20864; train accuracy: 0.729153\n",
      "epoch 2; batch 20992; loss 0.418947\n",
      "epoch:2; batch 20992; train accuracy: 0.729254\n",
      "epoch 2; batch 21120; loss 0.345127\n",
      "epoch:2; batch 21120; train accuracy: 0.729426\n",
      "epoch 2; batch 21248; loss 0.491906\n",
      "epoch:2; batch 21248; train accuracy: 0.729502\n",
      "epoch 2; batch 21376; loss 0.361335\n",
      "epoch:2; batch 21376; train accuracy: 0.729618\n",
      "epoch 2; batch 21504; loss 0.495238\n",
      "epoch:2; batch 21504; train accuracy: 0.729694\n",
      "epoch 2; batch 21632; loss 0.280914\n",
      "epoch:2; batch 21632; train accuracy: 0.729841\n",
      "epoch 2; batch 21760; loss 0.332455\n",
      "epoch:2; batch 21760; train accuracy: 0.729995\n",
      "epoch 2; batch 21888; loss 0.494520\n",
      "epoch:2; batch 21888; train accuracy: 0.730094\n",
      "epoch 2; batch 22016; loss 0.535394\n",
      "epoch:2; batch 22016; train accuracy: 0.730185\n",
      "epoch 2; batch 22144; loss 0.411361\n",
      "epoch:2; batch 22144; train accuracy: 0.730252\n",
      "epoch 2; batch 22272; loss 0.325931\n",
      "epoch:2; batch 22272; train accuracy: 0.730414\n",
      "epoch 2; batch 22400; loss 0.397260\n",
      "epoch:2; batch 22400; train accuracy: 0.730528\n",
      "epoch 2; batch 22528; loss 0.396723\n",
      "epoch:2; batch 22528; train accuracy: 0.730633\n",
      "epoch 2; batch 22656; loss 0.485069\n",
      "epoch:2; batch 22656; train accuracy: 0.730700\n",
      "epoch 2; batch 22784; loss 0.375799\n",
      "epoch:2; batch 22784; train accuracy: 0.730798\n",
      "epoch 2; batch 22912; loss 0.478478\n",
      "epoch:2; batch 22912; train accuracy: 0.730887\n",
      "epoch 2; batch 23040; loss 0.292944\n",
      "epoch:2; batch 23040; train accuracy: 0.731047\n",
      "epoch 2; batch 23168; loss 0.449619\n",
      "epoch:2; batch 23168; train accuracy: 0.731167\n",
      "epoch 2; batch 23296; loss 0.317986\n",
      "epoch:2; batch 23296; train accuracy: 0.731319\n",
      "epoch 2; batch 23424; loss 0.330636\n",
      "epoch:2; batch 23424; train accuracy: 0.731407\n",
      "epoch 2; batch 23552; loss 0.363684\n",
      "epoch:2; batch 23552; train accuracy: 0.731543\n",
      "epoch 2; batch 23680; loss 0.318436\n",
      "epoch:2; batch 23680; train accuracy: 0.731685\n",
      "epoch 2; batch 23808; loss 0.482153\n",
      "epoch:2; batch 23808; train accuracy: 0.731789\n",
      "epoch 2; batch 23936; loss 0.382721\n",
      "epoch:2; batch 23936; train accuracy: 0.731916\n",
      "epoch 2; batch 24064; loss 0.442538\n",
      "epoch:2; batch 24064; train accuracy: 0.732019\n",
      "epoch 2; batch 24192; loss 0.392514\n",
      "epoch:2; batch 24192; train accuracy: 0.732130\n",
      "epoch 2; batch 24320; loss 0.424072\n",
      "epoch:2; batch 24320; train accuracy: 0.732240\n",
      "epoch 2; batch 24448; loss 0.322513\n",
      "epoch:2; batch 24448; train accuracy: 0.732404\n",
      "epoch 2; batch 24576; loss 0.297561\n",
      "epoch:2; batch 24576; train accuracy: 0.732561\n",
      "epoch 2; batch 24704; loss 0.497468\n",
      "epoch:2; batch 24704; train accuracy: 0.732640\n",
      "epoch 2; batch 24832; loss 0.404649\n",
      "epoch:2; batch 24832; train accuracy: 0.732765\n",
      "epoch 2; batch 24960; loss 0.475727\n",
      "epoch:2; batch 24960; train accuracy: 0.732859\n",
      "epoch 2; batch 25088; loss 0.500597\n",
      "epoch:2; batch 25088; train accuracy: 0.732937\n",
      "epoch 2; batch 25216; loss 0.456859\n",
      "epoch:2; batch 25216; train accuracy: 0.733038\n",
      "epoch 2; batch 25344; loss 0.405523\n",
      "epoch:2; batch 25344; train accuracy: 0.733147\n",
      "epoch 2; batch 25472; loss 0.385645\n",
      "epoch:2; batch 25472; train accuracy: 0.733325\n",
      "epoch 2; batch 25600; loss 0.388366\n",
      "epoch:2; batch 25600; train accuracy: 0.733402\n",
      "epoch 2; batch 25728; loss 0.431484\n",
      "epoch:2; batch 25728; train accuracy: 0.733518\n",
      "epoch 2; batch 25856; loss 0.332278\n",
      "epoch:2; batch 25856; train accuracy: 0.733634\n",
      "epoch 2; batch 25984; loss 0.449159\n",
      "epoch:2; batch 25984; train accuracy: 0.733711\n",
      "epoch 2; batch 26112; loss 0.331510\n",
      "epoch:2; batch 26112; train accuracy: 0.733849\n",
      "epoch 2; batch 26240; loss 0.386683\n",
      "epoch:2; batch 26240; train accuracy: 0.733948\n",
      "epoch 2; batch 26368; loss 0.341425\n",
      "epoch:2; batch 26368; train accuracy: 0.734063\n",
      "epoch 2; batch 26496; loss 0.384341\n",
      "epoch:2; batch 26496; train accuracy: 0.734208\n",
      "epoch 2; batch 26624; loss 0.347528\n",
      "epoch:2; batch 26624; train accuracy: 0.734345\n",
      "epoch 2; batch 26752; loss 0.562419\n",
      "epoch:2; batch 26752; train accuracy: 0.734375\n",
      "epoch 2; batch 26880; loss 0.404923\n",
      "epoch:2; batch 26880; train accuracy: 0.734474\n",
      "epoch 2; batch 27008; loss 0.432340\n",
      "epoch:2; batch 27008; train accuracy: 0.734595\n",
      "epoch 2; batch 27136; loss 0.496444\n",
      "epoch:2; batch 27136; train accuracy: 0.734670\n",
      "epoch 2; batch 27264; loss 0.453262\n",
      "epoch:2; batch 27264; train accuracy: 0.734738\n",
      "epoch 2; batch 27392; loss 0.511591\n",
      "epoch:2; batch 27392; train accuracy: 0.734813\n",
      "epoch 2; batch 27520; loss 0.453416\n",
      "epoch:2; batch 27520; train accuracy: 0.734881\n",
      "epoch 2; batch 27648; loss 0.280618\n",
      "epoch:2; batch 27648; train accuracy: 0.735023\n",
      "epoch 2; batch 27776; loss 0.498758\n",
      "epoch:2; batch 27776; train accuracy: 0.735113\n",
      "epoch 2; batch 27904; loss 0.422757\n",
      "epoch:2; batch 27904; train accuracy: 0.735218\n",
      "epoch 2; batch 28032; loss 0.347240\n",
      "epoch:2; batch 28032; train accuracy: 0.735353\n",
      "epoch 2; batch 28160; loss 0.415529\n",
      "epoch:2; batch 28160; train accuracy: 0.735427\n",
      "epoch 2; batch 28288; loss 0.330477\n",
      "epoch:2; batch 28288; train accuracy: 0.735568\n",
      "epoch 2; batch 28416; loss 0.423522\n",
      "epoch:2; batch 28416; train accuracy: 0.735687\n",
      "epoch 2; batch 28544; loss 0.502378\n",
      "epoch:2; batch 28544; train accuracy: 0.735791\n",
      "epoch 2; batch 28672; loss 0.523989\n",
      "epoch:2; batch 28672; train accuracy: 0.735834\n",
      "epoch 2; batch 28800; loss 0.470867\n",
      "epoch:2; batch 28800; train accuracy: 0.735930\n",
      "epoch 2; batch 28928; loss 0.384795\n",
      "epoch:2; batch 28928; train accuracy: 0.736018\n",
      "epoch 2; batch 29056; loss 0.329927\n",
      "epoch:2; batch 29056; train accuracy: 0.736158\n",
      "epoch 2; batch 29184; loss 0.305318\n",
      "epoch:2; batch 29184; train accuracy: 0.736328\n",
      "epoch 2; batch 29312; loss 0.354307\n",
      "epoch:2; batch 29312; train accuracy: 0.736453\n",
      "epoch 2; batch 29440; loss 0.395677\n",
      "epoch:2; batch 29440; train accuracy: 0.736562\n",
      "epoch 2; batch 29568; loss 0.418507\n",
      "epoch:2; batch 29568; train accuracy: 0.736657\n",
      "epoch 2; batch 29696; loss 0.473121\n",
      "epoch:2; batch 29696; train accuracy: 0.736744\n",
      "epoch 2; batch 29824; loss 0.410176\n",
      "epoch:2; batch 29824; train accuracy: 0.736823\n",
      "epoch 2; batch 29952; loss 0.323181\n",
      "epoch:2; batch 29952; train accuracy: 0.736954\n",
      "epoch 2; batch 30080; loss 0.516606\n",
      "epoch:2; batch 30080; train accuracy: 0.737026\n",
      "epoch 2; batch 30208; loss 0.445471\n",
      "epoch:2; batch 30208; train accuracy: 0.737135\n",
      "epoch 2; batch 30336; loss 0.461211\n",
      "epoch:2; batch 30336; train accuracy: 0.737243\n",
      "epoch 2; batch 30464; loss 0.443548\n",
      "epoch:2; batch 30464; train accuracy: 0.737329\n",
      "epoch 2; batch 30592; loss 0.311734\n",
      "epoch:2; batch 30592; train accuracy: 0.737488\n",
      "epoch 2; batch 30720; loss 0.384854\n",
      "epoch:2; batch 30720; train accuracy: 0.737596\n",
      "epoch 2; batch 30848; loss 0.300533\n",
      "epoch:2; batch 30848; train accuracy: 0.737733\n",
      "epoch 2; batch 30976; loss 0.457439\n",
      "epoch:2; batch 30976; train accuracy: 0.737796\n",
      "epoch 2; batch 31104; loss 0.486968\n",
      "epoch:2; batch 31104; train accuracy: 0.737873\n",
      "epoch 2; batch 31232; loss 0.520043\n",
      "epoch:2; batch 31232; train accuracy: 0.737921\n",
      "epoch 2; batch 31360; loss 0.464521\n",
      "epoch:2; batch 31360; train accuracy: 0.738021\n",
      "epoch 2; batch 31488; loss 0.428838\n",
      "epoch:2; batch 31488; train accuracy: 0.738142\n",
      "epoch 2; batch 31616; loss 0.391156\n",
      "epoch:2; batch 31616; train accuracy: 0.738248\n",
      "epoch 2; batch 31744; loss 0.320709\n",
      "epoch:2; batch 31744; train accuracy: 0.738376\n",
      "epoch 2; batch 31872; loss 0.430823\n",
      "epoch:2; batch 31872; train accuracy: 0.738446\n",
      "epoch 2; batch 32000; loss 0.437417\n",
      "epoch:2; batch 32000; train accuracy: 0.738573\n",
      "epoch 2; batch 32128; loss 0.422848\n",
      "epoch:2; batch 32128; train accuracy: 0.738657\n",
      "epoch 2; batch 32256; loss 0.385728\n",
      "epoch:2; batch 32256; train accuracy: 0.738777\n",
      "epoch 2; batch 32384; loss 0.430979\n",
      "epoch:2; batch 32384; train accuracy: 0.738860\n",
      "epoch 2; batch 32512; loss 0.298360\n",
      "epoch:2; batch 32512; train accuracy: 0.739030\n",
      "epoch 2; batch 32640; loss 0.403696\n",
      "epoch:2; batch 32640; train accuracy: 0.739113\n",
      "epoch 2; batch 32768; loss 0.377081\n",
      "epoch:2; batch 32768; train accuracy: 0.739196\n",
      "epoch 2; batch 32896; loss 0.321020\n",
      "epoch:2; batch 32896; train accuracy: 0.739351\n",
      "epoch 2; batch 33024; loss 0.400501\n",
      "epoch:2; batch 33024; train accuracy: 0.739477\n",
      "epoch 2; batch 33152; loss 0.357513\n",
      "epoch:2; batch 33152; train accuracy: 0.739595\n",
      "epoch 2; batch 33280; loss 0.287395\n",
      "epoch:2; batch 33280; train accuracy: 0.739728\n",
      "epoch 2; batch 33408; loss 0.498075\n",
      "epoch:2; batch 33408; train accuracy: 0.739810\n",
      "epoch 2; batch 33536; loss 0.503993\n",
      "epoch:2; batch 33536; train accuracy: 0.739841\n",
      "epoch 2; batch 33664; loss 0.411490\n",
      "epoch:2; batch 33664; train accuracy: 0.739951\n",
      "epoch 2; batch 33792; loss 0.388505\n",
      "epoch:2; batch 33792; train accuracy: 0.740061\n",
      "epoch 2; batch 33920; loss 0.535171\n",
      "epoch:2; batch 33920; train accuracy: 0.740121\n",
      "epoch 2; batch 34048; loss 0.486828\n",
      "epoch:2; batch 34048; train accuracy: 0.740180\n",
      "epoch 2; batch 34176; loss 0.371038\n",
      "epoch:2; batch 34176; train accuracy: 0.740304\n",
      "epoch 2; batch 34304; loss 0.518545\n",
      "epoch:2; batch 34304; train accuracy: 0.740400\n",
      "epoch 2; batch 34432; loss 0.357176\n",
      "epoch:2; batch 34432; train accuracy: 0.740494\n",
      "epoch 2; batch 34560; loss 0.443342\n",
      "epoch:2; batch 34560; train accuracy: 0.740582\n",
      "epoch 2; batch 34688; loss 0.460475\n",
      "epoch:2; batch 34688; train accuracy: 0.740648\n",
      "epoch 2; batch 34816; loss 0.459982\n",
      "epoch:2; batch 34816; train accuracy: 0.740721\n",
      "epoch 2; batch 34944; loss 0.451659\n",
      "epoch:2; batch 34944; train accuracy: 0.740801\n",
      "epoch 2; batch 35072; loss 0.454225\n",
      "epoch:2; batch 35072; train accuracy: 0.740874\n",
      "epoch 2; batch 35200; loss 0.482318\n",
      "epoch:2; batch 35200; train accuracy: 0.740939\n",
      "epoch 2; batch 35328; loss 0.334674\n",
      "epoch:2; batch 35328; train accuracy: 0.741076\n",
      "epoch 2; batch 35456; loss 0.411310\n",
      "epoch:2; batch 35456; train accuracy: 0.741155\n",
      "epoch 2; batch 35584; loss 0.424864\n",
      "epoch:2; batch 35584; train accuracy: 0.741248\n",
      "epoch 2; batch 35712; loss 0.614240\n",
      "epoch:2; batch 35712; train accuracy: 0.741278\n",
      "epoch 2; batch 35840; loss 0.353557\n",
      "epoch:2; batch 35840; train accuracy: 0.741392\n",
      "epoch 2; batch 35968; loss 0.347716\n",
      "epoch:2; batch 35968; train accuracy: 0.741492\n",
      "epoch 2; batch 36096; loss 0.394691\n",
      "epoch:2; batch 36096; train accuracy: 0.741606\n",
      "epoch 2; batch 36224; loss 0.413168\n",
      "epoch:2; batch 36224; train accuracy: 0.741699\n",
      "epoch 2; batch 36352; loss 0.319771\n",
      "epoch:2; batch 36352; train accuracy: 0.741841\n",
      "epoch 2; batch 36480; loss 0.608555\n",
      "epoch:2; batch 36480; train accuracy: 0.741876\n",
      "epoch 2; batch 36608; loss 0.381650\n",
      "epoch:2; batch 36608; train accuracy: 0.741976\n",
      "epoch 2; batch 36736; loss 0.384521\n",
      "epoch:2; batch 36736; train accuracy: 0.742060\n",
      "epoch 2; batch 36864; loss 0.442362\n",
      "epoch:2; batch 36864; train accuracy: 0.742166\n",
      "epoch 2; batch 36992; loss 0.384093\n",
      "epoch:2; batch 36992; train accuracy: 0.742293\n",
      "epoch 2; batch 37120; loss 0.398317\n",
      "epoch:2; batch 37120; train accuracy: 0.742370\n",
      "epoch 2; batch 37248; loss 0.338747\n",
      "epoch:2; batch 37248; train accuracy: 0.742448\n",
      "epoch 2; batch 37376; loss 0.455946\n",
      "epoch:2; batch 37376; train accuracy: 0.742497\n",
      "epoch 2; batch 37504; loss 0.549435\n",
      "epoch:2; batch 37504; train accuracy: 0.742538\n",
      "epoch 2; batch 37632; loss 0.572728\n",
      "epoch:2; batch 37632; train accuracy: 0.742566\n",
      "epoch 2; batch 37760; loss 0.474837\n",
      "epoch:2; batch 37760; train accuracy: 0.742650\n",
      "epoch 2; batch 37888; loss 0.459401\n",
      "epoch:2; batch 37888; train accuracy: 0.742720\n",
      "epoch 2; batch 38016; loss 0.449635\n",
      "epoch:2; batch 38016; train accuracy: 0.742768\n",
      "epoch 2; batch 38144; loss 0.443727\n",
      "epoch:2; batch 38144; train accuracy: 0.742851\n",
      "epoch 2; batch 38272; loss 0.432772\n",
      "epoch:2; batch 38272; train accuracy: 0.742941\n",
      "epoch 2; batch 38400; loss 0.436959\n",
      "epoch:2; batch 38400; train accuracy: 0.742997\n",
      "epoch 2; batch 38528; loss 0.411948\n",
      "epoch:2; batch 38528; train accuracy: 0.743114\n",
      "epoch 2; batch 38656; loss 0.474004\n",
      "epoch:2; batch 38656; train accuracy: 0.743183\n",
      "epoch 2; batch 38784; loss 0.432927\n",
      "epoch:2; batch 38784; train accuracy: 0.743238\n",
      "epoch 2; batch 38912; loss 0.541012\n",
      "epoch:2; batch 38912; train accuracy: 0.743320\n",
      "epoch 2; batch 39040; loss 0.386349\n",
      "epoch:2; batch 39040; train accuracy: 0.743417\n",
      "epoch 2; batch 39168; loss 0.425856\n",
      "epoch:2; batch 39168; train accuracy: 0.743499\n",
      "epoch 2; batch 39296; loss 0.302751\n",
      "epoch:2; batch 39296; train accuracy: 0.743629\n",
      "epoch 2; batch 39424; loss 0.359253\n",
      "epoch:2; batch 39424; train accuracy: 0.743746\n",
      "epoch 2; batch 39552; loss 0.444196\n",
      "epoch:2; batch 39552; train accuracy: 0.743821\n",
      "epoch 2; batch 39680; loss 0.459770\n",
      "epoch:2; batch 39680; train accuracy: 0.743881\n",
      "epoch 2; batch 39808; loss 0.342754\n",
      "epoch:2; batch 39808; train accuracy: 0.743983\n",
      "epoch 2; batch 39936; loss 0.477035\n",
      "epoch:2; batch 39936; train accuracy: 0.744037\n",
      "epoch 2; batch 40064; loss 0.360293\n",
      "epoch:2; batch 40064; train accuracy: 0.744153\n",
      "epoch 2; batch 40192; loss 0.366635\n",
      "epoch:2; batch 40192; train accuracy: 0.744261\n",
      "epoch 2; batch 40320; loss 0.397405\n",
      "epoch:2; batch 40320; train accuracy: 0.744328\n",
      "epoch 2; batch 40448; loss 0.303649\n",
      "epoch:2; batch 40448; train accuracy: 0.744436\n",
      "epoch 2; batch 40576; loss 0.622711\n",
      "epoch:2; batch 40576; train accuracy: 0.744496\n",
      "epoch 2; batch 40704; loss 0.418269\n",
      "epoch:2; batch 40704; train accuracy: 0.744570\n",
      "epoch 2; batch 40832; loss 0.335371\n",
      "epoch:2; batch 40832; train accuracy: 0.744684\n",
      "epoch 2; batch 40960; loss 0.440152\n",
      "epoch:2; batch 40960; train accuracy: 0.744757\n",
      "epoch 2; batch 41088; loss 0.405686\n",
      "epoch:2; batch 41088; train accuracy: 0.744851\n",
      "epoch 2; batch 41216; loss 0.397347\n",
      "epoch:2; batch 41216; train accuracy: 0.744951\n",
      "epoch 2; batch 41344; loss 0.391404\n",
      "epoch:2; batch 41344; train accuracy: 0.745024\n",
      "epoch 2; batch 41472; loss 0.474184\n",
      "epoch:2; batch 41472; train accuracy: 0.745110\n",
      "epoch 2; batch 41600; loss 0.368973\n",
      "epoch:2; batch 41600; train accuracy: 0.745190\n",
      "epoch 2; batch 41728; loss 0.355803\n",
      "epoch:2; batch 41728; train accuracy: 0.745276\n",
      "epoch 2; batch 41856; loss 0.386287\n",
      "epoch:2; batch 41856; train accuracy: 0.745389\n",
      "epoch 2; batch 41984; loss 0.388113\n",
      "epoch:2; batch 41984; train accuracy: 0.745502\n",
      "epoch 2; batch 42112; loss 0.554982\n",
      "epoch:2; batch 42112; train accuracy: 0.745506\n",
      "epoch 2; batch 42240; loss 0.524476\n",
      "epoch:2; batch 42240; train accuracy: 0.745564\n",
      "epoch 2; batch 42368; loss 0.386581\n",
      "epoch:2; batch 42368; train accuracy: 0.745649\n",
      "epoch 2; batch 42496; loss 0.434118\n",
      "epoch:2; batch 42496; train accuracy: 0.745755\n",
      "epoch 2; batch 42624; loss 0.431991\n",
      "epoch:2; batch 42624; train accuracy: 0.745860\n",
      "epoch 2; batch 42752; loss 0.332520\n",
      "epoch:2; batch 42752; train accuracy: 0.745965\n",
      "epoch 2; batch 42880; loss 0.378968\n",
      "epoch:2; batch 42880; train accuracy: 0.746070\n",
      "epoch 2; batch 43008; loss 0.400087\n",
      "epoch:2; batch 43008; train accuracy: 0.746148\n",
      "epoch 2; batch 43136; loss 0.298346\n",
      "epoch:2; batch 43136; train accuracy: 0.746293\n",
      "epoch 2; batch 43264; loss 0.436691\n",
      "epoch:2; batch 43264; train accuracy: 0.746370\n",
      "epoch 2; batch 43392; loss 0.287424\n",
      "epoch:2; batch 43392; train accuracy: 0.746515\n",
      "epoch 2; batch 43520; loss 0.471381\n",
      "epoch:2; batch 43520; train accuracy: 0.746579\n",
      "epoch 2; batch 43648; loss 0.443730\n",
      "epoch:2; batch 43648; train accuracy: 0.746676\n",
      "epoch 2; batch 43776; loss 0.327017\n",
      "epoch:2; batch 43776; train accuracy: 0.746813\n",
      "epoch 2; batch 43904; loss 0.367120\n",
      "epoch:2; batch 43904; train accuracy: 0.746917\n",
      "epoch 2; batch 44032; loss 0.314482\n",
      "epoch:2; batch 44032; train accuracy: 0.747027\n",
      "epoch 2; batch 44160; loss 0.387849\n",
      "epoch:2; batch 44160; train accuracy: 0.747110\n",
      "epoch 2; batch 44288; loss 0.437879\n",
      "epoch:2; batch 44288; train accuracy: 0.747186\n",
      "epoch 2; batch 44416; loss 0.358893\n",
      "epoch:2; batch 44416; train accuracy: 0.747289\n",
      "epoch 2; batch 44544; loss 0.403308\n",
      "epoch:2; batch 44544; train accuracy: 0.747398\n",
      "epoch 2; batch 44672; loss 0.426949\n",
      "epoch:2; batch 44672; train accuracy: 0.747460\n",
      "epoch 2; batch 44800; loss 0.562550\n",
      "epoch:2; batch 44800; train accuracy: 0.747496\n",
      "epoch 2; batch 44928; loss 0.486384\n",
      "epoch:2; batch 44928; train accuracy: 0.747578\n",
      "epoch 2; batch 45056; loss 0.528651\n",
      "epoch:2; batch 45056; train accuracy: 0.747627\n",
      "epoch 2; batch 45184; loss 0.398130\n",
      "epoch:2; batch 45184; train accuracy: 0.747696\n",
      "epoch 2; batch 45312; loss 0.376944\n",
      "epoch:2; batch 45312; train accuracy: 0.747764\n",
      "epoch 2; batch 45440; loss 0.570499\n",
      "epoch:2; batch 45440; train accuracy: 0.747786\n",
      "epoch 2; batch 45568; loss 0.342484\n",
      "epoch:2; batch 45568; train accuracy: 0.747868\n",
      "epoch 2; batch 45696; loss 0.401978\n",
      "epoch:2; batch 45696; train accuracy: 0.747949\n",
      "epoch 2; batch 45824; loss 0.381365\n",
      "epoch:2; batch 45824; train accuracy: 0.748010\n",
      "epoch 2; batch 45952; loss 0.365812\n",
      "epoch:2; batch 45952; train accuracy: 0.748111\n",
      "epoch 2; batch 46080; loss 0.436085\n",
      "epoch:2; batch 46080; train accuracy: 0.748212\n",
      "epoch 2; batch 46208; loss 0.426582\n",
      "epoch:2; batch 46208; train accuracy: 0.748273\n",
      "epoch 2; batch 46336; loss 0.350232\n",
      "epoch:2; batch 46336; train accuracy: 0.748367\n",
      "epoch 2; batch 46464; loss 0.406491\n",
      "epoch:2; batch 46464; train accuracy: 0.748441\n",
      "epoch 2; batch 46592; loss 0.370495\n",
      "epoch:2; batch 46592; train accuracy: 0.748529\n",
      "epoch 2; batch 46720; loss 0.386668\n",
      "epoch:2; batch 46720; train accuracy: 0.748629\n",
      "epoch 2; batch 46848; loss 0.296800\n",
      "epoch:2; batch 46848; train accuracy: 0.748762\n",
      "epoch 2; batch 46976; loss 0.449276\n",
      "epoch:2; batch 46976; train accuracy: 0.748861\n",
      "epoch 2; batch 47104; loss 0.362455\n",
      "epoch:2; batch 47104; train accuracy: 0.748948\n",
      "epoch 2; batch 47232; loss 0.447682\n",
      "epoch:2; batch 47232; train accuracy: 0.749008\n",
      "epoch 2; batch 47360; loss 0.377941\n",
      "epoch:2; batch 47360; train accuracy: 0.749133\n",
      "epoch 2; batch 47488; loss 0.432904\n",
      "epoch:2; batch 47488; train accuracy: 0.749226\n",
      "epoch 2; batch 47616; loss 0.362974\n",
      "epoch:2; batch 47616; train accuracy: 0.749331\n",
      "epoch 2; batch 47744; loss 0.313709\n",
      "epoch:2; batch 47744; train accuracy: 0.749430\n",
      "epoch 2; batch 47872; loss 0.390426\n",
      "epoch:2; batch 47872; train accuracy: 0.749529\n",
      "epoch 2; batch 48000; loss 0.410890\n",
      "epoch:2; batch 48000; train accuracy: 0.749595\n",
      "epoch 2; batch 48128; loss 0.363829\n",
      "epoch:2; batch 48128; train accuracy: 0.749693\n",
      "epoch 2; batch 48256; loss 0.430675\n",
      "epoch:2; batch 48256; train accuracy: 0.749765\n",
      "epoch 2; batch 48384; loss 0.292991\n",
      "epoch:2; batch 48384; train accuracy: 0.749876\n",
      "epoch 2; batch 48512; loss 0.442982\n",
      "epoch:2; batch 48512; train accuracy: 0.749935\n",
      "epoch 2; batch 48640; loss 0.403981\n",
      "epoch:2; batch 48640; train accuracy: 0.750033\n",
      "epoch 2; batch 48768; loss 0.462654\n",
      "epoch:2; batch 48768; train accuracy: 0.750091\n",
      "epoch 2; batch 48896; loss 0.319335\n",
      "epoch:2; batch 48896; train accuracy: 0.750201\n",
      "epoch 2; batch 49024; loss 0.411747\n",
      "epoch:2; batch 49024; train accuracy: 0.750279\n",
      "epoch 2; batch 49152; loss 0.381366\n",
      "epoch:2; batch 49152; train accuracy: 0.750344\n",
      "epoch 2; batch 49280; loss 0.524509\n",
      "epoch:2; batch 49280; train accuracy: 0.750408\n",
      "epoch 2; batch 49408; loss 0.311766\n",
      "epoch:2; batch 49408; train accuracy: 0.750505\n",
      "epoch 2; batch 49536; loss 0.452137\n",
      "epoch:2; batch 49536; train accuracy: 0.750576\n",
      "epoch 2; batch 49664; loss 0.471411\n",
      "epoch:2; batch 49664; train accuracy: 0.750627\n",
      "epoch 2; batch 49792; loss 0.358537\n",
      "epoch:2; batch 49792; train accuracy: 0.750743\n",
      "epoch 2; batch 49920; loss 0.399943\n",
      "epoch:2; batch 49920; train accuracy: 0.750814\n",
      "epoch 2; batch 50048; loss 0.446771\n",
      "epoch:2; batch 50048; train accuracy: 0.750852\n",
      "epoch 2; batch 50176; loss 0.366644\n",
      "epoch:2; batch 50176; train accuracy: 0.750928\n",
      "epoch 2; batch 50304; loss 0.455414\n",
      "epoch:2; batch 50304; train accuracy: 0.751024\n",
      "epoch 2; batch 50432; loss 0.302869\n",
      "epoch:2; batch 50432; train accuracy: 0.751120\n",
      "epoch 2; batch 50560; loss 0.386050\n",
      "epoch:2; batch 50560; train accuracy: 0.751183\n",
      "epoch 2; batch 50688; loss 0.370153\n",
      "epoch:2; batch 50688; train accuracy: 0.751291\n",
      "epoch 2; batch 50816; loss 0.469342\n",
      "epoch:2; batch 50816; train accuracy: 0.751342\n",
      "epoch 2; batch 50944; loss 0.384319\n",
      "epoch:2; batch 50944; train accuracy: 0.751443\n",
      "epoch 2; batch 51072; loss 0.482146\n",
      "epoch:2; batch 51072; train accuracy: 0.751493\n",
      "epoch 2; batch 51200; loss 0.389681\n",
      "epoch:2; batch 51200; train accuracy: 0.751524\n",
      "epoch 2; batch 51328; loss 0.350542\n",
      "epoch:2; batch 51328; train accuracy: 0.751632\n",
      "epoch 2; batch 51456; loss 0.482334\n",
      "epoch:2; batch 51456; train accuracy: 0.751669\n",
      "epoch 2; batch 51584; loss 0.406200\n",
      "epoch:2; batch 51584; train accuracy: 0.751757\n",
      "epoch 2; batch 51712; loss 0.378830\n",
      "epoch:2; batch 51712; train accuracy: 0.751825\n",
      "epoch 2; batch 51840; loss 0.408062\n",
      "epoch:2; batch 51840; train accuracy: 0.751907\n",
      "epoch 2; batch 51968; loss 0.465992\n",
      "epoch:2; batch 51968; train accuracy: 0.751956\n",
      "epoch 2; batch 52096; loss 0.399985\n",
      "epoch:2; batch 52096; train accuracy: 0.752044\n",
      "epoch 2; batch 52224; loss 0.429547\n",
      "epoch:2; batch 52224; train accuracy: 0.752106\n",
      "epoch 2; batch 52352; loss 0.460343\n",
      "epoch:2; batch 52352; train accuracy: 0.752187\n",
      "epoch 2; batch 52480; loss 0.477609\n",
      "epoch:2; batch 52480; train accuracy: 0.752229\n",
      "epoch 2; batch 52608; loss 0.326654\n",
      "epoch:2; batch 52608; train accuracy: 0.752335\n",
      "epoch 2; batch 52736; loss 0.403588\n",
      "epoch:2; batch 52736; train accuracy: 0.752422\n",
      "epoch 2; batch 52864; loss 0.360311\n",
      "epoch:2; batch 52864; train accuracy: 0.752509\n",
      "epoch 2; batch 52992; loss 0.272253\n",
      "epoch:2; batch 52992; train accuracy: 0.752621\n",
      "epoch 2; batch 53120; loss 0.481938\n",
      "epoch:2; batch 53120; train accuracy: 0.752689\n",
      "epoch 2; batch 53248; loss 0.333658\n",
      "epoch:2; batch 53248; train accuracy: 0.752787\n",
      "epoch 2; batch 53376; loss 0.317260\n",
      "epoch:2; batch 53376; train accuracy: 0.752874\n",
      "epoch 2; batch 53504; loss 0.395792\n",
      "epoch:2; batch 53504; train accuracy: 0.752972\n",
      "epoch 2; batch 53632; loss 0.388474\n",
      "epoch:2; batch 53632; train accuracy: 0.753058\n",
      "epoch 2; batch 53760; loss 0.451810\n",
      "epoch:2; batch 53760; train accuracy: 0.753150\n",
      "epoch 2; batch 53888; loss 0.369435\n",
      "epoch:2; batch 53888; train accuracy: 0.753229\n",
      "epoch 2; batch 54016; loss 0.321400\n",
      "epoch:2; batch 54016; train accuracy: 0.753340\n",
      "epoch 2; batch 54144; loss 0.388121\n",
      "epoch:2; batch 54144; train accuracy: 0.753394\n",
      "epoch 2; batch 54272; loss 0.339635\n",
      "epoch:2; batch 54272; train accuracy: 0.753479\n",
      "epoch 2; batch 54400; loss 0.418055\n",
      "epoch:2; batch 54400; train accuracy: 0.753520\n",
      "epoch 2; batch 54528; loss 0.466886\n",
      "epoch:2; batch 54528; train accuracy: 0.753568\n",
      "epoch 2; batch 54656; loss 0.384113\n",
      "epoch:2; batch 54656; train accuracy: 0.753634\n",
      "epoch 2; batch 54784; loss 0.484479\n",
      "epoch:2; batch 54784; train accuracy: 0.753693\n",
      "epoch 2; batch 54912; loss 0.397856\n",
      "epoch:2; batch 54912; train accuracy: 0.753772\n",
      "epoch 2; batch 55040; loss 0.423616\n",
      "epoch:2; batch 55040; train accuracy: 0.753856\n",
      "epoch 2; batch 55168; loss 0.351291\n",
      "epoch:2; batch 55168; train accuracy: 0.753941\n",
      "epoch 2; batch 55296; loss 0.494646\n",
      "epoch:2; batch 55296; train accuracy: 0.753994\n",
      "epoch 2; batch 55424; loss 0.518561\n",
      "epoch:2; batch 55424; train accuracy: 0.753997\n",
      "epoch 2; batch 55552; loss 0.380542\n",
      "epoch:2; batch 55552; train accuracy: 0.754087\n",
      "epoch 2; batch 55680; loss 0.349336\n",
      "epoch:2; batch 55680; train accuracy: 0.754189\n",
      "epoch 2; batch 55808; loss 0.362065\n",
      "epoch:2; batch 55808; train accuracy: 0.754267\n",
      "epoch 2; batch 55936; loss 0.335497\n",
      "epoch:2; batch 55936; train accuracy: 0.754344\n",
      "epoch 2; batch 56064; loss 0.361493\n",
      "epoch:2; batch 56064; train accuracy: 0.754422\n",
      "epoch 2; batch 56192; loss 0.381688\n",
      "epoch:2; batch 56192; train accuracy: 0.754493\n",
      "epoch 2; batch 56320; loss 0.426296\n",
      "epoch:2; batch 56320; train accuracy: 0.754564\n",
      "epoch 2; batch 56448; loss 0.542162\n",
      "epoch:2; batch 56448; train accuracy: 0.754585\n",
      "epoch 2; batch 56576; loss 0.403397\n",
      "epoch:2; batch 56576; train accuracy: 0.754643\n",
      "epoch 2; batch 56704; loss 0.460648\n",
      "epoch:2; batch 56704; train accuracy: 0.754701\n",
      "epoch 2; batch 56832; loss 0.337484\n",
      "epoch:2; batch 56832; train accuracy: 0.754802\n",
      "epoch 2; batch 56960; loss 0.542458\n",
      "epoch:2; batch 56960; train accuracy: 0.754842\n",
      "epoch 2; batch 57088; loss 0.552941\n",
      "epoch:2; batch 57088; train accuracy: 0.754875\n",
      "epoch 2; batch 57216; loss 0.422655\n",
      "epoch:2; batch 57216; train accuracy: 0.754933\n",
      "epoch 2; batch 57344; loss 0.422415\n",
      "epoch:2; batch 57344; train accuracy: 0.754997\n",
      "epoch 2; batch 57472; loss 0.532594\n",
      "epoch:2; batch 57472; train accuracy: 0.755036\n",
      "epoch 2; batch 57600; loss 0.338612\n",
      "epoch:2; batch 57600; train accuracy: 0.755124\n",
      "epoch 2; batch 57728; loss 0.398025\n",
      "epoch:2; batch 57728; train accuracy: 0.755206\n",
      "epoch 2; batch 57856; loss 0.417911\n",
      "epoch:2; batch 57856; train accuracy: 0.755300\n",
      "epoch 2; batch 57984; loss 0.372718\n",
      "epoch:2; batch 57984; train accuracy: 0.755382\n",
      "epoch 2; batch 58112; loss 0.362457\n",
      "epoch:2; batch 58112; train accuracy: 0.755458\n",
      "epoch 2; batch 58240; loss 0.304911\n",
      "epoch:2; batch 58240; train accuracy: 0.755570\n",
      "epoch 2; batch 58368; loss 0.355834\n",
      "epoch:2; batch 58368; train accuracy: 0.755670\n",
      "epoch 2; batch 58496; loss 0.448685\n",
      "epoch:2; batch 58496; train accuracy: 0.755726\n",
      "epoch 2; batch 58624; loss 0.366547\n",
      "epoch:2; batch 58624; train accuracy: 0.755801\n",
      "epoch 2; batch 58752; loss 0.468271\n",
      "epoch:2; batch 58752; train accuracy: 0.755864\n",
      "epoch 2; batch 58880; loss 0.360625\n",
      "epoch:2; batch 58880; train accuracy: 0.755963\n",
      "epoch 2; batch 59008; loss 0.357196\n",
      "epoch:2; batch 59008; train accuracy: 0.756056\n",
      "epoch 2; batch 59136; loss 0.253404\n",
      "epoch:2; batch 59136; train accuracy: 0.756173\n",
      "epoch 2; batch 59264; loss 0.355285\n",
      "epoch:2; batch 59264; train accuracy: 0.756254\n",
      "epoch 2; batch 59392; loss 0.370717\n",
      "epoch:2; batch 59392; train accuracy: 0.756322\n",
      "epoch 2; batch 59520; loss 0.367483\n",
      "epoch:2; batch 59520; train accuracy: 0.756396\n",
      "epoch 2; batch 59648; loss 0.332454\n",
      "epoch:2; batch 59648; train accuracy: 0.756482\n",
      "epoch 2; batch 59776; loss 0.390254\n",
      "epoch:2; batch 59776; train accuracy: 0.756544\n",
      "epoch 2; batch 59904; loss 0.439384\n",
      "epoch:2; batch 59904; train accuracy: 0.756581\n",
      "epoch 2; batch 60032; loss 0.571897\n",
      "epoch:2; batch 60032; train accuracy: 0.756618\n",
      "epoch 2; batch 60160; loss 0.363661\n",
      "epoch:2; batch 60160; train accuracy: 0.756710\n",
      "epoch 2; batch 60288; loss 0.467378\n",
      "epoch:2; batch 60288; train accuracy: 0.756796\n",
      "epoch 2; batch 60416; loss 0.378801\n",
      "epoch:2; batch 60416; train accuracy: 0.756851\n",
      "epoch 2; batch 60544; loss 0.365056\n",
      "epoch:2; batch 60544; train accuracy: 0.756912\n",
      "epoch 2; batch 60672; loss 0.438323\n",
      "epoch:2; batch 60672; train accuracy: 0.756979\n",
      "epoch 2; batch 60800; loss 0.403438\n",
      "epoch:2; batch 60800; train accuracy: 0.757052\n",
      "epoch 2; batch 60928; loss 0.396628\n",
      "epoch:2; batch 60928; train accuracy: 0.757137\n",
      "epoch 2; batch 61056; loss 0.484073\n",
      "epoch:2; batch 61056; train accuracy: 0.757174\n",
      "epoch 2; batch 61184; loss 0.335544\n",
      "epoch:2; batch 61184; train accuracy: 0.757271\n",
      "epoch 2; batch 61312; loss 0.471103\n",
      "epoch:2; batch 61312; train accuracy: 0.757349\n",
      "epoch 2; batch 61440; loss 0.527237\n",
      "epoch:2; batch 61440; train accuracy: 0.757344\n",
      "epoch 2; batch 61568; loss 0.364478\n",
      "epoch:2; batch 61568; train accuracy: 0.757452\n",
      "epoch 2; batch 61696; loss 0.420675\n",
      "epoch:2; batch 61696; train accuracy: 0.757518\n",
      "epoch 2; batch 61824; loss 0.357001\n",
      "epoch:2; batch 61824; train accuracy: 0.757591\n",
      "epoch 2; batch 61952; loss 0.367182\n",
      "epoch:2; batch 61952; train accuracy: 0.757675\n",
      "epoch 2; batch 62080; loss 0.249513\n",
      "epoch:2; batch 62080; train accuracy: 0.757807\n",
      "epoch 2; batch 62208; loss 0.507503\n",
      "epoch:2; batch 62208; train accuracy: 0.757807\n",
      "epoch 2; batch 62336; loss 0.413345\n",
      "epoch:2; batch 62336; train accuracy: 0.757902\n",
      "epoch 2; batch 62464; loss 0.357842\n",
      "epoch:2; batch 62464; train accuracy: 0.757992\n",
      "epoch 2; batch 62592; loss 0.377662\n",
      "epoch:2; batch 62592; train accuracy: 0.758063\n",
      "epoch 2; batch 62720; loss 0.549996\n",
      "epoch:2; batch 62720; train accuracy: 0.758093\n",
      "epoch 2; batch 62848; loss 0.376286\n",
      "epoch:2; batch 62848; train accuracy: 0.758170\n",
      "epoch 2; batch 62976; loss 0.514089\n",
      "epoch:2; batch 62976; train accuracy: 0.758211\n",
      "epoch 2; batch 63104; loss 0.421353\n",
      "epoch:2; batch 63104; train accuracy: 0.758294\n",
      "epoch 2; batch 63232; loss 0.468130\n",
      "epoch:2; batch 63232; train accuracy: 0.758348\n",
      "epoch 2; batch 63360; loss 0.400992\n",
      "epoch:2; batch 63360; train accuracy: 0.758424\n",
      "epoch 2; batch 63488; loss 0.374833\n",
      "epoch:2; batch 63488; train accuracy: 0.758513\n",
      "epoch 2; batch 63616; loss 0.377146\n",
      "epoch:2; batch 63616; train accuracy: 0.758607\n",
      "epoch 2; batch 63744; loss 0.303818\n",
      "epoch:2; batch 63744; train accuracy: 0.758708\n",
      "epoch 2; batch 63872; loss 0.463239\n",
      "epoch:2; batch 63872; train accuracy: 0.758766\n",
      "epoch 2; batch 64000; loss 0.437526\n",
      "epoch:2; batch 64000; train accuracy: 0.758836\n",
      "epoch 2; batch 64128; loss 0.340678\n",
      "epoch:2; batch 64128; train accuracy: 0.758924\n",
      "epoch 2; batch 64256; loss 0.340351\n",
      "epoch:2; batch 64256; train accuracy: 0.758994\n",
      "epoch 2; batch 64384; loss 0.345711\n",
      "epoch:2; batch 64384; train accuracy: 0.759076\n",
      "epoch 2; batch 64512; loss 0.593790\n",
      "epoch:2; batch 64512; train accuracy: 0.759134\n",
      "epoch 2; batch 64640; loss 0.367856\n",
      "epoch:2; batch 64640; train accuracy: 0.759233\n",
      "epoch 2; batch 64768; loss 0.290871\n",
      "epoch:2; batch 64768; train accuracy: 0.759350\n",
      "epoch 2; batch 64896; loss 0.442945\n",
      "epoch:2; batch 64896; train accuracy: 0.759408\n",
      "epoch 2; batch 65024; loss 0.367103\n",
      "epoch:2; batch 65024; train accuracy: 0.759495\n",
      "epoch 2; batch 65152; loss 0.349410\n",
      "epoch:2; batch 65152; train accuracy: 0.759576\n",
      "epoch 2; batch 65280; loss 0.425749\n",
      "epoch:2; batch 65280; train accuracy: 0.759622\n",
      "epoch 2; batch 65408; loss 0.377980\n",
      "epoch:2; batch 65408; train accuracy: 0.759697\n",
      "epoch 2; batch 65536; loss 0.409783\n",
      "epoch:2; batch 65536; train accuracy: 0.759742\n",
      "epoch 2; batch 65664; loss 0.382795\n",
      "epoch:2; batch 65664; train accuracy: 0.759788\n",
      "epoch 2; batch 65792; loss 0.390792\n",
      "epoch:2; batch 65792; train accuracy: 0.759874\n",
      "epoch 2; batch 65920; loss 0.374074\n",
      "epoch:2; batch 65920; train accuracy: 0.759943\n",
      "epoch 2; batch 66048; loss 0.506510\n",
      "epoch:2; batch 66048; train accuracy: 0.760011\n",
      "epoch 2; batch 66176; loss 0.377073\n",
      "epoch:2; batch 66176; train accuracy: 0.760091\n",
      "epoch 2; batch 66304; loss 0.391406\n",
      "epoch:2; batch 66304; train accuracy: 0.760131\n",
      "epoch 2; batch 66432; loss 0.377917\n",
      "epoch:2; batch 66432; train accuracy: 0.760205\n",
      "epoch 2; batch 66560; loss 0.462754\n",
      "epoch:2; batch 66560; train accuracy: 0.760255\n",
      "epoch 2; batch 66688; loss 0.366781\n",
      "epoch:2; batch 66688; train accuracy: 0.760335\n",
      "epoch 2; batch 66816; loss 0.434676\n",
      "epoch:2; batch 66816; train accuracy: 0.760380\n",
      "epoch 2; batch 66944; loss 0.353693\n",
      "epoch:2; batch 66944; train accuracy: 0.760448\n",
      "epoch 2; batch 67072; loss 0.388425\n",
      "epoch:2; batch 67072; train accuracy: 0.760504\n",
      "epoch 2; batch 67200; loss 0.411392\n",
      "epoch:2; batch 67200; train accuracy: 0.760548\n",
      "epoch 2; batch 67328; loss 0.253234\n",
      "epoch:2; batch 67328; train accuracy: 0.760657\n",
      "epoch 2; batch 67456; loss 0.379006\n",
      "epoch:2; batch 67456; train accuracy: 0.760730\n",
      "epoch 2; batch 67584; loss 0.343488\n",
      "epoch:2; batch 67584; train accuracy: 0.760820\n",
      "epoch 2; batch 67712; loss 0.404653\n",
      "epoch:2; batch 67712; train accuracy: 0.760893\n",
      "epoch 2; batch 67840; loss 0.449634\n",
      "epoch:2; batch 67840; train accuracy: 0.760949\n",
      "epoch 2; batch 67968; loss 0.259950\n",
      "epoch:2; batch 67968; train accuracy: 0.761051\n",
      "epoch 2; batch 68096; loss 0.425345\n",
      "epoch:2; batch 68096; train accuracy: 0.761124\n",
      "epoch 2; batch 68224; loss 0.452492\n",
      "epoch:2; batch 68224; train accuracy: 0.761196\n",
      "epoch 2; batch 68352; loss 0.269089\n",
      "epoch:2; batch 68352; train accuracy: 0.761303\n",
      "epoch 2; batch 68480; loss 0.463200\n",
      "epoch:2; batch 68480; train accuracy: 0.761370\n",
      "epoch 2; batch 68608; loss 0.399141\n",
      "epoch:2; batch 68608; train accuracy: 0.761419\n",
      "epoch 2; batch 68736; loss 0.357828\n",
      "epoch:2; batch 68736; train accuracy: 0.761480\n",
      "epoch 2; batch 68864; loss 0.382683\n",
      "epoch:2; batch 68864; train accuracy: 0.761535\n",
      "epoch 2; batch 68992; loss 0.463661\n",
      "epoch:2; batch 68992; train accuracy: 0.761584\n",
      "epoch 2; batch 69120; loss 0.462243\n",
      "epoch:2; batch 69120; train accuracy: 0.761633\n",
      "epoch 2; batch 69248; loss 0.343748\n",
      "epoch:2; batch 69248; train accuracy: 0.761727\n",
      "epoch 2; batch 69376; loss 0.385999\n",
      "epoch:2; batch 69376; train accuracy: 0.761799\n",
      "epoch 2; batch 69504; loss 0.419167\n",
      "epoch:2; batch 69504; train accuracy: 0.761825\n",
      "epoch 2; batch 69632; loss 0.310307\n",
      "epoch:2; batch 69632; train accuracy: 0.761913\n",
      "epoch 2; batch 69760; loss 0.389560\n",
      "epoch:2; batch 69760; train accuracy: 0.761985\n",
      "epoch 2; batch 69888; loss 0.416205\n",
      "epoch:2; batch 69888; train accuracy: 0.762050\n",
      "epoch 2; batch 70016; loss 0.402989\n",
      "epoch:2; batch 70016; train accuracy: 0.762105\n",
      "epoch 2; batch 70144; loss 0.421086\n",
      "epoch:2; batch 70144; train accuracy: 0.762141\n",
      "epoch 2; batch 70272; loss 0.333891\n",
      "epoch:2; batch 70272; train accuracy: 0.762235\n",
      "epoch 2; batch 70400; loss 0.431719\n",
      "epoch:2; batch 70400; train accuracy: 0.762278\n",
      "epoch 2; batch 70528; loss 0.385288\n",
      "epoch:2; batch 70528; train accuracy: 0.762331\n",
      "epoch 2; batch 70656; loss 0.503942\n",
      "epoch:2; batch 70656; train accuracy: 0.762345\n",
      "epoch 2; batch 70784; loss 0.323898\n",
      "epoch:2; batch 70784; train accuracy: 0.762404\n",
      "epoch 2; batch 70912; loss 0.475454\n",
      "epoch:2; batch 70912; train accuracy: 0.762441\n",
      "epoch 2; batch 71040; loss 0.253439\n",
      "epoch:2; batch 71040; train accuracy: 0.762557\n",
      "epoch 2; batch 71168; loss 0.437844\n",
      "epoch:2; batch 71168; train accuracy: 0.762599\n",
      "epoch 2; batch 71296; loss 0.290795\n",
      "epoch:2; batch 71296; train accuracy: 0.762686\n",
      "epoch 2; batch 71424; loss 0.370725\n",
      "epoch:2; batch 71424; train accuracy: 0.762773\n",
      "epoch 2; batch 71552; loss 0.414022\n",
      "epoch:2; batch 71552; train accuracy: 0.762821\n",
      "epoch 2; batch 71680; loss 0.307201\n",
      "epoch:2; batch 71680; train accuracy: 0.762902\n",
      "epoch 2; batch 71808; loss 0.393323\n",
      "epoch:2; batch 71808; train accuracy: 0.762944\n",
      "epoch 2; batch 71936; loss 0.341170\n",
      "epoch:2; batch 71936; train accuracy: 0.763047\n",
      "epoch 2; batch 72064; loss 0.395373\n",
      "epoch:2; batch 72064; train accuracy: 0.763089\n",
      "epoch 2; batch 72192; loss 0.477901\n",
      "epoch:2; batch 72192; train accuracy: 0.763141\n",
      "epoch 2; batch 72320; loss 0.438003\n",
      "epoch:2; batch 72320; train accuracy: 0.763205\n",
      "epoch 2; batch 72448; loss 0.461088\n",
      "epoch:2; batch 72448; train accuracy: 0.763252\n",
      "epoch 2; batch 72576; loss 0.383617\n",
      "epoch:2; batch 72576; train accuracy: 0.763327\n",
      "epoch 2; batch 72704; loss 0.486632\n",
      "epoch:2; batch 72704; train accuracy: 0.763357\n",
      "epoch 2; batch 72832; loss 0.403543\n",
      "epoch:2; batch 72832; train accuracy: 0.763420\n",
      "epoch 2; batch 72960; loss 0.402537\n",
      "epoch:2; batch 72960; train accuracy: 0.763495\n",
      "epoch 2; batch 73088; loss 0.565032\n",
      "epoch:2; batch 73088; train accuracy: 0.763491\n",
      "epoch 2; batch 73216; loss 0.349077\n",
      "epoch:2; batch 73216; train accuracy: 0.763554\n",
      "epoch 2; batch 73344; loss 0.389346\n",
      "epoch:2; batch 73344; train accuracy: 0.763612\n",
      "epoch 2; batch 73472; loss 0.396124\n",
      "epoch:2; batch 73472; train accuracy: 0.763680\n",
      "epoch 2; batch 73600; loss 0.390751\n",
      "epoch:2; batch 73600; train accuracy: 0.763732\n",
      "epoch 2; batch 73728; loss 0.366640\n",
      "epoch:2; batch 73728; train accuracy: 0.763789\n",
      "epoch 2; batch 73856; loss 0.423364\n",
      "epoch:2; batch 73856; train accuracy: 0.763858\n",
      "epoch 2; batch 73984; loss 0.429097\n",
      "epoch:2; batch 73984; train accuracy: 0.763904\n",
      "epoch 2; batch 74112; loss 0.470526\n",
      "epoch:2; batch 74112; train accuracy: 0.763972\n",
      "epoch 2; batch 74240; loss 0.375390\n",
      "epoch:2; batch 74240; train accuracy: 0.764035\n",
      "epoch 2; batch 74368; loss 0.391251\n",
      "epoch:2; batch 74368; train accuracy: 0.764086\n",
      "epoch 2; batch 74496; loss 0.477290\n",
      "epoch:2; batch 74496; train accuracy: 0.764126\n",
      "epoch 2; batch 74624; loss 0.300369\n",
      "epoch:2; batch 74624; train accuracy: 0.764222\n",
      "epoch 2; batch 74752; loss 0.307486\n",
      "epoch:2; batch 74752; train accuracy: 0.764312\n",
      "epoch 2; batch 74880; loss 0.346615\n",
      "epoch:2; batch 74880; train accuracy: 0.764391\n",
      "epoch 2; batch 75008; loss 0.434592\n",
      "epoch:2; batch 75008; train accuracy: 0.764441\n",
      "epoch 2; batch 75136; loss 0.284477\n",
      "epoch:2; batch 75136; train accuracy: 0.764531\n",
      "epoch 2; batch 75264; loss 0.419440\n",
      "epoch:2; batch 75264; train accuracy: 0.764610\n",
      "epoch 2; batch 75392; loss 0.353222\n",
      "epoch:2; batch 75392; train accuracy: 0.764682\n",
      "epoch 2; batch 75520; loss 0.380663\n",
      "epoch:2; batch 75520; train accuracy: 0.764739\n",
      "epoch 2; batch 75648; loss 0.292302\n",
      "epoch:2; batch 75648; train accuracy: 0.764839\n",
      "epoch 2; batch 75776; loss 0.449363\n",
      "epoch:2; batch 75776; train accuracy: 0.764889\n",
      "epoch 2; batch 75904; loss 0.372769\n",
      "epoch:2; batch 75904; train accuracy: 0.764962\n",
      "epoch 2; batch 76032; loss 0.381680\n",
      "epoch:2; batch 76032; train accuracy: 0.765023\n",
      "epoch 2; batch 76160; loss 0.399569\n",
      "epoch:2; batch 76160; train accuracy: 0.765078\n",
      "epoch 2; batch 76288; loss 0.371984\n",
      "epoch:2; batch 76288; train accuracy: 0.765123\n",
      "epoch 2; batch 76416; loss 0.271689\n",
      "epoch:2; batch 76416; train accuracy: 0.765212\n",
      "epoch 2; batch 76544; loss 0.527690\n",
      "epoch:2; batch 76544; train accuracy: 0.765239\n",
      "epoch 2; batch 76672; loss 0.376458\n",
      "epoch:2; batch 76672; train accuracy: 0.765317\n",
      "epoch 2; batch 76800; loss 0.431786\n",
      "epoch:2; batch 76800; train accuracy: 0.765366\n",
      "epoch 2; batch 76928; loss 0.440818\n",
      "epoch:2; batch 76928; train accuracy: 0.765405\n",
      "epoch 2; batch 77056; loss 0.360889\n",
      "epoch:2; batch 77056; train accuracy: 0.765466\n",
      "epoch 2; batch 77184; loss 0.368268\n",
      "epoch:2; batch 77184; train accuracy: 0.765515\n",
      "epoch 2; batch 77312; loss 0.500844\n",
      "epoch:2; batch 77312; train accuracy: 0.765565\n",
      "epoch 2; batch 77440; loss 0.356655\n",
      "epoch:2; batch 77440; train accuracy: 0.765636\n",
      "epoch 2; batch 77568; loss 0.309153\n",
      "epoch:2; batch 77568; train accuracy: 0.765718\n",
      "epoch 2; batch 77696; loss 0.326055\n",
      "epoch:2; batch 77696; train accuracy: 0.765795\n",
      "epoch 2; batch 77824; loss 0.450400\n",
      "epoch:2; batch 77824; train accuracy: 0.765817\n",
      "epoch 2; batch 77952; loss 0.321439\n",
      "epoch:2; batch 77952; train accuracy: 0.765882\n",
      "epoch 2; batch 78080; loss 0.423698\n",
      "epoch:2; batch 78080; train accuracy: 0.765931\n",
      "epoch 2; batch 78208; loss 0.381040\n",
      "epoch:2; batch 78208; train accuracy: 0.765985\n",
      "epoch 2; batch 78336; loss 0.228515\n",
      "epoch:2; batch 78336; train accuracy: 0.766100\n",
      "epoch 2; batch 78464; loss 0.346918\n",
      "epoch:2; batch 78464; train accuracy: 0.766170\n",
      "epoch 2; batch 78592; loss 0.463433\n",
      "epoch:2; batch 78592; train accuracy: 0.766230\n",
      "epoch 2; batch 78720; loss 0.466372\n",
      "epoch:2; batch 78720; train accuracy: 0.766300\n",
      "epoch 2; batch 78848; loss 0.271674\n",
      "epoch:2; batch 78848; train accuracy: 0.766398\n",
      "epoch 2; batch 78976; loss 0.340323\n",
      "epoch:2; batch 78976; train accuracy: 0.766462\n",
      "epoch 2; batch 79104; loss 0.444796\n",
      "epoch:2; batch 79104; train accuracy: 0.766521\n",
      "epoch 2; batch 79232; loss 0.294383\n",
      "epoch:2; batch 79232; train accuracy: 0.766613\n",
      "epoch 2; batch 79360; loss 0.258999\n",
      "epoch:2; batch 79360; train accuracy: 0.766715\n",
      "epoch 2; batch 79488; loss 0.288342\n",
      "epoch:2; batch 79488; train accuracy: 0.766791\n",
      "epoch 2; batch 79616; loss 0.479027\n",
      "epoch:2; batch 79616; train accuracy: 0.766822\n",
      "epoch 2; batch 79744; loss 0.541110\n",
      "epoch:2; batch 79744; train accuracy: 0.766838\n",
      "epoch 2; batch 79872; loss 0.536978\n",
      "epoch:2; batch 79872; train accuracy: 0.766837\n",
      "epoch 2; batch 80000; loss 0.303961\n",
      "epoch:2; batch 80000; train accuracy: 0.766923\n",
      "epoch 2; batch 80128; loss 0.435061\n",
      "epoch:2; batch 80128; train accuracy: 0.766976\n",
      "epoch 2; batch 80256; loss 0.246658\n",
      "epoch:2; batch 80256; train accuracy: 0.767083\n",
      "epoch 2; batch 80384; loss 0.470087\n",
      "epoch:2; batch 80384; train accuracy: 0.767120\n",
      "epoch 2; batch 80512; loss 0.351773\n",
      "epoch:2; batch 80512; train accuracy: 0.767210\n",
      "epoch 2; batch 80640; loss 0.328974\n",
      "epoch:2; batch 80640; train accuracy: 0.767279\n",
      "epoch 2; batch 80768; loss 0.444364\n",
      "epoch:2; batch 80768; train accuracy: 0.767310\n",
      "epoch 2; batch 80896; loss 0.286215\n",
      "epoch:2; batch 80896; train accuracy: 0.767395\n",
      "epoch 2; batch 81024; loss 0.464297\n",
      "epoch:2; batch 81024; train accuracy: 0.767464\n",
      "epoch 2; batch 81152; loss 0.411081\n",
      "epoch:2; batch 81152; train accuracy: 0.767527\n",
      "epoch 2; batch 81280; loss 0.459313\n",
      "epoch:2; batch 81280; train accuracy: 0.767563\n",
      "epoch 2; batch 81408; loss 0.365210\n",
      "epoch:2; batch 81408; train accuracy: 0.767621\n",
      "epoch 2; batch 81536; loss 0.328744\n",
      "epoch:2; batch 81536; train accuracy: 0.767689\n",
      "epoch 2; batch 81664; loss 0.322821\n",
      "epoch:2; batch 81664; train accuracy: 0.767763\n",
      "epoch 2; batch 81792; loss 0.343420\n",
      "epoch:2; batch 81792; train accuracy: 0.767826\n",
      "epoch 2; batch 81920; loss 0.327508\n",
      "epoch:2; batch 81920; train accuracy: 0.767894\n",
      "epoch 2; batch 82048; loss 0.347442\n",
      "epoch:2; batch 82048; train accuracy: 0.767951\n",
      "epoch 2; batch 82176; loss 0.234399\n",
      "epoch:2; batch 82176; train accuracy: 0.768067\n",
      "epoch 2; batch 82304; loss 0.546507\n",
      "epoch:2; batch 82304; train accuracy: 0.768060\n",
      "epoch 2; batch 82432; loss 0.327171\n",
      "epoch:2; batch 82432; train accuracy: 0.768144\n",
      "epoch 2; batch 82560; loss 0.316773\n",
      "epoch:2; batch 82560; train accuracy: 0.768233\n",
      "epoch 2; batch 82688; loss 0.351675\n",
      "epoch:2; batch 82688; train accuracy: 0.768300\n",
      "epoch 2; batch 82816; loss 0.361065\n",
      "epoch:2; batch 82816; train accuracy: 0.768368\n",
      "epoch 2; batch 82944; loss 0.408764\n",
      "epoch:2; batch 82944; train accuracy: 0.768424\n",
      "epoch 2; batch 83072; loss 0.500499\n",
      "epoch:2; batch 83072; train accuracy: 0.768460\n",
      "epoch 2; batch 83200; loss 0.452020\n",
      "epoch:2; batch 83200; train accuracy: 0.768506\n",
      "epoch 2; batch 83328; loss 0.455925\n",
      "epoch:2; batch 83328; train accuracy: 0.768514\n",
      "epoch 2; batch 83456; loss 0.341879\n",
      "epoch:2; batch 83456; train accuracy: 0.768592\n",
      "epoch 2; batch 83584; loss 0.322139\n",
      "epoch:2; batch 83584; train accuracy: 0.768691\n",
      "epoch 2; batch 83712; loss 0.363326\n",
      "epoch:2; batch 83712; train accuracy: 0.768752\n",
      "epoch 2; batch 83840; loss 0.374268\n",
      "epoch:2; batch 83840; train accuracy: 0.768819\n",
      "epoch 2; batch 83968; loss 0.313222\n",
      "epoch:2; batch 83968; train accuracy: 0.768901\n",
      "epoch 2; batch 84096; loss 0.268118\n",
      "epoch:2; batch 84096; train accuracy: 0.768989\n",
      "epoch 2; batch 84224; loss 0.355433\n",
      "epoch:2; batch 84224; train accuracy: 0.769040\n",
      "epoch 2; batch 84352; loss 0.429570\n",
      "epoch:2; batch 84352; train accuracy: 0.769095\n",
      "epoch 2; batch 84480; loss 0.384757\n",
      "epoch:2; batch 84480; train accuracy: 0.769130\n",
      "epoch 2; batch 84608; loss 0.341338\n",
      "epoch:2; batch 84608; train accuracy: 0.769212\n",
      "epoch 2; batch 84736; loss 0.290372\n",
      "epoch:2; batch 84736; train accuracy: 0.769294\n",
      "epoch 2; batch 84864; loss 0.523975\n",
      "epoch:2; batch 84864; train accuracy: 0.769318\n",
      "epoch 2; batch 84992; loss 0.315660\n",
      "epoch:2; batch 84992; train accuracy: 0.769410\n",
      "epoch 2; batch 85120; loss 0.287130\n",
      "epoch:2; batch 85120; train accuracy: 0.769487\n",
      "epoch 2; batch 85248; loss 0.432250\n",
      "epoch:2; batch 85248; train accuracy: 0.769515\n",
      "epoch 2; batch 85376; loss 0.288941\n",
      "epoch:2; batch 85376; train accuracy: 0.769597\n",
      "epoch 2; batch 85504; loss 0.427614\n",
      "epoch:2; batch 85504; train accuracy: 0.769642\n",
      "epoch 2; batch 85632; loss 0.289745\n",
      "epoch:2; batch 85632; train accuracy: 0.769728\n",
      "epoch 2; batch 85760; loss 0.309228\n",
      "epoch:2; batch 85760; train accuracy: 0.769788\n",
      "epoch 2; batch 85888; loss 0.471861\n",
      "epoch:2; batch 85888; train accuracy: 0.769817\n",
      "epoch 2; batch 86016; loss 0.335605\n",
      "epoch:2; batch 86016; train accuracy: 0.769882\n",
      "epoch 2; batch 86144; loss 0.570478\n",
      "epoch:2; batch 86144; train accuracy: 0.769879\n",
      "epoch 2; batch 86272; loss 0.394694\n",
      "epoch:2; batch 86272; train accuracy: 0.769934\n",
      "epoch 2; batch 86400; loss 0.437046\n",
      "epoch:2; batch 86400; train accuracy: 0.769988\n",
      "epoch 2; batch 86528; loss 0.377206\n",
      "epoch:2; batch 86528; train accuracy: 0.770064\n",
      "epoch 2; batch 86656; loss 0.358220\n",
      "epoch:2; batch 86656; train accuracy: 0.770129\n",
      "epoch 2; batch 86784; loss 0.464683\n",
      "epoch:2; batch 86784; train accuracy: 0.770157\n",
      "epoch 2; batch 86912; loss 0.480834\n",
      "epoch:2; batch 86912; train accuracy: 0.770180\n",
      "epoch 2; batch 87040; loss 0.221700\n",
      "epoch:2; batch 87040; train accuracy: 0.770276\n",
      "epoch 2; batch 87168; loss 0.288295\n",
      "epoch:2; batch 87168; train accuracy: 0.770387\n",
      "epoch 2; batch 87296; loss 0.345041\n",
      "epoch:2; batch 87296; train accuracy: 0.770462\n",
      "epoch 2; batch 87424; loss 0.259774\n",
      "epoch:2; batch 87424; train accuracy: 0.770568\n",
      "epoch 2; batch 87552; loss 0.352501\n",
      "epoch:2; batch 87552; train accuracy: 0.770617\n",
      "epoch 2; batch 87680; loss 0.412560\n",
      "epoch:2; batch 87680; train accuracy: 0.770650\n",
      "epoch 2; batch 87808; loss 0.329643\n",
      "epoch:2; batch 87808; train accuracy: 0.770714\n",
      "epoch 2; batch 87936; loss 0.331081\n",
      "epoch:2; batch 87936; train accuracy: 0.770783\n",
      "epoch 2; batch 88064; loss 0.537008\n",
      "epoch:2; batch 88064; train accuracy: 0.770826\n",
      "epoch 2; batch 88192; loss 0.396095\n",
      "epoch:2; batch 88192; train accuracy: 0.770885\n",
      "epoch 2; batch 88320; loss 0.326565\n",
      "epoch:2; batch 88320; train accuracy: 0.770964\n",
      "epoch 2; batch 88448; loss 0.290784\n",
      "epoch:2; batch 88448; train accuracy: 0.771033\n",
      "epoch 2; batch 88576; loss 0.510887\n",
      "epoch:2; batch 88576; train accuracy: 0.771081\n",
      "epoch 2; batch 88704; loss 0.326988\n",
      "epoch:2; batch 88704; train accuracy: 0.771160\n",
      "epoch 2; batch 88832; loss 0.416709\n",
      "epoch:2; batch 88832; train accuracy: 0.771203\n",
      "epoch 2; batch 88960; loss 0.396514\n",
      "epoch:2; batch 88960; train accuracy: 0.771251\n",
      "epoch 2; batch 89088; loss 0.432996\n",
      "epoch:2; batch 89088; train accuracy: 0.771294\n",
      "epoch 2; batch 89216; loss 0.439006\n",
      "epoch:2; batch 89216; train accuracy: 0.771357\n",
      "epoch 2; batch 89344; loss 0.333871\n",
      "epoch:2; batch 89344; train accuracy: 0.771410\n",
      "epoch 2; batch 89472; loss 0.471349\n",
      "epoch:2; batch 89472; train accuracy: 0.771421\n",
      "epoch 2; batch 89600; loss 0.381824\n",
      "epoch:2; batch 89600; train accuracy: 0.771464\n",
      "epoch 2; batch 89728; loss 0.306315\n",
      "epoch:2; batch 89728; train accuracy: 0.771552\n",
      "epoch 2; batch 89856; loss 0.345179\n",
      "epoch:2; batch 89856; train accuracy: 0.771615\n",
      "epoch 2; batch 89984; loss 0.336085\n",
      "epoch:2; batch 89984; train accuracy: 0.771683\n",
      "epoch 2; batch 90112; loss 0.432349\n",
      "epoch:2; batch 90112; train accuracy: 0.771725\n",
      "epoch 2; batch 90240; loss 0.452349\n",
      "epoch:2; batch 90240; train accuracy: 0.771762\n",
      "epoch 2; batch 90368; loss 0.439581\n",
      "epoch:2; batch 90368; train accuracy: 0.771794\n",
      "epoch 2; batch 90496; loss 0.426629\n",
      "epoch:2; batch 90496; train accuracy: 0.771836\n",
      "epoch 2; batch 90624; loss 0.285915\n",
      "epoch:2; batch 90624; train accuracy: 0.771934\n",
      "epoch 2; batch 90752; loss 0.303441\n",
      "epoch:2; batch 90752; train accuracy: 0.772012\n",
      "epoch 2; batch 90880; loss 0.432141\n",
      "epoch:2; batch 90880; train accuracy: 0.772059\n",
      "epoch 2; batch 91008; loss 0.464693\n",
      "epoch:2; batch 91008; train accuracy: 0.772090\n",
      "epoch 2; batch 91136; loss 0.377259\n",
      "epoch:2; batch 91136; train accuracy: 0.772127\n",
      "epoch 2; batch 91264; loss 0.262039\n",
      "epoch:2; batch 91264; train accuracy: 0.772214\n",
      "epoch 2; batch 91392; loss 0.308244\n",
      "epoch:2; batch 91392; train accuracy: 0.772276\n",
      "epoch 2; batch 91520; loss 0.294759\n",
      "epoch:2; batch 91520; train accuracy: 0.772353\n",
      "epoch 2; batch 91648; loss 0.265092\n",
      "epoch:2; batch 91648; train accuracy: 0.772430\n",
      "epoch 2; batch 91776; loss 0.453115\n",
      "epoch:2; batch 91776; train accuracy: 0.772441\n",
      "epoch 2; batch 91904; loss 0.382738\n",
      "epoch:2; batch 91904; train accuracy: 0.772508\n",
      "epoch 2; batch 92032; loss 0.285141\n",
      "epoch:2; batch 92032; train accuracy: 0.772605\n",
      "epoch 2; batch 92160; loss 0.493169\n",
      "epoch:2; batch 92160; train accuracy: 0.772636\n",
      "epoch 2; batch 92288; loss 0.425873\n",
      "epoch:2; batch 92288; train accuracy: 0.772687\n",
      "epoch 2; batch 92416; loss 0.325800\n",
      "epoch:2; batch 92416; train accuracy: 0.772769\n",
      "epoch 2; batch 92544; loss 0.428813\n",
      "epoch:2; batch 92544; train accuracy: 0.772830\n",
      "epoch 2; batch 92672; loss 0.345541\n",
      "epoch:2; batch 92672; train accuracy: 0.772871\n",
      "epoch 2; batch 92800; loss 0.336252\n",
      "epoch:2; batch 92800; train accuracy: 0.772932\n",
      "epoch 2; batch 92928; loss 0.285669\n",
      "epoch:2; batch 92928; train accuracy: 0.773028\n",
      "epoch 2; batch 93056; loss 0.260605\n",
      "epoch:2; batch 93056; train accuracy: 0.773114\n",
      "epoch 2; batch 93184; loss 0.319393\n",
      "epoch:2; batch 93184; train accuracy: 0.773185\n",
      "epoch 2; batch 93312; loss 0.297478\n",
      "epoch:2; batch 93312; train accuracy: 0.773266\n",
      "epoch 2; batch 93440; loss 0.310266\n",
      "epoch:2; batch 93440; train accuracy: 0.773332\n",
      "epoch 2; batch 93568; loss 0.481974\n",
      "epoch:2; batch 93568; train accuracy: 0.773382\n",
      "epoch 2; batch 93696; loss 0.409704\n",
      "epoch:2; batch 93696; train accuracy: 0.773422\n",
      "epoch 2; batch 93824; loss 0.517700\n",
      "epoch:2; batch 93824; train accuracy: 0.773427\n",
      "epoch 2; batch 93952; loss 0.402032\n",
      "epoch:2; batch 93952; train accuracy: 0.773488\n",
      "epoch 2; batch 94080; loss 0.350684\n",
      "epoch:2; batch 94080; train accuracy: 0.773523\n",
      "epoch 2; batch 94208; loss 0.495774\n",
      "epoch:2; batch 94208; train accuracy: 0.773548\n",
      "epoch 2; batch 94336; loss 0.333711\n",
      "epoch:2; batch 94336; train accuracy: 0.773618\n",
      "epoch 2; batch 94464; loss 0.399985\n",
      "epoch:2; batch 94464; train accuracy: 0.773673\n",
      "epoch 2; batch 94592; loss 0.391844\n",
      "epoch:2; batch 94592; train accuracy: 0.773728\n",
      "epoch 2; batch 94720; loss 0.347248\n",
      "epoch:2; batch 94720; train accuracy: 0.773793\n",
      "epoch 2; batch 94848; loss 0.308372\n",
      "epoch:2; batch 94848; train accuracy: 0.773853\n",
      "epoch 2; batch 94976; loss 0.247796\n",
      "epoch:2; batch 94976; train accuracy: 0.773933\n",
      "epoch 2; batch 95104; loss 0.300193\n",
      "epoch:2; batch 95104; train accuracy: 0.774027\n",
      "epoch 2; batch 95232; loss 0.320160\n",
      "epoch:2; batch 95232; train accuracy: 0.774122\n",
      "epoch 2; batch 95360; loss 0.350025\n",
      "epoch:2; batch 95360; train accuracy: 0.774161\n",
      "epoch 2; batch 95488; loss 0.369793\n",
      "epoch:2; batch 95488; train accuracy: 0.774236\n",
      "epoch 2; batch 95616; loss 0.289639\n",
      "epoch:2; batch 95616; train accuracy: 0.774310\n",
      "epoch 2; batch 95744; loss 0.286645\n",
      "epoch:2; batch 95744; train accuracy: 0.774394\n",
      "epoch 2; batch 95872; loss 0.210644\n",
      "epoch:2; batch 95872; train accuracy: 0.774488\n",
      "epoch 2; batch 96000; loss 0.497591\n",
      "epoch:2; batch 96000; train accuracy: 0.774492\n",
      "epoch 2; batch 96128; loss 0.459129\n",
      "epoch:2; batch 96128; train accuracy: 0.774502\n",
      "epoch 2; batch 96256; loss 0.331629\n",
      "epoch:2; batch 96256; train accuracy: 0.774561\n",
      "epoch 2; batch 96384; loss 0.325186\n",
      "epoch:2; batch 96384; train accuracy: 0.774634\n",
      "epoch 2; batch 96512; loss 0.320874\n",
      "epoch:2; batch 96512; train accuracy: 0.774698\n",
      "epoch 2; batch 96640; loss 0.437286\n",
      "epoch:2; batch 96640; train accuracy: 0.774732\n",
      "epoch 2; batch 96768; loss 0.341301\n",
      "epoch:2; batch 96768; train accuracy: 0.774791\n",
      "epoch 2; batch 96896; loss 0.345620\n",
      "epoch:2; batch 96896; train accuracy: 0.774839\n",
      "epoch 2; batch 97024; loss 0.301654\n",
      "epoch:2; batch 97024; train accuracy: 0.774923\n",
      "epoch 2; batch 97152; loss 0.469330\n",
      "epoch:2; batch 97152; train accuracy: 0.774961\n",
      "epoch 2; batch 97280; loss 0.347412\n",
      "epoch:2; batch 97280; train accuracy: 0.775025\n",
      "epoch 2; batch 97408; loss 0.352960\n",
      "epoch:2; batch 97408; train accuracy: 0.775093\n",
      "epoch 2; batch 97536; loss 0.297774\n",
      "epoch:2; batch 97536; train accuracy: 0.775171\n",
      "epoch 2; batch 97664; loss 0.262979\n",
      "epoch:2; batch 97664; train accuracy: 0.775254\n",
      "epoch 2; batch 97792; loss 0.322401\n",
      "epoch:2; batch 97792; train accuracy: 0.775341\n",
      "epoch 2; batch 97920; loss 0.474355\n",
      "epoch:2; batch 97920; train accuracy: 0.775380\n",
      "epoch 2; batch 98048; loss 0.264033\n",
      "epoch:2; batch 98048; train accuracy: 0.775477\n",
      "epoch 2; batch 98176; loss 0.418409\n",
      "epoch:2; batch 98176; train accuracy: 0.775520\n",
      "epoch 2; batch 98304; loss 0.329613\n",
      "epoch:2; batch 98304; train accuracy: 0.775563\n",
      "epoch 2; batch 98432; loss 0.419390\n",
      "epoch:2; batch 98432; train accuracy: 0.775591\n",
      "epoch 2; batch 98560; loss 0.341631\n",
      "epoch:2; batch 98560; train accuracy: 0.775639\n",
      "epoch 2; batch 98688; loss 0.461316\n",
      "epoch:2; batch 98688; train accuracy: 0.775667\n",
      "epoch 2; batch 98816; loss 0.442051\n",
      "epoch:2; batch 98816; train accuracy: 0.775700\n",
      "epoch 2; batch 98944; loss 0.267328\n",
      "epoch:2; batch 98944; train accuracy: 0.775772\n",
      "epoch 2; batch 99072; loss 0.444754\n",
      "epoch:2; batch 99072; train accuracy: 0.775800\n",
      "epoch 2; batch 99200; loss 0.368933\n",
      "epoch:2; batch 99200; train accuracy: 0.775852\n",
      "epoch 2; batch 99328; loss 0.292864\n",
      "epoch:2; batch 99328; train accuracy: 0.775919\n",
      "epoch 2; batch 99456; loss 0.278670\n",
      "epoch:2; batch 99456; train accuracy: 0.775986\n",
      "epoch 2; batch 99584; loss 0.386932\n",
      "epoch:2; batch 99584; train accuracy: 0.776029\n",
      "epoch 2; batch 99712; loss 0.417000\n",
      "epoch:2; batch 99712; train accuracy: 0.776081\n",
      "epoch 2; batch 99840; loss 0.427612\n",
      "epoch:2; batch 99840; train accuracy: 0.776133\n",
      "epoch 2; batch 99968; loss 0.221649\n",
      "epoch:2; batch 99968; train accuracy: 0.776224\n",
      "epoch 2; batch 100096; loss 0.350173\n",
      "epoch:2; batch 100096; train accuracy: 0.776295\n",
      "epoch 2; batch 100224; loss 0.314257\n",
      "epoch:2; batch 100224; train accuracy: 0.776367\n",
      "epoch 2; batch 100352; loss 0.372781\n",
      "epoch:2; batch 100352; train accuracy: 0.776409\n",
      "epoch 2; batch 100480; loss 0.333489\n",
      "epoch:2; batch 100480; train accuracy: 0.776485\n",
      "epoch 2; batch 100608; loss 0.393533\n",
      "epoch:2; batch 100608; train accuracy: 0.776531\n",
      "epoch 2; batch 100736; loss 0.246217\n",
      "epoch:2; batch 100736; train accuracy: 0.776612\n",
      "epoch 2; batch 100864; loss 0.383838\n",
      "epoch:2; batch 100864; train accuracy: 0.776688\n",
      "epoch 2; batch 100992; loss 0.343777\n",
      "epoch:2; batch 100992; train accuracy: 0.776739\n",
      "epoch 2; batch 101120; loss 0.291911\n",
      "epoch:2; batch 101120; train accuracy: 0.776815\n",
      "epoch 2; batch 101248; loss 0.393915\n",
      "epoch:2; batch 101248; train accuracy: 0.776861\n",
      "epoch 2; batch 101376; loss 0.422667\n",
      "epoch:2; batch 101376; train accuracy: 0.776903\n",
      "epoch 2; batch 101504; loss 0.306768\n",
      "epoch:2; batch 101504; train accuracy: 0.776973\n",
      "epoch 2; batch 101632; loss 0.314067\n",
      "epoch:2; batch 101632; train accuracy: 0.777029\n",
      "epoch 2; batch 101760; loss 0.322330\n",
      "epoch:2; batch 101760; train accuracy: 0.777090\n",
      "epoch 2; batch 101888; loss 0.294327\n",
      "epoch:2; batch 101888; train accuracy: 0.777160\n",
      "epoch 2; batch 102016; loss 0.266651\n",
      "epoch:2; batch 102016; train accuracy: 0.777221\n",
      "epoch 2; batch 102144; loss 0.368581\n",
      "epoch:2; batch 102144; train accuracy: 0.777262\n",
      "epoch 2; batch 102272; loss 0.286978\n",
      "epoch:2; batch 102272; train accuracy: 0.777341\n",
      "epoch 2; batch 102400; loss 0.274424\n",
      "epoch:2; batch 102400; train accuracy: 0.777406\n",
      "epoch 2; batch 102528; loss 0.331791\n",
      "epoch:2; batch 102528; train accuracy: 0.777467\n",
      "epoch 2; batch 102656; loss 0.335356\n",
      "epoch:2; batch 102656; train accuracy: 0.777536\n",
      "epoch 2; batch 102784; loss 0.329881\n",
      "epoch:2; batch 102784; train accuracy: 0.777601\n",
      "epoch 2; batch 102912; loss 0.349359\n",
      "epoch:2; batch 102912; train accuracy: 0.777661\n",
      "epoch 2; batch 103040; loss 0.456944\n",
      "epoch:2; batch 103040; train accuracy: 0.777692\n",
      "epoch 2; batch 103168; loss 0.364350\n",
      "epoch:2; batch 103168; train accuracy: 0.777767\n",
      "epoch 2; batch 103296; loss 0.423732\n",
      "epoch:2; batch 103296; train accuracy: 0.777807\n",
      "epoch 2; batch 103424; loss 0.355182\n",
      "epoch:2; batch 103424; train accuracy: 0.777867\n",
      "epoch 2; batch 103552; loss 0.401983\n",
      "epoch:2; batch 103552; train accuracy: 0.777902\n",
      "epoch 2; batch 103680; loss 0.488116\n",
      "epoch:2; batch 103680; train accuracy: 0.777938\n",
      "epoch 2; batch 103808; loss 0.466912\n",
      "epoch:2; batch 103808; train accuracy: 0.777950\n",
      "epoch 2; batch 103936; loss 0.331744\n",
      "epoch:2; batch 103936; train accuracy: 0.778000\n",
      "epoch 2; batch 104064; loss 0.319216\n",
      "epoch:2; batch 104064; train accuracy: 0.778069\n",
      "epoch 2; batch 104192; loss 0.364335\n",
      "epoch:2; batch 104192; train accuracy: 0.778094\n",
      "epoch 2; batch 104320; loss 0.442458\n",
      "epoch:2; batch 104320; train accuracy: 0.778120\n",
      "epoch 2; batch 104448; loss 0.475414\n",
      "epoch:2; batch 104448; train accuracy: 0.778156\n",
      "epoch 2; batch 104576; loss 0.362242\n",
      "epoch:2; batch 104576; train accuracy: 0.778196\n",
      "epoch 2; batch 104704; loss 0.308583\n",
      "epoch:2; batch 104704; train accuracy: 0.778260\n",
      "epoch 2; batch 104832; loss 0.427961\n",
      "epoch:2; batch 104832; train accuracy: 0.778276\n",
      "epoch 2; batch 104960; loss 0.343996\n",
      "epoch:2; batch 104960; train accuracy: 0.778320\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 31767 ; rate: 0.302658\n",
      "y_true_label_1_num: 12636 ; rate: 0.120389\n",
      "y_true_label_2_num: 14791 ; rate: 0.140920\n",
      "y_true_label_3_num: 45766 ; rate: 0.436033\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.940587\n",
      "valid avg_precision: 0.951790\n",
      "valid avg_recall: 0.924314\n",
      "valid avg_f1: 0.935812\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 4475 ; rate: 0.298811\n",
      "y_true_label_1_num: 1789 ; rate: 0.119458\n",
      "y_true_label_2_num: 2176 ; rate: 0.145299\n",
      "y_true_label_3_num: 6536 ; rate: 0.436432\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.748865\n",
      "valid avg_precision: 0.760815\n",
      "valid avg_recall: 0.721888\n",
      "valid avg_f1: 0.737148\n",
      "epoch 3\n",
      "epoch 3; batch 128; loss 0.287335\n",
      "epoch:3; batch 128; train accuracy: 0.778408\n",
      "epoch 3; batch 256; loss 0.244880\n",
      "epoch:3; batch 256; train accuracy: 0.778500\n",
      "epoch 3; batch 384; loss 0.390179\n",
      "epoch:3; batch 384; train accuracy: 0.778554\n",
      "epoch 3; batch 512; loss 0.259086\n",
      "epoch:3; batch 512; train accuracy: 0.778632\n",
      "epoch 3; batch 640; loss 0.283846\n",
      "epoch:3; batch 640; train accuracy: 0.778719\n",
      "epoch 3; batch 768; loss 0.257276\n",
      "epoch:3; batch 768; train accuracy: 0.778782\n",
      "epoch 3; batch 896; loss 0.293959\n",
      "epoch:3; batch 896; train accuracy: 0.778864\n",
      "epoch 3; batch 1024; loss 0.300294\n",
      "epoch:3; batch 1024; train accuracy: 0.778951\n",
      "epoch 3; batch 1152; loss 0.278533\n",
      "epoch:3; batch 1152; train accuracy: 0.779023\n",
      "epoch 3; batch 1280; loss 0.252497\n",
      "epoch:3; batch 1280; train accuracy: 0.779100\n",
      "epoch 3; batch 1408; loss 0.191933\n",
      "epoch:3; batch 1408; train accuracy: 0.779182\n",
      "epoch 3; batch 1536; loss 0.222767\n",
      "epoch:3; batch 1536; train accuracy: 0.779259\n",
      "epoch 3; batch 1664; loss 0.289419\n",
      "epoch:3; batch 1664; train accuracy: 0.779322\n",
      "epoch 3; batch 1792; loss 0.318677\n",
      "epoch:3; batch 1792; train accuracy: 0.779375\n",
      "epoch 3; batch 1920; loss 0.207968\n",
      "epoch:3; batch 1920; train accuracy: 0.779461\n",
      "epoch 3; batch 2048; loss 0.197119\n",
      "epoch:3; batch 2048; train accuracy: 0.779537\n",
      "epoch 3; batch 2176; loss 0.275084\n",
      "epoch:3; batch 2176; train accuracy: 0.779623\n",
      "epoch 3; batch 2304; loss 0.250706\n",
      "epoch:3; batch 2304; train accuracy: 0.779719\n",
      "epoch 3; batch 2432; loss 0.197774\n",
      "epoch:3; batch 2432; train accuracy: 0.779823\n",
      "epoch 3; batch 2560; loss 0.208124\n",
      "epoch:3; batch 2560; train accuracy: 0.779918\n",
      "epoch 3; batch 2688; loss 0.178534\n",
      "epoch:3; batch 2688; train accuracy: 0.779999\n",
      "epoch 3; batch 2816; loss 0.396780\n",
      "epoch:3; batch 2816; train accuracy: 0.780037\n",
      "epoch 3; batch 2944; loss 0.284160\n",
      "epoch:3; batch 2944; train accuracy: 0.780104\n",
      "epoch 3; batch 3072; loss 0.326686\n",
      "epoch:3; batch 3072; train accuracy: 0.780175\n",
      "epoch 3; batch 3200; loss 0.203162\n",
      "epoch:3; batch 3200; train accuracy: 0.780265\n",
      "epoch 3; batch 3328; loss 0.215281\n",
      "epoch:3; batch 3328; train accuracy: 0.780345\n",
      "epoch 3; batch 3456; loss 0.177175\n",
      "epoch:3; batch 3456; train accuracy: 0.780444\n",
      "epoch 3; batch 3584; loss 0.237229\n",
      "epoch:3; batch 3584; train accuracy: 0.780538\n",
      "epoch 3; batch 3712; loss 0.211068\n",
      "epoch:3; batch 3712; train accuracy: 0.780618\n",
      "epoch 3; batch 3840; loss 0.211049\n",
      "epoch:3; batch 3840; train accuracy: 0.780698\n",
      "epoch 3; batch 3968; loss 0.255086\n",
      "epoch:3; batch 3968; train accuracy: 0.780782\n",
      "epoch 3; batch 4096; loss 0.276167\n",
      "epoch:3; batch 4096; train accuracy: 0.780848\n",
      "epoch 3; batch 4224; loss 0.362937\n",
      "epoch:3; batch 4224; train accuracy: 0.780900\n",
      "epoch 3; batch 4352; loss 0.168732\n",
      "epoch:3; batch 4352; train accuracy: 0.780998\n",
      "epoch 3; batch 4480; loss 0.151832\n",
      "epoch:3; batch 4480; train accuracy: 0.781101\n",
      "epoch 3; batch 4608; loss 0.275084\n",
      "epoch:3; batch 4608; train accuracy: 0.781180\n",
      "epoch 3; batch 4736; loss 0.159697\n",
      "epoch:3; batch 4736; train accuracy: 0.781287\n",
      "epoch 3; batch 4864; loss 0.114454\n",
      "epoch:3; batch 4864; train accuracy: 0.781404\n",
      "epoch 3; batch 4992; loss 0.265717\n",
      "epoch:3; batch 4992; train accuracy: 0.781464\n",
      "epoch 3; batch 5120; loss 0.172749\n",
      "epoch:3; batch 5120; train accuracy: 0.781548\n",
      "epoch 3; batch 5248; loss 0.228952\n",
      "epoch:3; batch 5248; train accuracy: 0.781626\n",
      "epoch 3; batch 5376; loss 0.147838\n",
      "epoch:3; batch 5376; train accuracy: 0.781714\n",
      "epoch 3; batch 5504; loss 0.424050\n",
      "epoch:3; batch 5504; train accuracy: 0.781761\n",
      "epoch 3; batch 5632; loss 0.293246\n",
      "epoch:3; batch 5632; train accuracy: 0.781848\n",
      "epoch 3; batch 5760; loss 0.164315\n",
      "epoch:3; batch 5760; train accuracy: 0.781941\n",
      "epoch 3; batch 5888; loss 0.206151\n",
      "epoch:3; batch 5888; train accuracy: 0.782033\n",
      "epoch 3; batch 6016; loss 0.217700\n",
      "epoch:3; batch 6016; train accuracy: 0.782102\n",
      "epoch 3; batch 6144; loss 0.240079\n",
      "epoch:3; batch 6144; train accuracy: 0.782180\n",
      "epoch 3; batch 6272; loss 0.184869\n",
      "epoch:3; batch 6272; train accuracy: 0.782268\n",
      "epoch 3; batch 6400; loss 0.256160\n",
      "epoch:3; batch 6400; train accuracy: 0.782327\n",
      "epoch 3; batch 6528; loss 0.191944\n",
      "epoch:3; batch 6528; train accuracy: 0.782414\n",
      "epoch 3; batch 6656; loss 0.225047\n",
      "epoch:3; batch 6656; train accuracy: 0.782506\n",
      "epoch 3; batch 6784; loss 0.238026\n",
      "epoch:3; batch 6784; train accuracy: 0.782574\n",
      "epoch 3; batch 6912; loss 0.251161\n",
      "epoch:3; batch 6912; train accuracy: 0.782652\n",
      "epoch 3; batch 7040; loss 0.235774\n",
      "epoch:3; batch 7040; train accuracy: 0.782730\n",
      "epoch 3; batch 7168; loss 0.188768\n",
      "epoch:3; batch 7168; train accuracy: 0.782812\n",
      "epoch 3; batch 7296; loss 0.235922\n",
      "epoch:3; batch 7296; train accuracy: 0.782866\n",
      "epoch 3; batch 7424; loss 0.141313\n",
      "epoch:3; batch 7424; train accuracy: 0.782962\n",
      "epoch 3; batch 7552; loss 0.146164\n",
      "epoch:3; batch 7552; train accuracy: 0.783066\n",
      "epoch 3; batch 7680; loss 0.193391\n",
      "epoch:3; batch 7680; train accuracy: 0.783134\n",
      "epoch 3; batch 7808; loss 0.149314\n",
      "epoch:3; batch 7808; train accuracy: 0.783234\n",
      "epoch 3; batch 7936; loss 0.215850\n",
      "epoch:3; batch 7936; train accuracy: 0.783325\n",
      "epoch 3; batch 8064; loss 0.202357\n",
      "epoch:3; batch 8064; train accuracy: 0.783415\n",
      "epoch 3; batch 8192; loss 0.260222\n",
      "epoch:3; batch 8192; train accuracy: 0.783478\n",
      "epoch 3; batch 8320; loss 0.206595\n",
      "epoch:3; batch 8320; train accuracy: 0.783559\n",
      "epoch 3; batch 8448; loss 0.203707\n",
      "epoch:3; batch 8448; train accuracy: 0.783650\n",
      "epoch 3; batch 8576; loss 0.249185\n",
      "epoch:3; batch 8576; train accuracy: 0.783735\n",
      "epoch 3; batch 8704; loss 0.286943\n",
      "epoch:3; batch 8704; train accuracy: 0.783811\n",
      "epoch 3; batch 8832; loss 0.250645\n",
      "epoch:3; batch 8832; train accuracy: 0.783879\n",
      "epoch 3; batch 8960; loss 0.150332\n",
      "epoch:3; batch 8960; train accuracy: 0.783968\n",
      "epoch 3; batch 9088; loss 0.316499\n",
      "epoch:3; batch 9088; train accuracy: 0.784031\n",
      "epoch 3; batch 9216; loss 0.137165\n",
      "epoch:3; batch 9216; train accuracy: 0.784130\n",
      "epoch 3; batch 9344; loss 0.213212\n",
      "epoch:3; batch 9344; train accuracy: 0.784205\n",
      "epoch 3; batch 9472; loss 0.204608\n",
      "epoch:3; batch 9472; train accuracy: 0.784290\n",
      "epoch 3; batch 9600; loss 0.203684\n",
      "epoch:3; batch 9600; train accuracy: 0.784384\n",
      "epoch 3; batch 9728; loss 0.153410\n",
      "epoch:3; batch 9728; train accuracy: 0.784482\n",
      "epoch 3; batch 9856; loss 0.154338\n",
      "epoch:3; batch 9856; train accuracy: 0.784585\n",
      "epoch 3; batch 9984; loss 0.209000\n",
      "epoch:3; batch 9984; train accuracy: 0.784683\n",
      "epoch 3; batch 10112; loss 0.153658\n",
      "epoch:3; batch 10112; train accuracy: 0.784781\n",
      "epoch 3; batch 10240; loss 0.160121\n",
      "epoch:3; batch 10240; train accuracy: 0.784870\n",
      "epoch 3; batch 10368; loss 0.243253\n",
      "epoch:3; batch 10368; train accuracy: 0.784927\n",
      "epoch 3; batch 10496; loss 0.172560\n",
      "epoch:3; batch 10496; train accuracy: 0.785016\n",
      "epoch 3; batch 10624; loss 0.295867\n",
      "epoch:3; batch 10624; train accuracy: 0.785081\n",
      "epoch 3; batch 10752; loss 0.091316\n",
      "epoch:3; batch 10752; train accuracy: 0.785188\n",
      "epoch 3; batch 10880; loss 0.209828\n",
      "epoch:3; batch 10880; train accuracy: 0.785258\n",
      "epoch 3; batch 11008; loss 0.203824\n",
      "epoch:3; batch 11008; train accuracy: 0.785337\n",
      "epoch 3; batch 11136; loss 0.165926\n",
      "epoch:3; batch 11136; train accuracy: 0.785434\n",
      "epoch 3; batch 11264; loss 0.207550\n",
      "epoch:3; batch 11264; train accuracy: 0.785513\n",
      "epoch 3; batch 11392; loss 0.282750\n",
      "epoch:3; batch 11392; train accuracy: 0.785588\n",
      "epoch 3; batch 11520; loss 0.148705\n",
      "epoch:3; batch 11520; train accuracy: 0.785671\n",
      "epoch 3; batch 11648; loss 0.192696\n",
      "epoch:3; batch 11648; train accuracy: 0.785745\n",
      "epoch 3; batch 11776; loss 0.159655\n",
      "epoch:3; batch 11776; train accuracy: 0.785833\n",
      "epoch 3; batch 11904; loss 0.138989\n",
      "epoch:3; batch 11904; train accuracy: 0.785925\n",
      "epoch 3; batch 12032; loss 0.262679\n",
      "epoch:3; batch 12032; train accuracy: 0.785990\n",
      "epoch 3; batch 12160; loss 0.148420\n",
      "epoch:3; batch 12160; train accuracy: 0.786073\n",
      "epoch 3; batch 12288; loss 0.168298\n",
      "epoch:3; batch 12288; train accuracy: 0.786164\n",
      "epoch 3; batch 12416; loss 0.175068\n",
      "epoch:3; batch 12416; train accuracy: 0.786251\n",
      "epoch 3; batch 12544; loss 0.199722\n",
      "epoch:3; batch 12544; train accuracy: 0.786334\n",
      "epoch 3; batch 12672; loss 0.187315\n",
      "epoch:3; batch 12672; train accuracy: 0.786407\n",
      "epoch 3; batch 12800; loss 0.173630\n",
      "epoch:3; batch 12800; train accuracy: 0.786494\n",
      "epoch 3; batch 12928; loss 0.297409\n",
      "epoch:3; batch 12928; train accuracy: 0.786563\n",
      "epoch 3; batch 13056; loss 0.104759\n",
      "epoch:3; batch 13056; train accuracy: 0.786663\n",
      "epoch 3; batch 13184; loss 0.158058\n",
      "epoch:3; batch 13184; train accuracy: 0.786754\n",
      "epoch 3; batch 13312; loss 0.228127\n",
      "epoch:3; batch 13312; train accuracy: 0.786832\n",
      "epoch 3; batch 13440; loss 0.193190\n",
      "epoch:3; batch 13440; train accuracy: 0.786914\n",
      "epoch 3; batch 13568; loss 0.120396\n",
      "epoch:3; batch 13568; train accuracy: 0.787018\n",
      "epoch 3; batch 13696; loss 0.195021\n",
      "epoch:3; batch 13696; train accuracy: 0.787086\n",
      "epoch 3; batch 13824; loss 0.101251\n",
      "epoch:3; batch 13824; train accuracy: 0.787185\n",
      "epoch 3; batch 13952; loss 0.187854\n",
      "epoch:3; batch 13952; train accuracy: 0.787276\n",
      "epoch 3; batch 14080; loss 0.173074\n",
      "epoch:3; batch 14080; train accuracy: 0.787339\n",
      "epoch 3; batch 14208; loss 0.182709\n",
      "epoch:3; batch 14208; train accuracy: 0.787421\n",
      "epoch 3; batch 14336; loss 0.131592\n",
      "epoch:3; batch 14336; train accuracy: 0.787515\n",
      "epoch 3; batch 14464; loss 0.284910\n",
      "epoch:3; batch 14464; train accuracy: 0.787592\n",
      "epoch 3; batch 14592; loss 0.139351\n",
      "epoch:3; batch 14592; train accuracy: 0.787686\n",
      "epoch 3; batch 14720; loss 0.195251\n",
      "epoch:3; batch 14720; train accuracy: 0.787758\n",
      "epoch 3; batch 14848; loss 0.196553\n",
      "epoch:3; batch 14848; train accuracy: 0.787839\n",
      "epoch 3; batch 14976; loss 0.122154\n",
      "epoch:3; batch 14976; train accuracy: 0.787933\n",
      "epoch 3; batch 15104; loss 0.406714\n",
      "epoch:3; batch 15104; train accuracy: 0.787992\n",
      "epoch 3; batch 15232; loss 0.258269\n",
      "epoch:3; batch 15232; train accuracy: 0.788045\n",
      "epoch 3; batch 15360; loss 0.210941\n",
      "epoch:3; batch 15360; train accuracy: 0.788135\n",
      "epoch 3; batch 15488; loss 0.187687\n",
      "epoch:3; batch 15488; train accuracy: 0.788211\n",
      "epoch 3; batch 15616; loss 0.240811\n",
      "epoch:3; batch 15616; train accuracy: 0.788278\n",
      "epoch 3; batch 15744; loss 0.269760\n",
      "epoch:3; batch 15744; train accuracy: 0.788340\n",
      "epoch 3; batch 15872; loss 0.209552\n",
      "epoch:3; batch 15872; train accuracy: 0.788416\n",
      "epoch 3; batch 16000; loss 0.305746\n",
      "epoch:3; batch 16000; train accuracy: 0.788469\n",
      "epoch 3; batch 16128; loss 0.128777\n",
      "epoch:3; batch 16128; train accuracy: 0.788567\n",
      "epoch 3; batch 16256; loss 0.193196\n",
      "epoch:3; batch 16256; train accuracy: 0.788651\n",
      "epoch 3; batch 16384; loss 0.166354\n",
      "epoch:3; batch 16384; train accuracy: 0.788740\n",
      "epoch 3; batch 16512; loss 0.182901\n",
      "epoch:3; batch 16512; train accuracy: 0.788811\n",
      "epoch 3; batch 16640; loss 0.187407\n",
      "epoch:3; batch 16640; train accuracy: 0.788904\n",
      "epoch 3; batch 16768; loss 0.119894\n",
      "epoch:3; batch 16768; train accuracy: 0.788996\n",
      "epoch 3; batch 16896; loss 0.203190\n",
      "epoch:3; batch 16896; train accuracy: 0.789071\n",
      "epoch 3; batch 17024; loss 0.140958\n",
      "epoch:3; batch 17024; train accuracy: 0.789164\n",
      "epoch 3; batch 17152; loss 0.151534\n",
      "epoch:3; batch 17152; train accuracy: 0.789247\n",
      "epoch 3; batch 17280; loss 0.142957\n",
      "epoch:3; batch 17280; train accuracy: 0.789344\n",
      "epoch 3; batch 17408; loss 0.154820\n",
      "epoch:3; batch 17408; train accuracy: 0.789432\n",
      "epoch 3; batch 17536; loss 0.236775\n",
      "epoch:3; batch 17536; train accuracy: 0.789498\n",
      "epoch 3; batch 17664; loss 0.204725\n",
      "epoch:3; batch 17664; train accuracy: 0.789563\n",
      "epoch 3; batch 17792; loss 0.123926\n",
      "epoch:3; batch 17792; train accuracy: 0.789651\n",
      "epoch 3; batch 17920; loss 0.189236\n",
      "epoch:3; batch 17920; train accuracy: 0.789730\n",
      "epoch 3; batch 18048; loss 0.204732\n",
      "epoch:3; batch 18048; train accuracy: 0.789808\n",
      "epoch 3; batch 18176; loss 0.164005\n",
      "epoch:3; batch 18176; train accuracy: 0.789891\n",
      "epoch 3; batch 18304; loss 0.204363\n",
      "epoch:3; batch 18304; train accuracy: 0.789978\n",
      "epoch 3; batch 18432; loss 0.126264\n",
      "epoch:3; batch 18432; train accuracy: 0.790070\n",
      "epoch 3; batch 18560; loss 0.127854\n",
      "epoch:3; batch 18560; train accuracy: 0.790161\n",
      "epoch 3; batch 18688; loss 0.227640\n",
      "epoch:3; batch 18688; train accuracy: 0.790226\n",
      "epoch 3; batch 18816; loss 0.145232\n",
      "epoch:3; batch 18816; train accuracy: 0.790317\n",
      "epoch 3; batch 18944; loss 0.164594\n",
      "epoch:3; batch 18944; train accuracy: 0.790400\n",
      "epoch 3; batch 19072; loss 0.191606\n",
      "epoch:3; batch 19072; train accuracy: 0.790477\n",
      "epoch 3; batch 19200; loss 0.138045\n",
      "epoch:3; batch 19200; train accuracy: 0.790573\n",
      "epoch 3; batch 19328; loss 0.177866\n",
      "epoch:3; batch 19328; train accuracy: 0.790655\n",
      "epoch 3; batch 19456; loss 0.387877\n",
      "epoch:3; batch 19456; train accuracy: 0.790719\n",
      "epoch 3; batch 19584; loss 0.112935\n",
      "epoch:3; batch 19584; train accuracy: 0.790805\n",
      "epoch 3; batch 19712; loss 0.192421\n",
      "epoch:3; batch 19712; train accuracy: 0.790874\n",
      "epoch 3; batch 19840; loss 0.184867\n",
      "epoch:3; batch 19840; train accuracy: 0.790956\n",
      "epoch 3; batch 19968; loss 0.243723\n",
      "epoch:3; batch 19968; train accuracy: 0.791016\n",
      "epoch 3; batch 20096; loss 0.073631\n",
      "epoch:3; batch 20096; train accuracy: 0.791128\n",
      "epoch 3; batch 20224; loss 0.171940\n",
      "epoch:3; batch 20224; train accuracy: 0.791192\n",
      "epoch 3; batch 20352; loss 0.183839\n",
      "epoch:3; batch 20352; train accuracy: 0.791277\n",
      "epoch 3; batch 20480; loss 0.196418\n",
      "epoch:3; batch 20480; train accuracy: 0.791363\n",
      "epoch 3; batch 20608; loss 0.144363\n",
      "epoch:3; batch 20608; train accuracy: 0.791448\n",
      "epoch 3; batch 20736; loss 0.105673\n",
      "epoch:3; batch 20736; train accuracy: 0.791542\n",
      "epoch 3; batch 20864; loss 0.196144\n",
      "epoch:3; batch 20864; train accuracy: 0.791623\n",
      "epoch 3; batch 20992; loss 0.132722\n",
      "epoch:3; batch 20992; train accuracy: 0.791713\n",
      "epoch 3; batch 21120; loss 0.080079\n",
      "epoch:3; batch 21120; train accuracy: 0.791811\n",
      "epoch 3; batch 21248; loss 0.225856\n",
      "epoch:3; batch 21248; train accuracy: 0.791883\n",
      "epoch 3; batch 21376; loss 0.319178\n",
      "epoch:3; batch 21376; train accuracy: 0.791959\n",
      "epoch 3; batch 21504; loss 0.195934\n",
      "epoch:3; batch 21504; train accuracy: 0.792027\n",
      "epoch 3; batch 21632; loss 0.185920\n",
      "epoch:3; batch 21632; train accuracy: 0.792112\n",
      "epoch 3; batch 21760; loss 0.191598\n",
      "epoch:3; batch 21760; train accuracy: 0.792192\n",
      "epoch 3; batch 21888; loss 0.232504\n",
      "epoch:3; batch 21888; train accuracy: 0.792268\n",
      "epoch 3; batch 22016; loss 0.177466\n",
      "epoch:3; batch 22016; train accuracy: 0.792356\n",
      "epoch 3; batch 22144; loss 0.142414\n",
      "epoch:3; batch 22144; train accuracy: 0.792437\n",
      "epoch 3; batch 22272; loss 0.146874\n",
      "epoch:3; batch 22272; train accuracy: 0.792508\n",
      "epoch 3; batch 22400; loss 0.155222\n",
      "epoch:3; batch 22400; train accuracy: 0.792588\n",
      "epoch 3; batch 22528; loss 0.106737\n",
      "epoch:3; batch 22528; train accuracy: 0.792676\n",
      "epoch 3; batch 22656; loss 0.156594\n",
      "epoch:3; batch 22656; train accuracy: 0.792765\n",
      "epoch 3; batch 22784; loss 0.188128\n",
      "epoch:3; batch 22784; train accuracy: 0.792831\n",
      "epoch 3; batch 22912; loss 0.142990\n",
      "epoch:3; batch 22912; train accuracy: 0.792915\n",
      "epoch 3; batch 23040; loss 0.149902\n",
      "epoch:3; batch 23040; train accuracy: 0.793007\n",
      "epoch 3; batch 23168; loss 0.211477\n",
      "epoch:3; batch 23168; train accuracy: 0.793087\n",
      "epoch 3; batch 23296; loss 0.191033\n",
      "epoch:3; batch 23296; train accuracy: 0.793149\n",
      "epoch 3; batch 23424; loss 0.187643\n",
      "epoch:3; batch 23424; train accuracy: 0.793228\n",
      "epoch 3; batch 23552; loss 0.127910\n",
      "epoch:3; batch 23552; train accuracy: 0.793316\n",
      "epoch 3; batch 23680; loss 0.202903\n",
      "epoch:3; batch 23680; train accuracy: 0.793395\n",
      "epoch 3; batch 23808; loss 0.157127\n",
      "epoch:3; batch 23808; train accuracy: 0.793474\n",
      "epoch 3; batch 23936; loss 0.211926\n",
      "epoch:3; batch 23936; train accuracy: 0.793552\n",
      "epoch 3; batch 24064; loss 0.126774\n",
      "epoch:3; batch 24064; train accuracy: 0.793644\n",
      "epoch 3; batch 24192; loss 0.136719\n",
      "epoch:3; batch 24192; train accuracy: 0.793727\n",
      "epoch 3; batch 24320; loss 0.097144\n",
      "epoch:3; batch 24320; train accuracy: 0.793831\n",
      "epoch 3; batch 24448; loss 0.082633\n",
      "epoch:3; batch 24448; train accuracy: 0.793927\n",
      "epoch 3; batch 24576; loss 0.117053\n",
      "epoch:3; batch 24576; train accuracy: 0.794018\n",
      "epoch 3; batch 24704; loss 0.159455\n",
      "epoch:3; batch 24704; train accuracy: 0.794092\n",
      "epoch 3; batch 24832; loss 0.165642\n",
      "epoch:3; batch 24832; train accuracy: 0.794170\n",
      "epoch 3; batch 24960; loss 0.135320\n",
      "epoch:3; batch 24960; train accuracy: 0.794252\n",
      "epoch 3; batch 25088; loss 0.073679\n",
      "epoch:3; batch 25088; train accuracy: 0.794347\n",
      "epoch 3; batch 25216; loss 0.102275\n",
      "epoch:3; batch 25216; train accuracy: 0.794438\n",
      "epoch 3; batch 25344; loss 0.142404\n",
      "epoch:3; batch 25344; train accuracy: 0.794516\n",
      "epoch 3; batch 25472; loss 0.124471\n",
      "epoch:3; batch 25472; train accuracy: 0.794602\n",
      "epoch 3; batch 25600; loss 0.161951\n",
      "epoch:3; batch 25600; train accuracy: 0.794693\n",
      "epoch 3; batch 25728; loss 0.133906\n",
      "epoch:3; batch 25728; train accuracy: 0.794779\n",
      "epoch 3; batch 25856; loss 0.137415\n",
      "epoch:3; batch 25856; train accuracy: 0.794860\n",
      "epoch 3; batch 25984; loss 0.155946\n",
      "epoch:3; batch 25984; train accuracy: 0.794950\n",
      "epoch 3; batch 26112; loss 0.177397\n",
      "epoch:3; batch 26112; train accuracy: 0.795040\n",
      "epoch 3; batch 26240; loss 0.110930\n",
      "epoch:3; batch 26240; train accuracy: 0.795130\n",
      "epoch 3; batch 26368; loss 0.123588\n",
      "epoch:3; batch 26368; train accuracy: 0.795216\n",
      "epoch 3; batch 26496; loss 0.150858\n",
      "epoch:3; batch 26496; train accuracy: 0.795293\n",
      "epoch 3; batch 26624; loss 0.124060\n",
      "epoch:3; batch 26624; train accuracy: 0.795387\n",
      "epoch 3; batch 26752; loss 0.225446\n",
      "epoch:3; batch 26752; train accuracy: 0.795438\n",
      "epoch 3; batch 26880; loss 0.181069\n",
      "epoch:3; batch 26880; train accuracy: 0.795515\n",
      "epoch 3; batch 27008; loss 0.105449\n",
      "epoch:3; batch 27008; train accuracy: 0.795600\n",
      "epoch 3; batch 27136; loss 0.126911\n",
      "epoch:3; batch 27136; train accuracy: 0.795702\n",
      "epoch 3; batch 27264; loss 0.192016\n",
      "epoch:3; batch 27264; train accuracy: 0.795783\n",
      "epoch 3; batch 27392; loss 0.090116\n",
      "epoch:3; batch 27392; train accuracy: 0.795876\n",
      "epoch 3; batch 27520; loss 0.186546\n",
      "epoch:3; batch 27520; train accuracy: 0.795953\n",
      "epoch 3; batch 27648; loss 0.127628\n",
      "epoch:3; batch 27648; train accuracy: 0.796042\n",
      "epoch 3; batch 27776; loss 0.160806\n",
      "epoch:3; batch 27776; train accuracy: 0.796130\n",
      "epoch 3; batch 27904; loss 0.203295\n",
      "epoch:3; batch 27904; train accuracy: 0.796206\n",
      "epoch 3; batch 28032; loss 0.133275\n",
      "epoch:3; batch 28032; train accuracy: 0.796282\n",
      "epoch 3; batch 28160; loss 0.272431\n",
      "epoch:3; batch 28160; train accuracy: 0.796354\n",
      "epoch 3; batch 28288; loss 0.125349\n",
      "epoch:3; batch 28288; train accuracy: 0.796438\n",
      "epoch 3; batch 28416; loss 0.138838\n",
      "epoch:3; batch 28416; train accuracy: 0.796523\n",
      "epoch 3; batch 28544; loss 0.189210\n",
      "epoch:3; batch 28544; train accuracy: 0.796602\n",
      "epoch 3; batch 28672; loss 0.148706\n",
      "epoch:3; batch 28672; train accuracy: 0.796678\n",
      "epoch 3; batch 28800; loss 0.306286\n",
      "epoch:3; batch 28800; train accuracy: 0.796741\n",
      "epoch 3; batch 28928; loss 0.170456\n",
      "epoch:3; batch 28928; train accuracy: 0.796821\n",
      "epoch 3; batch 29056; loss 0.168824\n",
      "epoch:3; batch 29056; train accuracy: 0.796896\n",
      "epoch 3; batch 29184; loss 0.103988\n",
      "epoch:3; batch 29184; train accuracy: 0.796984\n",
      "epoch 3; batch 29312; loss 0.181315\n",
      "epoch:3; batch 29312; train accuracy: 0.797051\n",
      "epoch 3; batch 29440; loss 0.109474\n",
      "epoch:3; batch 29440; train accuracy: 0.797138\n",
      "epoch 3; batch 29568; loss 0.220404\n",
      "epoch:3; batch 29568; train accuracy: 0.797201\n",
      "epoch 3; batch 29696; loss 0.181785\n",
      "epoch:3; batch 29696; train accuracy: 0.797276\n",
      "epoch 3; batch 29824; loss 0.150757\n",
      "epoch:3; batch 29824; train accuracy: 0.797363\n",
      "epoch 3; batch 29952; loss 0.079315\n",
      "epoch:3; batch 29952; train accuracy: 0.797467\n",
      "epoch 3; batch 30080; loss 0.127306\n",
      "epoch:3; batch 30080; train accuracy: 0.797562\n",
      "epoch 3; batch 30208; loss 0.244564\n",
      "epoch:3; batch 30208; train accuracy: 0.797625\n",
      "epoch 3; batch 30336; loss 0.167980\n",
      "epoch:3; batch 30336; train accuracy: 0.797699\n",
      "epoch 3; batch 30464; loss 0.092166\n",
      "epoch:3; batch 30464; train accuracy: 0.797790\n",
      "epoch 3; batch 30592; loss 0.117943\n",
      "epoch:3; batch 30592; train accuracy: 0.797885\n",
      "epoch 3; batch 30720; loss 0.177772\n",
      "epoch:3; batch 30720; train accuracy: 0.797964\n",
      "epoch 3; batch 30848; loss 0.315192\n",
      "epoch:3; batch 30848; train accuracy: 0.798021\n",
      "epoch 3; batch 30976; loss 0.186383\n",
      "epoch:3; batch 30976; train accuracy: 0.798091\n",
      "epoch 3; batch 31104; loss 0.145537\n",
      "epoch:3; batch 31104; train accuracy: 0.798169\n",
      "epoch 3; batch 31232; loss 0.166600\n",
      "epoch:3; batch 31232; train accuracy: 0.798260\n",
      "epoch 3; batch 31360; loss 0.186409\n",
      "epoch:3; batch 31360; train accuracy: 0.798326\n",
      "epoch 3; batch 31488; loss 0.262060\n",
      "epoch:3; batch 31488; train accuracy: 0.798387\n",
      "epoch 3; batch 31616; loss 0.131850\n",
      "epoch:3; batch 31616; train accuracy: 0.798469\n",
      "epoch 3; batch 31744; loss 0.097973\n",
      "epoch:3; batch 31744; train accuracy: 0.798551\n",
      "epoch 3; batch 31872; loss 0.151947\n",
      "epoch:3; batch 31872; train accuracy: 0.798624\n",
      "epoch 3; batch 32000; loss 0.145992\n",
      "epoch:3; batch 32000; train accuracy: 0.798702\n",
      "epoch 3; batch 32128; loss 0.099106\n",
      "epoch:3; batch 32128; train accuracy: 0.798784\n",
      "epoch 3; batch 32256; loss 0.081591\n",
      "epoch:3; batch 32256; train accuracy: 0.798882\n",
      "epoch 3; batch 32384; loss 0.138067\n",
      "epoch:3; batch 32384; train accuracy: 0.798963\n",
      "epoch 3; batch 32512; loss 0.113109\n",
      "epoch:3; batch 32512; train accuracy: 0.799057\n",
      "epoch 3; batch 32640; loss 0.127046\n",
      "epoch:3; batch 32640; train accuracy: 0.799147\n",
      "epoch 3; batch 32768; loss 0.116506\n",
      "epoch:3; batch 32768; train accuracy: 0.799232\n",
      "epoch 3; batch 32896; loss 0.123137\n",
      "epoch:3; batch 32896; train accuracy: 0.799317\n",
      "epoch 3; batch 33024; loss 0.069663\n",
      "epoch:3; batch 33024; train accuracy: 0.799406\n",
      "epoch 3; batch 33152; loss 0.140614\n",
      "epoch:3; batch 33152; train accuracy: 0.799483\n",
      "epoch 3; batch 33280; loss 0.130206\n",
      "epoch:3; batch 33280; train accuracy: 0.799564\n",
      "epoch 3; batch 33408; loss 0.128053\n",
      "epoch:3; batch 33408; train accuracy: 0.799645\n",
      "epoch 3; batch 33536; loss 0.181996\n",
      "epoch:3; batch 33536; train accuracy: 0.799717\n",
      "epoch 3; batch 33664; loss 0.129968\n",
      "epoch:3; batch 33664; train accuracy: 0.799798\n",
      "epoch 3; batch 33792; loss 0.151198\n",
      "epoch:3; batch 33792; train accuracy: 0.799883\n",
      "epoch 3; batch 33920; loss 0.167785\n",
      "epoch:3; batch 33920; train accuracy: 0.799938\n",
      "epoch 3; batch 34048; loss 0.217528\n",
      "epoch:3; batch 34048; train accuracy: 0.800002\n",
      "epoch 3; batch 34176; loss 0.144817\n",
      "epoch:3; batch 34176; train accuracy: 0.800070\n",
      "epoch 3; batch 34304; loss 0.074556\n",
      "epoch:3; batch 34304; train accuracy: 0.800163\n",
      "epoch 3; batch 34432; loss 0.133381\n",
      "epoch:3; batch 34432; train accuracy: 0.800243\n",
      "epoch 3; batch 34560; loss 0.076359\n",
      "epoch:3; batch 34560; train accuracy: 0.800335\n",
      "epoch 3; batch 34688; loss 0.116682\n",
      "epoch:3; batch 34688; train accuracy: 0.800424\n",
      "epoch 3; batch 34816; loss 0.075851\n",
      "epoch:3; batch 34816; train accuracy: 0.800520\n",
      "epoch 3; batch 34944; loss 0.145991\n",
      "epoch:3; batch 34944; train accuracy: 0.800591\n",
      "epoch 3; batch 35072; loss 0.115343\n",
      "epoch:3; batch 35072; train accuracy: 0.800667\n",
      "epoch 3; batch 35200; loss 0.171975\n",
      "epoch:3; batch 35200; train accuracy: 0.800738\n",
      "epoch 3; batch 35328; loss 0.124833\n",
      "epoch:3; batch 35328; train accuracy: 0.800818\n",
      "epoch 3; batch 35456; loss 0.124432\n",
      "epoch:3; batch 35456; train accuracy: 0.800901\n",
      "epoch 3; batch 35584; loss 0.208060\n",
      "epoch:3; batch 35584; train accuracy: 0.800960\n",
      "epoch 3; batch 35712; loss 0.176402\n",
      "epoch:3; batch 35712; train accuracy: 0.801028\n",
      "epoch 3; batch 35840; loss 0.110101\n",
      "epoch:3; batch 35840; train accuracy: 0.801115\n",
      "epoch 3; batch 35968; loss 0.166580\n",
      "epoch:3; batch 35968; train accuracy: 0.801194\n",
      "epoch 3; batch 36096; loss 0.134072\n",
      "epoch:3; batch 36096; train accuracy: 0.801269\n",
      "epoch 3; batch 36224; loss 0.125837\n",
      "epoch:3; batch 36224; train accuracy: 0.801352\n",
      "epoch 3; batch 36352; loss 0.193404\n",
      "epoch:3; batch 36352; train accuracy: 0.801419\n",
      "epoch 3; batch 36480; loss 0.143973\n",
      "epoch:3; batch 36480; train accuracy: 0.801485\n",
      "epoch 3; batch 36608; loss 0.147127\n",
      "epoch:3; batch 36608; train accuracy: 0.801564\n",
      "epoch 3; batch 36736; loss 0.127045\n",
      "epoch:3; batch 36736; train accuracy: 0.801643\n",
      "epoch 3; batch 36864; loss 0.155850\n",
      "epoch:3; batch 36864; train accuracy: 0.801717\n",
      "epoch 3; batch 36992; loss 0.096750\n",
      "epoch:3; batch 36992; train accuracy: 0.801804\n",
      "epoch 3; batch 37120; loss 0.148834\n",
      "epoch:3; batch 37120; train accuracy: 0.801878\n",
      "epoch 3; batch 37248; loss 0.173769\n",
      "epoch:3; batch 37248; train accuracy: 0.801948\n",
      "epoch 3; batch 37376; loss 0.228266\n",
      "epoch:3; batch 37376; train accuracy: 0.802011\n",
      "epoch 3; batch 37504; loss 0.140509\n",
      "epoch:3; batch 37504; train accuracy: 0.802089\n",
      "epoch 3; batch 37632; loss 0.135360\n",
      "epoch:3; batch 37632; train accuracy: 0.802163\n",
      "epoch 3; batch 37760; loss 0.133400\n",
      "epoch:3; batch 37760; train accuracy: 0.802241\n",
      "epoch 3; batch 37888; loss 0.165404\n",
      "epoch:3; batch 37888; train accuracy: 0.802307\n",
      "epoch 3; batch 38016; loss 0.115410\n",
      "epoch:3; batch 38016; train accuracy: 0.802389\n",
      "epoch 3; batch 38144; loss 0.165945\n",
      "epoch:3; batch 38144; train accuracy: 0.802458\n",
      "epoch 3; batch 38272; loss 0.171588\n",
      "epoch:3; batch 38272; train accuracy: 0.802532\n",
      "epoch 3; batch 38400; loss 0.148030\n",
      "epoch:3; batch 38400; train accuracy: 0.802606\n",
      "epoch 3; batch 38528; loss 0.129861\n",
      "epoch:3; batch 38528; train accuracy: 0.802687\n",
      "epoch 3; batch 38656; loss 0.098179\n",
      "epoch:3; batch 38656; train accuracy: 0.802773\n",
      "epoch 3; batch 38784; loss 0.059157\n",
      "epoch:3; batch 38784; train accuracy: 0.802858\n",
      "epoch 3; batch 38912; loss 0.169616\n",
      "epoch:3; batch 38912; train accuracy: 0.802927\n",
      "epoch 3; batch 39040; loss 0.131901\n",
      "epoch:3; batch 39040; train accuracy: 0.803001\n",
      "epoch 3; batch 39168; loss 0.171170\n",
      "epoch:3; batch 39168; train accuracy: 0.803070\n",
      "epoch 3; batch 39296; loss 0.072847\n",
      "epoch:3; batch 39296; train accuracy: 0.803159\n",
      "epoch 3; batch 39424; loss 0.249132\n",
      "epoch:3; batch 39424; train accuracy: 0.803228\n",
      "epoch 3; batch 39552; loss 0.120813\n",
      "epoch:3; batch 39552; train accuracy: 0.803309\n",
      "epoch 3; batch 39680; loss 0.094066\n",
      "epoch:3; batch 39680; train accuracy: 0.803389\n",
      "epoch 3; batch 39808; loss 0.192352\n",
      "epoch:3; batch 39808; train accuracy: 0.803454\n",
      "epoch 3; batch 39936; loss 0.171221\n",
      "epoch:3; batch 39936; train accuracy: 0.803523\n",
      "epoch 3; batch 40064; loss 0.115370\n",
      "epoch:3; batch 40064; train accuracy: 0.803603\n",
      "epoch 3; batch 40192; loss 0.125509\n",
      "epoch:3; batch 40192; train accuracy: 0.803680\n",
      "epoch 3; batch 40320; loss 0.102951\n",
      "epoch:3; batch 40320; train accuracy: 0.803764\n",
      "epoch 3; batch 40448; loss 0.164419\n",
      "epoch:3; batch 40448; train accuracy: 0.803845\n",
      "epoch 3; batch 40576; loss 0.097444\n",
      "epoch:3; batch 40576; train accuracy: 0.803929\n",
      "epoch 3; batch 40704; loss 0.113855\n",
      "epoch:3; batch 40704; train accuracy: 0.804009\n",
      "epoch 3; batch 40832; loss 0.143602\n",
      "epoch:3; batch 40832; train accuracy: 0.804073\n",
      "epoch 3; batch 40960; loss 0.155330\n",
      "epoch:3; batch 40960; train accuracy: 0.804141\n",
      "epoch 3; batch 41088; loss 0.088045\n",
      "epoch:3; batch 41088; train accuracy: 0.804225\n",
      "epoch 3; batch 41216; loss 0.157269\n",
      "epoch:3; batch 41216; train accuracy: 0.804293\n",
      "epoch 3; batch 41344; loss 0.120109\n",
      "epoch:3; batch 41344; train accuracy: 0.804369\n",
      "epoch 3; batch 41472; loss 0.088699\n",
      "epoch:3; batch 41472; train accuracy: 0.804457\n",
      "epoch 3; batch 41600; loss 0.093021\n",
      "epoch:3; batch 41600; train accuracy: 0.804536\n",
      "epoch 3; batch 41728; loss 0.241774\n",
      "epoch:3; batch 41728; train accuracy: 0.804592\n",
      "epoch 3; batch 41856; loss 0.122594\n",
      "epoch:3; batch 41856; train accuracy: 0.804672\n",
      "epoch 3; batch 41984; loss 0.119654\n",
      "epoch:3; batch 41984; train accuracy: 0.804747\n",
      "epoch 3; batch 42112; loss 0.122763\n",
      "epoch:3; batch 42112; train accuracy: 0.804814\n",
      "epoch 3; batch 42240; loss 0.134398\n",
      "epoch:3; batch 42240; train accuracy: 0.804890\n",
      "epoch 3; batch 42368; loss 0.194466\n",
      "epoch:3; batch 42368; train accuracy: 0.804961\n",
      "epoch 3; batch 42496; loss 0.104807\n",
      "epoch:3; batch 42496; train accuracy: 0.805044\n",
      "epoch 3; batch 42624; loss 0.064855\n",
      "epoch:3; batch 42624; train accuracy: 0.805127\n",
      "epoch 3; batch 42752; loss 0.136653\n",
      "epoch:3; batch 42752; train accuracy: 0.805210\n",
      "epoch 3; batch 42880; loss 0.122906\n",
      "epoch:3; batch 42880; train accuracy: 0.805277\n",
      "epoch 3; batch 43008; loss 0.201468\n",
      "epoch:3; batch 43008; train accuracy: 0.805336\n",
      "epoch 3; batch 43136; loss 0.124276\n",
      "epoch:3; batch 43136; train accuracy: 0.805415\n",
      "epoch 3; batch 43264; loss 0.090832\n",
      "epoch:3; batch 43264; train accuracy: 0.805493\n",
      "epoch 3; batch 43392; loss 0.163093\n",
      "epoch:3; batch 43392; train accuracy: 0.805564\n",
      "epoch 3; batch 43520; loss 0.180479\n",
      "epoch:3; batch 43520; train accuracy: 0.805623\n",
      "epoch 3; batch 43648; loss 0.107358\n",
      "epoch:3; batch 43648; train accuracy: 0.805697\n",
      "epoch 3; batch 43776; loss 0.175209\n",
      "epoch:3; batch 43776; train accuracy: 0.805760\n",
      "epoch 3; batch 43904; loss 0.129287\n",
      "epoch:3; batch 43904; train accuracy: 0.805826\n",
      "epoch 3; batch 44032; loss 0.132174\n",
      "epoch:3; batch 44032; train accuracy: 0.805896\n",
      "epoch 3; batch 44160; loss 0.146235\n",
      "epoch:3; batch 44160; train accuracy: 0.805971\n",
      "epoch 3; batch 44288; loss 0.200446\n",
      "epoch:3; batch 44288; train accuracy: 0.806033\n",
      "epoch 3; batch 44416; loss 0.137426\n",
      "epoch:3; batch 44416; train accuracy: 0.806107\n",
      "epoch 3; batch 44544; loss 0.204388\n",
      "epoch:3; batch 44544; train accuracy: 0.806161\n",
      "epoch 3; batch 44672; loss 0.124592\n",
      "epoch:3; batch 44672; train accuracy: 0.806223\n",
      "epoch 3; batch 44800; loss 0.134693\n",
      "epoch:3; batch 44800; train accuracy: 0.806293\n",
      "epoch 3; batch 44928; loss 0.121097\n",
      "epoch:3; batch 44928; train accuracy: 0.806367\n",
      "epoch 3; batch 45056; loss 0.199164\n",
      "epoch:3; batch 45056; train accuracy: 0.806433\n",
      "epoch 3; batch 45184; loss 0.078182\n",
      "epoch:3; batch 45184; train accuracy: 0.806522\n",
      "epoch 3; batch 45312; loss 0.127373\n",
      "epoch:3; batch 45312; train accuracy: 0.806584\n",
      "epoch 3; batch 45440; loss 0.105524\n",
      "epoch:3; batch 45440; train accuracy: 0.806657\n",
      "epoch 3; batch 45568; loss 0.096863\n",
      "epoch:3; batch 45568; train accuracy: 0.806738\n",
      "epoch 3; batch 45696; loss 0.122073\n",
      "epoch:3; batch 45696; train accuracy: 0.806812\n",
      "epoch 3; batch 45824; loss 0.150478\n",
      "epoch:3; batch 45824; train accuracy: 0.806885\n",
      "epoch 3; batch 45952; loss 0.070449\n",
      "epoch:3; batch 45952; train accuracy: 0.806970\n",
      "epoch 3; batch 46080; loss 0.124777\n",
      "epoch:3; batch 46080; train accuracy: 0.807047\n",
      "epoch 3; batch 46208; loss 0.110822\n",
      "epoch:3; batch 46208; train accuracy: 0.807128\n",
      "epoch 3; batch 46336; loss 0.136024\n",
      "epoch:3; batch 46336; train accuracy: 0.807208\n",
      "epoch 3; batch 46464; loss 0.124208\n",
      "epoch:3; batch 46464; train accuracy: 0.807285\n",
      "epoch 3; batch 46592; loss 0.073605\n",
      "epoch:3; batch 46592; train accuracy: 0.807370\n",
      "epoch 3; batch 46720; loss 0.166506\n",
      "epoch:3; batch 46720; train accuracy: 0.807431\n",
      "epoch 3; batch 46848; loss 0.143377\n",
      "epoch:3; batch 46848; train accuracy: 0.807495\n",
      "epoch 3; batch 46976; loss 0.071773\n",
      "epoch:3; batch 46976; train accuracy: 0.807580\n",
      "epoch 3; batch 47104; loss 0.107053\n",
      "epoch:3; batch 47104; train accuracy: 0.807652\n",
      "epoch 3; batch 47232; loss 0.227262\n",
      "epoch:3; batch 47232; train accuracy: 0.807729\n",
      "epoch 3; batch 47360; loss 0.130931\n",
      "epoch:3; batch 47360; train accuracy: 0.807809\n",
      "epoch 3; batch 47488; loss 0.094435\n",
      "epoch:3; batch 47488; train accuracy: 0.807885\n",
      "epoch 3; batch 47616; loss 0.116346\n",
      "epoch:3; batch 47616; train accuracy: 0.807961\n",
      "epoch 3; batch 47744; loss 0.136998\n",
      "epoch:3; batch 47744; train accuracy: 0.808029\n",
      "epoch 3; batch 47872; loss 0.077337\n",
      "epoch:3; batch 47872; train accuracy: 0.808109\n",
      "epoch 3; batch 48000; loss 0.155034\n",
      "epoch:3; batch 48000; train accuracy: 0.808192\n",
      "epoch 3; batch 48128; loss 0.162934\n",
      "epoch:3; batch 48128; train accuracy: 0.808261\n",
      "epoch 3; batch 48256; loss 0.095372\n",
      "epoch:3; batch 48256; train accuracy: 0.808336\n",
      "epoch 3; batch 48384; loss 0.117864\n",
      "epoch:3; batch 48384; train accuracy: 0.808408\n",
      "epoch 3; batch 48512; loss 0.082029\n",
      "epoch:3; batch 48512; train accuracy: 0.808491\n",
      "epoch 3; batch 48640; loss 0.147197\n",
      "epoch:3; batch 48640; train accuracy: 0.808555\n",
      "epoch 3; batch 48768; loss 0.101108\n",
      "epoch:3; batch 48768; train accuracy: 0.808638\n",
      "epoch 3; batch 48896; loss 0.075756\n",
      "epoch:3; batch 48896; train accuracy: 0.808725\n",
      "epoch 3; batch 49024; loss 0.077695\n",
      "epoch:3; batch 49024; train accuracy: 0.808812\n",
      "epoch 3; batch 49152; loss 0.097885\n",
      "epoch:3; batch 49152; train accuracy: 0.808887\n",
      "epoch 3; batch 49280; loss 0.134768\n",
      "epoch:3; batch 49280; train accuracy: 0.808958\n",
      "epoch 3; batch 49408; loss 0.207588\n",
      "epoch:3; batch 49408; train accuracy: 0.809022\n",
      "epoch 3; batch 49536; loss 0.160863\n",
      "epoch:3; batch 49536; train accuracy: 0.809093\n",
      "epoch 3; batch 49664; loss 0.155671\n",
      "epoch:3; batch 49664; train accuracy: 0.809160\n",
      "epoch 3; batch 49792; loss 0.046270\n",
      "epoch:3; batch 49792; train accuracy: 0.809246\n",
      "epoch 3; batch 49920; loss 0.084868\n",
      "epoch:3; batch 49920; train accuracy: 0.809325\n",
      "epoch 3; batch 50048; loss 0.166223\n",
      "epoch:3; batch 50048; train accuracy: 0.809396\n",
      "epoch 3; batch 50176; loss 0.137224\n",
      "epoch:3; batch 50176; train accuracy: 0.809459\n",
      "epoch 3; batch 50304; loss 0.082345\n",
      "epoch:3; batch 50304; train accuracy: 0.809537\n",
      "epoch 3; batch 50432; loss 0.130044\n",
      "epoch:3; batch 50432; train accuracy: 0.809608\n",
      "epoch 3; batch 50560; loss 0.087824\n",
      "epoch:3; batch 50560; train accuracy: 0.809686\n",
      "epoch 3; batch 50688; loss 0.051325\n",
      "epoch:3; batch 50688; train accuracy: 0.809776\n",
      "epoch 3; batch 50816; loss 0.185618\n",
      "epoch:3; batch 50816; train accuracy: 0.809842\n",
      "epoch 3; batch 50944; loss 0.119415\n",
      "epoch:3; batch 50944; train accuracy: 0.809924\n",
      "epoch 3; batch 51072; loss 0.161424\n",
      "epoch:3; batch 51072; train accuracy: 0.809983\n",
      "epoch 3; batch 51200; loss 0.085967\n",
      "epoch:3; batch 51200; train accuracy: 0.810064\n",
      "epoch 3; batch 51328; loss 0.067112\n",
      "epoch:3; batch 51328; train accuracy: 0.810150\n",
      "epoch 3; batch 51456; loss 0.111507\n",
      "epoch:3; batch 51456; train accuracy: 0.810224\n",
      "epoch 3; batch 51584; loss 0.137392\n",
      "epoch:3; batch 51584; train accuracy: 0.810282\n",
      "epoch 3; batch 51712; loss 0.091997\n",
      "epoch:3; batch 51712; train accuracy: 0.810352\n",
      "epoch 3; batch 51840; loss 0.166522\n",
      "epoch:3; batch 51840; train accuracy: 0.810429\n",
      "epoch 3; batch 51968; loss 0.129066\n",
      "epoch:3; batch 51968; train accuracy: 0.810499\n",
      "epoch 3; batch 52096; loss 0.144768\n",
      "epoch:3; batch 52096; train accuracy: 0.810576\n",
      "epoch 3; batch 52224; loss 0.085365\n",
      "epoch:3; batch 52224; train accuracy: 0.810658\n",
      "epoch 3; batch 52352; loss 0.097887\n",
      "epoch:3; batch 52352; train accuracy: 0.810731\n",
      "epoch 3; batch 52480; loss 0.146428\n",
      "epoch:3; batch 52480; train accuracy: 0.810793\n",
      "epoch 3; batch 52608; loss 0.107606\n",
      "epoch:3; batch 52608; train accuracy: 0.810862\n",
      "epoch 3; batch 52736; loss 0.095290\n",
      "epoch:3; batch 52736; train accuracy: 0.810935\n",
      "epoch 3; batch 52864; loss 0.147495\n",
      "epoch:3; batch 52864; train accuracy: 0.810997\n",
      "epoch 3; batch 52992; loss 0.095019\n",
      "epoch:3; batch 52992; train accuracy: 0.811077\n",
      "epoch 3; batch 53120; loss 0.047383\n",
      "epoch:3; batch 53120; train accuracy: 0.811158\n",
      "epoch 3; batch 53248; loss 0.081665\n",
      "epoch:3; batch 53248; train accuracy: 0.811235\n",
      "epoch 3; batch 53376; loss 0.144681\n",
      "epoch:3; batch 53376; train accuracy: 0.811292\n",
      "epoch 3; batch 53504; loss 0.063293\n",
      "epoch:3; batch 53504; train accuracy: 0.811373\n",
      "epoch 3; batch 53632; loss 0.135793\n",
      "epoch:3; batch 53632; train accuracy: 0.811445\n",
      "epoch 3; batch 53760; loss 0.063499\n",
      "epoch:3; batch 53760; train accuracy: 0.811525\n",
      "epoch 3; batch 53888; loss 0.083457\n",
      "epoch:3; batch 53888; train accuracy: 0.811605\n",
      "epoch 3; batch 54016; loss 0.052364\n",
      "epoch:3; batch 54016; train accuracy: 0.811693\n",
      "epoch 3; batch 54144; loss 0.144600\n",
      "epoch:3; batch 54144; train accuracy: 0.811750\n",
      "epoch 3; batch 54272; loss 0.140427\n",
      "epoch:3; batch 54272; train accuracy: 0.811815\n",
      "epoch 3; batch 54400; loss 0.085073\n",
      "epoch:3; batch 54400; train accuracy: 0.811891\n",
      "epoch 3; batch 54528; loss 0.086195\n",
      "epoch:3; batch 54528; train accuracy: 0.811971\n",
      "epoch 3; batch 54656; loss 0.061176\n",
      "epoch:3; batch 54656; train accuracy: 0.812046\n",
      "epoch 3; batch 54784; loss 0.094641\n",
      "epoch:3; batch 54784; train accuracy: 0.812126\n",
      "epoch 3; batch 54912; loss 0.182336\n",
      "epoch:3; batch 54912; train accuracy: 0.812194\n",
      "epoch 3; batch 55040; loss 0.118903\n",
      "epoch:3; batch 55040; train accuracy: 0.812274\n",
      "epoch 3; batch 55168; loss 0.132175\n",
      "epoch:3; batch 55168; train accuracy: 0.812342\n",
      "epoch 3; batch 55296; loss 0.123621\n",
      "epoch:3; batch 55296; train accuracy: 0.812410\n",
      "epoch 3; batch 55424; loss 0.073679\n",
      "epoch:3; batch 55424; train accuracy: 0.812489\n",
      "epoch 3; batch 55552; loss 0.104818\n",
      "epoch:3; batch 55552; train accuracy: 0.812564\n",
      "epoch 3; batch 55680; loss 0.161211\n",
      "epoch:3; batch 55680; train accuracy: 0.812639\n",
      "epoch 3; batch 55808; loss 0.105727\n",
      "epoch:3; batch 55808; train accuracy: 0.812711\n",
      "epoch 3; batch 55936; loss 0.104019\n",
      "epoch:3; batch 55936; train accuracy: 0.812786\n",
      "epoch 3; batch 56064; loss 0.145348\n",
      "epoch:3; batch 56064; train accuracy: 0.812853\n",
      "epoch 3; batch 56192; loss 0.151542\n",
      "epoch:3; batch 56192; train accuracy: 0.812913\n",
      "epoch 3; batch 56320; loss 0.102648\n",
      "epoch:3; batch 56320; train accuracy: 0.812985\n",
      "epoch 3; batch 56448; loss 0.215059\n",
      "epoch:3; batch 56448; train accuracy: 0.813044\n",
      "epoch 3; batch 56576; loss 0.089195\n",
      "epoch:3; batch 56576; train accuracy: 0.813115\n",
      "epoch 3; batch 56704; loss 0.060619\n",
      "epoch:3; batch 56704; train accuracy: 0.813201\n",
      "epoch 3; batch 56832; loss 0.151760\n",
      "epoch:3; batch 56832; train accuracy: 0.813272\n",
      "epoch 3; batch 56960; loss 0.144175\n",
      "epoch:3; batch 56960; train accuracy: 0.813336\n",
      "epoch 3; batch 57088; loss 0.139318\n",
      "epoch:3; batch 57088; train accuracy: 0.813406\n",
      "epoch 3; batch 57216; loss 0.146904\n",
      "epoch:3; batch 57216; train accuracy: 0.813470\n",
      "epoch 3; batch 57344; loss 0.077575\n",
      "epoch:3; batch 57344; train accuracy: 0.813548\n",
      "epoch 3; batch 57472; loss 0.100698\n",
      "epoch:3; batch 57472; train accuracy: 0.813611\n",
      "epoch 3; batch 57600; loss 0.084758\n",
      "epoch:3; batch 57600; train accuracy: 0.813692\n",
      "epoch 3; batch 57728; loss 0.105366\n",
      "epoch:3; batch 57728; train accuracy: 0.813755\n",
      "epoch 3; batch 57856; loss 0.219441\n",
      "epoch:3; batch 57856; train accuracy: 0.813822\n",
      "epoch 3; batch 57984; loss 0.159595\n",
      "epoch:3; batch 57984; train accuracy: 0.813885\n",
      "epoch 3; batch 58112; loss 0.135176\n",
      "epoch:3; batch 58112; train accuracy: 0.813951\n",
      "epoch 3; batch 58240; loss 0.080848\n",
      "epoch:3; batch 58240; train accuracy: 0.814033\n",
      "epoch 3; batch 58368; loss 0.103840\n",
      "epoch:3; batch 58368; train accuracy: 0.814103\n",
      "epoch 3; batch 58496; loss 0.074523\n",
      "epoch:3; batch 58496; train accuracy: 0.814180\n",
      "epoch 3; batch 58624; loss 0.132284\n",
      "epoch:3; batch 58624; train accuracy: 0.814239\n",
      "epoch 3; batch 58752; loss 0.148802\n",
      "epoch:3; batch 58752; train accuracy: 0.814309\n",
      "epoch 3; batch 58880; loss 0.120359\n",
      "epoch:3; batch 58880; train accuracy: 0.814379\n",
      "epoch 3; batch 59008; loss 0.083589\n",
      "epoch:3; batch 59008; train accuracy: 0.814460\n",
      "epoch 3; batch 59136; loss 0.077554\n",
      "epoch:3; batch 59136; train accuracy: 0.814540\n",
      "epoch 3; batch 59264; loss 0.067405\n",
      "epoch:3; batch 59264; train accuracy: 0.814618\n",
      "epoch 3; batch 59392; loss 0.135190\n",
      "epoch:3; batch 59392; train accuracy: 0.814683\n",
      "epoch 3; batch 59520; loss 0.108790\n",
      "epoch:3; batch 59520; train accuracy: 0.814760\n",
      "epoch 3; batch 59648; loss 0.078986\n",
      "epoch:3; batch 59648; train accuracy: 0.814833\n",
      "epoch 3; batch 59776; loss 0.071508\n",
      "epoch:3; batch 59776; train accuracy: 0.814914\n",
      "epoch 3; batch 59904; loss 0.125728\n",
      "epoch:3; batch 59904; train accuracy: 0.814979\n",
      "epoch 3; batch 60032; loss 0.069308\n",
      "epoch:3; batch 60032; train accuracy: 0.815056\n",
      "epoch 3; batch 60160; loss 0.139348\n",
      "epoch:3; batch 60160; train accuracy: 0.815121\n",
      "epoch 3; batch 60288; loss 0.133338\n",
      "epoch:3; batch 60288; train accuracy: 0.815187\n",
      "epoch 3; batch 60416; loss 0.167199\n",
      "epoch:3; batch 60416; train accuracy: 0.815248\n",
      "epoch 3; batch 60544; loss 0.165069\n",
      "epoch:3; batch 60544; train accuracy: 0.815299\n",
      "epoch 3; batch 60672; loss 0.085985\n",
      "epoch:3; batch 60672; train accuracy: 0.815371\n",
      "epoch 3; batch 60800; loss 0.123192\n",
      "epoch:3; batch 60800; train accuracy: 0.815437\n",
      "epoch 3; batch 60928; loss 0.110828\n",
      "epoch:3; batch 60928; train accuracy: 0.815505\n",
      "epoch 3; batch 61056; loss 0.060179\n",
      "epoch:3; batch 61056; train accuracy: 0.815585\n",
      "epoch 3; batch 61184; loss 0.087252\n",
      "epoch:3; batch 61184; train accuracy: 0.815650\n",
      "epoch 3; batch 61312; loss 0.080647\n",
      "epoch:3; batch 61312; train accuracy: 0.815730\n",
      "epoch 3; batch 61440; loss 0.099606\n",
      "epoch:3; batch 61440; train accuracy: 0.815806\n",
      "epoch 3; batch 61568; loss 0.116597\n",
      "epoch:3; batch 61568; train accuracy: 0.815867\n",
      "epoch 3; batch 61696; loss 0.088365\n",
      "epoch:3; batch 61696; train accuracy: 0.815935\n",
      "epoch 3; batch 61824; loss 0.147400\n",
      "epoch:3; batch 61824; train accuracy: 0.815996\n",
      "epoch 3; batch 61952; loss 0.111966\n",
      "epoch:3; batch 61952; train accuracy: 0.816068\n",
      "epoch 3; batch 62080; loss 0.206114\n",
      "epoch:3; batch 62080; train accuracy: 0.816118\n",
      "epoch 3; batch 62208; loss 0.139248\n",
      "epoch:3; batch 62208; train accuracy: 0.816189\n",
      "epoch 3; batch 62336; loss 0.037483\n",
      "epoch:3; batch 62336; train accuracy: 0.816272\n",
      "epoch 3; batch 62464; loss 0.162540\n",
      "epoch:3; batch 62464; train accuracy: 0.816333\n",
      "epoch 3; batch 62592; loss 0.062191\n",
      "epoch:3; batch 62592; train accuracy: 0.816408\n",
      "epoch 3; batch 62720; loss 0.147834\n",
      "epoch:3; batch 62720; train accuracy: 0.816465\n",
      "epoch 3; batch 62848; loss 0.066942\n",
      "epoch:3; batch 62848; train accuracy: 0.816540\n",
      "epoch 3; batch 62976; loss 0.091994\n",
      "epoch:3; batch 62976; train accuracy: 0.816611\n",
      "epoch 3; batch 63104; loss 0.287839\n",
      "epoch:3; batch 63104; train accuracy: 0.816650\n",
      "epoch 3; batch 63232; loss 0.123545\n",
      "epoch:3; batch 63232; train accuracy: 0.816710\n",
      "epoch 3; batch 63360; loss 0.101195\n",
      "epoch:3; batch 63360; train accuracy: 0.816781\n",
      "epoch 3; batch 63488; loss 0.153656\n",
      "epoch:3; batch 63488; train accuracy: 0.816856\n",
      "epoch 3; batch 63616; loss 0.091420\n",
      "epoch:3; batch 63616; train accuracy: 0.816927\n",
      "epoch 3; batch 63744; loss 0.143175\n",
      "epoch:3; batch 63744; train accuracy: 0.816984\n",
      "epoch 3; batch 63872; loss 0.077110\n",
      "epoch:3; batch 63872; train accuracy: 0.817055\n",
      "epoch 3; batch 64000; loss 0.093941\n",
      "epoch:3; batch 64000; train accuracy: 0.817125\n",
      "epoch 3; batch 64128; loss 0.052015\n",
      "epoch:3; batch 64128; train accuracy: 0.817204\n",
      "epoch 3; batch 64256; loss 0.149107\n",
      "epoch:3; batch 64256; train accuracy: 0.817260\n",
      "epoch 3; batch 64384; loss 0.066104\n",
      "epoch:3; batch 64384; train accuracy: 0.817330\n",
      "epoch 3; batch 64512; loss 0.189983\n",
      "epoch:3; batch 64512; train accuracy: 0.817386\n",
      "epoch 3; batch 64640; loss 0.062125\n",
      "epoch:3; batch 64640; train accuracy: 0.817464\n",
      "epoch 3; batch 64768; loss 0.120569\n",
      "epoch:3; batch 64768; train accuracy: 0.817528\n",
      "epoch 3; batch 64896; loss 0.121635\n",
      "epoch:3; batch 64896; train accuracy: 0.817598\n",
      "epoch 3; batch 65024; loss 0.091300\n",
      "epoch:3; batch 65024; train accuracy: 0.817668\n",
      "epoch 3; batch 65152; loss 0.162019\n",
      "epoch:3; batch 65152; train accuracy: 0.817735\n",
      "epoch 3; batch 65280; loss 0.093349\n",
      "epoch:3; batch 65280; train accuracy: 0.817802\n",
      "epoch 3; batch 65408; loss 0.125619\n",
      "epoch:3; batch 65408; train accuracy: 0.817861\n",
      "epoch 3; batch 65536; loss 0.104766\n",
      "epoch:3; batch 65536; train accuracy: 0.817931\n",
      "epoch 3; batch 65664; loss 0.145477\n",
      "epoch:3; batch 65664; train accuracy: 0.817997\n",
      "epoch 3; batch 65792; loss 0.160420\n",
      "epoch:3; batch 65792; train accuracy: 0.818053\n",
      "epoch 3; batch 65920; loss 0.136471\n",
      "epoch:3; batch 65920; train accuracy: 0.818112\n",
      "epoch 3; batch 66048; loss 0.191357\n",
      "epoch:3; batch 66048; train accuracy: 0.818164\n",
      "epoch 3; batch 66176; loss 0.129149\n",
      "epoch:3; batch 66176; train accuracy: 0.818226\n",
      "epoch 3; batch 66304; loss 0.145419\n",
      "epoch:3; batch 66304; train accuracy: 0.818285\n",
      "epoch 3; batch 66432; loss 0.095094\n",
      "epoch:3; batch 66432; train accuracy: 0.818355\n",
      "epoch 3; batch 66560; loss 0.114394\n",
      "epoch:3; batch 66560; train accuracy: 0.818421\n",
      "epoch 3; batch 66688; loss 0.123664\n",
      "epoch:3; batch 66688; train accuracy: 0.818483\n",
      "epoch 3; batch 66816; loss 0.114916\n",
      "epoch:3; batch 66816; train accuracy: 0.818549\n",
      "epoch 3; batch 66944; loss 0.086734\n",
      "epoch:3; batch 66944; train accuracy: 0.818622\n",
      "epoch 3; batch 67072; loss 0.089043\n",
      "epoch:3; batch 67072; train accuracy: 0.818684\n",
      "epoch 3; batch 67200; loss 0.063291\n",
      "epoch:3; batch 67200; train accuracy: 0.818754\n",
      "epoch 3; batch 67328; loss 0.146456\n",
      "epoch:3; batch 67328; train accuracy: 0.818812\n",
      "epoch 3; batch 67456; loss 0.076184\n",
      "epoch:3; batch 67456; train accuracy: 0.818885\n",
      "epoch 3; batch 67584; loss 0.124179\n",
      "epoch:3; batch 67584; train accuracy: 0.818947\n",
      "epoch 3; batch 67712; loss 0.232257\n",
      "epoch:3; batch 67712; train accuracy: 0.818991\n",
      "epoch 3; batch 67840; loss 0.047568\n",
      "epoch:3; batch 67840; train accuracy: 0.819067\n",
      "epoch 3; batch 67968; loss 0.098259\n",
      "epoch:3; batch 67968; train accuracy: 0.819143\n",
      "epoch 3; batch 68096; loss 0.097454\n",
      "epoch:3; batch 68096; train accuracy: 0.819215\n",
      "epoch 3; batch 68224; loss 0.104142\n",
      "epoch:3; batch 68224; train accuracy: 0.819288\n",
      "epoch 3; batch 68352; loss 0.175292\n",
      "epoch:3; batch 68352; train accuracy: 0.819346\n",
      "epoch 3; batch 68480; loss 0.035170\n",
      "epoch:3; batch 68480; train accuracy: 0.819425\n",
      "epoch 3; batch 68608; loss 0.164546\n",
      "epoch:3; batch 68608; train accuracy: 0.819483\n",
      "epoch 3; batch 68736; loss 0.134435\n",
      "epoch:3; batch 68736; train accuracy: 0.819541\n",
      "epoch 3; batch 68864; loss 0.067386\n",
      "epoch:3; batch 68864; train accuracy: 0.819606\n",
      "epoch 3; batch 68992; loss 0.071462\n",
      "epoch:3; batch 68992; train accuracy: 0.819671\n",
      "epoch 3; batch 69120; loss 0.072807\n",
      "epoch:3; batch 69120; train accuracy: 0.819743\n",
      "epoch 3; batch 69248; loss 0.165017\n",
      "epoch:3; batch 69248; train accuracy: 0.819797\n",
      "epoch 3; batch 69376; loss 0.091865\n",
      "epoch:3; batch 69376; train accuracy: 0.819858\n",
      "epoch 3; batch 69504; loss 0.120587\n",
      "epoch:3; batch 69504; train accuracy: 0.819915\n",
      "epoch 3; batch 69632; loss 0.078565\n",
      "epoch:3; batch 69632; train accuracy: 0.819980\n",
      "epoch 3; batch 69760; loss 0.052688\n",
      "epoch:3; batch 69760; train accuracy: 0.820051\n",
      "epoch 3; batch 69888; loss 0.114757\n",
      "epoch:3; batch 69888; train accuracy: 0.820116\n",
      "epoch 3; batch 70016; loss 0.118334\n",
      "epoch:3; batch 70016; train accuracy: 0.820173\n",
      "epoch 3; batch 70144; loss 0.070546\n",
      "epoch:3; batch 70144; train accuracy: 0.820248\n",
      "epoch 3; batch 70272; loss 0.111812\n",
      "epoch:3; batch 70272; train accuracy: 0.820302\n",
      "epoch 3; batch 70400; loss 0.153353\n",
      "epoch:3; batch 70400; train accuracy: 0.820362\n",
      "epoch 3; batch 70528; loss 0.113238\n",
      "epoch:3; batch 70528; train accuracy: 0.820423\n",
      "epoch 3; batch 70656; loss 0.152527\n",
      "epoch:3; batch 70656; train accuracy: 0.820487\n",
      "epoch 3; batch 70784; loss 0.099977\n",
      "epoch:3; batch 70784; train accuracy: 0.820548\n",
      "epoch 3; batch 70912; loss 0.118910\n",
      "epoch:3; batch 70912; train accuracy: 0.820605\n",
      "epoch 3; batch 71040; loss 0.146205\n",
      "epoch:3; batch 71040; train accuracy: 0.820665\n",
      "epoch 3; batch 71168; loss 0.094268\n",
      "epoch:3; batch 71168; train accuracy: 0.820736\n",
      "epoch 3; batch 71296; loss 0.126796\n",
      "epoch:3; batch 71296; train accuracy: 0.820789\n",
      "epoch 3; batch 71424; loss 0.102568\n",
      "epoch:3; batch 71424; train accuracy: 0.820853\n",
      "epoch 3; batch 71552; loss 0.082242\n",
      "epoch:3; batch 71552; train accuracy: 0.820927\n",
      "epoch 3; batch 71680; loss 0.125392\n",
      "epoch:3; batch 71680; train accuracy: 0.820991\n",
      "epoch 3; batch 71808; loss 0.105121\n",
      "epoch:3; batch 71808; train accuracy: 0.821051\n",
      "epoch 3; batch 71936; loss 0.078871\n",
      "epoch:3; batch 71936; train accuracy: 0.821118\n",
      "epoch 3; batch 72064; loss 0.141626\n",
      "epoch:3; batch 72064; train accuracy: 0.821174\n",
      "epoch 3; batch 72192; loss 0.134141\n",
      "epoch:3; batch 72192; train accuracy: 0.821231\n",
      "epoch 3; batch 72320; loss 0.146570\n",
      "epoch:3; batch 72320; train accuracy: 0.821294\n",
      "epoch 3; batch 72448; loss 0.054808\n",
      "epoch:3; batch 72448; train accuracy: 0.821371\n",
      "epoch 3; batch 72576; loss 0.095156\n",
      "epoch:3; batch 72576; train accuracy: 0.821438\n",
      "epoch 3; batch 72704; loss 0.086205\n",
      "epoch:3; batch 72704; train accuracy: 0.821501\n",
      "epoch 3; batch 72832; loss 0.085107\n",
      "epoch:3; batch 72832; train accuracy: 0.821568\n",
      "epoch 3; batch 72960; loss 0.140491\n",
      "epoch:3; batch 72960; train accuracy: 0.821628\n",
      "epoch 3; batch 73088; loss 0.106782\n",
      "epoch:3; batch 73088; train accuracy: 0.821694\n",
      "epoch 3; batch 73216; loss 0.157294\n",
      "epoch:3; batch 73216; train accuracy: 0.821746\n",
      "epoch 3; batch 73344; loss 0.072524\n",
      "epoch:3; batch 73344; train accuracy: 0.821806\n",
      "epoch 3; batch 73472; loss 0.177892\n",
      "epoch:3; batch 73472; train accuracy: 0.821872\n",
      "epoch 3; batch 73600; loss 0.073053\n",
      "epoch:3; batch 73600; train accuracy: 0.821939\n",
      "epoch 3; batch 73728; loss 0.204579\n",
      "epoch:3; batch 73728; train accuracy: 0.821994\n",
      "epoch 3; batch 73856; loss 0.118526\n",
      "epoch:3; batch 73856; train accuracy: 0.822050\n",
      "epoch 3; batch 73984; loss 0.095282\n",
      "epoch:3; batch 73984; train accuracy: 0.822116\n",
      "epoch 3; batch 74112; loss 0.058169\n",
      "epoch:3; batch 74112; train accuracy: 0.822193\n",
      "epoch 3; batch 74240; loss 0.088316\n",
      "epoch:3; batch 74240; train accuracy: 0.822255\n",
      "epoch 3; batch 74368; loss 0.148404\n",
      "epoch:3; batch 74368; train accuracy: 0.822321\n",
      "epoch 3; batch 74496; loss 0.182879\n",
      "epoch:3; batch 74496; train accuracy: 0.822366\n",
      "epoch 3; batch 74624; loss 0.093005\n",
      "epoch:3; batch 74624; train accuracy: 0.822435\n",
      "epoch 3; batch 74752; loss 0.091046\n",
      "epoch:3; batch 74752; train accuracy: 0.822494\n",
      "epoch 3; batch 74880; loss 0.078427\n",
      "epoch:3; batch 74880; train accuracy: 0.822560\n",
      "epoch 3; batch 75008; loss 0.111474\n",
      "epoch:3; batch 75008; train accuracy: 0.822625\n",
      "epoch 3; batch 75136; loss 0.174196\n",
      "epoch:3; batch 75136; train accuracy: 0.822677\n",
      "epoch 3; batch 75264; loss 0.186824\n",
      "epoch:3; batch 75264; train accuracy: 0.822721\n",
      "epoch 3; batch 75392; loss 0.148036\n",
      "epoch:3; batch 75392; train accuracy: 0.822776\n",
      "epoch 3; batch 75520; loss 0.215239\n",
      "epoch:3; batch 75520; train accuracy: 0.822828\n",
      "epoch 3; batch 75648; loss 0.136064\n",
      "epoch:3; batch 75648; train accuracy: 0.822897\n",
      "epoch 3; batch 75776; loss 0.164660\n",
      "epoch:3; batch 75776; train accuracy: 0.822948\n",
      "epoch 3; batch 75904; loss 0.187176\n",
      "epoch:3; batch 75904; train accuracy: 0.822996\n",
      "epoch 3; batch 76032; loss 0.079494\n",
      "epoch:3; batch 76032; train accuracy: 0.823065\n",
      "epoch 3; batch 76160; loss 0.090457\n",
      "epoch:3; batch 76160; train accuracy: 0.823130\n",
      "epoch 3; batch 76288; loss 0.147437\n",
      "epoch:3; batch 76288; train accuracy: 0.823192\n",
      "epoch 3; batch 76416; loss 0.116789\n",
      "epoch:3; batch 76416; train accuracy: 0.823253\n",
      "epoch 3; batch 76544; loss 0.081119\n",
      "epoch:3; batch 76544; train accuracy: 0.823322\n",
      "epoch 3; batch 76672; loss 0.246041\n",
      "epoch:3; batch 76672; train accuracy: 0.823362\n",
      "epoch 3; batch 76800; loss 0.079600\n",
      "epoch:3; batch 76800; train accuracy: 0.823424\n",
      "epoch 3; batch 76928; loss 0.092373\n",
      "epoch:3; batch 76928; train accuracy: 0.823495\n",
      "epoch 3; batch 77056; loss 0.154057\n",
      "epoch:3; batch 77056; train accuracy: 0.823546\n",
      "epoch 3; batch 77184; loss 0.146004\n",
      "epoch:3; batch 77184; train accuracy: 0.823607\n",
      "epoch 3; batch 77312; loss 0.099922\n",
      "epoch:3; batch 77312; train accuracy: 0.823669\n",
      "epoch 3; batch 77440; loss 0.137925\n",
      "epoch:3; batch 77440; train accuracy: 0.823709\n",
      "epoch 3; batch 77568; loss 0.167158\n",
      "epoch:3; batch 77568; train accuracy: 0.823763\n",
      "epoch 3; batch 77696; loss 0.071048\n",
      "epoch:3; batch 77696; train accuracy: 0.823835\n",
      "epoch 3; batch 77824; loss 0.095646\n",
      "epoch:3; batch 77824; train accuracy: 0.823899\n",
      "epoch 3; batch 77952; loss 0.094012\n",
      "epoch:3; batch 77952; train accuracy: 0.823967\n",
      "epoch 3; batch 78080; loss 0.138008\n",
      "epoch:3; batch 78080; train accuracy: 0.824028\n",
      "epoch 3; batch 78208; loss 0.143996\n",
      "epoch:3; batch 78208; train accuracy: 0.824078\n",
      "epoch 3; batch 78336; loss 0.063145\n",
      "epoch:3; batch 78336; train accuracy: 0.824146\n",
      "epoch 3; batch 78464; loss 0.070163\n",
      "epoch:3; batch 78464; train accuracy: 0.824214\n",
      "epoch 3; batch 78592; loss 0.101082\n",
      "epoch:3; batch 78592; train accuracy: 0.824274\n",
      "epoch 3; batch 78720; loss 0.115697\n",
      "epoch:3; batch 78720; train accuracy: 0.824338\n",
      "epoch 3; batch 78848; loss 0.113397\n",
      "epoch:3; batch 78848; train accuracy: 0.824399\n",
      "epoch 3; batch 78976; loss 0.092607\n",
      "epoch:3; batch 78976; train accuracy: 0.824470\n",
      "epoch 3; batch 79104; loss 0.069630\n",
      "epoch:3; batch 79104; train accuracy: 0.824534\n",
      "epoch 3; batch 79232; loss 0.096111\n",
      "epoch:3; batch 79232; train accuracy: 0.824594\n",
      "epoch 3; batch 79360; loss 0.109414\n",
      "epoch:3; batch 79360; train accuracy: 0.824658\n",
      "epoch 3; batch 79488; loss 0.142308\n",
      "epoch:3; batch 79488; train accuracy: 0.824718\n",
      "epoch 3; batch 79616; loss 0.091821\n",
      "epoch:3; batch 79616; train accuracy: 0.824778\n",
      "epoch 3; batch 79744; loss 0.115680\n",
      "epoch:3; batch 79744; train accuracy: 0.824842\n",
      "epoch 3; batch 79872; loss 0.050192\n",
      "epoch:3; batch 79872; train accuracy: 0.824912\n",
      "epoch 3; batch 80000; loss 0.143230\n",
      "epoch:3; batch 80000; train accuracy: 0.824966\n",
      "epoch 3; batch 80128; loss 0.109305\n",
      "epoch:3; batch 80128; train accuracy: 0.825022\n",
      "epoch 3; batch 80256; loss 0.119638\n",
      "epoch:3; batch 80256; train accuracy: 0.825079\n",
      "epoch 3; batch 80384; loss 0.112835\n",
      "epoch:3; batch 80384; train accuracy: 0.825142\n",
      "epoch 3; batch 80512; loss 0.062077\n",
      "epoch:3; batch 80512; train accuracy: 0.825209\n",
      "epoch 3; batch 80640; loss 0.053702\n",
      "epoch:3; batch 80640; train accuracy: 0.825279\n",
      "epoch 3; batch 80768; loss 0.152566\n",
      "epoch:3; batch 80768; train accuracy: 0.825335\n",
      "epoch 3; batch 80896; loss 0.080912\n",
      "epoch:3; batch 80896; train accuracy: 0.825402\n",
      "epoch 3; batch 81024; loss 0.073811\n",
      "epoch:3; batch 81024; train accuracy: 0.825465\n",
      "epoch 3; batch 81152; loss 0.088970\n",
      "epoch:3; batch 81152; train accuracy: 0.825524\n",
      "epoch 3; batch 81280; loss 0.084142\n",
      "epoch:3; batch 81280; train accuracy: 0.825584\n",
      "epoch 3; batch 81408; loss 0.132424\n",
      "epoch:3; batch 81408; train accuracy: 0.825640\n",
      "epoch 3; batch 81536; loss 0.205076\n",
      "epoch:3; batch 81536; train accuracy: 0.825699\n",
      "epoch 3; batch 81664; loss 0.074463\n",
      "epoch:3; batch 81664; train accuracy: 0.825765\n",
      "epoch 3; batch 81792; loss 0.107514\n",
      "epoch:3; batch 81792; train accuracy: 0.825825\n",
      "epoch 3; batch 81920; loss 0.179206\n",
      "epoch:3; batch 81920; train accuracy: 0.825881\n",
      "epoch 3; batch 82048; loss 0.046379\n",
      "epoch:3; batch 82048; train accuracy: 0.825950\n",
      "epoch 3; batch 82176; loss 0.132662\n",
      "epoch:3; batch 82176; train accuracy: 0.826009\n",
      "epoch 3; batch 82304; loss 0.055209\n",
      "epoch:3; batch 82304; train accuracy: 0.826079\n",
      "epoch 3; batch 82432; loss 0.183457\n",
      "epoch:3; batch 82432; train accuracy: 0.826134\n",
      "epoch 3; batch 82560; loss 0.086551\n",
      "epoch:3; batch 82560; train accuracy: 0.826200\n",
      "epoch 3; batch 82688; loss 0.123170\n",
      "epoch:3; batch 82688; train accuracy: 0.826256\n",
      "epoch 3; batch 82816; loss 0.129252\n",
      "epoch:3; batch 82816; train accuracy: 0.826311\n",
      "epoch 3; batch 82944; loss 0.114605\n",
      "epoch:3; batch 82944; train accuracy: 0.826380\n",
      "epoch 3; batch 83072; loss 0.053467\n",
      "epoch:3; batch 83072; train accuracy: 0.826446\n",
      "epoch 3; batch 83200; loss 0.101079\n",
      "epoch:3; batch 83200; train accuracy: 0.826515\n",
      "epoch 3; batch 83328; loss 0.224261\n",
      "epoch:3; batch 83328; train accuracy: 0.826567\n",
      "epoch 3; batch 83456; loss 0.053811\n",
      "epoch:3; batch 83456; train accuracy: 0.826635\n",
      "epoch 3; batch 83584; loss 0.067497\n",
      "epoch:3; batch 83584; train accuracy: 0.826694\n",
      "epoch 3; batch 83712; loss 0.097380\n",
      "epoch:3; batch 83712; train accuracy: 0.826759\n",
      "epoch 3; batch 83840; loss 0.098427\n",
      "epoch:3; batch 83840; train accuracy: 0.826818\n",
      "epoch 3; batch 83968; loss 0.114078\n",
      "epoch:3; batch 83968; train accuracy: 0.826873\n",
      "epoch 3; batch 84096; loss 0.114925\n",
      "epoch:3; batch 84096; train accuracy: 0.826938\n",
      "epoch 3; batch 84224; loss 0.081102\n",
      "epoch:3; batch 84224; train accuracy: 0.827000\n",
      "epoch 3; batch 84352; loss 0.075135\n",
      "epoch:3; batch 84352; train accuracy: 0.827058\n",
      "epoch 3; batch 84480; loss 0.080394\n",
      "epoch:3; batch 84480; train accuracy: 0.827123\n",
      "epoch 3; batch 84608; loss 0.068859\n",
      "epoch:3; batch 84608; train accuracy: 0.827188\n",
      "epoch 3; batch 84736; loss 0.046556\n",
      "epoch:3; batch 84736; train accuracy: 0.827260\n",
      "epoch 3; batch 84864; loss 0.093590\n",
      "epoch:3; batch 84864; train accuracy: 0.827321\n",
      "epoch 3; batch 84992; loss 0.071496\n",
      "epoch:3; batch 84992; train accuracy: 0.827379\n",
      "epoch 3; batch 85120; loss 0.150320\n",
      "epoch:3; batch 85120; train accuracy: 0.827440\n",
      "epoch 3; batch 85248; loss 0.069942\n",
      "epoch:3; batch 85248; train accuracy: 0.827505\n",
      "epoch 3; batch 85376; loss 0.044623\n",
      "epoch:3; batch 85376; train accuracy: 0.827573\n",
      "epoch 3; batch 85504; loss 0.087769\n",
      "epoch:3; batch 85504; train accuracy: 0.827638\n",
      "epoch 3; batch 85632; loss 0.083619\n",
      "epoch:3; batch 85632; train accuracy: 0.827695\n",
      "epoch 3; batch 85760; loss 0.097058\n",
      "epoch:3; batch 85760; train accuracy: 0.827750\n",
      "epoch 3; batch 85888; loss 0.092736\n",
      "epoch:3; batch 85888; train accuracy: 0.827811\n",
      "epoch 3; batch 86016; loss 0.085887\n",
      "epoch:3; batch 86016; train accuracy: 0.827875\n",
      "epoch 3; batch 86144; loss 0.146986\n",
      "epoch:3; batch 86144; train accuracy: 0.827929\n",
      "epoch 3; batch 86272; loss 0.094760\n",
      "epoch:3; batch 86272; train accuracy: 0.827990\n",
      "epoch 3; batch 86400; loss 0.048388\n",
      "epoch:3; batch 86400; train accuracy: 0.828058\n",
      "epoch 3; batch 86528; loss 0.107306\n",
      "epoch:3; batch 86528; train accuracy: 0.828115\n",
      "epoch 3; batch 86656; loss 0.043186\n",
      "epoch:3; batch 86656; train accuracy: 0.828182\n",
      "epoch 3; batch 86784; loss 0.075237\n",
      "epoch:3; batch 86784; train accuracy: 0.828240\n",
      "epoch 3; batch 86912; loss 0.072286\n",
      "epoch:3; batch 86912; train accuracy: 0.828300\n",
      "epoch 3; batch 87040; loss 0.072009\n",
      "epoch:3; batch 87040; train accuracy: 0.828364\n",
      "epoch 3; batch 87168; loss 0.162166\n",
      "epoch:3; batch 87168; train accuracy: 0.828418\n",
      "epoch 3; batch 87296; loss 0.080519\n",
      "epoch:3; batch 87296; train accuracy: 0.828482\n",
      "epoch 3; batch 87424; loss 0.044069\n",
      "epoch:3; batch 87424; train accuracy: 0.828545\n",
      "epoch 3; batch 87552; loss 0.092871\n",
      "epoch:3; batch 87552; train accuracy: 0.828599\n",
      "epoch 3; batch 87680; loss 0.138160\n",
      "epoch:3; batch 87680; train accuracy: 0.828653\n",
      "epoch 3; batch 87808; loss 0.055123\n",
      "epoch:3; batch 87808; train accuracy: 0.828716\n",
      "epoch 3; batch 87936; loss 0.075998\n",
      "epoch:3; batch 87936; train accuracy: 0.828786\n",
      "epoch 3; batch 88064; loss 0.042864\n",
      "epoch:3; batch 88064; train accuracy: 0.828853\n",
      "epoch 3; batch 88192; loss 0.166171\n",
      "epoch:3; batch 88192; train accuracy: 0.828900\n",
      "epoch 3; batch 88320; loss 0.155294\n",
      "epoch:3; batch 88320; train accuracy: 0.828953\n",
      "epoch 3; batch 88448; loss 0.146049\n",
      "epoch:3; batch 88448; train accuracy: 0.829006\n",
      "epoch 3; batch 88576; loss 0.063729\n",
      "epoch:3; batch 88576; train accuracy: 0.829070\n",
      "epoch 3; batch 88704; loss 0.053933\n",
      "epoch:3; batch 88704; train accuracy: 0.829140\n",
      "epoch 3; batch 88832; loss 0.093458\n",
      "epoch:3; batch 88832; train accuracy: 0.829193\n",
      "epoch 3; batch 88960; loss 0.056265\n",
      "epoch:3; batch 88960; train accuracy: 0.829253\n",
      "epoch 3; batch 89088; loss 0.103749\n",
      "epoch:3; batch 89088; train accuracy: 0.829316\n",
      "epoch 3; batch 89216; loss 0.088850\n",
      "epoch:3; batch 89216; train accuracy: 0.829379\n",
      "epoch 3; batch 89344; loss 0.043438\n",
      "epoch:3; batch 89344; train accuracy: 0.829448\n",
      "epoch 3; batch 89472; loss 0.235983\n",
      "epoch:3; batch 89472; train accuracy: 0.829488\n",
      "epoch 3; batch 89600; loss 0.154174\n",
      "epoch:3; batch 89600; train accuracy: 0.829537\n",
      "epoch 3; batch 89728; loss 0.134824\n",
      "epoch:3; batch 89728; train accuracy: 0.829583\n",
      "epoch 3; batch 89856; loss 0.063727\n",
      "epoch:3; batch 89856; train accuracy: 0.829649\n",
      "epoch 3; batch 89984; loss 0.095729\n",
      "epoch:3; batch 89984; train accuracy: 0.829709\n",
      "epoch 3; batch 90112; loss 0.066009\n",
      "epoch:3; batch 90112; train accuracy: 0.829765\n",
      "epoch 3; batch 90240; loss 0.031791\n",
      "epoch:3; batch 90240; train accuracy: 0.829834\n",
      "epoch 3; batch 90368; loss 0.038783\n",
      "epoch:3; batch 90368; train accuracy: 0.829907\n",
      "epoch 3; batch 90496; loss 0.146788\n",
      "epoch:3; batch 90496; train accuracy: 0.829966\n",
      "epoch 3; batch 90624; loss 0.079605\n",
      "epoch:3; batch 90624; train accuracy: 0.830028\n",
      "epoch 3; batch 90752; loss 0.039318\n",
      "epoch:3; batch 90752; train accuracy: 0.830091\n",
      "epoch 3; batch 90880; loss 0.045327\n",
      "epoch:3; batch 90880; train accuracy: 0.830153\n",
      "epoch 3; batch 91008; loss 0.125237\n",
      "epoch:3; batch 91008; train accuracy: 0.830202\n",
      "epoch 3; batch 91136; loss 0.079301\n",
      "epoch:3; batch 91136; train accuracy: 0.830264\n",
      "epoch 3; batch 91264; loss 0.069422\n",
      "epoch:3; batch 91264; train accuracy: 0.830330\n",
      "epoch 3; batch 91392; loss 0.027876\n",
      "epoch:3; batch 91392; train accuracy: 0.830398\n",
      "epoch 3; batch 91520; loss 0.111827\n",
      "epoch:3; batch 91520; train accuracy: 0.830454\n",
      "epoch 3; batch 91648; loss 0.036344\n",
      "epoch:3; batch 91648; train accuracy: 0.830519\n",
      "epoch 3; batch 91776; loss 0.096940\n",
      "epoch:3; batch 91776; train accuracy: 0.830575\n",
      "epoch 3; batch 91904; loss 0.042416\n",
      "epoch:3; batch 91904; train accuracy: 0.830640\n",
      "epoch 3; batch 92032; loss 0.053184\n",
      "epoch:3; batch 92032; train accuracy: 0.830698\n",
      "epoch 3; batch 92160; loss 0.113093\n",
      "epoch:3; batch 92160; train accuracy: 0.830760\n",
      "epoch 3; batch 92288; loss 0.101415\n",
      "epoch:3; batch 92288; train accuracy: 0.830825\n",
      "epoch 3; batch 92416; loss 0.129509\n",
      "epoch:3; batch 92416; train accuracy: 0.830874\n",
      "epoch 3; batch 92544; loss 0.048630\n",
      "epoch:3; batch 92544; train accuracy: 0.830939\n",
      "epoch 3; batch 92672; loss 0.068636\n",
      "epoch:3; batch 92672; train accuracy: 0.830990\n",
      "epoch 3; batch 92800; loss 0.042026\n",
      "epoch:3; batch 92800; train accuracy: 0.831055\n",
      "epoch 3; batch 92928; loss 0.053464\n",
      "epoch:3; batch 92928; train accuracy: 0.831120\n",
      "epoch 3; batch 93056; loss 0.144757\n",
      "epoch:3; batch 93056; train accuracy: 0.831171\n",
      "epoch 3; batch 93184; loss 0.044289\n",
      "epoch:3; batch 93184; train accuracy: 0.831239\n",
      "epoch 3; batch 93312; loss 0.181258\n",
      "epoch:3; batch 93312; train accuracy: 0.831298\n",
      "epoch 3; batch 93440; loss 0.077202\n",
      "epoch:3; batch 93440; train accuracy: 0.831359\n",
      "epoch 3; batch 93568; loss 0.079975\n",
      "epoch:3; batch 93568; train accuracy: 0.831413\n",
      "epoch 3; batch 93696; loss 0.089902\n",
      "epoch:3; batch 93696; train accuracy: 0.831475\n",
      "epoch 3; batch 93824; loss 0.090057\n",
      "epoch:3; batch 93824; train accuracy: 0.831532\n",
      "epoch 3; batch 93952; loss 0.083134\n",
      "epoch:3; batch 93952; train accuracy: 0.831584\n",
      "epoch 3; batch 94080; loss 0.078662\n",
      "epoch:3; batch 94080; train accuracy: 0.831645\n",
      "epoch 3; batch 94208; loss 0.089108\n",
      "epoch:3; batch 94208; train accuracy: 0.831693\n",
      "epoch 3; batch 94336; loss 0.031699\n",
      "epoch:3; batch 94336; train accuracy: 0.831757\n",
      "epoch 3; batch 94464; loss 0.214718\n",
      "epoch:3; batch 94464; train accuracy: 0.831805\n",
      "epoch 3; batch 94592; loss 0.092253\n",
      "epoch:3; batch 94592; train accuracy: 0.831862\n",
      "epoch 3; batch 94720; loss 0.098888\n",
      "epoch:3; batch 94720; train accuracy: 0.831923\n",
      "epoch 3; batch 94848; loss 0.068462\n",
      "epoch:3; batch 94848; train accuracy: 0.831980\n",
      "epoch 3; batch 94976; loss 0.157017\n",
      "epoch:3; batch 94976; train accuracy: 0.832038\n",
      "epoch 3; batch 95104; loss 0.113052\n",
      "epoch:3; batch 95104; train accuracy: 0.832089\n",
      "epoch 3; batch 95232; loss 0.120927\n",
      "epoch:3; batch 95232; train accuracy: 0.832139\n",
      "epoch 3; batch 95360; loss 0.142935\n",
      "epoch:3; batch 95360; train accuracy: 0.832187\n",
      "epoch 3; batch 95488; loss 0.102196\n",
      "epoch:3; batch 95488; train accuracy: 0.832241\n",
      "epoch 3; batch 95616; loss 0.067467\n",
      "epoch:3; batch 95616; train accuracy: 0.832298\n",
      "epoch 3; batch 95744; loss 0.097934\n",
      "epoch:3; batch 95744; train accuracy: 0.832355\n",
      "epoch 3; batch 95872; loss 0.127418\n",
      "epoch:3; batch 95872; train accuracy: 0.832409\n",
      "epoch 3; batch 96000; loss 0.050310\n",
      "epoch:3; batch 96000; train accuracy: 0.832473\n",
      "epoch 3; batch 96128; loss 0.110546\n",
      "epoch:3; batch 96128; train accuracy: 0.832526\n",
      "epoch 3; batch 96256; loss 0.171401\n",
      "epoch:3; batch 96256; train accuracy: 0.832583\n",
      "epoch 3; batch 96384; loss 0.128355\n",
      "epoch:3; batch 96384; train accuracy: 0.832634\n",
      "epoch 3; batch 96512; loss 0.058416\n",
      "epoch:3; batch 96512; train accuracy: 0.832697\n",
      "epoch 3; batch 96640; loss 0.099325\n",
      "epoch:3; batch 96640; train accuracy: 0.832757\n",
      "epoch 3; batch 96768; loss 0.042544\n",
      "epoch:3; batch 96768; train accuracy: 0.832820\n",
      "epoch 3; batch 96896; loss 0.138762\n",
      "epoch:3; batch 96896; train accuracy: 0.832861\n",
      "epoch 3; batch 97024; loss 0.076428\n",
      "epoch:3; batch 97024; train accuracy: 0.832921\n",
      "epoch 3; batch 97152; loss 0.310641\n",
      "epoch:3; batch 97152; train accuracy: 0.832948\n",
      "epoch 3; batch 97280; loss 0.144489\n",
      "epoch:3; batch 97280; train accuracy: 0.833001\n",
      "epoch 3; batch 97408; loss 0.057738\n",
      "epoch:3; batch 97408; train accuracy: 0.833068\n",
      "epoch 3; batch 97536; loss 0.080751\n",
      "epoch:3; batch 97536; train accuracy: 0.833124\n",
      "epoch 3; batch 97664; loss 0.150402\n",
      "epoch:3; batch 97664; train accuracy: 0.833181\n",
      "epoch 3; batch 97792; loss 0.054874\n",
      "epoch:3; batch 97792; train accuracy: 0.833243\n",
      "epoch 3; batch 97920; loss 0.053850\n",
      "epoch:3; batch 97920; train accuracy: 0.833306\n",
      "epoch 3; batch 98048; loss 0.065024\n",
      "epoch:3; batch 98048; train accuracy: 0.833366\n",
      "epoch 3; batch 98176; loss 0.108418\n",
      "epoch:3; batch 98176; train accuracy: 0.833416\n",
      "epoch 3; batch 98304; loss 0.110189\n",
      "epoch:3; batch 98304; train accuracy: 0.833468\n",
      "epoch 3; batch 98432; loss 0.099369\n",
      "epoch:3; batch 98432; train accuracy: 0.833525\n",
      "epoch 3; batch 98560; loss 0.044518\n",
      "epoch:3; batch 98560; train accuracy: 0.833587\n",
      "epoch 3; batch 98688; loss 0.093486\n",
      "epoch:3; batch 98688; train accuracy: 0.833643\n",
      "epoch 3; batch 98816; loss 0.083411\n",
      "epoch:3; batch 98816; train accuracy: 0.833699\n",
      "epoch 3; batch 98944; loss 0.040474\n",
      "epoch:3; batch 98944; train accuracy: 0.833762\n",
      "epoch 3; batch 99072; loss 0.095612\n",
      "epoch:3; batch 99072; train accuracy: 0.833814\n",
      "epoch 3; batch 99200; loss 0.083824\n",
      "epoch:3; batch 99200; train accuracy: 0.833870\n",
      "epoch 3; batch 99328; loss 0.070801\n",
      "epoch:3; batch 99328; train accuracy: 0.833933\n",
      "epoch 3; batch 99456; loss 0.048034\n",
      "epoch:3; batch 99456; train accuracy: 0.833988\n",
      "epoch 3; batch 99584; loss 0.078334\n",
      "epoch:3; batch 99584; train accuracy: 0.834047\n",
      "epoch 3; batch 99712; loss 0.046135\n",
      "epoch:3; batch 99712; train accuracy: 0.834110\n",
      "epoch 3; batch 99840; loss 0.110097\n",
      "epoch:3; batch 99840; train accuracy: 0.834165\n",
      "epoch 3; batch 99968; loss 0.049846\n",
      "epoch:3; batch 99968; train accuracy: 0.834230\n",
      "epoch 3; batch 100096; loss 0.051974\n",
      "epoch:3; batch 100096; train accuracy: 0.834296\n",
      "epoch 3; batch 100224; loss 0.112854\n",
      "epoch:3; batch 100224; train accuracy: 0.834348\n",
      "epoch 3; batch 100352; loss 0.092529\n",
      "epoch:3; batch 100352; train accuracy: 0.834407\n",
      "epoch 3; batch 100480; loss 0.061994\n",
      "epoch:3; batch 100480; train accuracy: 0.834459\n",
      "epoch 3; batch 100608; loss 0.108885\n",
      "epoch:3; batch 100608; train accuracy: 0.834498\n",
      "epoch 3; batch 100736; loss 0.083329\n",
      "epoch:3; batch 100736; train accuracy: 0.834550\n",
      "epoch 3; batch 100864; loss 0.086244\n",
      "epoch:3; batch 100864; train accuracy: 0.834609\n",
      "epoch 3; batch 100992; loss 0.128926\n",
      "epoch:3; batch 100992; train accuracy: 0.834664\n",
      "epoch 3; batch 101120; loss 0.043420\n",
      "epoch:3; batch 101120; train accuracy: 0.834725\n",
      "epoch 3; batch 101248; loss 0.067195\n",
      "epoch:3; batch 101248; train accuracy: 0.834781\n",
      "epoch 3; batch 101376; loss 0.097349\n",
      "epoch:3; batch 101376; train accuracy: 0.834836\n",
      "epoch 3; batch 101504; loss 0.080153\n",
      "epoch:3; batch 101504; train accuracy: 0.834891\n",
      "epoch 3; batch 101632; loss 0.136002\n",
      "epoch:3; batch 101632; train accuracy: 0.834946\n",
      "epoch 3; batch 101760; loss 0.048205\n",
      "epoch:3; batch 101760; train accuracy: 0.835010\n",
      "epoch 3; batch 101888; loss 0.147601\n",
      "epoch:3; batch 101888; train accuracy: 0.835046\n",
      "epoch 3; batch 102016; loss 0.106641\n",
      "epoch:3; batch 102016; train accuracy: 0.835107\n",
      "epoch 3; batch 102144; loss 0.019364\n",
      "epoch:3; batch 102144; train accuracy: 0.835175\n",
      "epoch 3; batch 102272; loss 0.087880\n",
      "epoch:3; batch 102272; train accuracy: 0.835226\n",
      "epoch 3; batch 102400; loss 0.026154\n",
      "epoch:3; batch 102400; train accuracy: 0.835291\n",
      "epoch 3; batch 102528; loss 0.185223\n",
      "epoch:3; batch 102528; train accuracy: 0.835333\n",
      "epoch 3; batch 102656; loss 0.048841\n",
      "epoch:3; batch 102656; train accuracy: 0.835394\n",
      "epoch 3; batch 102784; loss 0.124242\n",
      "epoch:3; batch 102784; train accuracy: 0.835445\n",
      "epoch 3; batch 102912; loss 0.191424\n",
      "epoch:3; batch 102912; train accuracy: 0.835480\n",
      "epoch 3; batch 103040; loss 0.064823\n",
      "epoch:3; batch 103040; train accuracy: 0.835538\n",
      "epoch 3; batch 103168; loss 0.069146\n",
      "epoch:3; batch 103168; train accuracy: 0.835593\n",
      "epoch 3; batch 103296; loss 0.075659\n",
      "epoch:3; batch 103296; train accuracy: 0.835650\n",
      "epoch 3; batch 103424; loss 0.078759\n",
      "epoch:3; batch 103424; train accuracy: 0.835708\n",
      "epoch 3; batch 103552; loss 0.091696\n",
      "epoch:3; batch 103552; train accuracy: 0.835759\n",
      "epoch 3; batch 103680; loss 0.052718\n",
      "epoch:3; batch 103680; train accuracy: 0.835816\n",
      "epoch 3; batch 103808; loss 0.076566\n",
      "epoch:3; batch 103808; train accuracy: 0.835874\n",
      "epoch 3; batch 103936; loss 0.063576\n",
      "epoch:3; batch 103936; train accuracy: 0.835928\n",
      "epoch 3; batch 104064; loss 0.075010\n",
      "epoch:3; batch 104064; train accuracy: 0.835982\n",
      "epoch 3; batch 104192; loss 0.104686\n",
      "epoch:3; batch 104192; train accuracy: 0.836033\n",
      "epoch 3; batch 104320; loss 0.097120\n",
      "epoch:3; batch 104320; train accuracy: 0.836084\n",
      "epoch 3; batch 104448; loss 0.137020\n",
      "epoch:3; batch 104448; train accuracy: 0.836128\n",
      "epoch 3; batch 104576; loss 0.103771\n",
      "epoch:3; batch 104576; train accuracy: 0.836182\n",
      "epoch 3; batch 104704; loss 0.113347\n",
      "epoch:3; batch 104704; train accuracy: 0.836230\n",
      "epoch 3; batch 104832; loss 0.074966\n",
      "epoch:3; batch 104832; train accuracy: 0.836281\n",
      "epoch 3; batch 104960; loss 0.097273\n",
      "epoch:3; batch 104960; train accuracy: 0.836328\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 31772 ; rate: 0.302706\n",
      "y_true_label_1_num: 12635 ; rate: 0.120379\n",
      "y_true_label_2_num: 14791 ; rate: 0.140920\n",
      "y_true_label_3_num: 45762 ; rate: 0.435995\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.982403\n",
      "valid avg_precision: 0.983467\n",
      "valid avg_recall: 0.981193\n",
      "valid avg_f1: 0.982201\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 4476 ; rate: 0.298878\n",
      "y_true_label_1_num: 1783 ; rate: 0.119057\n",
      "y_true_label_2_num: 2172 ; rate: 0.145032\n",
      "y_true_label_3_num: 6545 ; rate: 0.437033\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.726229\n",
      "valid avg_precision: 0.727587\n",
      "valid avg_recall: 0.717882\n",
      "valid avg_f1: 0.722032\n",
      "epoch 4\n",
      "epoch 4; batch 128; loss 0.036969\n",
      "epoch:4; batch 128; train accuracy: 0.836391\n",
      "epoch 4; batch 256; loss 0.177127\n",
      "epoch:4; batch 256; train accuracy: 0.836436\n",
      "epoch 4; batch 384; loss 0.056269\n",
      "epoch:4; batch 384; train accuracy: 0.836496\n",
      "epoch 4; batch 512; loss 0.103910\n",
      "epoch:4; batch 512; train accuracy: 0.836546\n",
      "epoch 4; batch 640; loss 0.037572\n",
      "epoch:4; batch 640; train accuracy: 0.836606\n",
      "epoch 4; batch 768; loss 0.077323\n",
      "epoch:4; batch 768; train accuracy: 0.836663\n",
      "epoch 4; batch 896; loss 0.071468\n",
      "epoch:4; batch 896; train accuracy: 0.836717\n",
      "epoch 4; batch 1024; loss 0.070253\n",
      "epoch:4; batch 1024; train accuracy: 0.836770\n",
      "epoch 4; batch 1152; loss 0.047854\n",
      "epoch:4; batch 1152; train accuracy: 0.836827\n",
      "epoch 4; batch 1280; loss 0.023182\n",
      "epoch:4; batch 1280; train accuracy: 0.836893\n",
      "epoch 4; batch 1408; loss 0.083585\n",
      "epoch:4; batch 1408; train accuracy: 0.836946\n",
      "epoch 4; batch 1536; loss 0.053841\n",
      "epoch:4; batch 1536; train accuracy: 0.836999\n",
      "epoch 4; batch 1664; loss 0.083328\n",
      "epoch:4; batch 1664; train accuracy: 0.837056\n",
      "epoch 4; batch 1792; loss 0.137638\n",
      "epoch:4; batch 1792; train accuracy: 0.837115\n",
      "epoch 4; batch 1920; loss 0.065490\n",
      "epoch:4; batch 1920; train accuracy: 0.837178\n",
      "epoch 4; batch 2048; loss 0.088039\n",
      "epoch:4; batch 2048; train accuracy: 0.837231\n",
      "epoch 4; batch 2176; loss 0.114161\n",
      "epoch:4; batch 2176; train accuracy: 0.837281\n",
      "epoch 4; batch 2304; loss 0.066032\n",
      "epoch:4; batch 2304; train accuracy: 0.837337\n",
      "epoch 4; batch 2432; loss 0.073196\n",
      "epoch:4; batch 2432; train accuracy: 0.837393\n",
      "epoch 4; batch 2560; loss 0.054212\n",
      "epoch:4; batch 2560; train accuracy: 0.837450\n",
      "epoch 4; batch 2688; loss 0.063968\n",
      "epoch:4; batch 2688; train accuracy: 0.837509\n",
      "epoch 4; batch 2816; loss 0.065831\n",
      "epoch:4; batch 2816; train accuracy: 0.837562\n",
      "epoch 4; batch 2944; loss 0.029988\n",
      "epoch:4; batch 2944; train accuracy: 0.837624\n",
      "epoch 4; batch 3072; loss 0.076796\n",
      "epoch:4; batch 3072; train accuracy: 0.837677\n",
      "epoch 4; batch 3200; loss 0.113606\n",
      "epoch:4; batch 3200; train accuracy: 0.837730\n",
      "epoch 4; batch 3328; loss 0.087198\n",
      "epoch:4; batch 3328; train accuracy: 0.837788\n",
      "epoch 4; batch 3456; loss 0.029336\n",
      "epoch:4; batch 3456; train accuracy: 0.837847\n",
      "epoch 4; batch 3584; loss 0.088332\n",
      "epoch:4; batch 3584; train accuracy: 0.837900\n",
      "epoch 4; batch 3712; loss 0.032931\n",
      "epoch:4; batch 3712; train accuracy: 0.837962\n",
      "epoch 4; batch 3840; loss 0.051805\n",
      "epoch:4; batch 3840; train accuracy: 0.838024\n",
      "epoch 4; batch 3968; loss 0.026668\n",
      "epoch:4; batch 3968; train accuracy: 0.838086\n",
      "epoch 4; batch 4096; loss 0.042744\n",
      "epoch:4; batch 4096; train accuracy: 0.838141\n",
      "epoch 4; batch 4224; loss 0.094515\n",
      "epoch:4; batch 4224; train accuracy: 0.838200\n",
      "epoch 4; batch 4352; loss 0.035157\n",
      "epoch:4; batch 4352; train accuracy: 0.838259\n",
      "epoch 4; batch 4480; loss 0.071746\n",
      "epoch:4; batch 4480; train accuracy: 0.838311\n",
      "epoch 4; batch 4608; loss 0.057284\n",
      "epoch:4; batch 4608; train accuracy: 0.838366\n",
      "epoch 4; batch 4736; loss 0.065609\n",
      "epoch:4; batch 4736; train accuracy: 0.838422\n",
      "epoch 4; batch 4864; loss 0.049749\n",
      "epoch:4; batch 4864; train accuracy: 0.838480\n",
      "epoch 4; batch 4992; loss 0.044871\n",
      "epoch:4; batch 4992; train accuracy: 0.838539\n",
      "epoch 4; batch 5120; loss 0.068918\n",
      "epoch:4; batch 5120; train accuracy: 0.838594\n",
      "epoch 4; batch 5248; loss 0.118337\n",
      "epoch:4; batch 5248; train accuracy: 0.838643\n",
      "epoch 4; batch 5376; loss 0.053240\n",
      "epoch:4; batch 5376; train accuracy: 0.838695\n",
      "epoch 4; batch 5504; loss 0.019485\n",
      "epoch:4; batch 5504; train accuracy: 0.838756\n",
      "epoch 4; batch 5632; loss 0.054948\n",
      "epoch:4; batch 5632; train accuracy: 0.838814\n",
      "epoch 4; batch 5760; loss 0.066390\n",
      "epoch:4; batch 5760; train accuracy: 0.838869\n",
      "epoch 4; batch 5888; loss 0.102137\n",
      "epoch:4; batch 5888; train accuracy: 0.838912\n",
      "epoch 4; batch 6016; loss 0.045319\n",
      "epoch:4; batch 6016; train accuracy: 0.838967\n",
      "epoch 4; batch 6144; loss 0.040455\n",
      "epoch:4; batch 6144; train accuracy: 0.839021\n",
      "epoch 4; batch 6272; loss 0.149223\n",
      "epoch:4; batch 6272; train accuracy: 0.839067\n",
      "epoch 4; batch 6400; loss 0.035391\n",
      "epoch:4; batch 6400; train accuracy: 0.839125\n",
      "epoch 4; batch 6528; loss 0.153975\n",
      "epoch:4; batch 6528; train accuracy: 0.839170\n",
      "epoch 4; batch 6656; loss 0.023904\n",
      "epoch:4; batch 6656; train accuracy: 0.839231\n",
      "epoch 4; batch 6784; loss 0.023220\n",
      "epoch:4; batch 6784; train accuracy: 0.839292\n",
      "epoch 4; batch 6912; loss 0.020530\n",
      "epoch:4; batch 6912; train accuracy: 0.839356\n",
      "epoch 4; batch 7040; loss 0.064433\n",
      "epoch:4; batch 7040; train accuracy: 0.839404\n",
      "epoch 4; batch 7168; loss 0.043144\n",
      "epoch:4; batch 7168; train accuracy: 0.839462\n",
      "epoch 4; batch 7296; loss 0.128544\n",
      "epoch:4; batch 7296; train accuracy: 0.839516\n",
      "epoch 4; batch 7424; loss 0.009392\n",
      "epoch:4; batch 7424; train accuracy: 0.839580\n",
      "epoch 4; batch 7552; loss 0.033166\n",
      "epoch:4; batch 7552; train accuracy: 0.839641\n",
      "epoch 4; batch 7680; loss 0.144765\n",
      "epoch:4; batch 7680; train accuracy: 0.839692\n",
      "epoch 4; batch 7808; loss 0.172269\n",
      "epoch:4; batch 7808; train accuracy: 0.839724\n",
      "epoch 4; batch 7936; loss 0.025362\n",
      "epoch:4; batch 7936; train accuracy: 0.839782\n",
      "epoch 4; batch 8064; loss 0.019531\n",
      "epoch:4; batch 8064; train accuracy: 0.839842\n",
      "epoch 4; batch 8192; loss 0.059833\n",
      "epoch:4; batch 8192; train accuracy: 0.839896\n",
      "epoch 4; batch 8320; loss 0.055321\n",
      "epoch:4; batch 8320; train accuracy: 0.839951\n",
      "epoch 4; batch 8448; loss 0.054301\n",
      "epoch:4; batch 8448; train accuracy: 0.840005\n",
      "epoch 4; batch 8576; loss 0.106953\n",
      "epoch:4; batch 8576; train accuracy: 0.840052\n",
      "epoch 4; batch 8704; loss 0.087499\n",
      "epoch:4; batch 8704; train accuracy: 0.840106\n",
      "epoch 4; batch 8832; loss 0.022687\n",
      "epoch:4; batch 8832; train accuracy: 0.840170\n",
      "epoch 4; batch 8960; loss 0.027226\n",
      "epoch:4; batch 8960; train accuracy: 0.840230\n",
      "epoch 4; batch 9088; loss 0.054860\n",
      "epoch:4; batch 9088; train accuracy: 0.840281\n",
      "epoch 4; batch 9216; loss 0.104083\n",
      "epoch:4; batch 9216; train accuracy: 0.840328\n",
      "epoch 4; batch 9344; loss 0.050306\n",
      "epoch:4; batch 9344; train accuracy: 0.840382\n",
      "epoch 4; batch 9472; loss 0.074064\n",
      "epoch:4; batch 9472; train accuracy: 0.840436\n",
      "epoch 4; batch 9600; loss 0.072498\n",
      "epoch:4; batch 9600; train accuracy: 0.840489\n",
      "epoch 4; batch 9728; loss 0.049506\n",
      "epoch:4; batch 9728; train accuracy: 0.840549\n",
      "epoch 4; batch 9856; loss 0.057563\n",
      "epoch:4; batch 9856; train accuracy: 0.840606\n",
      "epoch 4; batch 9984; loss 0.071037\n",
      "epoch:4; batch 9984; train accuracy: 0.840659\n",
      "epoch 4; batch 10112; loss 0.015827\n",
      "epoch:4; batch 10112; train accuracy: 0.840722\n",
      "epoch 4; batch 10240; loss 0.019425\n",
      "epoch:4; batch 10240; train accuracy: 0.840782\n",
      "epoch 4; batch 10368; loss 0.033526\n",
      "epoch:4; batch 10368; train accuracy: 0.840845\n",
      "epoch 4; batch 10496; loss 0.045624\n",
      "epoch:4; batch 10496; train accuracy: 0.840901\n",
      "epoch 4; batch 10624; loss 0.011781\n",
      "epoch:4; batch 10624; train accuracy: 0.840964\n",
      "epoch 4; batch 10752; loss 0.060858\n",
      "epoch:4; batch 10752; train accuracy: 0.841023\n",
      "epoch 4; batch 10880; loss 0.018507\n",
      "epoch:4; batch 10880; train accuracy: 0.841085\n",
      "epoch 4; batch 11008; loss 0.086609\n",
      "epoch:4; batch 11008; train accuracy: 0.841142\n",
      "epoch 4; batch 11136; loss 0.029175\n",
      "epoch:4; batch 11136; train accuracy: 0.841198\n",
      "epoch 4; batch 11264; loss 0.097731\n",
      "epoch:4; batch 11264; train accuracy: 0.841251\n",
      "epoch 4; batch 11392; loss 0.016208\n",
      "epoch:4; batch 11392; train accuracy: 0.841313\n",
      "epoch 4; batch 11520; loss 0.083361\n",
      "epoch:4; batch 11520; train accuracy: 0.841366\n",
      "epoch 4; batch 11648; loss 0.056753\n",
      "epoch:4; batch 11648; train accuracy: 0.841422\n",
      "epoch 4; batch 11776; loss 0.040706\n",
      "epoch:4; batch 11776; train accuracy: 0.841482\n",
      "epoch 4; batch 11904; loss 0.014634\n",
      "epoch:4; batch 11904; train accuracy: 0.841541\n",
      "epoch 4; batch 12032; loss 0.027798\n",
      "epoch:4; batch 12032; train accuracy: 0.841600\n",
      "epoch 4; batch 12160; loss 0.026345\n",
      "epoch:4; batch 12160; train accuracy: 0.841659\n",
      "epoch 4; batch 12288; loss 0.034238\n",
      "epoch:4; batch 12288; train accuracy: 0.841714\n",
      "epoch 4; batch 12416; loss 0.008200\n",
      "epoch:4; batch 12416; train accuracy: 0.841776\n",
      "epoch 4; batch 12544; loss 0.019927\n",
      "epoch:4; batch 12544; train accuracy: 0.841835\n",
      "epoch 4; batch 12672; loss 0.045337\n",
      "epoch:4; batch 12672; train accuracy: 0.841891\n",
      "epoch 4; batch 12800; loss 0.024618\n",
      "epoch:4; batch 12800; train accuracy: 0.841949\n",
      "epoch 4; batch 12928; loss 0.029363\n",
      "epoch:4; batch 12928; train accuracy: 0.842005\n",
      "epoch 4; batch 13056; loss 0.022476\n",
      "epoch:4; batch 13056; train accuracy: 0.842064\n",
      "epoch 4; batch 13184; loss 0.019420\n",
      "epoch:4; batch 13184; train accuracy: 0.842122\n",
      "epoch 4; batch 13312; loss 0.014712\n",
      "epoch:4; batch 13312; train accuracy: 0.842184\n",
      "epoch 4; batch 13440; loss 0.064893\n",
      "epoch:4; batch 13440; train accuracy: 0.842236\n",
      "epoch 4; batch 13568; loss 0.012005\n",
      "epoch:4; batch 13568; train accuracy: 0.842298\n",
      "epoch 4; batch 13696; loss 0.053770\n",
      "epoch:4; batch 13696; train accuracy: 0.842353\n",
      "epoch 4; batch 13824; loss 0.046254\n",
      "epoch:4; batch 13824; train accuracy: 0.842408\n",
      "epoch 4; batch 13952; loss 0.054266\n",
      "epoch:4; batch 13952; train accuracy: 0.842461\n",
      "epoch 4; batch 14080; loss 0.032295\n",
      "epoch:4; batch 14080; train accuracy: 0.842516\n",
      "epoch 4; batch 14208; loss 0.015189\n",
      "epoch:4; batch 14208; train accuracy: 0.842577\n",
      "epoch 4; batch 14336; loss 0.026570\n",
      "epoch:4; batch 14336; train accuracy: 0.842635\n",
      "epoch 4; batch 14464; loss 0.034071\n",
      "epoch:4; batch 14464; train accuracy: 0.842690\n",
      "epoch 4; batch 14592; loss 0.095091\n",
      "epoch:4; batch 14592; train accuracy: 0.842739\n",
      "epoch 4; batch 14720; loss 0.063945\n",
      "epoch:4; batch 14720; train accuracy: 0.842791\n",
      "epoch 4; batch 14848; loss 0.065489\n",
      "epoch:4; batch 14848; train accuracy: 0.842846\n",
      "epoch 4; batch 14976; loss 0.029005\n",
      "epoch:4; batch 14976; train accuracy: 0.842904\n",
      "epoch 4; batch 15104; loss 0.040103\n",
      "epoch:4; batch 15104; train accuracy: 0.842959\n",
      "epoch 4; batch 15232; loss 0.052212\n",
      "epoch:4; batch 15232; train accuracy: 0.843014\n",
      "epoch 4; batch 15360; loss 0.079914\n",
      "epoch:4; batch 15360; train accuracy: 0.843069\n",
      "epoch 4; batch 15488; loss 0.021236\n",
      "epoch:4; batch 15488; train accuracy: 0.843126\n",
      "epoch 4; batch 15616; loss 0.039923\n",
      "epoch:4; batch 15616; train accuracy: 0.843184\n",
      "epoch 4; batch 15744; loss 0.049166\n",
      "epoch:4; batch 15744; train accuracy: 0.843239\n",
      "epoch 4; batch 15872; loss 0.020309\n",
      "epoch:4; batch 15872; train accuracy: 0.843296\n",
      "epoch 4; batch 16000; loss 0.027229\n",
      "epoch:4; batch 16000; train accuracy: 0.843354\n",
      "epoch 4; batch 16128; loss 0.015602\n",
      "epoch:4; batch 16128; train accuracy: 0.843415\n",
      "epoch 4; batch 16256; loss 0.018290\n",
      "epoch:4; batch 16256; train accuracy: 0.843472\n",
      "epoch 4; batch 16384; loss 0.021124\n",
      "epoch:4; batch 16384; train accuracy: 0.843527\n",
      "epoch 4; batch 16512; loss 0.073615\n",
      "epoch:4; batch 16512; train accuracy: 0.843581\n",
      "epoch 4; batch 16640; loss 0.014394\n",
      "epoch:4; batch 16640; train accuracy: 0.843638\n",
      "epoch 4; batch 16768; loss 0.035594\n",
      "epoch:4; batch 16768; train accuracy: 0.843696\n",
      "epoch 4; batch 16896; loss 0.022408\n",
      "epoch:4; batch 16896; train accuracy: 0.843753\n",
      "epoch 4; batch 17024; loss 0.115458\n",
      "epoch:4; batch 17024; train accuracy: 0.843798\n",
      "epoch 4; batch 17152; loss 0.062952\n",
      "epoch:4; batch 17152; train accuracy: 0.843849\n",
      "epoch 4; batch 17280; loss 0.011881\n",
      "epoch:4; batch 17280; train accuracy: 0.843910\n",
      "epoch 4; batch 17408; loss 0.029350\n",
      "epoch:4; batch 17408; train accuracy: 0.843964\n",
      "epoch 4; batch 17536; loss 0.041674\n",
      "epoch:4; batch 17536; train accuracy: 0.844018\n",
      "epoch 4; batch 17664; loss 0.035316\n",
      "epoch:4; batch 17664; train accuracy: 0.844075\n",
      "epoch 4; batch 17792; loss 0.077880\n",
      "epoch:4; batch 17792; train accuracy: 0.844126\n",
      "epoch 4; batch 17920; loss 0.047578\n",
      "epoch:4; batch 17920; train accuracy: 0.844180\n",
      "epoch 4; batch 18048; loss 0.030225\n",
      "epoch:4; batch 18048; train accuracy: 0.844234\n",
      "epoch 4; batch 18176; loss 0.028317\n",
      "epoch:4; batch 18176; train accuracy: 0.844290\n",
      "epoch 4; batch 18304; loss 0.062770\n",
      "epoch:4; batch 18304; train accuracy: 0.844341\n",
      "epoch 4; batch 18432; loss 0.038872\n",
      "epoch:4; batch 18432; train accuracy: 0.844395\n",
      "epoch 4; batch 18560; loss 0.083302\n",
      "epoch:4; batch 18560; train accuracy: 0.844449\n",
      "epoch 4; batch 18688; loss 0.006388\n",
      "epoch:4; batch 18688; train accuracy: 0.844508\n",
      "epoch 4; batch 18816; loss 0.027482\n",
      "epoch:4; batch 18816; train accuracy: 0.844562\n",
      "epoch 4; batch 18944; loss 0.030268\n",
      "epoch:4; batch 18944; train accuracy: 0.844619\n",
      "epoch 4; batch 19072; loss 0.049653\n",
      "epoch:4; batch 19072; train accuracy: 0.844675\n",
      "epoch 4; batch 19200; loss 0.009670\n",
      "epoch:4; batch 19200; train accuracy: 0.844735\n",
      "epoch 4; batch 19328; loss 0.053013\n",
      "epoch:4; batch 19328; train accuracy: 0.844788\n",
      "epoch 4; batch 19456; loss 0.024244\n",
      "epoch:4; batch 19456; train accuracy: 0.844845\n",
      "epoch 4; batch 19584; loss 0.033896\n",
      "epoch:4; batch 19584; train accuracy: 0.844898\n",
      "epoch 4; batch 19712; loss 0.032079\n",
      "epoch:4; batch 19712; train accuracy: 0.844954\n",
      "epoch 4; batch 19840; loss 0.020971\n",
      "epoch:4; batch 19840; train accuracy: 0.845011\n",
      "epoch 4; batch 19968; loss 0.041332\n",
      "epoch:4; batch 19968; train accuracy: 0.845064\n",
      "epoch 4; batch 20096; loss 0.040267\n",
      "epoch:4; batch 20096; train accuracy: 0.845120\n",
      "epoch 4; batch 20224; loss 0.029581\n",
      "epoch:4; batch 20224; train accuracy: 0.845173\n",
      "epoch 4; batch 20352; loss 0.043165\n",
      "epoch:4; batch 20352; train accuracy: 0.845227\n",
      "epoch 4; batch 20480; loss 0.054432\n",
      "epoch:4; batch 20480; train accuracy: 0.845274\n",
      "epoch 4; batch 20608; loss 0.138371\n",
      "epoch:4; batch 20608; train accuracy: 0.845309\n",
      "epoch 4; batch 20736; loss 0.021426\n",
      "epoch:4; batch 20736; train accuracy: 0.845368\n",
      "epoch 4; batch 20864; loss 0.027992\n",
      "epoch:4; batch 20864; train accuracy: 0.845424\n",
      "epoch 4; batch 20992; loss 0.089108\n",
      "epoch:4; batch 20992; train accuracy: 0.845474\n",
      "epoch 4; batch 21120; loss 0.049004\n",
      "epoch:4; batch 21120; train accuracy: 0.845527\n",
      "epoch 4; batch 21248; loss 0.090368\n",
      "epoch:4; batch 21248; train accuracy: 0.845574\n",
      "epoch 4; batch 21376; loss 0.035551\n",
      "epoch:4; batch 21376; train accuracy: 0.845627\n",
      "epoch 4; batch 21504; loss 0.046651\n",
      "epoch:4; batch 21504; train accuracy: 0.845679\n",
      "epoch 4; batch 21632; loss 0.014978\n",
      "epoch:4; batch 21632; train accuracy: 0.845738\n",
      "epoch 4; batch 21760; loss 0.021926\n",
      "epoch:4; batch 21760; train accuracy: 0.845791\n",
      "epoch 4; batch 21888; loss 0.034431\n",
      "epoch:4; batch 21888; train accuracy: 0.845843\n",
      "epoch 4; batch 22016; loss 0.075761\n",
      "epoch:4; batch 22016; train accuracy: 0.845896\n",
      "epoch 4; batch 22144; loss 0.017860\n",
      "epoch:4; batch 22144; train accuracy: 0.845952\n",
      "epoch 4; batch 22272; loss 0.029908\n",
      "epoch:4; batch 22272; train accuracy: 0.846004\n",
      "epoch 4; batch 22400; loss 0.041474\n",
      "epoch:4; batch 22400; train accuracy: 0.846060\n",
      "epoch 4; batch 22528; loss 0.056736\n",
      "epoch:4; batch 22528; train accuracy: 0.846112\n",
      "epoch 4; batch 22656; loss 0.091552\n",
      "epoch:4; batch 22656; train accuracy: 0.846165\n",
      "epoch 4; batch 22784; loss 0.025913\n",
      "epoch:4; batch 22784; train accuracy: 0.846217\n",
      "epoch 4; batch 22912; loss 0.014209\n",
      "epoch:4; batch 22912; train accuracy: 0.846275\n",
      "epoch 4; batch 23040; loss 0.023497\n",
      "epoch:4; batch 23040; train accuracy: 0.846330\n",
      "epoch 4; batch 23168; loss 0.051905\n",
      "epoch:4; batch 23168; train accuracy: 0.846383\n",
      "epoch 4; batch 23296; loss 0.038591\n",
      "epoch:4; batch 23296; train accuracy: 0.846438\n",
      "epoch 4; batch 23424; loss 0.037328\n",
      "epoch:4; batch 23424; train accuracy: 0.846487\n",
      "epoch 4; batch 23552; loss 0.012658\n",
      "epoch:4; batch 23552; train accuracy: 0.846545\n",
      "epoch 4; batch 23680; loss 0.042128\n",
      "epoch:4; batch 23680; train accuracy: 0.846600\n",
      "epoch 4; batch 23808; loss 0.023629\n",
      "epoch:4; batch 23808; train accuracy: 0.846655\n",
      "epoch 4; batch 23936; loss 0.019251\n",
      "epoch:4; batch 23936; train accuracy: 0.846710\n",
      "epoch 4; batch 24064; loss 0.023519\n",
      "epoch:4; batch 24064; train accuracy: 0.846765\n",
      "epoch 4; batch 24192; loss 0.010313\n",
      "epoch:4; batch 24192; train accuracy: 0.846823\n",
      "epoch 4; batch 24320; loss 0.041512\n",
      "epoch:4; batch 24320; train accuracy: 0.846878\n",
      "epoch 4; batch 24448; loss 0.053625\n",
      "epoch:4; batch 24448; train accuracy: 0.846927\n",
      "epoch 4; batch 24576; loss 0.053956\n",
      "epoch:4; batch 24576; train accuracy: 0.846976\n",
      "epoch 4; batch 24704; loss 0.021017\n",
      "epoch:4; batch 24704; train accuracy: 0.847030\n",
      "epoch 4; batch 24832; loss 0.008972\n",
      "epoch:4; batch 24832; train accuracy: 0.847088\n",
      "epoch 4; batch 24960; loss 0.033579\n",
      "epoch:4; batch 24960; train accuracy: 0.847143\n",
      "epoch 4; batch 25088; loss 0.022072\n",
      "epoch:4; batch 25088; train accuracy: 0.847197\n",
      "epoch 4; batch 25216; loss 0.014314\n",
      "epoch:4; batch 25216; train accuracy: 0.847255\n",
      "epoch 4; batch 25344; loss 0.044446\n",
      "epoch:4; batch 25344; train accuracy: 0.847309\n",
      "epoch 4; batch 25472; loss 0.021049\n",
      "epoch:4; batch 25472; train accuracy: 0.847364\n",
      "epoch 4; batch 25600; loss 0.006671\n",
      "epoch:4; batch 25600; train accuracy: 0.847421\n",
      "epoch 4; batch 25728; loss 0.052464\n",
      "epoch:4; batch 25728; train accuracy: 0.847473\n",
      "epoch 4; batch 25856; loss 0.013568\n",
      "epoch:4; batch 25856; train accuracy: 0.847527\n",
      "epoch 4; batch 25984; loss 0.023515\n",
      "epoch:4; batch 25984; train accuracy: 0.847581\n",
      "epoch 4; batch 26112; loss 0.030471\n",
      "epoch:4; batch 26112; train accuracy: 0.847636\n",
      "epoch 4; batch 26240; loss 0.033186\n",
      "epoch:4; batch 26240; train accuracy: 0.847687\n",
      "epoch 4; batch 26368; loss 0.050105\n",
      "epoch:4; batch 26368; train accuracy: 0.847735\n",
      "epoch 4; batch 26496; loss 0.043859\n",
      "epoch:4; batch 26496; train accuracy: 0.847787\n",
      "epoch 4; batch 26624; loss 0.039680\n",
      "epoch:4; batch 26624; train accuracy: 0.847838\n",
      "epoch 4; batch 26752; loss 0.046481\n",
      "epoch:4; batch 26752; train accuracy: 0.847886\n",
      "epoch 4; batch 26880; loss 0.023687\n",
      "epoch:4; batch 26880; train accuracy: 0.847940\n",
      "epoch 4; batch 27008; loss 0.048106\n",
      "epoch:4; batch 27008; train accuracy: 0.847988\n",
      "epoch 4; batch 27136; loss 0.065526\n",
      "epoch:4; batch 27136; train accuracy: 0.848036\n",
      "epoch 4; batch 27264; loss 0.030868\n",
      "epoch:4; batch 27264; train accuracy: 0.848090\n",
      "epoch 4; batch 27392; loss 0.038904\n",
      "epoch:4; batch 27392; train accuracy: 0.848144\n",
      "epoch 4; batch 27520; loss 0.005423\n",
      "epoch:4; batch 27520; train accuracy: 0.848201\n",
      "epoch 4; batch 27648; loss 0.023276\n",
      "epoch:4; batch 27648; train accuracy: 0.848255\n",
      "epoch 4; batch 27776; loss 0.025574\n",
      "epoch:4; batch 27776; train accuracy: 0.848303\n",
      "epoch 4; batch 27904; loss 0.033275\n",
      "epoch:4; batch 27904; train accuracy: 0.848356\n",
      "epoch 4; batch 28032; loss 0.040827\n",
      "epoch:4; batch 28032; train accuracy: 0.848407\n",
      "epoch 4; batch 28160; loss 0.053457\n",
      "epoch:4; batch 28160; train accuracy: 0.848458\n",
      "epoch 4; batch 28288; loss 0.040464\n",
      "epoch:4; batch 28288; train accuracy: 0.848509\n",
      "epoch 4; batch 28416; loss 0.160614\n",
      "epoch:4; batch 28416; train accuracy: 0.848556\n",
      "epoch 4; batch 28544; loss 0.013344\n",
      "epoch:4; batch 28544; train accuracy: 0.848610\n",
      "epoch 4; batch 28672; loss 0.202018\n",
      "epoch:4; batch 28672; train accuracy: 0.848658\n",
      "epoch 4; batch 28800; loss 0.034245\n",
      "epoch:4; batch 28800; train accuracy: 0.848711\n",
      "epoch 4; batch 28928; loss 0.031290\n",
      "epoch:4; batch 28928; train accuracy: 0.848761\n",
      "epoch 4; batch 29056; loss 0.033105\n",
      "epoch:4; batch 29056; train accuracy: 0.848812\n",
      "epoch 4; batch 29184; loss 0.051830\n",
      "epoch:4; batch 29184; train accuracy: 0.848859\n",
      "epoch 4; batch 29312; loss 0.012276\n",
      "epoch:4; batch 29312; train accuracy: 0.848916\n",
      "epoch 4; batch 29440; loss 0.042507\n",
      "epoch:4; batch 29440; train accuracy: 0.848969\n",
      "epoch 4; batch 29568; loss 0.019364\n",
      "epoch:4; batch 29568; train accuracy: 0.849022\n",
      "epoch 4; batch 29696; loss 0.033040\n",
      "epoch:4; batch 29696; train accuracy: 0.849072\n",
      "epoch 4; batch 29824; loss 0.035990\n",
      "epoch:4; batch 29824; train accuracy: 0.849123\n",
      "epoch 4; batch 29952; loss 0.053795\n",
      "epoch:4; batch 29952; train accuracy: 0.849167\n",
      "epoch 4; batch 30080; loss 0.046655\n",
      "epoch:4; batch 30080; train accuracy: 0.849214\n",
      "epoch 4; batch 30208; loss 0.059785\n",
      "epoch:4; batch 30208; train accuracy: 0.849265\n",
      "epoch 4; batch 30336; loss 0.026752\n",
      "epoch:4; batch 30336; train accuracy: 0.849318\n",
      "epoch 4; batch 30464; loss 0.040987\n",
      "epoch:4; batch 30464; train accuracy: 0.849368\n",
      "epoch 4; batch 30592; loss 0.077147\n",
      "epoch:4; batch 30592; train accuracy: 0.849418\n",
      "epoch 4; batch 30720; loss 0.065466\n",
      "epoch:4; batch 30720; train accuracy: 0.849462\n",
      "epoch 4; batch 30848; loss 0.004695\n",
      "epoch:4; batch 30848; train accuracy: 0.849518\n",
      "epoch 4; batch 30976; loss 0.010838\n",
      "epoch:4; batch 30976; train accuracy: 0.849573\n",
      "epoch 4; batch 31104; loss 0.038653\n",
      "epoch:4; batch 31104; train accuracy: 0.849623\n",
      "epoch 4; batch 31232; loss 0.045091\n",
      "epoch:4; batch 31232; train accuracy: 0.849676\n",
      "epoch 4; batch 31360; loss 0.049100\n",
      "epoch:4; batch 31360; train accuracy: 0.849726\n",
      "epoch 4; batch 31488; loss 0.009612\n",
      "epoch:4; batch 31488; train accuracy: 0.849781\n",
      "epoch 4; batch 31616; loss 0.030742\n",
      "epoch:4; batch 31616; train accuracy: 0.849834\n",
      "epoch 4; batch 31744; loss 0.028616\n",
      "epoch:4; batch 31744; train accuracy: 0.849886\n",
      "epoch 4; batch 31872; loss 0.022555\n",
      "epoch:4; batch 31872; train accuracy: 0.849939\n",
      "epoch 4; batch 32000; loss 0.016035\n",
      "epoch:4; batch 32000; train accuracy: 0.849991\n",
      "epoch 4; batch 32128; loss 0.033085\n",
      "epoch:4; batch 32128; train accuracy: 0.850044\n",
      "epoch 4; batch 32256; loss 0.056799\n",
      "epoch:4; batch 32256; train accuracy: 0.850093\n",
      "epoch 4; batch 32384; loss 0.035853\n",
      "epoch:4; batch 32384; train accuracy: 0.850143\n",
      "epoch 4; batch 32512; loss 0.052328\n",
      "epoch:4; batch 32512; train accuracy: 0.850189\n",
      "epoch 4; batch 32640; loss 0.006511\n",
      "epoch:4; batch 32640; train accuracy: 0.850245\n",
      "epoch 4; batch 32768; loss 0.005873\n",
      "epoch:4; batch 32768; train accuracy: 0.850300\n",
      "epoch 4; batch 32896; loss 0.021168\n",
      "epoch:4; batch 32896; train accuracy: 0.850355\n",
      "epoch 4; batch 33024; loss 0.044244\n",
      "epoch:4; batch 33024; train accuracy: 0.850401\n",
      "epoch 4; batch 33152; loss 0.040323\n",
      "epoch:4; batch 33152; train accuracy: 0.850451\n",
      "epoch 4; batch 33280; loss 0.051143\n",
      "epoch:4; batch 33280; train accuracy: 0.850503\n",
      "epoch 4; batch 33408; loss 0.060154\n",
      "epoch:4; batch 33408; train accuracy: 0.850552\n",
      "epoch 4; batch 33536; loss 0.015310\n",
      "epoch:4; batch 33536; train accuracy: 0.850607\n",
      "epoch 4; batch 33664; loss 0.030258\n",
      "epoch:4; batch 33664; train accuracy: 0.850656\n",
      "epoch 4; batch 33792; loss 0.017615\n",
      "epoch:4; batch 33792; train accuracy: 0.850708\n",
      "epoch 4; batch 33920; loss 0.026978\n",
      "epoch:4; batch 33920; train accuracy: 0.850760\n",
      "epoch 4; batch 34048; loss 0.033763\n",
      "epoch:4; batch 34048; train accuracy: 0.850812\n",
      "epoch 4; batch 34176; loss 0.105825\n",
      "epoch:4; batch 34176; train accuracy: 0.850855\n",
      "epoch 4; batch 34304; loss 0.018580\n",
      "epoch:4; batch 34304; train accuracy: 0.850907\n",
      "epoch 4; batch 34432; loss 0.019081\n",
      "epoch:4; batch 34432; train accuracy: 0.850958\n",
      "epoch 4; batch 34560; loss 0.027475\n",
      "epoch:4; batch 34560; train accuracy: 0.851007\n",
      "epoch 4; batch 34688; loss 0.019496\n",
      "epoch:4; batch 34688; train accuracy: 0.851059\n",
      "epoch 4; batch 34816; loss 0.024275\n",
      "epoch:4; batch 34816; train accuracy: 0.851111\n",
      "epoch 4; batch 34944; loss 0.019482\n",
      "epoch:4; batch 34944; train accuracy: 0.851162\n",
      "epoch 4; batch 35072; loss 0.088284\n",
      "epoch:4; batch 35072; train accuracy: 0.851208\n",
      "epoch 4; batch 35200; loss 0.023202\n",
      "epoch:4; batch 35200; train accuracy: 0.851260\n",
      "epoch 4; batch 35328; loss 0.042889\n",
      "epoch:4; batch 35328; train accuracy: 0.851308\n",
      "epoch 4; batch 35456; loss 0.031419\n",
      "epoch:4; batch 35456; train accuracy: 0.851357\n",
      "epoch 4; batch 35584; loss 0.012441\n",
      "epoch:4; batch 35584; train accuracy: 0.851411\n",
      "epoch 4; batch 35712; loss 0.009278\n",
      "epoch:4; batch 35712; train accuracy: 0.851466\n",
      "epoch 4; batch 35840; loss 0.029941\n",
      "epoch:4; batch 35840; train accuracy: 0.851514\n",
      "epoch 4; batch 35968; loss 0.017260\n",
      "epoch:4; batch 35968; train accuracy: 0.851565\n",
      "epoch 4; batch 36096; loss 0.039852\n",
      "epoch:4; batch 36096; train accuracy: 0.851614\n",
      "epoch 4; batch 36224; loss 0.019641\n",
      "epoch:4; batch 36224; train accuracy: 0.851665\n",
      "epoch 4; batch 36352; loss 0.091041\n",
      "epoch:4; batch 36352; train accuracy: 0.851713\n",
      "epoch 4; batch 36480; loss 0.024029\n",
      "epoch:4; batch 36480; train accuracy: 0.851762\n",
      "epoch 4; batch 36608; loss 0.076534\n",
      "epoch:4; batch 36608; train accuracy: 0.851804\n",
      "epoch 4; batch 36736; loss 0.026434\n",
      "epoch:4; batch 36736; train accuracy: 0.851855\n",
      "epoch 4; batch 36864; loss 0.055940\n",
      "epoch:4; batch 36864; train accuracy: 0.851901\n",
      "epoch 4; batch 36992; loss 0.059732\n",
      "epoch:4; batch 36992; train accuracy: 0.851946\n",
      "epoch 4; batch 37120; loss 0.010593\n",
      "epoch:4; batch 37120; train accuracy: 0.852000\n",
      "epoch 4; batch 37248; loss 0.039225\n",
      "epoch:4; batch 37248; train accuracy: 0.852045\n",
      "epoch 4; batch 37376; loss 0.006818\n",
      "epoch:4; batch 37376; train accuracy: 0.852099\n",
      "epoch 4; batch 37504; loss 0.089987\n",
      "epoch:4; batch 37504; train accuracy: 0.852141\n",
      "epoch 4; batch 37632; loss 0.034971\n",
      "epoch:4; batch 37632; train accuracy: 0.852192\n",
      "epoch 4; batch 37760; loss 0.029150\n",
      "epoch:4; batch 37760; train accuracy: 0.852240\n",
      "epoch 4; batch 37888; loss 0.057332\n",
      "epoch:4; batch 37888; train accuracy: 0.852283\n",
      "epoch 4; batch 38016; loss 0.004452\n",
      "epoch:4; batch 38016; train accuracy: 0.852336\n",
      "epoch 4; batch 38144; loss 0.020414\n",
      "epoch:4; batch 38144; train accuracy: 0.852387\n",
      "epoch 4; batch 38272; loss 0.066744\n",
      "epoch:4; batch 38272; train accuracy: 0.852426\n",
      "epoch 4; batch 38400; loss 0.016227\n",
      "epoch:4; batch 38400; train accuracy: 0.852480\n",
      "epoch 4; batch 38528; loss 0.059732\n",
      "epoch:4; batch 38528; train accuracy: 0.852522\n",
      "epoch 4; batch 38656; loss 0.029183\n",
      "epoch:4; batch 38656; train accuracy: 0.852572\n",
      "epoch 4; batch 38784; loss 0.043459\n",
      "epoch:4; batch 38784; train accuracy: 0.852620\n",
      "epoch 4; batch 38912; loss 0.005511\n",
      "epoch:4; batch 38912; train accuracy: 0.852673\n",
      "epoch 4; batch 39040; loss 0.029556\n",
      "epoch:4; batch 39040; train accuracy: 0.852724\n",
      "epoch 4; batch 39168; loss 0.071701\n",
      "epoch:4; batch 39168; train accuracy: 0.852769\n",
      "epoch 4; batch 39296; loss 0.031759\n",
      "epoch:4; batch 39296; train accuracy: 0.852813\n",
      "epoch 4; batch 39424; loss 0.018019\n",
      "epoch:4; batch 39424; train accuracy: 0.852864\n",
      "epoch 4; batch 39552; loss 0.015988\n",
      "epoch:4; batch 39552; train accuracy: 0.852911\n",
      "epoch 4; batch 39680; loss 0.041447\n",
      "epoch:4; batch 39680; train accuracy: 0.852961\n",
      "epoch 4; batch 39808; loss 0.004420\n",
      "epoch:4; batch 39808; train accuracy: 0.853014\n",
      "epoch 4; batch 39936; loss 0.056380\n",
      "epoch:4; batch 39936; train accuracy: 0.853062\n",
      "epoch 4; batch 40064; loss 0.027839\n",
      "epoch:4; batch 40064; train accuracy: 0.853112\n",
      "epoch 4; batch 40192; loss 0.019524\n",
      "epoch:4; batch 40192; train accuracy: 0.853162\n",
      "epoch 4; batch 40320; loss 0.026634\n",
      "epoch:4; batch 40320; train accuracy: 0.853212\n",
      "epoch 4; batch 40448; loss 0.026772\n",
      "epoch:4; batch 40448; train accuracy: 0.853260\n",
      "epoch 4; batch 40576; loss 0.016151\n",
      "epoch:4; batch 40576; train accuracy: 0.853310\n",
      "epoch 4; batch 40704; loss 0.011234\n",
      "epoch:4; batch 40704; train accuracy: 0.853362\n",
      "epoch 4; batch 40832; loss 0.013462\n",
      "epoch:4; batch 40832; train accuracy: 0.853412\n",
      "epoch 4; batch 40960; loss 0.032206\n",
      "epoch:4; batch 40960; train accuracy: 0.853459\n",
      "epoch 4; batch 41088; loss 0.026719\n",
      "epoch:4; batch 41088; train accuracy: 0.853504\n",
      "epoch 4; batch 41216; loss 0.040254\n",
      "epoch:4; batch 41216; train accuracy: 0.853551\n",
      "epoch 4; batch 41344; loss 0.027951\n",
      "epoch:4; batch 41344; train accuracy: 0.853601\n",
      "epoch 4; batch 41472; loss 0.009411\n",
      "epoch:4; batch 41472; train accuracy: 0.853653\n",
      "epoch 4; batch 41600; loss 0.018879\n",
      "epoch:4; batch 41600; train accuracy: 0.853703\n",
      "epoch 4; batch 41728; loss 0.025340\n",
      "epoch:4; batch 41728; train accuracy: 0.853753\n",
      "epoch 4; batch 41856; loss 0.004832\n",
      "epoch:4; batch 41856; train accuracy: 0.853805\n",
      "epoch 4; batch 41984; loss 0.017420\n",
      "epoch:4; batch 41984; train accuracy: 0.853855\n",
      "epoch 4; batch 42112; loss 0.043321\n",
      "epoch:4; batch 42112; train accuracy: 0.853902\n",
      "epoch 4; batch 42240; loss 0.007954\n",
      "epoch:4; batch 42240; train accuracy: 0.853954\n",
      "epoch 4; batch 42368; loss 0.079336\n",
      "epoch:4; batch 42368; train accuracy: 0.853995\n",
      "epoch 4; batch 42496; loss 0.062466\n",
      "epoch:4; batch 42496; train accuracy: 0.854039\n",
      "epoch 4; batch 42624; loss 0.043774\n",
      "epoch:4; batch 42624; train accuracy: 0.854083\n",
      "epoch 4; batch 42752; loss 0.033547\n",
      "epoch:4; batch 42752; train accuracy: 0.854132\n",
      "epoch 4; batch 42880; loss 0.053413\n",
      "epoch:4; batch 42880; train accuracy: 0.854173\n",
      "epoch 4; batch 43008; loss 0.023198\n",
      "epoch:4; batch 43008; train accuracy: 0.854223\n",
      "epoch 4; batch 43136; loss 0.031538\n",
      "epoch:4; batch 43136; train accuracy: 0.854272\n",
      "epoch 4; batch 43264; loss 0.014371\n",
      "epoch:4; batch 43264; train accuracy: 0.854321\n",
      "epoch 4; batch 43392; loss 0.017603\n",
      "epoch:4; batch 43392; train accuracy: 0.854370\n",
      "epoch 4; batch 43520; loss 0.056840\n",
      "epoch:4; batch 43520; train accuracy: 0.854414\n",
      "epoch 4; batch 43648; loss 0.020240\n",
      "epoch:4; batch 43648; train accuracy: 0.854463\n",
      "epoch 4; batch 43776; loss 0.100889\n",
      "epoch:4; batch 43776; train accuracy: 0.854512\n",
      "epoch 4; batch 43904; loss 0.057446\n",
      "epoch:4; batch 43904; train accuracy: 0.854559\n",
      "epoch 4; batch 44032; loss 0.011078\n",
      "epoch:4; batch 44032; train accuracy: 0.854611\n",
      "epoch 4; batch 44160; loss 0.028988\n",
      "epoch:4; batch 44160; train accuracy: 0.854654\n",
      "epoch 4; batch 44288; loss 0.057424\n",
      "epoch:4; batch 44288; train accuracy: 0.854695\n",
      "epoch 4; batch 44416; loss 0.004772\n",
      "epoch:4; batch 44416; train accuracy: 0.854747\n",
      "epoch 4; batch 44544; loss 0.031958\n",
      "epoch:4; batch 44544; train accuracy: 0.854795\n",
      "epoch 4; batch 44672; loss 0.013306\n",
      "epoch:4; batch 44672; train accuracy: 0.854847\n",
      "epoch 4; batch 44800; loss 0.010847\n",
      "epoch:4; batch 44800; train accuracy: 0.854899\n",
      "epoch 4; batch 44928; loss 0.020105\n",
      "epoch:4; batch 44928; train accuracy: 0.854950\n",
      "epoch 4; batch 45056; loss 0.015953\n",
      "epoch:4; batch 45056; train accuracy: 0.854999\n",
      "epoch 4; batch 45184; loss 0.005906\n",
      "epoch:4; batch 45184; train accuracy: 0.855051\n",
      "epoch 4; batch 45312; loss 0.027306\n",
      "epoch:4; batch 45312; train accuracy: 0.855099\n",
      "epoch 4; batch 45440; loss 0.010476\n",
      "epoch:4; batch 45440; train accuracy: 0.855151\n",
      "epoch 4; batch 45568; loss 0.008941\n",
      "epoch:4; batch 45568; train accuracy: 0.855202\n",
      "epoch 4; batch 45696; loss 0.035135\n",
      "epoch:4; batch 45696; train accuracy: 0.855251\n",
      "epoch 4; batch 45824; loss 0.024078\n",
      "epoch:4; batch 45824; train accuracy: 0.855297\n",
      "epoch 4; batch 45952; loss 0.004026\n",
      "epoch:4; batch 45952; train accuracy: 0.855348\n",
      "epoch 4; batch 46080; loss 0.037489\n",
      "epoch:4; batch 46080; train accuracy: 0.855397\n",
      "epoch 4; batch 46208; loss 0.007523\n",
      "epoch:4; batch 46208; train accuracy: 0.855448\n",
      "epoch 4; batch 46336; loss 0.083690\n",
      "epoch:4; batch 46336; train accuracy: 0.855494\n",
      "epoch 4; batch 46464; loss 0.010225\n",
      "epoch:4; batch 46464; train accuracy: 0.855545\n",
      "epoch 4; batch 46592; loss 0.005288\n",
      "epoch:4; batch 46592; train accuracy: 0.855596\n",
      "epoch 4; batch 46720; loss 0.049357\n",
      "epoch:4; batch 46720; train accuracy: 0.855642\n",
      "epoch 4; batch 46848; loss 0.055624\n",
      "epoch:4; batch 46848; train accuracy: 0.855690\n",
      "epoch 4; batch 46976; loss 0.086861\n",
      "epoch:4; batch 46976; train accuracy: 0.855730\n",
      "epoch 4; batch 47104; loss 0.025070\n",
      "epoch:4; batch 47104; train accuracy: 0.855778\n",
      "epoch 4; batch 47232; loss 0.027044\n",
      "epoch:4; batch 47232; train accuracy: 0.855826\n",
      "epoch 4; batch 47360; loss 0.023011\n",
      "epoch:4; batch 47360; train accuracy: 0.855875\n",
      "epoch 4; batch 47488; loss 0.143163\n",
      "epoch:4; batch 47488; train accuracy: 0.855920\n",
      "epoch 4; batch 47616; loss 0.009423\n",
      "epoch:4; batch 47616; train accuracy: 0.855971\n",
      "epoch 4; batch 47744; loss 0.012081\n",
      "epoch:4; batch 47744; train accuracy: 0.856022\n",
      "epoch 4; batch 47872; loss 0.057342\n",
      "epoch:4; batch 47872; train accuracy: 0.856064\n",
      "epoch 4; batch 48000; loss 0.067095\n",
      "epoch:4; batch 48000; train accuracy: 0.856109\n",
      "epoch 4; batch 48128; loss 0.024263\n",
      "epoch:4; batch 48128; train accuracy: 0.856157\n",
      "epoch 4; batch 48256; loss 0.015485\n",
      "epoch:4; batch 48256; train accuracy: 0.856205\n",
      "epoch 4; batch 48384; loss 0.023745\n",
      "epoch:4; batch 48384; train accuracy: 0.856253\n",
      "epoch 4; batch 48512; loss 0.052077\n",
      "epoch:4; batch 48512; train accuracy: 0.856298\n",
      "epoch 4; batch 48640; loss 0.018606\n",
      "epoch:4; batch 48640; train accuracy: 0.856346\n",
      "epoch 4; batch 48768; loss 0.079327\n",
      "epoch:4; batch 48768; train accuracy: 0.856389\n",
      "epoch 4; batch 48896; loss 0.012655\n",
      "epoch:4; batch 48896; train accuracy: 0.856439\n",
      "epoch 4; batch 49024; loss 0.024423\n",
      "epoch:4; batch 49024; train accuracy: 0.856490\n",
      "epoch 4; batch 49152; loss 0.035402\n",
      "epoch:4; batch 49152; train accuracy: 0.856535\n",
      "epoch 4; batch 49280; loss 0.136697\n",
      "epoch:4; batch 49280; train accuracy: 0.856569\n",
      "epoch 4; batch 49408; loss 0.018779\n",
      "epoch:4; batch 49408; train accuracy: 0.856616\n",
      "epoch 4; batch 49536; loss 0.074307\n",
      "epoch:4; batch 49536; train accuracy: 0.856661\n",
      "epoch 4; batch 49664; loss 0.018558\n",
      "epoch:4; batch 49664; train accuracy: 0.856709\n",
      "epoch 4; batch 49792; loss 0.041322\n",
      "epoch:4; batch 49792; train accuracy: 0.856756\n",
      "epoch 4; batch 49920; loss 0.012234\n",
      "epoch:4; batch 49920; train accuracy: 0.856806\n",
      "epoch 4; batch 50048; loss 0.013576\n",
      "epoch:4; batch 50048; train accuracy: 0.856854\n",
      "epoch 4; batch 50176; loss 0.009801\n",
      "epoch:4; batch 50176; train accuracy: 0.856901\n",
      "epoch 4; batch 50304; loss 0.020530\n",
      "epoch:4; batch 50304; train accuracy: 0.856949\n",
      "epoch 4; batch 50432; loss 0.015719\n",
      "epoch:4; batch 50432; train accuracy: 0.856996\n",
      "epoch 4; batch 50560; loss 0.050204\n",
      "epoch:4; batch 50560; train accuracy: 0.857041\n",
      "epoch 4; batch 50688; loss 0.015656\n",
      "epoch:4; batch 50688; train accuracy: 0.857088\n",
      "epoch 4; batch 50816; loss 0.017709\n",
      "epoch:4; batch 50816; train accuracy: 0.857135\n",
      "epoch 4; batch 50944; loss 0.035782\n",
      "epoch:4; batch 50944; train accuracy: 0.857183\n",
      "epoch 4; batch 51072; loss 0.035907\n",
      "epoch:4; batch 51072; train accuracy: 0.857227\n",
      "epoch 4; batch 51200; loss 0.049019\n",
      "epoch:4; batch 51200; train accuracy: 0.857269\n",
      "epoch 4; batch 51328; loss 0.020668\n",
      "epoch:4; batch 51328; train accuracy: 0.857316\n",
      "epoch 4; batch 51456; loss 0.029372\n",
      "epoch:4; batch 51456; train accuracy: 0.857360\n",
      "epoch 4; batch 51584; loss 0.018204\n",
      "epoch:4; batch 51584; train accuracy: 0.857408\n",
      "epoch 4; batch 51712; loss 0.077754\n",
      "epoch:4; batch 51712; train accuracy: 0.857452\n",
      "epoch 4; batch 51840; loss 0.008274\n",
      "epoch:4; batch 51840; train accuracy: 0.857502\n",
      "epoch 4; batch 51968; loss 0.077009\n",
      "epoch:4; batch 51968; train accuracy: 0.857540\n",
      "epoch 4; batch 52096; loss 0.013159\n",
      "epoch:4; batch 52096; train accuracy: 0.857587\n",
      "epoch 4; batch 52224; loss 0.001306\n",
      "epoch:4; batch 52224; train accuracy: 0.857637\n",
      "epoch 4; batch 52352; loss 0.045585\n",
      "epoch:4; batch 52352; train accuracy: 0.857684\n",
      "epoch 4; batch 52480; loss 0.051061\n",
      "epoch:4; batch 52480; train accuracy: 0.857725\n",
      "epoch 4; batch 52608; loss 0.025798\n",
      "epoch:4; batch 52608; train accuracy: 0.857772\n",
      "epoch 4; batch 52736; loss 0.013529\n",
      "epoch:4; batch 52736; train accuracy: 0.857819\n",
      "epoch 4; batch 52864; loss 0.005398\n",
      "epoch:4; batch 52864; train accuracy: 0.857868\n",
      "epoch 4; batch 52992; loss 0.008168\n",
      "epoch:4; batch 52992; train accuracy: 0.857918\n",
      "epoch 4; batch 53120; loss 0.007859\n",
      "epoch:4; batch 53120; train accuracy: 0.857967\n",
      "epoch 4; batch 53248; loss 0.020024\n",
      "epoch:4; batch 53248; train accuracy: 0.858014\n",
      "epoch 4; batch 53376; loss 0.004044\n",
      "epoch:4; batch 53376; train accuracy: 0.858063\n",
      "epoch 4; batch 53504; loss 0.013079\n",
      "epoch:4; batch 53504; train accuracy: 0.858113\n",
      "epoch 4; batch 53632; loss 0.009154\n",
      "epoch:4; batch 53632; train accuracy: 0.858162\n",
      "epoch 4; batch 53760; loss 0.085626\n",
      "epoch:4; batch 53760; train accuracy: 0.858203\n",
      "epoch 4; batch 53888; loss 0.023511\n",
      "epoch:4; batch 53888; train accuracy: 0.858250\n",
      "epoch 4; batch 54016; loss 0.015622\n",
      "epoch:4; batch 54016; train accuracy: 0.858296\n",
      "epoch 4; batch 54144; loss 0.006583\n",
      "epoch:4; batch 54144; train accuracy: 0.858345\n",
      "epoch 4; batch 54272; loss 0.017873\n",
      "epoch:4; batch 54272; train accuracy: 0.858392\n",
      "epoch 4; batch 54400; loss 0.028521\n",
      "epoch:4; batch 54400; train accuracy: 0.858435\n",
      "epoch 4; batch 54528; loss 0.015607\n",
      "epoch:4; batch 54528; train accuracy: 0.858482\n",
      "epoch 4; batch 54656; loss 0.019838\n",
      "epoch:4; batch 54656; train accuracy: 0.858525\n",
      "epoch 4; batch 54784; loss 0.030698\n",
      "epoch:4; batch 54784; train accuracy: 0.858566\n",
      "epoch 4; batch 54912; loss 0.014423\n",
      "epoch:4; batch 54912; train accuracy: 0.858612\n",
      "epoch 4; batch 55040; loss 0.011316\n",
      "epoch:4; batch 55040; train accuracy: 0.858659\n",
      "epoch 4; batch 55168; loss 0.020686\n",
      "epoch:4; batch 55168; train accuracy: 0.858705\n",
      "epoch 4; batch 55296; loss 0.026005\n",
      "epoch:4; batch 55296; train accuracy: 0.858751\n",
      "epoch 4; batch 55424; loss 0.003054\n",
      "epoch:4; batch 55424; train accuracy: 0.858800\n",
      "epoch 4; batch 55552; loss 0.010297\n",
      "epoch:4; batch 55552; train accuracy: 0.858846\n",
      "epoch 4; batch 55680; loss 0.022820\n",
      "epoch:4; batch 55680; train accuracy: 0.858892\n",
      "epoch 4; batch 55808; loss 0.034682\n",
      "epoch:4; batch 55808; train accuracy: 0.858935\n",
      "epoch 4; batch 55936; loss 0.026280\n",
      "epoch:4; batch 55936; train accuracy: 0.858979\n",
      "epoch 4; batch 56064; loss 0.035698\n",
      "epoch:4; batch 56064; train accuracy: 0.859022\n",
      "epoch 4; batch 56192; loss 0.078063\n",
      "epoch:4; batch 56192; train accuracy: 0.859060\n",
      "epoch 4; batch 56320; loss 0.021537\n",
      "epoch:4; batch 56320; train accuracy: 0.859106\n",
      "epoch 4; batch 56448; loss 0.053424\n",
      "epoch:4; batch 56448; train accuracy: 0.859151\n",
      "epoch 4; batch 56576; loss 0.008906\n",
      "epoch:4; batch 56576; train accuracy: 0.859197\n",
      "epoch 4; batch 56704; loss 0.034344\n",
      "epoch:4; batch 56704; train accuracy: 0.859238\n",
      "epoch 4; batch 56832; loss 0.011280\n",
      "epoch:4; batch 56832; train accuracy: 0.859286\n",
      "epoch 4; batch 56960; loss 0.040368\n",
      "epoch:4; batch 56960; train accuracy: 0.859329\n",
      "epoch 4; batch 57088; loss 0.010710\n",
      "epoch:4; batch 57088; train accuracy: 0.859378\n",
      "epoch 4; batch 57216; loss 0.020542\n",
      "epoch:4; batch 57216; train accuracy: 0.859421\n",
      "epoch 4; batch 57344; loss 0.027399\n",
      "epoch:4; batch 57344; train accuracy: 0.859466\n",
      "epoch 4; batch 57472; loss 0.028565\n",
      "epoch:4; batch 57472; train accuracy: 0.859512\n",
      "epoch 4; batch 57600; loss 0.100577\n",
      "epoch:4; batch 57600; train accuracy: 0.859552\n",
      "epoch 4; batch 57728; loss 0.011622\n",
      "epoch:4; batch 57728; train accuracy: 0.859600\n",
      "epoch 4; batch 57856; loss 0.009279\n",
      "epoch:4; batch 57856; train accuracy: 0.859646\n",
      "epoch 4; batch 57984; loss 0.008095\n",
      "epoch:4; batch 57984; train accuracy: 0.859694\n",
      "epoch 4; batch 58112; loss 0.035754\n",
      "epoch:4; batch 58112; train accuracy: 0.859737\n",
      "epoch 4; batch 58240; loss 0.001186\n",
      "epoch:4; batch 58240; train accuracy: 0.859785\n",
      "epoch 4; batch 58368; loss 0.025978\n",
      "epoch:4; batch 58368; train accuracy: 0.859830\n",
      "epoch 4; batch 58496; loss 0.007499\n",
      "epoch:4; batch 58496; train accuracy: 0.859879\n",
      "epoch 4; batch 58624; loss 0.017067\n",
      "epoch:4; batch 58624; train accuracy: 0.859924\n",
      "epoch 4; batch 58752; loss 0.017014\n",
      "epoch:4; batch 58752; train accuracy: 0.859969\n",
      "epoch 4; batch 58880; loss 0.019608\n",
      "epoch:4; batch 58880; train accuracy: 0.860014\n",
      "epoch 4; batch 59008; loss 0.055618\n",
      "epoch:4; batch 59008; train accuracy: 0.860054\n",
      "epoch 4; batch 59136; loss 0.005920\n",
      "epoch:4; batch 59136; train accuracy: 0.860102\n",
      "epoch 4; batch 59264; loss 0.071239\n",
      "epoch:4; batch 59264; train accuracy: 0.860142\n",
      "epoch 4; batch 59392; loss 0.032417\n",
      "epoch:4; batch 59392; train accuracy: 0.860185\n",
      "epoch 4; batch 59520; loss 0.013369\n",
      "epoch:4; batch 59520; train accuracy: 0.860230\n",
      "epoch 4; batch 59648; loss 0.002527\n",
      "epoch:4; batch 59648; train accuracy: 0.860277\n",
      "epoch 4; batch 59776; loss 0.019482\n",
      "epoch:4; batch 59776; train accuracy: 0.860323\n",
      "epoch 4; batch 59904; loss 0.093909\n",
      "epoch:4; batch 59904; train accuracy: 0.860365\n",
      "epoch 4; batch 60032; loss 0.073872\n",
      "epoch:4; batch 60032; train accuracy: 0.860405\n",
      "epoch 4; batch 60160; loss 0.017475\n",
      "epoch:4; batch 60160; train accuracy: 0.860450\n",
      "epoch 4; batch 60288; loss 0.030077\n",
      "epoch:4; batch 60288; train accuracy: 0.860494\n",
      "epoch 4; batch 60416; loss 0.005875\n",
      "epoch:4; batch 60416; train accuracy: 0.860542\n",
      "epoch 4; batch 60544; loss 0.061160\n",
      "epoch:4; batch 60544; train accuracy: 0.860587\n",
      "epoch 4; batch 60672; loss 0.028532\n",
      "epoch:4; batch 60672; train accuracy: 0.860632\n",
      "epoch 4; batch 60800; loss 0.041714\n",
      "epoch:4; batch 60800; train accuracy: 0.860674\n",
      "epoch 4; batch 60928; loss 0.014748\n",
      "epoch:4; batch 60928; train accuracy: 0.860721\n",
      "epoch 4; batch 61056; loss 0.078975\n",
      "epoch:4; batch 61056; train accuracy: 0.860761\n",
      "epoch 4; batch 61184; loss 0.036641\n",
      "epoch:4; batch 61184; train accuracy: 0.860806\n",
      "epoch 4; batch 61312; loss 0.010955\n",
      "epoch:4; batch 61312; train accuracy: 0.860853\n",
      "epoch 4; batch 61440; loss 0.050446\n",
      "epoch:4; batch 61440; train accuracy: 0.860892\n",
      "epoch 4; batch 61568; loss 0.074945\n",
      "epoch:4; batch 61568; train accuracy: 0.860929\n",
      "epoch 4; batch 61696; loss 0.023358\n",
      "epoch:4; batch 61696; train accuracy: 0.860971\n",
      "epoch 4; batch 61824; loss 0.017045\n",
      "epoch:4; batch 61824; train accuracy: 0.861016\n",
      "epoch 4; batch 61952; loss 0.014745\n",
      "epoch:4; batch 61952; train accuracy: 0.861060\n",
      "epoch 4; batch 62080; loss 0.013254\n",
      "epoch:4; batch 62080; train accuracy: 0.861107\n",
      "epoch 4; batch 62208; loss 0.030474\n",
      "epoch:4; batch 62208; train accuracy: 0.861149\n",
      "epoch 4; batch 62336; loss 0.080211\n",
      "epoch:4; batch 62336; train accuracy: 0.861191\n",
      "epoch 4; batch 62464; loss 0.055102\n",
      "epoch:4; batch 62464; train accuracy: 0.861233\n",
      "epoch 4; batch 62592; loss 0.032382\n",
      "epoch:4; batch 62592; train accuracy: 0.861274\n",
      "epoch 4; batch 62720; loss 0.133042\n",
      "epoch:4; batch 62720; train accuracy: 0.861311\n",
      "epoch 4; batch 62848; loss 0.039738\n",
      "epoch:4; batch 62848; train accuracy: 0.861353\n",
      "epoch 4; batch 62976; loss 0.086261\n",
      "epoch:4; batch 62976; train accuracy: 0.861389\n",
      "epoch 4; batch 63104; loss 0.018482\n",
      "epoch:4; batch 63104; train accuracy: 0.861433\n",
      "epoch 4; batch 63232; loss 0.024722\n",
      "epoch:4; batch 63232; train accuracy: 0.861475\n",
      "epoch 4; batch 63360; loss 0.037185\n",
      "epoch:4; batch 63360; train accuracy: 0.861514\n",
      "epoch 4; batch 63488; loss 0.032673\n",
      "epoch:4; batch 63488; train accuracy: 0.861555\n",
      "epoch 4; batch 63616; loss 0.029725\n",
      "epoch:4; batch 63616; train accuracy: 0.861597\n",
      "epoch 4; batch 63744; loss 0.043458\n",
      "epoch:4; batch 63744; train accuracy: 0.861638\n",
      "epoch 4; batch 63872; loss 0.022181\n",
      "epoch:4; batch 63872; train accuracy: 0.861683\n",
      "epoch 4; batch 64000; loss 0.037208\n",
      "epoch:4; batch 64000; train accuracy: 0.861721\n",
      "epoch 4; batch 64128; loss 0.029390\n",
      "epoch:4; batch 64128; train accuracy: 0.861760\n",
      "epoch 4; batch 64256; loss 0.019928\n",
      "epoch:4; batch 64256; train accuracy: 0.861802\n",
      "epoch 4; batch 64384; loss 0.053778\n",
      "epoch:4; batch 64384; train accuracy: 0.861843\n",
      "epoch 4; batch 64512; loss 0.066684\n",
      "epoch:4; batch 64512; train accuracy: 0.861884\n",
      "epoch 4; batch 64640; loss 0.014424\n",
      "epoch:4; batch 64640; train accuracy: 0.861931\n",
      "epoch 4; batch 64768; loss 0.016341\n",
      "epoch:4; batch 64768; train accuracy: 0.861975\n",
      "epoch 4; batch 64896; loss 0.098276\n",
      "epoch:4; batch 64896; train accuracy: 0.862016\n",
      "epoch 4; batch 65024; loss 0.004450\n",
      "epoch:4; batch 65024; train accuracy: 0.862063\n",
      "epoch 4; batch 65152; loss 0.034004\n",
      "epoch:4; batch 65152; train accuracy: 0.862106\n",
      "epoch 4; batch 65280; loss 0.019291\n",
      "epoch:4; batch 65280; train accuracy: 0.862150\n",
      "epoch 4; batch 65408; loss 0.003867\n",
      "epoch:4; batch 65408; train accuracy: 0.862197\n",
      "epoch 4; batch 65536; loss 0.029313\n",
      "epoch:4; batch 65536; train accuracy: 0.862240\n",
      "epoch 4; batch 65664; loss 0.038002\n",
      "epoch:4; batch 65664; train accuracy: 0.862284\n",
      "epoch 4; batch 65792; loss 0.042096\n",
      "epoch:4; batch 65792; train accuracy: 0.862328\n",
      "epoch 4; batch 65920; loss 0.019957\n",
      "epoch:4; batch 65920; train accuracy: 0.862371\n",
      "epoch 4; batch 66048; loss 0.140779\n",
      "epoch:4; batch 66048; train accuracy: 0.862412\n",
      "epoch 4; batch 66176; loss 0.027690\n",
      "epoch:4; batch 66176; train accuracy: 0.862453\n",
      "epoch 4; batch 66304; loss 0.006785\n",
      "epoch:4; batch 66304; train accuracy: 0.862499\n",
      "epoch 4; batch 66432; loss 0.033075\n",
      "epoch:4; batch 66432; train accuracy: 0.862543\n",
      "epoch 4; batch 66560; loss 0.022818\n",
      "epoch:4; batch 66560; train accuracy: 0.862584\n",
      "epoch 4; batch 66688; loss 0.116514\n",
      "epoch:4; batch 66688; train accuracy: 0.862622\n",
      "epoch 4; batch 66816; loss 0.021060\n",
      "epoch:4; batch 66816; train accuracy: 0.862668\n",
      "epoch 4; batch 66944; loss 0.028380\n",
      "epoch:4; batch 66944; train accuracy: 0.862712\n",
      "epoch 4; batch 67072; loss 0.059613\n",
      "epoch:4; batch 67072; train accuracy: 0.862755\n",
      "epoch 4; batch 67200; loss 0.058503\n",
      "epoch:4; batch 67200; train accuracy: 0.862798\n",
      "epoch 4; batch 67328; loss 0.007445\n",
      "epoch:4; batch 67328; train accuracy: 0.862844\n",
      "epoch 4; batch 67456; loss 0.008106\n",
      "epoch:4; batch 67456; train accuracy: 0.862890\n",
      "epoch 4; batch 67584; loss 0.010452\n",
      "epoch:4; batch 67584; train accuracy: 0.862936\n",
      "epoch 4; batch 67712; loss 0.012240\n",
      "epoch:4; batch 67712; train accuracy: 0.862982\n",
      "epoch 4; batch 67840; loss 0.020611\n",
      "epoch:4; batch 67840; train accuracy: 0.863023\n",
      "epoch 4; batch 67968; loss 0.022078\n",
      "epoch:4; batch 67968; train accuracy: 0.863066\n",
      "epoch 4; batch 68096; loss 0.028919\n",
      "epoch:4; batch 68096; train accuracy: 0.863109\n",
      "epoch 4; batch 68224; loss 0.021313\n",
      "epoch:4; batch 68224; train accuracy: 0.863149\n",
      "epoch 4; batch 68352; loss 0.022202\n",
      "epoch:4; batch 68352; train accuracy: 0.863193\n",
      "epoch 4; batch 68480; loss 0.012480\n",
      "epoch:4; batch 68480; train accuracy: 0.863238\n",
      "epoch 4; batch 68608; loss 0.048555\n",
      "epoch:4; batch 68608; train accuracy: 0.863276\n",
      "epoch 4; batch 68736; loss 0.044275\n",
      "epoch:4; batch 68736; train accuracy: 0.863316\n",
      "epoch 4; batch 68864; loss 0.031344\n",
      "epoch:4; batch 68864; train accuracy: 0.863357\n",
      "epoch 4; batch 68992; loss 0.027023\n",
      "epoch:4; batch 68992; train accuracy: 0.863400\n",
      "epoch 4; batch 69120; loss 0.028255\n",
      "epoch:4; batch 69120; train accuracy: 0.863440\n",
      "epoch 4; batch 69248; loss 0.010856\n",
      "epoch:4; batch 69248; train accuracy: 0.863483\n",
      "epoch 4; batch 69376; loss 0.032520\n",
      "epoch:4; batch 69376; train accuracy: 0.863523\n",
      "epoch 4; batch 69504; loss 0.005399\n",
      "epoch:4; batch 69504; train accuracy: 0.863569\n",
      "epoch 4; batch 69632; loss 0.022971\n",
      "epoch:4; batch 69632; train accuracy: 0.863612\n",
      "epoch 4; batch 69760; loss 0.010799\n",
      "epoch:4; batch 69760; train accuracy: 0.863657\n",
      "epoch 4; batch 69888; loss 0.009169\n",
      "epoch:4; batch 69888; train accuracy: 0.863700\n",
      "epoch 4; batch 70016; loss 0.027611\n",
      "epoch:4; batch 70016; train accuracy: 0.863740\n",
      "epoch 4; batch 70144; loss 0.013290\n",
      "epoch:4; batch 70144; train accuracy: 0.863783\n",
      "epoch 4; batch 70272; loss 0.008172\n",
      "epoch:4; batch 70272; train accuracy: 0.863828\n",
      "epoch 4; batch 70400; loss 0.019879\n",
      "epoch:4; batch 70400; train accuracy: 0.863870\n",
      "epoch 4; batch 70528; loss 0.017561\n",
      "epoch:4; batch 70528; train accuracy: 0.863913\n",
      "epoch 4; batch 70656; loss 0.045171\n",
      "epoch:4; batch 70656; train accuracy: 0.863953\n",
      "epoch 4; batch 70784; loss 0.087852\n",
      "epoch:4; batch 70784; train accuracy: 0.863996\n",
      "epoch 4; batch 70912; loss 0.021700\n",
      "epoch:4; batch 70912; train accuracy: 0.864038\n",
      "epoch 4; batch 71040; loss 0.043971\n",
      "epoch:4; batch 71040; train accuracy: 0.864081\n",
      "epoch 4; batch 71168; loss 0.010941\n",
      "epoch:4; batch 71168; train accuracy: 0.864123\n",
      "epoch 4; batch 71296; loss 0.022672\n",
      "epoch:4; batch 71296; train accuracy: 0.864166\n",
      "epoch 4; batch 71424; loss 0.054278\n",
      "epoch:4; batch 71424; train accuracy: 0.864208\n",
      "epoch 4; batch 71552; loss 0.004748\n",
      "epoch:4; batch 71552; train accuracy: 0.864253\n",
      "epoch 4; batch 71680; loss 0.026181\n",
      "epoch:4; batch 71680; train accuracy: 0.864293\n",
      "epoch 4; batch 71808; loss 0.004264\n",
      "epoch:4; batch 71808; train accuracy: 0.864338\n",
      "epoch 4; batch 71936; loss 0.030788\n",
      "epoch:4; batch 71936; train accuracy: 0.864380\n",
      "epoch 4; batch 72064; loss 0.001944\n",
      "epoch:4; batch 72064; train accuracy: 0.864425\n",
      "epoch 4; batch 72192; loss 0.034071\n",
      "epoch:4; batch 72192; train accuracy: 0.864462\n",
      "epoch 4; batch 72320; loss 0.051289\n",
      "epoch:4; batch 72320; train accuracy: 0.864494\n",
      "epoch 4; batch 72448; loss 0.025278\n",
      "epoch:4; batch 72448; train accuracy: 0.864536\n",
      "epoch 4; batch 72576; loss 0.063367\n",
      "epoch:4; batch 72576; train accuracy: 0.864565\n",
      "epoch 4; batch 72704; loss 0.015070\n",
      "epoch:4; batch 72704; train accuracy: 0.864607\n",
      "epoch 4; batch 72832; loss 0.024489\n",
      "epoch:4; batch 72832; train accuracy: 0.864650\n",
      "epoch 4; batch 72960; loss 0.060126\n",
      "epoch:4; batch 72960; train accuracy: 0.864689\n",
      "epoch 4; batch 73088; loss 0.008140\n",
      "epoch:4; batch 73088; train accuracy: 0.864734\n",
      "epoch 4; batch 73216; loss 0.017612\n",
      "epoch:4; batch 73216; train accuracy: 0.864776\n",
      "epoch 4; batch 73344; loss 0.033156\n",
      "epoch:4; batch 73344; train accuracy: 0.864818\n",
      "epoch 4; batch 73472; loss 0.005198\n",
      "epoch:4; batch 73472; train accuracy: 0.864862\n",
      "epoch 4; batch 73600; loss 0.016415\n",
      "epoch:4; batch 73600; train accuracy: 0.864902\n",
      "epoch 4; batch 73728; loss 0.014899\n",
      "epoch:4; batch 73728; train accuracy: 0.864946\n",
      "epoch 4; batch 73856; loss 0.025879\n",
      "epoch:4; batch 73856; train accuracy: 0.864988\n",
      "epoch 4; batch 73984; loss 0.006444\n",
      "epoch:4; batch 73984; train accuracy: 0.865032\n",
      "epoch 4; batch 74112; loss 0.005778\n",
      "epoch:4; batch 74112; train accuracy: 0.865077\n",
      "epoch 4; batch 74240; loss 0.015899\n",
      "epoch:4; batch 74240; train accuracy: 0.865121\n",
      "epoch 4; batch 74368; loss 0.005663\n",
      "epoch:4; batch 74368; train accuracy: 0.865166\n",
      "epoch 4; batch 74496; loss 0.002976\n",
      "epoch:4; batch 74496; train accuracy: 0.865210\n",
      "epoch 4; batch 74624; loss 0.012430\n",
      "epoch:4; batch 74624; train accuracy: 0.865254\n",
      "epoch 4; batch 74752; loss 0.020684\n",
      "epoch:4; batch 74752; train accuracy: 0.865296\n",
      "epoch 4; batch 74880; loss 0.016260\n",
      "epoch:4; batch 74880; train accuracy: 0.865338\n",
      "epoch 4; batch 75008; loss 0.036105\n",
      "epoch:4; batch 75008; train accuracy: 0.865377\n",
      "epoch 4; batch 75136; loss 0.004951\n",
      "epoch:4; batch 75136; train accuracy: 0.865421\n",
      "epoch 4; batch 75264; loss 0.011248\n",
      "epoch:4; batch 75264; train accuracy: 0.865462\n",
      "epoch 4; batch 75392; loss 0.006652\n",
      "epoch:4; batch 75392; train accuracy: 0.865507\n",
      "epoch 4; batch 75520; loss 0.030650\n",
      "epoch:4; batch 75520; train accuracy: 0.865548\n",
      "epoch 4; batch 75648; loss 0.042075\n",
      "epoch:4; batch 75648; train accuracy: 0.865587\n",
      "epoch 4; batch 75776; loss 0.053059\n",
      "epoch:4; batch 75776; train accuracy: 0.865623\n",
      "epoch 4; batch 75904; loss 0.010248\n",
      "epoch:4; batch 75904; train accuracy: 0.865667\n",
      "epoch 4; batch 76032; loss 0.019930\n",
      "epoch:4; batch 76032; train accuracy: 0.865709\n",
      "epoch 4; batch 76160; loss 0.056694\n",
      "epoch:4; batch 76160; train accuracy: 0.865748\n",
      "epoch 4; batch 76288; loss 0.020072\n",
      "epoch:4; batch 76288; train accuracy: 0.865792\n",
      "epoch 4; batch 76416; loss 0.010850\n",
      "epoch:4; batch 76416; train accuracy: 0.865836\n",
      "epoch 4; batch 76544; loss 0.014924\n",
      "epoch:4; batch 76544; train accuracy: 0.865877\n",
      "epoch 4; batch 76672; loss 0.003839\n",
      "epoch:4; batch 76672; train accuracy: 0.865921\n",
      "epoch 4; batch 76800; loss 0.005350\n",
      "epoch:4; batch 76800; train accuracy: 0.865965\n",
      "epoch 4; batch 76928; loss 0.009187\n",
      "epoch:4; batch 76928; train accuracy: 0.866006\n",
      "epoch 4; batch 77056; loss 0.043180\n",
      "epoch:4; batch 77056; train accuracy: 0.866042\n",
      "epoch 4; batch 77184; loss 0.136568\n",
      "epoch:4; batch 77184; train accuracy: 0.866078\n",
      "epoch 4; batch 77312; loss 0.003909\n",
      "epoch:4; batch 77312; train accuracy: 0.866122\n",
      "epoch 4; batch 77440; loss 0.003683\n",
      "epoch:4; batch 77440; train accuracy: 0.866165\n",
      "epoch 4; batch 77568; loss 0.031363\n",
      "epoch:4; batch 77568; train accuracy: 0.866206\n",
      "epoch 4; batch 77696; loss 0.006800\n",
      "epoch:4; batch 77696; train accuracy: 0.866250\n",
      "epoch 4; batch 77824; loss 0.021479\n",
      "epoch:4; batch 77824; train accuracy: 0.866291\n",
      "epoch 4; batch 77952; loss 0.054720\n",
      "epoch:4; batch 77952; train accuracy: 0.866327\n",
      "epoch 4; batch 78080; loss 0.031095\n",
      "epoch:4; batch 78080; train accuracy: 0.866363\n",
      "epoch 4; batch 78208; loss 0.130347\n",
      "epoch:4; batch 78208; train accuracy: 0.866404\n",
      "epoch 4; batch 78336; loss 0.022363\n",
      "epoch:4; batch 78336; train accuracy: 0.866442\n",
      "epoch 4; batch 78464; loss 0.033150\n",
      "epoch:4; batch 78464; train accuracy: 0.866478\n",
      "epoch 4; batch 78592; loss 0.009917\n",
      "epoch:4; batch 78592; train accuracy: 0.866519\n",
      "epoch 4; batch 78720; loss 0.014255\n",
      "epoch:4; batch 78720; train accuracy: 0.866560\n",
      "epoch 4; batch 78848; loss 0.045627\n",
      "epoch:4; batch 78848; train accuracy: 0.866596\n",
      "epoch 4; batch 78976; loss 0.082269\n",
      "epoch:4; batch 78976; train accuracy: 0.866631\n",
      "epoch 4; batch 79104; loss 0.027867\n",
      "epoch:4; batch 79104; train accuracy: 0.866670\n",
      "epoch 4; batch 79232; loss 0.141526\n",
      "epoch:4; batch 79232; train accuracy: 0.866705\n",
      "epoch 4; batch 79360; loss 0.036067\n",
      "epoch:4; batch 79360; train accuracy: 0.866744\n",
      "epoch 4; batch 79488; loss 0.006562\n",
      "epoch:4; batch 79488; train accuracy: 0.866787\n",
      "epoch 4; batch 79616; loss 0.036381\n",
      "epoch:4; batch 79616; train accuracy: 0.866828\n",
      "epoch 4; batch 79744; loss 0.020254\n",
      "epoch:4; batch 79744; train accuracy: 0.866868\n",
      "epoch 4; batch 79872; loss 0.038613\n",
      "epoch:4; batch 79872; train accuracy: 0.866909\n",
      "epoch 4; batch 80000; loss 0.005966\n",
      "epoch:4; batch 80000; train accuracy: 0.866952\n",
      "epoch 4; batch 80128; loss 0.002242\n",
      "epoch:4; batch 80128; train accuracy: 0.866995\n",
      "epoch 4; batch 80256; loss 0.019855\n",
      "epoch:4; batch 80256; train accuracy: 0.867036\n",
      "epoch 4; batch 80384; loss 0.012268\n",
      "epoch:4; batch 80384; train accuracy: 0.867079\n",
      "epoch 4; batch 80512; loss 0.023168\n",
      "epoch:4; batch 80512; train accuracy: 0.867119\n",
      "epoch 4; batch 80640; loss 0.067907\n",
      "epoch:4; batch 80640; train accuracy: 0.867160\n",
      "epoch 4; batch 80768; loss 0.012364\n",
      "epoch:4; batch 80768; train accuracy: 0.867200\n",
      "epoch 4; batch 80896; loss 0.002827\n",
      "epoch:4; batch 80896; train accuracy: 0.867243\n",
      "epoch 4; batch 81024; loss 0.002576\n",
      "epoch:4; batch 81024; train accuracy: 0.867286\n",
      "epoch 4; batch 81152; loss 0.004869\n",
      "epoch:4; batch 81152; train accuracy: 0.867329\n",
      "epoch 4; batch 81280; loss 0.015691\n",
      "epoch:4; batch 81280; train accuracy: 0.867372\n",
      "epoch 4; batch 81408; loss 0.056785\n",
      "epoch:4; batch 81408; train accuracy: 0.867412\n",
      "epoch 4; batch 81536; loss 0.003766\n",
      "epoch:4; batch 81536; train accuracy: 0.867455\n",
      "epoch 4; batch 81664; loss 0.013333\n",
      "epoch:4; batch 81664; train accuracy: 0.867495\n",
      "epoch 4; batch 81792; loss 0.014239\n",
      "epoch:4; batch 81792; train accuracy: 0.867535\n",
      "epoch 4; batch 81920; loss 0.014258\n",
      "epoch:4; batch 81920; train accuracy: 0.867576\n",
      "epoch 4; batch 82048; loss 0.021864\n",
      "epoch:4; batch 82048; train accuracy: 0.867613\n",
      "epoch 4; batch 82176; loss 0.047034\n",
      "epoch:4; batch 82176; train accuracy: 0.867651\n",
      "epoch 4; batch 82304; loss 0.004813\n",
      "epoch:4; batch 82304; train accuracy: 0.867694\n",
      "epoch 4; batch 82432; loss 0.006005\n",
      "epoch:4; batch 82432; train accuracy: 0.867736\n",
      "epoch 4; batch 82560; loss 0.005770\n",
      "epoch:4; batch 82560; train accuracy: 0.867779\n",
      "epoch 4; batch 82688; loss 0.006804\n",
      "epoch:4; batch 82688; train accuracy: 0.867821\n",
      "epoch 4; batch 82816; loss 0.050545\n",
      "epoch:4; batch 82816; train accuracy: 0.867859\n",
      "epoch 4; batch 82944; loss 0.023848\n",
      "epoch:4; batch 82944; train accuracy: 0.867899\n",
      "epoch 4; batch 83072; loss 0.009257\n",
      "epoch:4; batch 83072; train accuracy: 0.867941\n",
      "epoch 4; batch 83200; loss 0.016483\n",
      "epoch:4; batch 83200; train accuracy: 0.867984\n",
      "epoch 4; batch 83328; loss 0.065466\n",
      "epoch:4; batch 83328; train accuracy: 0.868021\n",
      "epoch 4; batch 83456; loss 0.017925\n",
      "epoch:4; batch 83456; train accuracy: 0.868061\n",
      "epoch 4; batch 83584; loss 0.010388\n",
      "epoch:4; batch 83584; train accuracy: 0.868104\n",
      "epoch 4; batch 83712; loss 0.010458\n",
      "epoch:4; batch 83712; train accuracy: 0.868146\n",
      "epoch 4; batch 83840; loss 0.012439\n",
      "epoch:4; batch 83840; train accuracy: 0.868188\n",
      "epoch 4; batch 83968; loss 0.041633\n",
      "epoch:4; batch 83968; train accuracy: 0.868226\n",
      "epoch 4; batch 84096; loss 0.018592\n",
      "epoch:4; batch 84096; train accuracy: 0.868265\n",
      "epoch 4; batch 84224; loss 0.019188\n",
      "epoch:4; batch 84224; train accuracy: 0.868305\n",
      "epoch 4; batch 84352; loss 0.006285\n",
      "epoch:4; batch 84352; train accuracy: 0.868347\n",
      "epoch 4; batch 84480; loss 0.007264\n",
      "epoch:4; batch 84480; train accuracy: 0.868389\n",
      "epoch 4; batch 84608; loss 0.002782\n",
      "epoch:4; batch 84608; train accuracy: 0.868432\n",
      "epoch 4; batch 84736; loss 0.006369\n",
      "epoch:4; batch 84736; train accuracy: 0.868474\n",
      "epoch 4; batch 84864; loss 0.047231\n",
      "epoch:4; batch 84864; train accuracy: 0.868511\n",
      "epoch 4; batch 84992; loss 0.011802\n",
      "epoch:4; batch 84992; train accuracy: 0.868553\n",
      "epoch 4; batch 85120; loss 0.032650\n",
      "epoch:4; batch 85120; train accuracy: 0.868590\n",
      "epoch 4; batch 85248; loss 0.015534\n",
      "epoch:4; batch 85248; train accuracy: 0.868630\n",
      "epoch 4; batch 85376; loss 0.007261\n",
      "epoch:4; batch 85376; train accuracy: 0.868672\n",
      "epoch 4; batch 85504; loss 0.015056\n",
      "epoch:4; batch 85504; train accuracy: 0.868714\n",
      "epoch 4; batch 85632; loss 0.010964\n",
      "epoch:4; batch 85632; train accuracy: 0.868753\n",
      "epoch 4; batch 85760; loss 0.127622\n",
      "epoch:4; batch 85760; train accuracy: 0.868787\n",
      "epoch 4; batch 85888; loss 0.030495\n",
      "epoch:4; batch 85888; train accuracy: 0.868824\n",
      "epoch 4; batch 86016; loss 0.037538\n",
      "epoch:4; batch 86016; train accuracy: 0.868861\n",
      "epoch 4; batch 86144; loss 0.015898\n",
      "epoch:4; batch 86144; train accuracy: 0.868901\n",
      "epoch 4; batch 86272; loss 0.031963\n",
      "epoch:4; batch 86272; train accuracy: 0.868940\n",
      "epoch 4; batch 86400; loss 0.016355\n",
      "epoch:4; batch 86400; train accuracy: 0.868982\n",
      "epoch 4; batch 86528; loss 0.064238\n",
      "epoch:4; batch 86528; train accuracy: 0.869019\n",
      "epoch 4; batch 86656; loss 0.014039\n",
      "epoch:4; batch 86656; train accuracy: 0.869058\n",
      "epoch 4; batch 86784; loss 0.016003\n",
      "epoch:4; batch 86784; train accuracy: 0.869097\n",
      "epoch 4; batch 86912; loss 0.008062\n",
      "epoch:4; batch 86912; train accuracy: 0.869136\n",
      "epoch 4; batch 87040; loss 0.021950\n",
      "epoch:4; batch 87040; train accuracy: 0.869175\n",
      "epoch 4; batch 87168; loss 0.036143\n",
      "epoch:4; batch 87168; train accuracy: 0.869215\n",
      "epoch 4; batch 87296; loss 0.004043\n",
      "epoch:4; batch 87296; train accuracy: 0.869256\n",
      "epoch 4; batch 87424; loss 0.034024\n",
      "epoch:4; batch 87424; train accuracy: 0.869293\n",
      "epoch 4; batch 87552; loss 0.012837\n",
      "epoch:4; batch 87552; train accuracy: 0.869332\n",
      "epoch 4; batch 87680; loss 0.001523\n",
      "epoch:4; batch 87680; train accuracy: 0.869374\n",
      "epoch 4; batch 87808; loss 0.045555\n",
      "epoch:4; batch 87808; train accuracy: 0.869413\n",
      "epoch 4; batch 87936; loss 0.020783\n",
      "epoch:4; batch 87936; train accuracy: 0.869452\n",
      "epoch 4; batch 88064; loss 0.013146\n",
      "epoch:4; batch 88064; train accuracy: 0.869493\n",
      "epoch 4; batch 88192; loss 0.006502\n",
      "epoch:4; batch 88192; train accuracy: 0.869534\n",
      "epoch 4; batch 88320; loss 0.020298\n",
      "epoch:4; batch 88320; train accuracy: 0.869573\n",
      "epoch 4; batch 88448; loss 0.010466\n",
      "epoch:4; batch 88448; train accuracy: 0.869612\n",
      "epoch 4; batch 88576; loss 0.034775\n",
      "epoch:4; batch 88576; train accuracy: 0.869649\n",
      "epoch 4; batch 88704; loss 0.004746\n",
      "epoch:4; batch 88704; train accuracy: 0.869690\n",
      "epoch 4; batch 88832; loss 0.043894\n",
      "epoch:4; batch 88832; train accuracy: 0.869729\n",
      "epoch 4; batch 88960; loss 0.024413\n",
      "epoch:4; batch 88960; train accuracy: 0.869768\n",
      "epoch 4; batch 89088; loss 0.075476\n",
      "epoch:4; batch 89088; train accuracy: 0.869807\n",
      "epoch 4; batch 89216; loss 0.024952\n",
      "epoch:4; batch 89216; train accuracy: 0.869845\n",
      "epoch 4; batch 89344; loss 0.025315\n",
      "epoch:4; batch 89344; train accuracy: 0.869882\n",
      "epoch 4; batch 89472; loss 0.022559\n",
      "epoch:4; batch 89472; train accuracy: 0.869920\n",
      "epoch 4; batch 89600; loss 0.009502\n",
      "epoch:4; batch 89600; train accuracy: 0.869959\n",
      "epoch 4; batch 89728; loss 0.027420\n",
      "epoch:4; batch 89728; train accuracy: 0.869998\n",
      "epoch 4; batch 89856; loss 0.045672\n",
      "epoch:4; batch 89856; train accuracy: 0.870036\n",
      "epoch 4; batch 89984; loss 0.011120\n",
      "epoch:4; batch 89984; train accuracy: 0.870077\n",
      "epoch 4; batch 90112; loss 0.013578\n",
      "epoch:4; batch 90112; train accuracy: 0.870118\n",
      "epoch 4; batch 90240; loss 0.026384\n",
      "epoch:4; batch 90240; train accuracy: 0.870157\n",
      "epoch 4; batch 90368; loss 0.011790\n",
      "epoch:4; batch 90368; train accuracy: 0.870198\n",
      "epoch 4; batch 90496; loss 0.163425\n",
      "epoch:4; batch 90496; train accuracy: 0.870234\n",
      "epoch 4; batch 90624; loss 0.022190\n",
      "epoch:4; batch 90624; train accuracy: 0.870273\n",
      "epoch 4; batch 90752; loss 0.017100\n",
      "epoch:4; batch 90752; train accuracy: 0.870313\n",
      "epoch 4; batch 90880; loss 0.015256\n",
      "epoch:4; batch 90880; train accuracy: 0.870354\n",
      "epoch 4; batch 91008; loss 0.009894\n",
      "epoch:4; batch 91008; train accuracy: 0.870395\n",
      "epoch 4; batch 91136; loss 0.016536\n",
      "epoch:4; batch 91136; train accuracy: 0.870434\n",
      "epoch 4; batch 91264; loss 0.009354\n",
      "epoch:4; batch 91264; train accuracy: 0.870475\n",
      "epoch 4; batch 91392; loss 0.018435\n",
      "epoch:4; batch 91392; train accuracy: 0.870513\n",
      "epoch 4; batch 91520; loss 0.007623\n",
      "epoch:4; batch 91520; train accuracy: 0.870554\n",
      "epoch 4; batch 91648; loss 0.007319\n",
      "epoch:4; batch 91648; train accuracy: 0.870594\n",
      "epoch 4; batch 91776; loss 0.004792\n",
      "epoch:4; batch 91776; train accuracy: 0.870635\n",
      "epoch 4; batch 91904; loss 0.043583\n",
      "epoch:4; batch 91904; train accuracy: 0.870673\n",
      "epoch 4; batch 92032; loss 0.005637\n",
      "epoch:4; batch 92032; train accuracy: 0.870714\n",
      "epoch 4; batch 92160; loss 0.005622\n",
      "epoch:4; batch 92160; train accuracy: 0.870755\n",
      "epoch 4; batch 92288; loss 0.043456\n",
      "epoch:4; batch 92288; train accuracy: 0.870793\n",
      "epoch 4; batch 92416; loss 0.013093\n",
      "epoch:4; batch 92416; train accuracy: 0.870834\n",
      "epoch 4; batch 92544; loss 0.013893\n",
      "epoch:4; batch 92544; train accuracy: 0.870874\n",
      "epoch 4; batch 92672; loss 0.051211\n",
      "epoch:4; batch 92672; train accuracy: 0.870910\n",
      "epoch 4; batch 92800; loss 0.004258\n",
      "epoch:4; batch 92800; train accuracy: 0.870950\n",
      "epoch 4; batch 92928; loss 0.128974\n",
      "epoch:4; batch 92928; train accuracy: 0.870988\n",
      "epoch 4; batch 93056; loss 0.028621\n",
      "epoch:4; batch 93056; train accuracy: 0.871021\n",
      "epoch 4; batch 93184; loss 0.012945\n",
      "epoch:4; batch 93184; train accuracy: 0.871062\n",
      "epoch 4; batch 93312; loss 0.005392\n",
      "epoch:4; batch 93312; train accuracy: 0.871102\n",
      "epoch 4; batch 93440; loss 0.008970\n",
      "epoch:4; batch 93440; train accuracy: 0.871143\n",
      "epoch 4; batch 93568; loss 0.014125\n",
      "epoch:4; batch 93568; train accuracy: 0.871181\n",
      "epoch 4; batch 93696; loss 0.015606\n",
      "epoch:4; batch 93696; train accuracy: 0.871219\n",
      "epoch 4; batch 93824; loss 0.026431\n",
      "epoch:4; batch 93824; train accuracy: 0.871256\n",
      "epoch 4; batch 93952; loss 0.081554\n",
      "epoch:4; batch 93952; train accuracy: 0.871294\n",
      "epoch 4; batch 94080; loss 0.001365\n",
      "epoch:4; batch 94080; train accuracy: 0.871335\n",
      "epoch 4; batch 94208; loss 0.007600\n",
      "epoch:4; batch 94208; train accuracy: 0.871375\n",
      "epoch 4; batch 94336; loss 0.103034\n",
      "epoch:4; batch 94336; train accuracy: 0.871410\n",
      "epoch 4; batch 94464; loss 0.014074\n",
      "epoch:4; batch 94464; train accuracy: 0.871448\n",
      "epoch 4; batch 94592; loss 0.003122\n",
      "epoch:4; batch 94592; train accuracy: 0.871488\n",
      "epoch 4; batch 94720; loss 0.014207\n",
      "epoch:4; batch 94720; train accuracy: 0.871528\n",
      "epoch 4; batch 94848; loss 0.056196\n",
      "epoch:4; batch 94848; train accuracy: 0.871561\n",
      "epoch 4; batch 94976; loss 0.004037\n",
      "epoch:4; batch 94976; train accuracy: 0.871601\n",
      "epoch 4; batch 95104; loss 0.005276\n",
      "epoch:4; batch 95104; train accuracy: 0.871641\n",
      "epoch 4; batch 95232; loss 0.031429\n",
      "epoch:4; batch 95232; train accuracy: 0.871679\n",
      "epoch 4; batch 95360; loss 0.009891\n",
      "epoch:4; batch 95360; train accuracy: 0.871717\n",
      "epoch 4; batch 95488; loss 0.071121\n",
      "epoch:4; batch 95488; train accuracy: 0.871749\n",
      "epoch 4; batch 95616; loss 0.005710\n",
      "epoch:4; batch 95616; train accuracy: 0.871789\n",
      "epoch 4; batch 95744; loss 0.103664\n",
      "epoch:4; batch 95744; train accuracy: 0.871824\n",
      "epoch 4; batch 95872; loss 0.013856\n",
      "epoch:4; batch 95872; train accuracy: 0.871862\n",
      "epoch 4; batch 96000; loss 0.009820\n",
      "epoch:4; batch 96000; train accuracy: 0.871899\n",
      "epoch 4; batch 96128; loss 0.008703\n",
      "epoch:4; batch 96128; train accuracy: 0.871939\n",
      "epoch 4; batch 96256; loss 0.015792\n",
      "epoch:4; batch 96256; train accuracy: 0.871977\n",
      "epoch 4; batch 96384; loss 0.019130\n",
      "epoch:4; batch 96384; train accuracy: 0.872014\n",
      "epoch 4; batch 96512; loss 0.004841\n",
      "epoch:4; batch 96512; train accuracy: 0.872054\n",
      "epoch 4; batch 96640; loss 0.020140\n",
      "epoch:4; batch 96640; train accuracy: 0.872091\n",
      "epoch 4; batch 96768; loss 0.011208\n",
      "epoch:4; batch 96768; train accuracy: 0.872131\n",
      "epoch 4; batch 96896; loss 0.014672\n",
      "epoch:4; batch 96896; train accuracy: 0.872171\n",
      "epoch 4; batch 97024; loss 0.028905\n",
      "epoch:4; batch 97024; train accuracy: 0.872208\n",
      "epoch 4; batch 97152; loss 0.032475\n",
      "epoch:4; batch 97152; train accuracy: 0.872245\n",
      "epoch 4; batch 97280; loss 0.003635\n",
      "epoch:4; batch 97280; train accuracy: 0.872285\n",
      "epoch 4; batch 97408; loss 0.031496\n",
      "epoch:4; batch 97408; train accuracy: 0.872322\n",
      "epoch 4; batch 97536; loss 0.005993\n",
      "epoch:4; batch 97536; train accuracy: 0.872362\n",
      "epoch 4; batch 97664; loss 0.013640\n",
      "epoch:4; batch 97664; train accuracy: 0.872399\n",
      "epoch 4; batch 97792; loss 0.014097\n",
      "epoch:4; batch 97792; train accuracy: 0.872436\n",
      "epoch 4; batch 97920; loss 0.018416\n",
      "epoch:4; batch 97920; train accuracy: 0.872473\n",
      "epoch 4; batch 98048; loss 0.002670\n",
      "epoch:4; batch 98048; train accuracy: 0.872513\n",
      "epoch 4; batch 98176; loss 0.010711\n",
      "epoch:4; batch 98176; train accuracy: 0.872552\n",
      "epoch 4; batch 98304; loss 0.019820\n",
      "epoch:4; batch 98304; train accuracy: 0.872589\n",
      "epoch 4; batch 98432; loss 0.031219\n",
      "epoch:4; batch 98432; train accuracy: 0.872626\n",
      "epoch 4; batch 98560; loss 0.006207\n",
      "epoch:4; batch 98560; train accuracy: 0.872666\n",
      "epoch 4; batch 98688; loss 0.007132\n",
      "epoch:4; batch 98688; train accuracy: 0.872705\n",
      "epoch 4; batch 98816; loss 0.053150\n",
      "epoch:4; batch 98816; train accuracy: 0.872740\n",
      "epoch 4; batch 98944; loss 0.007797\n",
      "epoch:4; batch 98944; train accuracy: 0.872779\n",
      "epoch 4; batch 99072; loss 0.046645\n",
      "epoch:4; batch 99072; train accuracy: 0.872816\n",
      "epoch 4; batch 99200; loss 0.038914\n",
      "epoch:4; batch 99200; train accuracy: 0.872851\n",
      "epoch 4; batch 99328; loss 0.006613\n",
      "epoch:4; batch 99328; train accuracy: 0.872890\n",
      "epoch 4; batch 99456; loss 0.020342\n",
      "epoch:4; batch 99456; train accuracy: 0.872927\n",
      "epoch 4; batch 99584; loss 0.032409\n",
      "epoch:4; batch 99584; train accuracy: 0.872961\n",
      "epoch 4; batch 99712; loss 0.005954\n",
      "epoch:4; batch 99712; train accuracy: 0.873000\n",
      "epoch 4; batch 99840; loss 0.008457\n",
      "epoch:4; batch 99840; train accuracy: 0.873040\n",
      "epoch 4; batch 99968; loss 0.005618\n",
      "epoch:4; batch 99968; train accuracy: 0.873079\n",
      "epoch 4; batch 100096; loss 0.031639\n",
      "epoch:4; batch 100096; train accuracy: 0.873113\n",
      "epoch 4; batch 100224; loss 0.029043\n",
      "epoch:4; batch 100224; train accuracy: 0.873150\n",
      "epoch 4; batch 100352; loss 0.005314\n",
      "epoch:4; batch 100352; train accuracy: 0.873189\n",
      "epoch 4; batch 100480; loss 0.010041\n",
      "epoch:4; batch 100480; train accuracy: 0.873226\n",
      "epoch 4; batch 100608; loss 0.012226\n",
      "epoch:4; batch 100608; train accuracy: 0.873262\n",
      "epoch 4; batch 100736; loss 0.004307\n",
      "epoch:4; batch 100736; train accuracy: 0.873301\n",
      "epoch 4; batch 100864; loss 0.041429\n",
      "epoch:4; batch 100864; train accuracy: 0.873338\n",
      "epoch 4; batch 100992; loss 0.005273\n",
      "epoch:4; batch 100992; train accuracy: 0.873377\n",
      "epoch 4; batch 101120; loss 0.023945\n",
      "epoch:4; batch 101120; train accuracy: 0.873411\n",
      "epoch 4; batch 101248; loss 0.068221\n",
      "epoch:4; batch 101248; train accuracy: 0.873448\n",
      "epoch 4; batch 101376; loss 0.010615\n",
      "epoch:4; batch 101376; train accuracy: 0.873487\n",
      "epoch 4; batch 101504; loss 0.018949\n",
      "epoch:4; batch 101504; train accuracy: 0.873523\n",
      "epoch 4; batch 101632; loss 0.007791\n",
      "epoch:4; batch 101632; train accuracy: 0.873562\n",
      "epoch 4; batch 101760; loss 0.073873\n",
      "epoch:4; batch 101760; train accuracy: 0.873594\n",
      "epoch 4; batch 101888; loss 0.007923\n",
      "epoch:4; batch 101888; train accuracy: 0.873632\n",
      "epoch 4; batch 102016; loss 0.012121\n",
      "epoch:4; batch 102016; train accuracy: 0.873669\n",
      "epoch 4; batch 102144; loss 0.010307\n",
      "epoch:4; batch 102144; train accuracy: 0.873708\n",
      "epoch 4; batch 102272; loss 0.008017\n",
      "epoch:4; batch 102272; train accuracy: 0.873746\n",
      "epoch 4; batch 102400; loss 0.001682\n",
      "epoch:4; batch 102400; train accuracy: 0.873785\n",
      "epoch 4; batch 102528; loss 0.002730\n",
      "epoch:4; batch 102528; train accuracy: 0.873824\n",
      "epoch 4; batch 102656; loss 0.030693\n",
      "epoch:4; batch 102656; train accuracy: 0.873860\n",
      "epoch 4; batch 102784; loss 0.004463\n",
      "epoch:4; batch 102784; train accuracy: 0.873899\n",
      "epoch 4; batch 102912; loss 0.005783\n",
      "epoch:4; batch 102912; train accuracy: 0.873937\n",
      "epoch 4; batch 103040; loss 0.085912\n",
      "epoch:4; batch 103040; train accuracy: 0.873966\n",
      "epoch 4; batch 103168; loss 0.006415\n",
      "epoch:4; batch 103168; train accuracy: 0.874005\n",
      "epoch 4; batch 103296; loss 0.043570\n",
      "epoch:4; batch 103296; train accuracy: 0.874039\n",
      "epoch 4; batch 103424; loss 0.008431\n",
      "epoch:4; batch 103424; train accuracy: 0.874077\n",
      "epoch 4; batch 103552; loss 0.010315\n",
      "epoch:4; batch 103552; train accuracy: 0.874113\n",
      "epoch 4; batch 103680; loss 0.071800\n",
      "epoch:4; batch 103680; train accuracy: 0.874147\n",
      "epoch 4; batch 103808; loss 0.010408\n",
      "epoch:4; batch 103808; train accuracy: 0.874186\n",
      "epoch 4; batch 103936; loss 0.009262\n",
      "epoch:4; batch 103936; train accuracy: 0.874224\n",
      "epoch 4; batch 104064; loss 0.055930\n",
      "epoch:4; batch 104064; train accuracy: 0.874253\n",
      "epoch 4; batch 104192; loss 0.021409\n",
      "epoch:4; batch 104192; train accuracy: 0.874287\n",
      "epoch 4; batch 104320; loss 0.003840\n",
      "epoch:4; batch 104320; train accuracy: 0.874325\n",
      "epoch 4; batch 104448; loss 0.057823\n",
      "epoch:4; batch 104448; train accuracy: 0.874361\n",
      "epoch 4; batch 104576; loss 0.028194\n",
      "epoch:4; batch 104576; train accuracy: 0.874397\n",
      "epoch 4; batch 104704; loss 0.003781\n",
      "epoch:4; batch 104704; train accuracy: 0.874435\n",
      "epoch 4; batch 104832; loss 0.003684\n",
      "epoch:4; batch 104832; train accuracy: 0.874473\n",
      "epoch 4; batch 104960; loss 0.008763\n",
      "epoch:4; batch 104960; train accuracy: 0.874512\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 31773 ; rate: 0.302715\n",
      "y_true_label_1_num: 12633 ; rate: 0.120360\n",
      "y_true_label_2_num: 14787 ; rate: 0.140882\n",
      "y_true_label_3_num: 45767 ; rate: 0.436042\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.991263\n",
      "valid avg_precision: 0.991639\n",
      "valid avg_recall: 0.991111\n",
      "valid avg_f1: 0.991340\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 4475 ; rate: 0.298811\n",
      "y_true_label_1_num: 1788 ; rate: 0.119391\n",
      "y_true_label_2_num: 2174 ; rate: 0.145166\n",
      "y_true_label_3_num: 6539 ; rate: 0.436632\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.721688\n",
      "valid avg_precision: 0.719008\n",
      "valid avg_recall: 0.719685\n",
      "valid avg_f1: 0.718972\n"
     ]
    }
   ],
   "source": [
    "#训练过程\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "for num in range(epochs):\n",
    "    print('epoch %d' % (num+1))\n",
    "    train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=10000).batch(batch_size,drop_remainder=True)\n",
    "    index = 0\n",
    "    for x,y in train:\n",
    "        index +=1\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(x)\n",
    "            #print(y.shape)\n",
    "            #print(labels_pred.shape)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            sparse_categorical_accuracy.update_state(y_true = y, y_pred= labels_pred)\n",
    "            #loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=labels_pred)\n",
    "            print(\"epoch %d; batch %d; loss %f\" % (num+1, index*batch_size, loss.numpy()))\n",
    "            print(\"epoch:%d; batch %d; train accuracy: %f\" % (num+1, index*batch_size, sparse_categorical_accuracy.result()))\n",
    "            grads = tape.gradient(loss, model.trainable_variables) #梯度\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables)) #梯度和变量\n",
    "    \n",
    "    # 模型在训练集上 的评估参数,batch_size 128\n",
    "    print('模型在训练集上的评价指标：')\n",
    "    eval_train = eval_of_experiment((train_x,train_y), batch_size ,model)\n",
    "    eval_train.eval_of_valid()\n",
    "    eval_train.result()\n",
    "    #  模型在验证集上 的评估参数，batch_size 128\n",
    "    print('模型在验证集上的评价指标：')\n",
    "    eval_valid = eval_of_experiment((valid_x,valid_y), batch_size ,model)\n",
    "    eval_valid.eval_of_valid()\n",
    "    eval_valid.result()\n",
    "       "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6  price 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "embedding_matrix = load_obj('./data2/train_sales_embedding_matrix.pkl')\n",
    "\n",
    "model = single_attention_aspect(\n",
    "                maxlen=maxlen, \n",
    "                embedding_matrix=embedding_matrix, \n",
    "                # aspect=loc,\n",
    "                # aspect=ser,\n",
    "                aspect=pri,\n",
    "                embedding_dim=200,\n",
    "                aspect_len=20,\n",
    "                hidden_size=100,\n",
    "                activation=None,\n",
    "                output_size=4\n",
    "            )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "epoch 1; batch 128; loss 1.386337\n",
      "epoch:1; batch 128; train accuracy: 0.320312\n",
      "epoch 1; batch 256; loss 1.345195\n",
      "epoch:1; batch 256; train accuracy: 0.343750\n",
      "epoch 1; batch 384; loss 1.324832\n",
      "epoch:1; batch 384; train accuracy: 0.354167\n",
      "epoch 1; batch 512; loss 1.342003\n",
      "epoch:1; batch 512; train accuracy: 0.359375\n",
      "epoch 1; batch 640; loss 1.305407\n",
      "epoch:1; batch 640; train accuracy: 0.371875\n",
      "epoch 1; batch 768; loss 1.212658\n",
      "epoch:1; batch 768; train accuracy: 0.385417\n",
      "epoch 1; batch 896; loss 1.359240\n",
      "epoch:1; batch 896; train accuracy: 0.379464\n",
      "epoch 1; batch 1024; loss 1.345639\n",
      "epoch:1; batch 1024; train accuracy: 0.373047\n",
      "epoch 1; batch 1152; loss 1.300195\n",
      "epoch:1; batch 1152; train accuracy: 0.373264\n",
      "epoch 1; batch 1280; loss 1.302877\n",
      "epoch:1; batch 1280; train accuracy: 0.380469\n",
      "epoch 1; batch 1408; loss 1.311572\n",
      "epoch:1; batch 1408; train accuracy: 0.382102\n",
      "epoch 1; batch 1536; loss 1.315498\n",
      "epoch:1; batch 1536; train accuracy: 0.380208\n",
      "epoch 1; batch 1664; loss 1.317807\n",
      "epoch:1; batch 1664; train accuracy: 0.378005\n",
      "epoch 1; batch 1792; loss 1.299432\n",
      "epoch:1; batch 1792; train accuracy: 0.384487\n",
      "epoch 1; batch 1920; loss 1.292719\n",
      "epoch:1; batch 1920; train accuracy: 0.384896\n",
      "epoch 1; batch 2048; loss 1.312043\n",
      "epoch:1; batch 2048; train accuracy: 0.386230\n",
      "epoch 1; batch 2176; loss 1.273530\n",
      "epoch:1; batch 2176; train accuracy: 0.389246\n",
      "epoch 1; batch 2304; loss 1.307500\n",
      "epoch:1; batch 2304; train accuracy: 0.386719\n",
      "epoch 1; batch 2432; loss 1.327690\n",
      "epoch:1; batch 2432; train accuracy: 0.381579\n",
      "epoch 1; batch 2560; loss 1.238565\n",
      "epoch:1; batch 2560; train accuracy: 0.382422\n",
      "epoch 1; batch 2688; loss 1.288937\n",
      "epoch:1; batch 2688; train accuracy: 0.383557\n",
      "epoch 1; batch 2816; loss 1.269817\n",
      "epoch:1; batch 2816; train accuracy: 0.384233\n",
      "epoch 1; batch 2944; loss 1.306716\n",
      "epoch:1; batch 2944; train accuracy: 0.383152\n",
      "epoch 1; batch 3072; loss 1.345084\n",
      "epoch:1; batch 3072; train accuracy: 0.381836\n",
      "epoch 1; batch 3200; loss 1.342546\n",
      "epoch:1; batch 3200; train accuracy: 0.381875\n",
      "epoch 1; batch 3328; loss 1.271360\n",
      "epoch:1; batch 3328; train accuracy: 0.381611\n",
      "epoch 1; batch 3456; loss 1.323141\n",
      "epoch:1; batch 3456; train accuracy: 0.382523\n",
      "epoch 1; batch 3584; loss 1.298531\n",
      "epoch:1; batch 3584; train accuracy: 0.382812\n",
      "epoch 1; batch 3712; loss 1.328450\n",
      "epoch:1; batch 3712; train accuracy: 0.381735\n",
      "epoch 1; batch 3840; loss 1.289745\n",
      "epoch:1; batch 3840; train accuracy: 0.384375\n",
      "epoch 1; batch 3968; loss 1.266971\n",
      "epoch:1; batch 3968; train accuracy: 0.384577\n",
      "epoch 1; batch 4096; loss 1.365887\n",
      "epoch:1; batch 4096; train accuracy: 0.381836\n",
      "epoch 1; batch 4224; loss 1.306275\n",
      "epoch:1; batch 4224; train accuracy: 0.382812\n",
      "epoch 1; batch 4352; loss 1.301347\n",
      "epoch:1; batch 4352; train accuracy: 0.384421\n",
      "epoch 1; batch 4480; loss 1.260975\n",
      "epoch:1; batch 4480; train accuracy: 0.384152\n",
      "epoch 1; batch 4608; loss 1.259711\n",
      "epoch:1; batch 4608; train accuracy: 0.383898\n",
      "epoch 1; batch 4736; loss 1.256965\n",
      "epoch:1; batch 4736; train accuracy: 0.385346\n",
      "epoch 1; batch 4864; loss 1.216781\n",
      "epoch:1; batch 4864; train accuracy: 0.388775\n",
      "epoch 1; batch 4992; loss 1.220212\n",
      "epoch:1; batch 4992; train accuracy: 0.391226\n",
      "epoch 1; batch 5120; loss 1.152250\n",
      "epoch:1; batch 5120; train accuracy: 0.393945\n",
      "epoch 1; batch 5248; loss 1.213637\n",
      "epoch:1; batch 5248; train accuracy: 0.397104\n",
      "epoch 1; batch 5376; loss 1.641068\n",
      "epoch:1; batch 5376; train accuracy: 0.395833\n",
      "epoch 1; batch 5504; loss 1.207249\n",
      "epoch:1; batch 5504; train accuracy: 0.396621\n",
      "epoch 1; batch 5632; loss 1.213511\n",
      "epoch:1; batch 5632; train accuracy: 0.398260\n",
      "epoch 1; batch 5760; loss 1.130252\n",
      "epoch:1; batch 5760; train accuracy: 0.401215\n",
      "epoch 1; batch 5888; loss 1.170419\n",
      "epoch:1; batch 5888; train accuracy: 0.402514\n",
      "epoch 1; batch 6016; loss 1.206795\n",
      "epoch:1; batch 6016; train accuracy: 0.405086\n",
      "epoch 1; batch 6144; loss 1.175067\n",
      "epoch:1; batch 6144; train accuracy: 0.406250\n",
      "epoch 1; batch 6272; loss 1.171548\n",
      "epoch:1; batch 6272; train accuracy: 0.408163\n",
      "epoch 1; batch 6400; loss 1.170262\n",
      "epoch:1; batch 6400; train accuracy: 0.410469\n",
      "epoch 1; batch 6528; loss 1.203507\n",
      "epoch:1; batch 6528; train accuracy: 0.411612\n",
      "epoch 1; batch 6656; loss 1.183216\n",
      "epoch:1; batch 6656; train accuracy: 0.412410\n",
      "epoch 1; batch 6784; loss 1.196778\n",
      "epoch:1; batch 6784; train accuracy: 0.413325\n",
      "epoch 1; batch 6912; loss 1.152206\n",
      "epoch:1; batch 6912; train accuracy: 0.415654\n",
      "epoch 1; batch 7040; loss 1.158586\n",
      "epoch:1; batch 7040; train accuracy: 0.417045\n",
      "epoch 1; batch 7168; loss 1.222504\n",
      "epoch:1; batch 7168; train accuracy: 0.417690\n",
      "epoch 1; batch 7296; loss 1.185303\n",
      "epoch:1; batch 7296; train accuracy: 0.418448\n",
      "epoch 1; batch 7424; loss 1.196148\n",
      "epoch:1; batch 7424; train accuracy: 0.420259\n",
      "epoch 1; batch 7552; loss 1.118945\n",
      "epoch:1; batch 7552; train accuracy: 0.422140\n",
      "epoch 1; batch 7680; loss 1.119113\n",
      "epoch:1; batch 7680; train accuracy: 0.423307\n",
      "epoch 1; batch 7808; loss 1.095427\n",
      "epoch:1; batch 7808; train accuracy: 0.425077\n",
      "epoch 1; batch 7936; loss 1.182268\n",
      "epoch:1; batch 7936; train accuracy: 0.426033\n",
      "epoch 1; batch 8064; loss 1.165067\n",
      "epoch:1; batch 8064; train accuracy: 0.426339\n",
      "epoch 1; batch 8192; loss 1.173419\n",
      "epoch:1; batch 8192; train accuracy: 0.427124\n",
      "epoch 1; batch 8320; loss 1.173295\n",
      "epoch:1; batch 8320; train accuracy: 0.427764\n",
      "epoch 1; batch 8448; loss 1.084886\n",
      "epoch:1; batch 8448; train accuracy: 0.429569\n",
      "epoch 1; batch 8576; loss 1.088889\n",
      "epoch:1; batch 8576; train accuracy: 0.430970\n",
      "epoch 1; batch 8704; loss 0.986485\n",
      "epoch:1; batch 8704; train accuracy: 0.433364\n",
      "epoch 1; batch 8832; loss 0.991409\n",
      "epoch:1; batch 8832; train accuracy: 0.436028\n",
      "epoch 1; batch 8960; loss 1.055262\n",
      "epoch:1; batch 8960; train accuracy: 0.437500\n",
      "epoch 1; batch 9088; loss 1.068780\n",
      "epoch:1; batch 9088; train accuracy: 0.439371\n",
      "epoch 1; batch 9216; loss 1.117080\n",
      "epoch:1; batch 9216; train accuracy: 0.440430\n",
      "epoch 1; batch 9344; loss 1.087753\n",
      "epoch:1; batch 9344; train accuracy: 0.441781\n",
      "epoch 1; batch 9472; loss 1.029439\n",
      "epoch:1; batch 9472; train accuracy: 0.444151\n",
      "epoch 1; batch 9600; loss 1.067645\n",
      "epoch:1; batch 9600; train accuracy: 0.445521\n",
      "epoch 1; batch 9728; loss 0.980024\n",
      "epoch:1; batch 9728; train accuracy: 0.447882\n",
      "epoch 1; batch 9856; loss 1.088111\n",
      "epoch:1; batch 9856; train accuracy: 0.449675\n",
      "epoch 1; batch 9984; loss 1.137361\n",
      "epoch:1; batch 9984; train accuracy: 0.450220\n",
      "epoch 1; batch 10112; loss 1.158965\n",
      "epoch:1; batch 10112; train accuracy: 0.451741\n",
      "epoch 1; batch 10240; loss 1.092620\n",
      "epoch:1; batch 10240; train accuracy: 0.452832\n",
      "epoch 1; batch 10368; loss 1.004256\n",
      "epoch:1; batch 10368; train accuracy: 0.454089\n",
      "epoch 1; batch 10496; loss 1.046431\n",
      "epoch:1; batch 10496; train accuracy: 0.455793\n",
      "epoch 1; batch 10624; loss 1.165210\n",
      "epoch:1; batch 10624; train accuracy: 0.456231\n",
      "epoch 1; batch 10752; loss 1.015240\n",
      "epoch:1; batch 10752; train accuracy: 0.458240\n",
      "epoch 1; batch 10880; loss 0.900162\n",
      "epoch:1; batch 10880; train accuracy: 0.460294\n",
      "epoch 1; batch 11008; loss 1.106223\n",
      "epoch:1; batch 11008; train accuracy: 0.461210\n",
      "epoch 1; batch 11136; loss 1.058246\n",
      "epoch:1; batch 11136; train accuracy: 0.462015\n",
      "epoch 1; batch 11264; loss 1.120859\n",
      "epoch:1; batch 11264; train accuracy: 0.462358\n",
      "epoch 1; batch 11392; loss 0.974366\n",
      "epoch:1; batch 11392; train accuracy: 0.463746\n",
      "epoch 1; batch 11520; loss 1.079261\n",
      "epoch:1; batch 11520; train accuracy: 0.464497\n",
      "epoch 1; batch 11648; loss 1.074027\n",
      "epoch:1; batch 11648; train accuracy: 0.465230\n",
      "epoch 1; batch 11776; loss 1.055319\n",
      "epoch:1; batch 11776; train accuracy: 0.466627\n",
      "epoch 1; batch 11904; loss 1.114676\n",
      "epoch:1; batch 11904; train accuracy: 0.467406\n",
      "epoch 1; batch 12032; loss 1.057453\n",
      "epoch:1; batch 12032; train accuracy: 0.467919\n",
      "epoch 1; batch 12160; loss 0.896951\n",
      "epoch:1; batch 12160; train accuracy: 0.469655\n",
      "epoch 1; batch 12288; loss 1.051677\n",
      "epoch:1; batch 12288; train accuracy: 0.470378\n",
      "epoch 1; batch 12416; loss 1.084559\n",
      "epoch:1; batch 12416; train accuracy: 0.470764\n",
      "epoch 1; batch 12544; loss 0.955111\n",
      "epoch:1; batch 12544; train accuracy: 0.471620\n",
      "epoch 1; batch 12672; loss 0.935480\n",
      "epoch:1; batch 12672; train accuracy: 0.473327\n",
      "epoch 1; batch 12800; loss 0.930649\n",
      "epoch:1; batch 12800; train accuracy: 0.474922\n",
      "epoch 1; batch 12928; loss 0.908785\n",
      "epoch:1; batch 12928; train accuracy: 0.476872\n",
      "epoch 1; batch 13056; loss 0.890643\n",
      "epoch:1; batch 13056; train accuracy: 0.478477\n",
      "epoch 1; batch 13184; loss 1.002950\n",
      "epoch:1; batch 13184; train accuracy: 0.479066\n",
      "epoch 1; batch 13312; loss 0.979390\n",
      "epoch:1; batch 13312; train accuracy: 0.480018\n",
      "epoch 1; batch 13440; loss 1.095010\n",
      "epoch:1; batch 13440; train accuracy: 0.480506\n",
      "epoch 1; batch 13568; loss 1.254273\n",
      "epoch:1; batch 13568; train accuracy: 0.481574\n",
      "epoch 1; batch 13696; loss 0.910978\n",
      "epoch:1; batch 13696; train accuracy: 0.482842\n",
      "epoch 1; batch 13824; loss 1.052398\n",
      "epoch:1; batch 13824; train accuracy: 0.483290\n",
      "epoch 1; batch 13952; loss 0.865764\n",
      "epoch:1; batch 13952; train accuracy: 0.484948\n",
      "epoch 1; batch 14080; loss 0.878650\n",
      "epoch:1; batch 14080; train accuracy: 0.486009\n",
      "epoch 1; batch 14208; loss 0.967163\n",
      "epoch:1; batch 14208; train accuracy: 0.486486\n",
      "epoch 1; batch 14336; loss 1.001086\n",
      "epoch:1; batch 14336; train accuracy: 0.487305\n",
      "epoch 1; batch 14464; loss 1.036712\n",
      "epoch:1; batch 14464; train accuracy: 0.487694\n",
      "epoch 1; batch 14592; loss 0.982871\n",
      "epoch:1; batch 14592; train accuracy: 0.488829\n",
      "epoch 1; batch 14720; loss 0.874680\n",
      "epoch:1; batch 14720; train accuracy: 0.490014\n",
      "epoch 1; batch 14848; loss 0.909322\n",
      "epoch:1; batch 14848; train accuracy: 0.491177\n",
      "epoch 1; batch 14976; loss 1.008891\n",
      "epoch:1; batch 14976; train accuracy: 0.492254\n",
      "epoch 1; batch 15104; loss 0.846941\n",
      "epoch:1; batch 15104; train accuracy: 0.493512\n",
      "epoch 1; batch 15232; loss 0.917324\n",
      "epoch:1; batch 15232; train accuracy: 0.494288\n",
      "epoch 1; batch 15360; loss 0.893648\n",
      "epoch:1; batch 15360; train accuracy: 0.495508\n",
      "epoch 1; batch 15488; loss 0.937591\n",
      "epoch:1; batch 15488; train accuracy: 0.496578\n",
      "epoch 1; batch 15616; loss 0.909043\n",
      "epoch:1; batch 15616; train accuracy: 0.497567\n",
      "epoch 1; batch 15744; loss 0.894345\n",
      "epoch:1; batch 15744; train accuracy: 0.498476\n",
      "epoch 1; batch 15872; loss 0.875709\n",
      "epoch:1; batch 15872; train accuracy: 0.499370\n",
      "epoch 1; batch 16000; loss 0.848308\n",
      "epoch:1; batch 16000; train accuracy: 0.500437\n",
      "epoch 1; batch 16128; loss 0.839392\n",
      "epoch:1; batch 16128; train accuracy: 0.501798\n",
      "epoch 1; batch 16256; loss 0.920487\n",
      "epoch:1; batch 16256; train accuracy: 0.502215\n",
      "epoch 1; batch 16384; loss 0.930313\n",
      "epoch:1; batch 16384; train accuracy: 0.502991\n",
      "epoch 1; batch 16512; loss 0.813443\n",
      "epoch:1; batch 16512; train accuracy: 0.504421\n",
      "epoch 1; batch 16640; loss 0.874292\n",
      "epoch:1; batch 16640; train accuracy: 0.505349\n",
      "epoch 1; batch 16768; loss 0.872588\n",
      "epoch:1; batch 16768; train accuracy: 0.505964\n",
      "epoch 1; batch 16896; loss 0.926269\n",
      "epoch:1; batch 16896; train accuracy: 0.506925\n",
      "epoch 1; batch 17024; loss 0.938878\n",
      "epoch:1; batch 17024; train accuracy: 0.508165\n",
      "epoch 1; batch 17152; loss 0.900399\n",
      "epoch:1; batch 17152; train accuracy: 0.509037\n",
      "epoch 1; batch 17280; loss 1.110831\n",
      "epoch:1; batch 17280; train accuracy: 0.508796\n",
      "epoch 1; batch 17408; loss 0.918758\n",
      "epoch:1; batch 17408; train accuracy: 0.509881\n",
      "epoch 1; batch 17536; loss 0.878619\n",
      "epoch:1; batch 17536; train accuracy: 0.510835\n",
      "epoch 1; batch 17664; loss 0.888044\n",
      "epoch:1; batch 17664; train accuracy: 0.511945\n",
      "epoch 1; batch 17792; loss 0.748304\n",
      "epoch:1; batch 17792; train accuracy: 0.513321\n",
      "epoch 1; batch 17920; loss 0.866517\n",
      "epoch:1; batch 17920; train accuracy: 0.514509\n",
      "epoch 1; batch 18048; loss 0.905421\n",
      "epoch:1; batch 18048; train accuracy: 0.515016\n",
      "epoch 1; batch 18176; loss 0.858213\n",
      "epoch:1; batch 18176; train accuracy: 0.515735\n",
      "epoch 1; batch 18304; loss 0.883935\n",
      "epoch:1; batch 18304; train accuracy: 0.516554\n",
      "epoch 1; batch 18432; loss 0.748605\n",
      "epoch:1; batch 18432; train accuracy: 0.517795\n",
      "epoch 1; batch 18560; loss 0.932828\n",
      "epoch:1; batch 18560; train accuracy: 0.518642\n",
      "epoch 1; batch 18688; loss 0.806101\n",
      "epoch:1; batch 18688; train accuracy: 0.519745\n",
      "epoch 1; batch 18816; loss 0.964807\n",
      "epoch:1; batch 18816; train accuracy: 0.520621\n",
      "epoch 1; batch 18944; loss 0.805591\n",
      "epoch:1; batch 18944; train accuracy: 0.521959\n",
      "epoch 1; batch 19072; loss 0.804924\n",
      "epoch:1; batch 19072; train accuracy: 0.522756\n",
      "epoch 1; batch 19200; loss 0.942825\n",
      "epoch:1; batch 19200; train accuracy: 0.523333\n",
      "epoch 1; batch 19328; loss 0.986949\n",
      "epoch:1; batch 19328; train accuracy: 0.523851\n",
      "epoch 1; batch 19456; loss 0.913199\n",
      "epoch:1; batch 19456; train accuracy: 0.524620\n",
      "epoch 1; batch 19584; loss 0.846487\n",
      "epoch:1; batch 19584; train accuracy: 0.525531\n",
      "epoch 1; batch 19712; loss 0.858890\n",
      "epoch:1; batch 19712; train accuracy: 0.526431\n",
      "epoch 1; batch 19840; loss 0.794267\n",
      "epoch:1; batch 19840; train accuracy: 0.527419\n",
      "epoch 1; batch 19968; loss 0.787819\n",
      "epoch:1; batch 19968; train accuracy: 0.528546\n",
      "epoch 1; batch 20096; loss 0.899934\n",
      "epoch:1; batch 20096; train accuracy: 0.529409\n",
      "epoch 1; batch 20224; loss 0.799131\n",
      "epoch:1; batch 20224; train accuracy: 0.530162\n",
      "epoch 1; batch 20352; loss 0.905273\n",
      "epoch:1; batch 20352; train accuracy: 0.530611\n",
      "epoch 1; batch 20480; loss 0.834483\n",
      "epoch:1; batch 20480; train accuracy: 0.531299\n",
      "epoch 1; batch 20608; loss 0.768263\n",
      "epoch:1; batch 20608; train accuracy: 0.532415\n",
      "epoch 1; batch 20736; loss 0.762474\n",
      "epoch:1; batch 20736; train accuracy: 0.533372\n",
      "epoch 1; batch 20864; loss 0.843980\n",
      "epoch:1; batch 20864; train accuracy: 0.534174\n",
      "epoch 1; batch 20992; loss 1.078327\n",
      "epoch:1; batch 20992; train accuracy: 0.534870\n",
      "epoch 1; batch 21120; loss 0.715827\n",
      "epoch:1; batch 21120; train accuracy: 0.535701\n",
      "epoch 1; batch 21248; loss 0.972262\n",
      "epoch:1; batch 21248; train accuracy: 0.535956\n",
      "epoch 1; batch 21376; loss 0.783305\n",
      "epoch:1; batch 21376; train accuracy: 0.536911\n",
      "epoch 1; batch 21504; loss 0.940488\n",
      "epoch:1; batch 21504; train accuracy: 0.537202\n",
      "epoch 1; batch 21632; loss 1.013745\n",
      "epoch:1; batch 21632; train accuracy: 0.537398\n",
      "epoch 1; batch 21760; loss 0.756661\n",
      "epoch:1; batch 21760; train accuracy: 0.538327\n",
      "epoch 1; batch 21888; loss 0.925085\n",
      "epoch:1; batch 21888; train accuracy: 0.538788\n",
      "epoch 1; batch 22016; loss 0.839005\n",
      "epoch:1; batch 22016; train accuracy: 0.539380\n",
      "epoch 1; batch 22144; loss 0.766245\n",
      "epoch:1; batch 22144; train accuracy: 0.540327\n",
      "epoch 1; batch 22272; loss 0.719624\n",
      "epoch:1; batch 22272; train accuracy: 0.541397\n",
      "epoch 1; batch 22400; loss 0.738752\n",
      "epoch:1; batch 22400; train accuracy: 0.542188\n",
      "epoch 1; batch 22528; loss 0.730554\n",
      "epoch:1; batch 22528; train accuracy: 0.543279\n",
      "epoch 1; batch 22656; loss 0.890728\n",
      "epoch:1; batch 22656; train accuracy: 0.543874\n",
      "epoch 1; batch 22784; loss 0.785213\n",
      "epoch:1; batch 22784; train accuracy: 0.544593\n",
      "epoch 1; batch 22912; loss 0.812547\n",
      "epoch:1; batch 22912; train accuracy: 0.545435\n",
      "epoch 1; batch 23040; loss 0.847492\n",
      "epoch:1; batch 23040; train accuracy: 0.545920\n",
      "epoch 1; batch 23168; loss 0.627522\n",
      "epoch:1; batch 23168; train accuracy: 0.547004\n",
      "epoch 1; batch 23296; loss 0.775622\n",
      "epoch:1; batch 23296; train accuracy: 0.547905\n",
      "epoch 1; batch 23424; loss 0.753333\n",
      "epoch:1; batch 23424; train accuracy: 0.548839\n",
      "epoch 1; batch 23552; loss 0.745415\n",
      "epoch:1; batch 23552; train accuracy: 0.549550\n",
      "epoch 1; batch 23680; loss 0.701602\n",
      "epoch:1; batch 23680; train accuracy: 0.550465\n",
      "epoch 1; batch 23808; loss 0.805571\n",
      "epoch:1; batch 23808; train accuracy: 0.550949\n",
      "epoch 1; batch 23936; loss 0.709445\n",
      "epoch:1; batch 23936; train accuracy: 0.551721\n",
      "epoch 1; batch 24064; loss 0.761741\n",
      "epoch:1; batch 24064; train accuracy: 0.552402\n",
      "epoch 1; batch 24192; loss 0.787809\n",
      "epoch:1; batch 24192; train accuracy: 0.553158\n",
      "epoch 1; batch 24320; loss 0.755578\n",
      "epoch:1; batch 24320; train accuracy: 0.553906\n",
      "epoch 1; batch 24448; loss 0.855357\n",
      "epoch:1; batch 24448; train accuracy: 0.554524\n",
      "epoch 1; batch 24576; loss 0.700179\n",
      "epoch:1; batch 24576; train accuracy: 0.555176\n",
      "epoch 1; batch 24704; loss 0.701804\n",
      "epoch:1; batch 24704; train accuracy: 0.556064\n",
      "epoch 1; batch 24832; loss 0.828631\n",
      "epoch:1; batch 24832; train accuracy: 0.556580\n",
      "epoch 1; batch 24960; loss 0.766916\n",
      "epoch:1; batch 24960; train accuracy: 0.557532\n",
      "epoch 1; batch 25088; loss 0.852498\n",
      "epoch:1; batch 25088; train accuracy: 0.558155\n",
      "epoch 1; batch 25216; loss 0.756525\n",
      "epoch:1; batch 25216; train accuracy: 0.558772\n",
      "epoch 1; batch 25344; loss 0.881896\n",
      "epoch:1; batch 25344; train accuracy: 0.559186\n",
      "epoch 1; batch 25472; loss 0.784491\n",
      "epoch:1; batch 25472; train accuracy: 0.559987\n",
      "epoch 1; batch 25600; loss 0.738732\n",
      "epoch:1; batch 25600; train accuracy: 0.560781\n",
      "epoch 1; batch 25728; loss 0.802101\n",
      "epoch:1; batch 25728; train accuracy: 0.561295\n",
      "epoch 1; batch 25856; loss 0.951540\n",
      "epoch:1; batch 25856; train accuracy: 0.561378\n",
      "epoch 1; batch 25984; loss 0.762706\n",
      "epoch:1; batch 25984; train accuracy: 0.562077\n",
      "epoch 1; batch 26112; loss 0.738185\n",
      "epoch:1; batch 26112; train accuracy: 0.562538\n",
      "epoch 1; batch 26240; loss 0.879382\n",
      "epoch:1; batch 26240; train accuracy: 0.562843\n",
      "epoch 1; batch 26368; loss 0.740295\n",
      "epoch:1; batch 26368; train accuracy: 0.563486\n",
      "epoch 1; batch 26496; loss 0.719616\n",
      "epoch:1; batch 26496; train accuracy: 0.564236\n",
      "epoch 1; batch 26624; loss 0.795676\n",
      "epoch:1; batch 26624; train accuracy: 0.564866\n",
      "epoch 1; batch 26752; loss 0.859627\n",
      "epoch:1; batch 26752; train accuracy: 0.565378\n",
      "epoch 1; batch 26880; loss 0.701578\n",
      "epoch:1; batch 26880; train accuracy: 0.566146\n",
      "epoch 1; batch 27008; loss 0.892283\n",
      "epoch:1; batch 27008; train accuracy: 0.566536\n",
      "epoch 1; batch 27136; loss 0.666319\n",
      "epoch:1; batch 27136; train accuracy: 0.567106\n",
      "epoch 1; batch 27264; loss 0.903981\n",
      "epoch:1; batch 27264; train accuracy: 0.567378\n",
      "epoch 1; batch 27392; loss 0.977682\n",
      "epoch:1; batch 27392; train accuracy: 0.567501\n",
      "epoch 1; batch 27520; loss 0.818714\n",
      "epoch:1; batch 27520; train accuracy: 0.568060\n",
      "epoch 1; batch 27648; loss 0.913148\n",
      "epoch:1; batch 27648; train accuracy: 0.568468\n",
      "epoch 1; batch 27776; loss 0.878854\n",
      "epoch:1; batch 27776; train accuracy: 0.568836\n",
      "epoch 1; batch 27904; loss 0.992343\n",
      "epoch:1; batch 27904; train accuracy: 0.569237\n",
      "epoch 1; batch 28032; loss 0.874250\n",
      "epoch:1; batch 28032; train accuracy: 0.569563\n",
      "epoch 1; batch 28160; loss 0.776359\n",
      "epoch:1; batch 28160; train accuracy: 0.570170\n",
      "epoch 1; batch 28288; loss 0.777347\n",
      "epoch:1; batch 28288; train accuracy: 0.570666\n",
      "epoch 1; batch 28416; loss 0.893600\n",
      "epoch:1; batch 28416; train accuracy: 0.571016\n",
      "epoch 1; batch 28544; loss 0.858355\n",
      "epoch:1; batch 28544; train accuracy: 0.571328\n",
      "epoch 1; batch 28672; loss 0.667524\n",
      "epoch:1; batch 28672; train accuracy: 0.572161\n",
      "epoch 1; batch 28800; loss 0.646142\n",
      "epoch:1; batch 28800; train accuracy: 0.572951\n",
      "epoch 1; batch 28928; loss 0.871852\n",
      "epoch:1; batch 28928; train accuracy: 0.573216\n",
      "epoch 1; batch 29056; loss 0.719393\n",
      "epoch:1; batch 29056; train accuracy: 0.573926\n",
      "epoch 1; batch 29184; loss 0.811869\n",
      "epoch:1; batch 29184; train accuracy: 0.574322\n",
      "epoch 1; batch 29312; loss 0.746530\n",
      "epoch:1; batch 29312; train accuracy: 0.574782\n",
      "epoch 1; batch 29440; loss 0.681329\n",
      "epoch:1; batch 29440; train accuracy: 0.575509\n",
      "epoch 1; batch 29568; loss 0.739954\n",
      "epoch:1; batch 29568; train accuracy: 0.575927\n",
      "epoch 1; batch 29696; loss 0.756282\n",
      "epoch:1; batch 29696; train accuracy: 0.576206\n",
      "epoch 1; batch 29824; loss 0.970966\n",
      "epoch:1; batch 29824; train accuracy: 0.576516\n",
      "epoch 1; batch 29952; loss 0.734338\n",
      "epoch:1; batch 29952; train accuracy: 0.577090\n",
      "epoch 1; batch 30080; loss 0.643972\n",
      "epoch:1; batch 30080; train accuracy: 0.577926\n",
      "epoch 1; batch 30208; loss 0.750223\n",
      "epoch:1; batch 30208; train accuracy: 0.578622\n",
      "epoch 1; batch 30336; loss 0.915362\n",
      "epoch:1; batch 30336; train accuracy: 0.578883\n",
      "epoch 1; batch 30464; loss 0.760658\n",
      "epoch:1; batch 30464; train accuracy: 0.579241\n",
      "epoch 1; batch 30592; loss 0.792545\n",
      "epoch:1; batch 30592; train accuracy: 0.579629\n",
      "epoch 1; batch 30720; loss 0.692245\n",
      "epoch:1; batch 30720; train accuracy: 0.580143\n",
      "epoch 1; batch 30848; loss 0.823579\n",
      "epoch:1; batch 30848; train accuracy: 0.580621\n",
      "epoch 1; batch 30976; loss 0.710099\n",
      "epoch:1; batch 30976; train accuracy: 0.581192\n",
      "epoch 1; batch 31104; loss 0.701781\n",
      "epoch:1; batch 31104; train accuracy: 0.581726\n",
      "epoch 1; batch 31232; loss 0.833055\n",
      "epoch:1; batch 31232; train accuracy: 0.582031\n",
      "epoch 1; batch 31360; loss 0.728147\n",
      "epoch:1; batch 31360; train accuracy: 0.582462\n",
      "epoch 1; batch 31488; loss 0.740901\n",
      "epoch:1; batch 31488; train accuracy: 0.582793\n",
      "epoch 1; batch 31616; loss 0.775564\n",
      "epoch:1; batch 31616; train accuracy: 0.583091\n",
      "epoch 1; batch 31744; loss 0.795197\n",
      "epoch:1; batch 31744; train accuracy: 0.583449\n",
      "epoch 1; batch 31872; loss 0.854817\n",
      "epoch:1; batch 31872; train accuracy: 0.583616\n",
      "epoch 1; batch 32000; loss 0.879470\n",
      "epoch:1; batch 32000; train accuracy: 0.584031\n",
      "epoch 1; batch 32128; loss 0.730184\n",
      "epoch:1; batch 32128; train accuracy: 0.584692\n",
      "epoch 1; batch 32256; loss 0.781421\n",
      "epoch:1; batch 32256; train accuracy: 0.584945\n",
      "epoch 1; batch 32384; loss 0.748566\n",
      "epoch:1; batch 32384; train accuracy: 0.585351\n",
      "epoch 1; batch 32512; loss 0.651396\n",
      "epoch:1; batch 32512; train accuracy: 0.585814\n",
      "epoch 1; batch 32640; loss 0.734587\n",
      "epoch:1; batch 32640; train accuracy: 0.586336\n",
      "epoch 1; batch 32768; loss 0.808675\n",
      "epoch:1; batch 32768; train accuracy: 0.586670\n",
      "epoch 1; batch 32896; loss 0.619016\n",
      "epoch:1; batch 32896; train accuracy: 0.587397\n",
      "epoch 1; batch 33024; loss 0.610626\n",
      "epoch:1; batch 33024; train accuracy: 0.587997\n",
      "epoch 1; batch 33152; loss 0.891638\n",
      "epoch:1; batch 33152; train accuracy: 0.588170\n",
      "epoch 1; batch 33280; loss 0.768036\n",
      "epoch:1; batch 33280; train accuracy: 0.588672\n",
      "epoch 1; batch 33408; loss 0.765000\n",
      "epoch:1; batch 33408; train accuracy: 0.588841\n",
      "epoch 1; batch 33536; loss 0.897606\n",
      "epoch:1; batch 33536; train accuracy: 0.589158\n",
      "epoch 1; batch 33664; loss 0.727126\n",
      "epoch:1; batch 33664; train accuracy: 0.589472\n",
      "epoch 1; batch 33792; loss 0.822305\n",
      "epoch:1; batch 33792; train accuracy: 0.589696\n",
      "epoch 1; batch 33920; loss 0.702559\n",
      "epoch:1; batch 33920; train accuracy: 0.590330\n",
      "epoch 1; batch 34048; loss 0.782334\n",
      "epoch:1; batch 34048; train accuracy: 0.590607\n",
      "epoch 1; batch 34176; loss 0.714089\n",
      "epoch:1; batch 34176; train accuracy: 0.590912\n",
      "epoch 1; batch 34304; loss 0.780815\n",
      "epoch:1; batch 34304; train accuracy: 0.591272\n",
      "epoch 1; batch 34432; loss 0.894896\n",
      "epoch:1; batch 34432; train accuracy: 0.591514\n",
      "epoch 1; batch 34560; loss 0.644643\n",
      "epoch:1; batch 34560; train accuracy: 0.592188\n",
      "epoch 1; batch 34688; loss 0.760691\n",
      "epoch:1; batch 34688; train accuracy: 0.592626\n",
      "epoch 1; batch 34816; loss 0.813472\n",
      "epoch:1; batch 34816; train accuracy: 0.592888\n",
      "epoch 1; batch 34944; loss 0.827229\n",
      "epoch:1; batch 34944; train accuracy: 0.593235\n",
      "epoch 1; batch 35072; loss 0.770158\n",
      "epoch:1; batch 35072; train accuracy: 0.593778\n",
      "epoch 1; batch 35200; loss 0.669928\n",
      "epoch:1; batch 35200; train accuracy: 0.594318\n",
      "epoch 1; batch 35328; loss 0.821177\n",
      "epoch:1; batch 35328; train accuracy: 0.594571\n",
      "epoch 1; batch 35456; loss 0.798757\n",
      "epoch:1; batch 35456; train accuracy: 0.594935\n",
      "epoch 1; batch 35584; loss 0.781698\n",
      "epoch:1; batch 35584; train accuracy: 0.595352\n",
      "epoch 1; batch 35712; loss 0.650645\n",
      "epoch:1; batch 35712; train accuracy: 0.595850\n",
      "epoch 1; batch 35840; loss 0.812940\n",
      "epoch:1; batch 35840; train accuracy: 0.596038\n",
      "epoch 1; batch 35968; loss 0.705996\n",
      "epoch:1; batch 35968; train accuracy: 0.596447\n",
      "epoch 1; batch 36096; loss 0.856395\n",
      "epoch:1; batch 36096; train accuracy: 0.596631\n",
      "epoch 1; batch 36224; loss 0.715436\n",
      "epoch:1; batch 36224; train accuracy: 0.596980\n",
      "epoch 1; batch 36352; loss 0.642604\n",
      "epoch:1; batch 36352; train accuracy: 0.597629\n",
      "epoch 1; batch 36480; loss 0.716067\n",
      "epoch:1; batch 36480; train accuracy: 0.597999\n",
      "epoch 1; batch 36608; loss 0.755147\n",
      "epoch:1; batch 36608; train accuracy: 0.598312\n",
      "epoch 1; batch 36736; loss 0.675804\n",
      "epoch:1; batch 36736; train accuracy: 0.598677\n",
      "epoch 1; batch 36864; loss 0.837886\n",
      "epoch:1; batch 36864; train accuracy: 0.598931\n",
      "epoch 1; batch 36992; loss 0.724945\n",
      "epoch:1; batch 36992; train accuracy: 0.599346\n",
      "epoch 1; batch 37120; loss 0.769564\n",
      "epoch:1; batch 37120; train accuracy: 0.599650\n",
      "epoch 1; batch 37248; loss 0.698475\n",
      "epoch:1; batch 37248; train accuracy: 0.600140\n",
      "epoch 1; batch 37376; loss 0.662471\n",
      "epoch:1; batch 37376; train accuracy: 0.600680\n",
      "epoch 1; batch 37504; loss 0.852331\n",
      "epoch:1; batch 37504; train accuracy: 0.600816\n",
      "epoch 1; batch 37632; loss 0.546525\n",
      "epoch:1; batch 37632; train accuracy: 0.601509\n",
      "epoch 1; batch 37760; loss 0.763213\n",
      "epoch:1; batch 37760; train accuracy: 0.601748\n",
      "epoch 1; batch 37888; loss 0.699433\n",
      "epoch:1; batch 37888; train accuracy: 0.602196\n",
      "epoch 1; batch 38016; loss 0.747160\n",
      "epoch:1; batch 38016; train accuracy: 0.602483\n",
      "epoch 1; batch 38144; loss 0.778391\n",
      "epoch:1; batch 38144; train accuracy: 0.602900\n",
      "epoch 1; batch 38272; loss 0.747485\n",
      "epoch:1; batch 38272; train accuracy: 0.603182\n",
      "epoch 1; batch 38400; loss 0.617655\n",
      "epoch:1; batch 38400; train accuracy: 0.603542\n",
      "epoch 1; batch 38528; loss 0.892403\n",
      "epoch:1; batch 38528; train accuracy: 0.603717\n",
      "epoch 1; batch 38656; loss 0.825204\n",
      "epoch:1; batch 38656; train accuracy: 0.604175\n",
      "epoch 1; batch 38784; loss 0.763836\n",
      "epoch:1; batch 38784; train accuracy: 0.604502\n",
      "epoch 1; batch 38912; loss 0.719996\n",
      "epoch:1; batch 38912; train accuracy: 0.604955\n",
      "epoch 1; batch 39040; loss 0.711271\n",
      "epoch:1; batch 39040; train accuracy: 0.605379\n",
      "epoch 1; batch 39168; loss 0.854390\n",
      "epoch:1; batch 39168; train accuracy: 0.605699\n",
      "epoch 1; batch 39296; loss 0.636397\n",
      "epoch:1; batch 39296; train accuracy: 0.606169\n",
      "epoch 1; batch 39424; loss 0.732328\n",
      "epoch:1; batch 39424; train accuracy: 0.606407\n",
      "epoch 1; batch 39552; loss 0.712060\n",
      "epoch:1; batch 39552; train accuracy: 0.606821\n",
      "epoch 1; batch 39680; loss 0.524764\n",
      "epoch:1; batch 39680; train accuracy: 0.607409\n",
      "epoch 1; batch 39808; loss 0.753824\n",
      "epoch:1; batch 39808; train accuracy: 0.607617\n",
      "epoch 1; batch 39936; loss 0.690245\n",
      "epoch:1; batch 39936; train accuracy: 0.607948\n",
      "epoch 1; batch 40064; loss 0.668667\n",
      "epoch:1; batch 40064; train accuracy: 0.608476\n",
      "epoch 1; batch 40192; loss 0.801481\n",
      "epoch:1; batch 40192; train accuracy: 0.608828\n",
      "epoch 1; batch 40320; loss 0.760873\n",
      "epoch:1; batch 40320; train accuracy: 0.609201\n",
      "epoch 1; batch 40448; loss 0.742409\n",
      "epoch:1; batch 40448; train accuracy: 0.609449\n",
      "epoch 1; batch 40576; loss 0.828281\n",
      "epoch:1; batch 40576; train accuracy: 0.609597\n",
      "epoch 1; batch 40704; loss 0.704050\n",
      "epoch:1; batch 40704; train accuracy: 0.609965\n",
      "epoch 1; batch 40832; loss 0.552822\n",
      "epoch:1; batch 40832; train accuracy: 0.610575\n",
      "epoch 1; batch 40960; loss 0.908544\n",
      "epoch:1; batch 40960; train accuracy: 0.610742\n",
      "epoch 1; batch 41088; loss 0.646411\n",
      "epoch:1; batch 41088; train accuracy: 0.611152\n",
      "epoch 1; batch 41216; loss 0.779919\n",
      "epoch:1; batch 41216; train accuracy: 0.611413\n",
      "epoch 1; batch 41344; loss 0.668343\n",
      "epoch:1; batch 41344; train accuracy: 0.611721\n",
      "epoch 1; batch 41472; loss 0.702240\n",
      "epoch:1; batch 41472; train accuracy: 0.612124\n",
      "epoch 1; batch 41600; loss 0.780481\n",
      "epoch:1; batch 41600; train accuracy: 0.612308\n",
      "epoch 1; batch 41728; loss 0.697036\n",
      "epoch:1; batch 41728; train accuracy: 0.612658\n",
      "epoch 1; batch 41856; loss 0.778453\n",
      "epoch:1; batch 41856; train accuracy: 0.612839\n",
      "epoch 1; batch 41984; loss 0.737485\n",
      "epoch:1; batch 41984; train accuracy: 0.613043\n",
      "epoch 1; batch 42112; loss 0.729352\n",
      "epoch:1; batch 42112; train accuracy: 0.613317\n",
      "epoch 1; batch 42240; loss 0.794955\n",
      "epoch:1; batch 42240; train accuracy: 0.613494\n",
      "epoch 1; batch 42368; loss 0.655756\n",
      "epoch:1; batch 42368; train accuracy: 0.613812\n",
      "epoch 1; batch 42496; loss 0.799220\n",
      "epoch:1; batch 42496; train accuracy: 0.613940\n",
      "epoch 1; batch 42624; loss 0.625975\n",
      "epoch:1; batch 42624; train accuracy: 0.614255\n",
      "epoch 1; batch 42752; loss 0.640107\n",
      "epoch:1; batch 42752; train accuracy: 0.614521\n",
      "epoch 1; batch 42880; loss 0.788532\n",
      "epoch:1; batch 42880; train accuracy: 0.614715\n",
      "epoch 1; batch 43008; loss 0.703713\n",
      "epoch:1; batch 43008; train accuracy: 0.615002\n",
      "epoch 1; batch 43136; loss 0.656662\n",
      "epoch:1; batch 43136; train accuracy: 0.615310\n",
      "epoch 1; batch 43264; loss 0.704793\n",
      "epoch:1; batch 43264; train accuracy: 0.615662\n",
      "epoch 1; batch 43392; loss 0.659113\n",
      "epoch:1; batch 43392; train accuracy: 0.616104\n",
      "epoch 1; batch 43520; loss 0.815228\n",
      "epoch:1; batch 43520; train accuracy: 0.616360\n",
      "epoch 1; batch 43648; loss 0.769093\n",
      "epoch:1; batch 43648; train accuracy: 0.616615\n",
      "epoch 1; batch 43776; loss 0.702148\n",
      "epoch:1; batch 43776; train accuracy: 0.616936\n",
      "epoch 1; batch 43904; loss 0.796087\n",
      "epoch:1; batch 43904; train accuracy: 0.617142\n",
      "epoch 1; batch 44032; loss 0.797882\n",
      "epoch:1; batch 44032; train accuracy: 0.617301\n",
      "epoch 1; batch 44160; loss 0.768785\n",
      "epoch:1; batch 44160; train accuracy: 0.617595\n",
      "epoch 1; batch 44288; loss 0.675155\n",
      "epoch:1; batch 44288; train accuracy: 0.617933\n",
      "epoch 1; batch 44416; loss 0.663619\n",
      "epoch:1; batch 44416; train accuracy: 0.618381\n",
      "epoch 1; batch 44544; loss 0.773477\n",
      "epoch:1; batch 44544; train accuracy: 0.618624\n",
      "epoch 1; batch 44672; loss 0.814426\n",
      "epoch:1; batch 44672; train accuracy: 0.618710\n",
      "epoch 1; batch 44800; loss 0.664272\n",
      "epoch:1; batch 44800; train accuracy: 0.619040\n",
      "epoch 1; batch 44928; loss 0.725828\n",
      "epoch:1; batch 44928; train accuracy: 0.619257\n",
      "epoch 1; batch 45056; loss 0.765450\n",
      "epoch:1; batch 45056; train accuracy: 0.619673\n",
      "epoch 1; batch 45184; loss 0.584185\n",
      "epoch:1; batch 45184; train accuracy: 0.620197\n",
      "epoch 1; batch 45312; loss 0.686267\n",
      "epoch:1; batch 45312; train accuracy: 0.620454\n",
      "epoch 1; batch 45440; loss 0.688482\n",
      "epoch:1; batch 45440; train accuracy: 0.620621\n",
      "epoch 1; batch 45568; loss 0.677816\n",
      "epoch:1; batch 45568; train accuracy: 0.621050\n",
      "epoch 1; batch 45696; loss 0.652606\n",
      "epoch:1; batch 45696; train accuracy: 0.621433\n",
      "epoch 1; batch 45824; loss 0.672925\n",
      "epoch:1; batch 45824; train accuracy: 0.621748\n",
      "epoch 1; batch 45952; loss 1.019124\n",
      "epoch:1; batch 45952; train accuracy: 0.621845\n",
      "epoch 1; batch 46080; loss 0.865917\n",
      "epoch:1; batch 46080; train accuracy: 0.621984\n",
      "epoch 1; batch 46208; loss 0.715603\n",
      "epoch:1; batch 46208; train accuracy: 0.622295\n",
      "epoch 1; batch 46336; loss 0.786795\n",
      "epoch:1; batch 46336; train accuracy: 0.622389\n",
      "epoch 1; batch 46464; loss 0.774599\n",
      "epoch:1; batch 46464; train accuracy: 0.622568\n",
      "epoch 1; batch 46592; loss 0.750344\n",
      "epoch:1; batch 46592; train accuracy: 0.622682\n",
      "epoch 1; batch 46720; loss 0.720838\n",
      "epoch:1; batch 46720; train accuracy: 0.622924\n",
      "epoch 1; batch 46848; loss 0.732350\n",
      "epoch:1; batch 46848; train accuracy: 0.623207\n",
      "epoch 1; batch 46976; loss 0.638105\n",
      "epoch:1; batch 46976; train accuracy: 0.623531\n",
      "epoch 1; batch 47104; loss 0.731085\n",
      "epoch:1; batch 47104; train accuracy: 0.623705\n",
      "epoch 1; batch 47232; loss 0.690899\n",
      "epoch:1; batch 47232; train accuracy: 0.623963\n",
      "epoch 1; batch 47360; loss 0.653562\n",
      "epoch:1; batch 47360; train accuracy: 0.624240\n",
      "epoch 1; batch 47488; loss 0.747224\n",
      "epoch:1; batch 47488; train accuracy: 0.624326\n",
      "epoch 1; batch 47616; loss 0.721865\n",
      "epoch:1; batch 47616; train accuracy: 0.624601\n",
      "epoch 1; batch 47744; loss 0.541858\n",
      "epoch:1; batch 47744; train accuracy: 0.625105\n",
      "epoch 1; batch 47872; loss 0.903336\n",
      "epoch:1; batch 47872; train accuracy: 0.625167\n",
      "epoch 1; batch 48000; loss 0.863337\n",
      "epoch:1; batch 48000; train accuracy: 0.625250\n",
      "epoch 1; batch 48128; loss 0.622662\n",
      "epoch:1; batch 48128; train accuracy: 0.625686\n",
      "epoch 1; batch 48256; loss 0.797307\n",
      "epoch:1; batch 48256; train accuracy: 0.625787\n",
      "epoch 1; batch 48384; loss 0.830390\n",
      "epoch:1; batch 48384; train accuracy: 0.625951\n",
      "epoch 1; batch 48512; loss 0.624962\n",
      "epoch:1; batch 48512; train accuracy: 0.626340\n",
      "epoch 1; batch 48640; loss 0.745718\n",
      "epoch:1; batch 48640; train accuracy: 0.626604\n",
      "epoch 1; batch 48768; loss 0.665557\n",
      "epoch:1; batch 48768; train accuracy: 0.626845\n",
      "epoch 1; batch 48896; loss 0.697085\n",
      "epoch:1; batch 48896; train accuracy: 0.627147\n",
      "epoch 1; batch 49024; loss 0.682965\n",
      "epoch:1; batch 49024; train accuracy: 0.627468\n",
      "epoch 1; batch 49152; loss 0.633994\n",
      "epoch:1; batch 49152; train accuracy: 0.627665\n",
      "epoch 1; batch 49280; loss 0.696082\n",
      "epoch:1; batch 49280; train accuracy: 0.627963\n",
      "epoch 1; batch 49408; loss 0.663213\n",
      "epoch:1; batch 49408; train accuracy: 0.628157\n",
      "epoch 1; batch 49536; loss 0.688605\n",
      "epoch:1; batch 49536; train accuracy: 0.628331\n",
      "epoch 1; batch 49664; loss 0.648641\n",
      "epoch:1; batch 49664; train accuracy: 0.628705\n",
      "epoch 1; batch 49792; loss 0.714840\n",
      "epoch:1; batch 49792; train accuracy: 0.628956\n",
      "epoch 1; batch 49920; loss 0.794949\n",
      "epoch:1; batch 49920; train accuracy: 0.629026\n",
      "epoch 1; batch 50048; loss 0.543831\n",
      "epoch:1; batch 50048; train accuracy: 0.629436\n",
      "epoch 1; batch 50176; loss 0.644772\n",
      "epoch:1; batch 50176; train accuracy: 0.629843\n",
      "epoch 1; batch 50304; loss 0.662722\n",
      "epoch:1; batch 50304; train accuracy: 0.630089\n",
      "epoch 1; batch 50432; loss 0.746073\n",
      "epoch:1; batch 50432; train accuracy: 0.630334\n",
      "epoch 1; batch 50560; loss 0.616592\n",
      "epoch:1; batch 50560; train accuracy: 0.630578\n",
      "epoch 1; batch 50688; loss 0.822955\n",
      "epoch:1; batch 50688; train accuracy: 0.630642\n",
      "epoch 1; batch 50816; loss 0.799358\n",
      "epoch:1; batch 50816; train accuracy: 0.630786\n",
      "epoch 1; batch 50944; loss 0.653089\n",
      "epoch:1; batch 50944; train accuracy: 0.631026\n",
      "epoch 1; batch 51072; loss 0.740414\n",
      "epoch:1; batch 51072; train accuracy: 0.631246\n",
      "epoch 1; batch 51200; loss 0.538890\n",
      "epoch:1; batch 51200; train accuracy: 0.631719\n",
      "epoch 1; batch 51328; loss 0.715041\n",
      "epoch:1; batch 51328; train accuracy: 0.631994\n",
      "epoch 1; batch 51456; loss 0.681539\n",
      "epoch:1; batch 51456; train accuracy: 0.632249\n",
      "epoch 1; batch 51584; loss 0.749830\n",
      "epoch:1; batch 51584; train accuracy: 0.632386\n",
      "epoch 1; batch 51712; loss 0.851419\n",
      "epoch:1; batch 51712; train accuracy: 0.632387\n",
      "epoch 1; batch 51840; loss 0.798928\n",
      "epoch:1; batch 51840; train accuracy: 0.632427\n",
      "epoch 1; batch 51968; loss 0.674246\n",
      "epoch:1; batch 51968; train accuracy: 0.632639\n",
      "epoch 1; batch 52096; loss 0.645443\n",
      "epoch:1; batch 52096; train accuracy: 0.633043\n",
      "epoch 1; batch 52224; loss 0.840608\n",
      "epoch:1; batch 52224; train accuracy: 0.633061\n",
      "epoch 1; batch 52352; loss 0.746750\n",
      "epoch:1; batch 52352; train accuracy: 0.633175\n",
      "epoch 1; batch 52480; loss 0.755123\n",
      "epoch:1; batch 52480; train accuracy: 0.633365\n",
      "epoch 1; batch 52608; loss 0.719184\n",
      "epoch:1; batch 52608; train accuracy: 0.633535\n",
      "epoch 1; batch 52736; loss 0.666085\n",
      "epoch:1; batch 52736; train accuracy: 0.633855\n",
      "epoch 1; batch 52864; loss 0.651481\n",
      "epoch:1; batch 52864; train accuracy: 0.634212\n",
      "epoch 1; batch 52992; loss 0.653337\n",
      "epoch:1; batch 52992; train accuracy: 0.634435\n",
      "epoch 1; batch 53120; loss 0.643998\n",
      "epoch:1; batch 53120; train accuracy: 0.634714\n",
      "epoch 1; batch 53248; loss 0.733186\n",
      "epoch:1; batch 53248; train accuracy: 0.634841\n",
      "epoch 1; batch 53376; loss 0.840879\n",
      "epoch:1; batch 53376; train accuracy: 0.634948\n",
      "epoch 1; batch 53504; loss 0.853114\n",
      "epoch:1; batch 53504; train accuracy: 0.635111\n",
      "epoch 1; batch 53632; loss 0.778820\n",
      "epoch:1; batch 53632; train accuracy: 0.635330\n",
      "epoch 1; batch 53760; loss 0.819584\n",
      "epoch:1; batch 53760; train accuracy: 0.635305\n",
      "epoch 1; batch 53888; loss 0.771655\n",
      "epoch:1; batch 53888; train accuracy: 0.635466\n",
      "epoch 1; batch 54016; loss 0.735743\n",
      "epoch:1; batch 54016; train accuracy: 0.635682\n",
      "epoch 1; batch 54144; loss 0.778678\n",
      "epoch:1; batch 54144; train accuracy: 0.635823\n",
      "epoch 1; batch 54272; loss 0.618593\n",
      "epoch:1; batch 54272; train accuracy: 0.636055\n",
      "epoch 1; batch 54400; loss 0.689493\n",
      "epoch:1; batch 54400; train accuracy: 0.636250\n",
      "epoch 1; batch 54528; loss 0.691615\n",
      "epoch:1; batch 54528; train accuracy: 0.636462\n",
      "epoch 1; batch 54656; loss 0.700632\n",
      "epoch:1; batch 54656; train accuracy: 0.636636\n",
      "epoch 1; batch 54784; loss 0.751717\n",
      "epoch:1; batch 54784; train accuracy: 0.636828\n",
      "epoch 1; batch 54912; loss 0.697965\n",
      "epoch:1; batch 54912; train accuracy: 0.637056\n",
      "epoch 1; batch 55040; loss 0.635479\n",
      "epoch:1; batch 55040; train accuracy: 0.637318\n",
      "epoch 1; batch 55168; loss 0.739503\n",
      "epoch:1; batch 55168; train accuracy: 0.637435\n",
      "epoch 1; batch 55296; loss 0.725452\n",
      "epoch:1; batch 55296; train accuracy: 0.637677\n",
      "epoch 1; batch 55424; loss 0.753088\n",
      "epoch:1; batch 55424; train accuracy: 0.637810\n",
      "epoch 1; batch 55552; loss 0.736524\n",
      "epoch:1; batch 55552; train accuracy: 0.637925\n",
      "epoch 1; batch 55680; loss 0.724462\n",
      "epoch:1; batch 55680; train accuracy: 0.638182\n",
      "epoch 1; batch 55808; loss 0.666204\n",
      "epoch:1; batch 55808; train accuracy: 0.638475\n",
      "epoch 1; batch 55936; loss 0.659348\n",
      "epoch:1; batch 55936; train accuracy: 0.638712\n",
      "epoch 1; batch 56064; loss 0.847710\n",
      "epoch:1; batch 56064; train accuracy: 0.638734\n",
      "epoch 1; batch 56192; loss 0.630792\n",
      "epoch:1; batch 56192; train accuracy: 0.638934\n",
      "epoch 1; batch 56320; loss 0.638519\n",
      "epoch:1; batch 56320; train accuracy: 0.639151\n",
      "epoch 1; batch 56448; loss 0.761243\n",
      "epoch:1; batch 56448; train accuracy: 0.639296\n",
      "epoch 1; batch 56576; loss 0.715817\n",
      "epoch:1; batch 56576; train accuracy: 0.639476\n",
      "epoch 1; batch 56704; loss 0.662079\n",
      "epoch:1; batch 56704; train accuracy: 0.639708\n",
      "epoch 1; batch 56832; loss 0.777743\n",
      "epoch:1; batch 56832; train accuracy: 0.639833\n",
      "epoch 1; batch 56960; loss 0.794269\n",
      "epoch:1; batch 56960; train accuracy: 0.639870\n",
      "epoch 1; batch 57088; loss 0.614282\n",
      "epoch:1; batch 57088; train accuracy: 0.640257\n",
      "epoch 1; batch 57216; loss 0.768993\n",
      "epoch:1; batch 57216; train accuracy: 0.640450\n",
      "epoch 1; batch 57344; loss 0.946217\n",
      "epoch:1; batch 57344; train accuracy: 0.640346\n",
      "epoch 1; batch 57472; loss 0.662702\n",
      "epoch:1; batch 57472; train accuracy: 0.640503\n",
      "epoch 1; batch 57600; loss 0.656387\n",
      "epoch:1; batch 57600; train accuracy: 0.640712\n",
      "epoch 1; batch 57728; loss 0.735722\n",
      "epoch:1; batch 57728; train accuracy: 0.640798\n",
      "epoch 1; batch 57856; loss 0.721374\n",
      "epoch:1; batch 57856; train accuracy: 0.640936\n",
      "epoch 1; batch 57984; loss 0.708159\n",
      "epoch:1; batch 57984; train accuracy: 0.641039\n",
      "epoch 1; batch 58112; loss 0.621224\n",
      "epoch:1; batch 58112; train accuracy: 0.641244\n",
      "epoch 1; batch 58240; loss 0.688415\n",
      "epoch:1; batch 58240; train accuracy: 0.641363\n",
      "epoch 1; batch 58368; loss 0.685814\n",
      "epoch:1; batch 58368; train accuracy: 0.641499\n",
      "epoch 1; batch 58496; loss 0.666471\n",
      "epoch:1; batch 58496; train accuracy: 0.641685\n",
      "epoch 1; batch 58624; loss 0.790850\n",
      "epoch:1; batch 58624; train accuracy: 0.641751\n",
      "epoch 1; batch 58752; loss 0.770746\n",
      "epoch:1; batch 58752; train accuracy: 0.641919\n",
      "epoch 1; batch 58880; loss 0.637232\n",
      "epoch:1; batch 58880; train accuracy: 0.642204\n",
      "epoch 1; batch 59008; loss 0.763577\n",
      "epoch:1; batch 59008; train accuracy: 0.642269\n",
      "epoch 1; batch 59136; loss 0.596655\n",
      "epoch:1; batch 59136; train accuracy: 0.642485\n",
      "epoch 1; batch 59264; loss 0.813255\n",
      "epoch:1; batch 59264; train accuracy: 0.642633\n",
      "epoch 1; batch 59392; loss 0.727308\n",
      "epoch:1; batch 59392; train accuracy: 0.642747\n",
      "epoch 1; batch 59520; loss 0.814251\n",
      "epoch:1; batch 59520; train accuracy: 0.642809\n",
      "epoch 1; batch 59648; loss 0.801553\n",
      "epoch:1; batch 59648; train accuracy: 0.642939\n",
      "epoch 1; batch 59776; loss 0.788285\n",
      "epoch:1; batch 59776; train accuracy: 0.643101\n",
      "epoch 1; batch 59904; loss 0.583622\n",
      "epoch:1; batch 59904; train accuracy: 0.643413\n",
      "epoch 1; batch 60032; loss 0.699100\n",
      "epoch:1; batch 60032; train accuracy: 0.643590\n",
      "epoch 1; batch 60160; loss 0.724155\n",
      "epoch:1; batch 60160; train accuracy: 0.643700\n",
      "epoch 1; batch 60288; loss 0.633585\n",
      "epoch:1; batch 60288; train accuracy: 0.643926\n",
      "epoch 1; batch 60416; loss 0.716174\n",
      "epoch:1; batch 60416; train accuracy: 0.644134\n",
      "epoch 1; batch 60544; loss 0.820799\n",
      "epoch:1; batch 60544; train accuracy: 0.644176\n",
      "epoch 1; batch 60672; loss 0.846066\n",
      "epoch:1; batch 60672; train accuracy: 0.644185\n",
      "epoch 1; batch 60800; loss 0.636424\n",
      "epoch:1; batch 60800; train accuracy: 0.644457\n",
      "epoch 1; batch 60928; loss 0.716166\n",
      "epoch:1; batch 60928; train accuracy: 0.644663\n",
      "epoch 1; batch 61056; loss 0.675678\n",
      "epoch:1; batch 61056; train accuracy: 0.644883\n",
      "epoch 1; batch 61184; loss 0.692537\n",
      "epoch:1; batch 61184; train accuracy: 0.645120\n",
      "epoch 1; batch 61312; loss 0.690928\n",
      "epoch:1; batch 61312; train accuracy: 0.645127\n",
      "epoch 1; batch 61440; loss 0.680789\n",
      "epoch:1; batch 61440; train accuracy: 0.645345\n",
      "epoch 1; batch 61568; loss 0.643575\n",
      "epoch:1; batch 61568; train accuracy: 0.645514\n",
      "epoch 1; batch 61696; loss 0.791212\n",
      "epoch:1; batch 61696; train accuracy: 0.645714\n",
      "epoch 1; batch 61824; loss 0.716553\n",
      "epoch:1; batch 61824; train accuracy: 0.645866\n",
      "epoch 1; batch 61952; loss 0.866850\n",
      "epoch:1; batch 61952; train accuracy: 0.645903\n",
      "epoch 1; batch 62080; loss 0.596651\n",
      "epoch:1; batch 62080; train accuracy: 0.646118\n",
      "epoch 1; batch 62208; loss 0.727350\n",
      "epoch:1; batch 62208; train accuracy: 0.646283\n",
      "epoch 1; batch 62336; loss 0.637605\n",
      "epoch:1; batch 62336; train accuracy: 0.646464\n",
      "epoch 1; batch 62464; loss 0.627245\n",
      "epoch:1; batch 62464; train accuracy: 0.646676\n",
      "epoch 1; batch 62592; loss 0.771014\n",
      "epoch:1; batch 62592; train accuracy: 0.646760\n",
      "epoch 1; batch 62720; loss 0.647931\n",
      "epoch:1; batch 62720; train accuracy: 0.647003\n",
      "epoch 1; batch 62848; loss 0.851077\n",
      "epoch:1; batch 62848; train accuracy: 0.647117\n",
      "epoch 1; batch 62976; loss 0.530250\n",
      "epoch:1; batch 62976; train accuracy: 0.647437\n",
      "epoch 1; batch 63104; loss 0.528987\n",
      "epoch:1; batch 63104; train accuracy: 0.647724\n",
      "epoch 1; batch 63232; loss 0.828846\n",
      "epoch:1; batch 63232; train accuracy: 0.647837\n",
      "epoch 1; batch 63360; loss 0.644783\n",
      "epoch:1; batch 63360; train accuracy: 0.647996\n",
      "epoch 1; batch 63488; loss 0.604676\n",
      "epoch:1; batch 63488; train accuracy: 0.648233\n",
      "epoch 1; batch 63616; loss 0.652299\n",
      "epoch:1; batch 63616; train accuracy: 0.648500\n",
      "epoch 1; batch 63744; loss 0.664359\n",
      "epoch:1; batch 63744; train accuracy: 0.648751\n",
      "epoch 1; batch 63872; loss 0.612960\n",
      "epoch:1; batch 63872; train accuracy: 0.648907\n",
      "epoch 1; batch 64000; loss 0.823061\n",
      "epoch:1; batch 64000; train accuracy: 0.649031\n",
      "epoch 1; batch 64128; loss 0.798819\n",
      "epoch:1; batch 64128; train accuracy: 0.649077\n",
      "epoch 1; batch 64256; loss 0.752319\n",
      "epoch:1; batch 64256; train accuracy: 0.649169\n",
      "epoch 1; batch 64384; loss 0.628183\n",
      "epoch:1; batch 64384; train accuracy: 0.649432\n",
      "epoch 1; batch 64512; loss 0.847318\n",
      "epoch:1; batch 64512; train accuracy: 0.649492\n",
      "epoch 1; batch 64640; loss 0.696929\n",
      "epoch:1; batch 64640; train accuracy: 0.649613\n",
      "epoch 1; batch 64768; loss 0.643390\n",
      "epoch:1; batch 64768; train accuracy: 0.649812\n",
      "epoch 1; batch 64896; loss 0.604346\n",
      "epoch:1; batch 64896; train accuracy: 0.650071\n",
      "epoch 1; batch 65024; loss 0.674899\n",
      "epoch:1; batch 65024; train accuracy: 0.650221\n",
      "epoch 1; batch 65152; loss 0.648209\n",
      "epoch:1; batch 65152; train accuracy: 0.650479\n",
      "epoch 1; batch 65280; loss 0.775465\n",
      "epoch:1; batch 65280; train accuracy: 0.650536\n",
      "epoch 1; batch 65408; loss 0.596608\n",
      "epoch:1; batch 65408; train accuracy: 0.650807\n",
      "epoch 1; batch 65536; loss 0.645588\n",
      "epoch:1; batch 65536; train accuracy: 0.650955\n",
      "epoch 1; batch 65664; loss 0.745507\n",
      "epoch:1; batch 65664; train accuracy: 0.651042\n",
      "epoch 1; batch 65792; loss 0.678467\n",
      "epoch:1; batch 65792; train accuracy: 0.651158\n",
      "epoch 1; batch 65920; loss 0.618074\n",
      "epoch:1; batch 65920; train accuracy: 0.651350\n",
      "epoch 1; batch 66048; loss 0.763031\n",
      "epoch:1; batch 66048; train accuracy: 0.651405\n",
      "epoch 1; batch 66176; loss 0.666256\n",
      "epoch:1; batch 66176; train accuracy: 0.651460\n",
      "epoch 1; batch 66304; loss 0.634119\n",
      "epoch:1; batch 66304; train accuracy: 0.651650\n",
      "epoch 1; batch 66432; loss 0.726954\n",
      "epoch:1; batch 66432; train accuracy: 0.651779\n",
      "epoch 1; batch 66560; loss 0.666493\n",
      "epoch:1; batch 66560; train accuracy: 0.651923\n",
      "epoch 1; batch 66688; loss 0.772337\n",
      "epoch:1; batch 66688; train accuracy: 0.652036\n",
      "epoch 1; batch 66816; loss 0.642104\n",
      "epoch:1; batch 66816; train accuracy: 0.652119\n",
      "epoch 1; batch 66944; loss 0.680689\n",
      "epoch:1; batch 66944; train accuracy: 0.652187\n",
      "epoch 1; batch 67072; loss 0.706828\n",
      "epoch:1; batch 67072; train accuracy: 0.652359\n",
      "epoch 1; batch 67200; loss 0.766453\n",
      "epoch:1; batch 67200; train accuracy: 0.652455\n",
      "epoch 1; batch 67328; loss 0.697770\n",
      "epoch:1; batch 67328; train accuracy: 0.652581\n",
      "epoch 1; batch 67456; loss 0.680644\n",
      "epoch:1; batch 67456; train accuracy: 0.652707\n",
      "epoch 1; batch 67584; loss 0.695894\n",
      "epoch:1; batch 67584; train accuracy: 0.652862\n",
      "epoch 1; batch 67712; loss 0.578576\n",
      "epoch:1; batch 67712; train accuracy: 0.653208\n",
      "epoch 1; batch 67840; loss 0.796539\n",
      "epoch:1; batch 67840; train accuracy: 0.653317\n",
      "epoch 1; batch 67968; loss 0.687353\n",
      "epoch:1; batch 67968; train accuracy: 0.653513\n",
      "epoch 1; batch 68096; loss 0.741123\n",
      "epoch:1; batch 68096; train accuracy: 0.653577\n",
      "epoch 1; batch 68224; loss 0.589730\n",
      "epoch:1; batch 68224; train accuracy: 0.653875\n",
      "epoch 1; batch 68352; loss 0.662538\n",
      "epoch:1; batch 68352; train accuracy: 0.654012\n",
      "epoch 1; batch 68480; loss 0.627827\n",
      "epoch:1; batch 68480; train accuracy: 0.654293\n",
      "epoch 1; batch 68608; loss 0.629419\n",
      "epoch:1; batch 68608; train accuracy: 0.654516\n",
      "epoch 1; batch 68736; loss 0.726300\n",
      "epoch:1; batch 68736; train accuracy: 0.654592\n",
      "epoch 1; batch 68864; loss 0.765129\n",
      "epoch:1; batch 68864; train accuracy: 0.654682\n",
      "epoch 1; batch 68992; loss 0.623185\n",
      "epoch:1; batch 68992; train accuracy: 0.654859\n",
      "epoch 1; batch 69120; loss 0.673611\n",
      "epoch:1; batch 69120; train accuracy: 0.654948\n",
      "epoch 1; batch 69248; loss 0.659401\n",
      "epoch:1; batch 69248; train accuracy: 0.655109\n",
      "epoch 1; batch 69376; loss 0.673477\n",
      "epoch:1; batch 69376; train accuracy: 0.655255\n",
      "epoch 1; batch 69504; loss 0.595897\n",
      "epoch:1; batch 69504; train accuracy: 0.655516\n",
      "epoch 1; batch 69632; loss 0.678043\n",
      "epoch:1; batch 69632; train accuracy: 0.655690\n",
      "epoch 1; batch 69760; loss 0.741471\n",
      "epoch:1; batch 69760; train accuracy: 0.655834\n",
      "epoch 1; batch 69888; loss 0.733154\n",
      "epoch:1; batch 69888; train accuracy: 0.655935\n",
      "epoch 1; batch 70016; loss 0.710214\n",
      "epoch:1; batch 70016; train accuracy: 0.656021\n",
      "epoch 1; batch 70144; loss 0.589385\n",
      "epoch:1; batch 70144; train accuracy: 0.656207\n",
      "epoch 1; batch 70272; loss 0.656313\n",
      "epoch:1; batch 70272; train accuracy: 0.656321\n",
      "epoch 1; batch 70400; loss 0.708899\n",
      "epoch:1; batch 70400; train accuracy: 0.656406\n",
      "epoch 1; batch 70528; loss 0.690832\n",
      "epoch:1; batch 70528; train accuracy: 0.656534\n",
      "epoch 1; batch 70656; loss 0.677845\n",
      "epoch:1; batch 70656; train accuracy: 0.656731\n",
      "epoch 1; batch 70784; loss 0.662749\n",
      "epoch:1; batch 70784; train accuracy: 0.656942\n",
      "epoch 1; batch 70912; loss 0.654992\n",
      "epoch:1; batch 70912; train accuracy: 0.657040\n",
      "epoch 1; batch 71040; loss 0.742044\n",
      "epoch:1; batch 71040; train accuracy: 0.657151\n",
      "epoch 1; batch 71168; loss 0.635831\n",
      "epoch:1; batch 71168; train accuracy: 0.657248\n",
      "epoch 1; batch 71296; loss 0.651211\n",
      "epoch:1; batch 71296; train accuracy: 0.657386\n",
      "epoch 1; batch 71424; loss 0.744207\n",
      "epoch:1; batch 71424; train accuracy: 0.657468\n",
      "epoch 1; batch 71552; loss 0.648457\n",
      "epoch:1; batch 71552; train accuracy: 0.657703\n",
      "epoch 1; batch 71680; loss 0.642762\n",
      "epoch:1; batch 71680; train accuracy: 0.657868\n",
      "epoch 1; batch 71808; loss 0.636352\n",
      "epoch:1; batch 71808; train accuracy: 0.657991\n",
      "epoch 1; batch 71936; loss 0.720011\n",
      "epoch:1; batch 71936; train accuracy: 0.658071\n",
      "epoch 1; batch 72064; loss 0.822436\n",
      "epoch:1; batch 72064; train accuracy: 0.658165\n",
      "epoch 1; batch 72192; loss 0.713365\n",
      "epoch:1; batch 72192; train accuracy: 0.658286\n",
      "epoch 1; batch 72320; loss 0.577315\n",
      "epoch:1; batch 72320; train accuracy: 0.658518\n",
      "epoch 1; batch 72448; loss 0.627618\n",
      "epoch:1; batch 72448; train accuracy: 0.658666\n",
      "epoch 1; batch 72576; loss 0.651803\n",
      "epoch:1; batch 72576; train accuracy: 0.658799\n",
      "epoch 1; batch 72704; loss 0.620243\n",
      "epoch:1; batch 72704; train accuracy: 0.658946\n",
      "epoch 1; batch 72832; loss 0.639924\n",
      "epoch:1; batch 72832; train accuracy: 0.659092\n",
      "epoch 1; batch 72960; loss 0.723494\n",
      "epoch:1; batch 72960; train accuracy: 0.659238\n",
      "epoch 1; batch 73088; loss 0.645020\n",
      "epoch:1; batch 73088; train accuracy: 0.659356\n",
      "epoch 1; batch 73216; loss 0.760480\n",
      "epoch:1; batch 73216; train accuracy: 0.659419\n",
      "epoch 1; batch 73344; loss 0.548340\n",
      "epoch:1; batch 73344; train accuracy: 0.659659\n",
      "epoch 1; batch 73472; loss 0.576295\n",
      "epoch:1; batch 73472; train accuracy: 0.659830\n",
      "epoch 1; batch 73600; loss 0.761035\n",
      "epoch:1; batch 73600; train accuracy: 0.659878\n",
      "epoch 1; batch 73728; loss 0.589174\n",
      "epoch:1; batch 73728; train accuracy: 0.660034\n",
      "epoch 1; batch 73856; loss 0.666949\n",
      "epoch:1; batch 73856; train accuracy: 0.660163\n",
      "epoch 1; batch 73984; loss 0.612039\n",
      "epoch:1; batch 73984; train accuracy: 0.660372\n",
      "epoch 1; batch 74112; loss 0.721770\n",
      "epoch:1; batch 74112; train accuracy: 0.660446\n",
      "epoch 1; batch 74240; loss 0.808897\n",
      "epoch:1; batch 74240; train accuracy: 0.660560\n",
      "epoch 1; batch 74368; loss 0.771207\n",
      "epoch:1; batch 74368; train accuracy: 0.660553\n",
      "epoch 1; batch 74496; loss 0.694694\n",
      "epoch:1; batch 74496; train accuracy: 0.660653\n",
      "epoch 1; batch 74624; loss 0.683401\n",
      "epoch:1; batch 74624; train accuracy: 0.660779\n",
      "epoch 1; batch 74752; loss 0.805523\n",
      "epoch:1; batch 74752; train accuracy: 0.660905\n",
      "epoch 1; batch 74880; loss 0.718993\n",
      "epoch:1; batch 74880; train accuracy: 0.660991\n",
      "epoch 1; batch 75008; loss 0.707254\n",
      "epoch:1; batch 75008; train accuracy: 0.661023\n",
      "epoch 1; batch 75136; loss 0.717020\n",
      "epoch:1; batch 75136; train accuracy: 0.661134\n",
      "epoch 1; batch 75264; loss 0.633387\n",
      "epoch:1; batch 75264; train accuracy: 0.661286\n",
      "epoch 1; batch 75392; loss 0.692966\n",
      "epoch:1; batch 75392; train accuracy: 0.661383\n",
      "epoch 1; batch 75520; loss 0.647418\n",
      "epoch:1; batch 75520; train accuracy: 0.661533\n",
      "epoch 1; batch 75648; loss 0.731671\n",
      "epoch:1; batch 75648; train accuracy: 0.661564\n",
      "epoch 1; batch 75776; loss 0.691202\n",
      "epoch:1; batch 75776; train accuracy: 0.661674\n",
      "epoch 1; batch 75904; loss 0.683325\n",
      "epoch:1; batch 75904; train accuracy: 0.661836\n",
      "epoch 1; batch 76032; loss 0.680536\n",
      "epoch:1; batch 76032; train accuracy: 0.661984\n",
      "epoch 1; batch 76160; loss 0.859302\n",
      "epoch:1; batch 76160; train accuracy: 0.662027\n",
      "epoch 1; batch 76288; loss 0.624335\n",
      "epoch:1; batch 76288; train accuracy: 0.662227\n",
      "epoch 1; batch 76416; loss 0.720999\n",
      "epoch:1; batch 76416; train accuracy: 0.662335\n",
      "epoch 1; batch 76544; loss 0.835559\n",
      "epoch:1; batch 76544; train accuracy: 0.662351\n",
      "epoch 1; batch 76672; loss 0.777224\n",
      "epoch:1; batch 76672; train accuracy: 0.662406\n",
      "epoch 1; batch 76800; loss 0.656685\n",
      "epoch:1; batch 76800; train accuracy: 0.662591\n",
      "epoch 1; batch 76928; loss 0.590211\n",
      "epoch:1; batch 76928; train accuracy: 0.662776\n",
      "epoch 1; batch 77056; loss 0.642652\n",
      "epoch:1; batch 77056; train accuracy: 0.662882\n",
      "epoch 1; batch 77184; loss 0.629798\n",
      "epoch:1; batch 77184; train accuracy: 0.663039\n",
      "epoch 1; batch 77312; loss 0.703489\n",
      "epoch:1; batch 77312; train accuracy: 0.663131\n",
      "epoch 1; batch 77440; loss 0.603020\n",
      "epoch:1; batch 77440; train accuracy: 0.663365\n",
      "epoch 1; batch 77568; loss 0.607066\n",
      "epoch:1; batch 77568; train accuracy: 0.663508\n",
      "epoch 1; batch 77696; loss 0.878754\n",
      "epoch:1; batch 77696; train accuracy: 0.663573\n",
      "epoch 1; batch 77824; loss 0.589768\n",
      "epoch:1; batch 77824; train accuracy: 0.663767\n",
      "epoch 1; batch 77952; loss 0.495343\n",
      "epoch:1; batch 77952; train accuracy: 0.664037\n",
      "epoch 1; batch 78080; loss 0.790064\n",
      "epoch:1; batch 78080; train accuracy: 0.664152\n",
      "epoch 1; batch 78208; loss 0.775714\n",
      "epoch:1; batch 78208; train accuracy: 0.664267\n",
      "epoch 1; batch 78336; loss 0.757359\n",
      "epoch:1; batch 78336; train accuracy: 0.664331\n",
      "epoch 1; batch 78464; loss 0.610034\n",
      "epoch:1; batch 78464; train accuracy: 0.664470\n",
      "epoch 1; batch 78592; loss 0.660545\n",
      "epoch:1; batch 78592; train accuracy: 0.664571\n",
      "epoch 1; batch 78720; loss 0.613581\n",
      "epoch:1; batch 78720; train accuracy: 0.664761\n",
      "epoch 1; batch 78848; loss 0.656318\n",
      "epoch:1; batch 78848; train accuracy: 0.664887\n",
      "epoch 1; batch 78976; loss 0.737040\n",
      "epoch:1; batch 78976; train accuracy: 0.665025\n",
      "epoch 1; batch 79104; loss 0.697152\n",
      "epoch:1; batch 79104; train accuracy: 0.665213\n",
      "epoch 1; batch 79232; loss 0.778427\n",
      "epoch:1; batch 79232; train accuracy: 0.665312\n",
      "epoch 1; batch 79360; loss 0.711851\n",
      "epoch:1; batch 79360; train accuracy: 0.665360\n",
      "epoch 1; batch 79488; loss 0.508151\n",
      "epoch:1; batch 79488; train accuracy: 0.665597\n",
      "epoch 1; batch 79616; loss 0.555196\n",
      "epoch:1; batch 79616; train accuracy: 0.665796\n",
      "epoch 1; batch 79744; loss 0.786127\n",
      "epoch:1; batch 79744; train accuracy: 0.665843\n",
      "epoch 1; batch 79872; loss 0.487242\n",
      "epoch:1; batch 79872; train accuracy: 0.666028\n",
      "epoch 1; batch 80000; loss 0.718053\n",
      "epoch:1; batch 80000; train accuracy: 0.666138\n",
      "epoch 1; batch 80128; loss 0.829311\n",
      "epoch:1; batch 80128; train accuracy: 0.666172\n",
      "epoch 1; batch 80256; loss 0.664407\n",
      "epoch:1; batch 80256; train accuracy: 0.666293\n",
      "epoch 1; batch 80384; loss 0.729014\n",
      "epoch:1; batch 80384; train accuracy: 0.666401\n",
      "epoch 1; batch 80512; loss 0.653487\n",
      "epoch:1; batch 80512; train accuracy: 0.666534\n",
      "epoch 1; batch 80640; loss 0.687929\n",
      "epoch:1; batch 80640; train accuracy: 0.666654\n",
      "epoch 1; batch 80768; loss 0.678946\n",
      "epoch:1; batch 80768; train accuracy: 0.666724\n",
      "epoch 1; batch 80896; loss 0.778254\n",
      "epoch:1; batch 80896; train accuracy: 0.666782\n",
      "epoch 1; batch 81024; loss 0.749281\n",
      "epoch:1; batch 81024; train accuracy: 0.666914\n",
      "epoch 1; batch 81152; loss 0.606510\n",
      "epoch:1; batch 81152; train accuracy: 0.667045\n",
      "epoch 1; batch 81280; loss 0.633142\n",
      "epoch:1; batch 81280; train accuracy: 0.667224\n",
      "epoch 1; batch 81408; loss 0.689393\n",
      "epoch:1; batch 81408; train accuracy: 0.667232\n",
      "epoch 1; batch 81536; loss 0.660433\n",
      "epoch:1; batch 81536; train accuracy: 0.667374\n",
      "epoch 1; batch 81664; loss 0.677640\n",
      "epoch:1; batch 81664; train accuracy: 0.667442\n",
      "epoch 1; batch 81792; loss 0.665192\n",
      "epoch:1; batch 81792; train accuracy: 0.667535\n",
      "epoch 1; batch 81920; loss 0.607702\n",
      "epoch:1; batch 81920; train accuracy: 0.667651\n",
      "epoch 1; batch 82048; loss 0.575110\n",
      "epoch:1; batch 82048; train accuracy: 0.667841\n",
      "epoch 1; batch 82176; loss 0.581441\n",
      "epoch:1; batch 82176; train accuracy: 0.668005\n",
      "epoch 1; batch 82304; loss 0.698585\n",
      "epoch:1; batch 82304; train accuracy: 0.668157\n",
      "epoch 1; batch 82432; loss 0.638845\n",
      "epoch:1; batch 82432; train accuracy: 0.668333\n",
      "epoch 1; batch 82560; loss 0.546964\n",
      "epoch:1; batch 82560; train accuracy: 0.668532\n",
      "epoch 1; batch 82688; loss 0.698178\n",
      "epoch:1; batch 82688; train accuracy: 0.668622\n",
      "epoch 1; batch 82816; loss 0.726160\n",
      "epoch:1; batch 82816; train accuracy: 0.668651\n",
      "epoch 1; batch 82944; loss 0.651729\n",
      "epoch:1; batch 82944; train accuracy: 0.668789\n",
      "epoch 1; batch 83072; loss 0.611751\n",
      "epoch:1; batch 83072; train accuracy: 0.668986\n",
      "epoch 1; batch 83200; loss 0.800443\n",
      "epoch:1; batch 83200; train accuracy: 0.669014\n",
      "epoch 1; batch 83328; loss 0.604251\n",
      "epoch:1; batch 83328; train accuracy: 0.669139\n",
      "epoch 1; batch 83456; loss 0.776332\n",
      "epoch:1; batch 83456; train accuracy: 0.669167\n",
      "epoch 1; batch 83584; loss 0.670516\n",
      "epoch:1; batch 83584; train accuracy: 0.669315\n",
      "epoch 1; batch 83712; loss 0.581320\n",
      "epoch:1; batch 83712; train accuracy: 0.669474\n",
      "epoch 1; batch 83840; loss 0.606293\n",
      "epoch:1; batch 83840; train accuracy: 0.669621\n",
      "epoch 1; batch 83968; loss 0.686227\n",
      "epoch:1; batch 83968; train accuracy: 0.669684\n",
      "epoch 1; batch 84096; loss 0.676814\n",
      "epoch:1; batch 84096; train accuracy: 0.669746\n",
      "epoch 1; batch 84224; loss 0.784648\n",
      "epoch:1; batch 84224; train accuracy: 0.669762\n",
      "epoch 1; batch 84352; loss 0.674331\n",
      "epoch:1; batch 84352; train accuracy: 0.669895\n",
      "epoch 1; batch 84480; loss 0.715016\n",
      "epoch:1; batch 84480; train accuracy: 0.669969\n",
      "epoch 1; batch 84608; loss 0.858672\n",
      "epoch:1; batch 84608; train accuracy: 0.669913\n",
      "epoch 1; batch 84736; loss 0.662672\n",
      "epoch:1; batch 84736; train accuracy: 0.670069\n",
      "epoch 1; batch 84864; loss 0.595156\n",
      "epoch:1; batch 84864; train accuracy: 0.670237\n",
      "epoch 1; batch 84992; loss 0.620956\n",
      "epoch:1; batch 84992; train accuracy: 0.670345\n",
      "epoch 1; batch 85120; loss 0.635131\n",
      "epoch:1; batch 85120; train accuracy: 0.670500\n",
      "epoch 1; batch 85248; loss 0.656749\n",
      "epoch:1; batch 85248; train accuracy: 0.670608\n",
      "epoch 1; batch 85376; loss 0.632801\n",
      "epoch:1; batch 85376; train accuracy: 0.670727\n",
      "epoch 1; batch 85504; loss 0.893384\n",
      "epoch:1; batch 85504; train accuracy: 0.670705\n",
      "epoch 1; batch 85632; loss 0.616211\n",
      "epoch:1; batch 85632; train accuracy: 0.670847\n",
      "epoch 1; batch 85760; loss 0.538505\n",
      "epoch:1; batch 85760; train accuracy: 0.671012\n",
      "epoch 1; batch 85888; loss 0.767492\n",
      "epoch:1; batch 85888; train accuracy: 0.671107\n",
      "epoch 1; batch 86016; loss 0.615571\n",
      "epoch:1; batch 86016; train accuracy: 0.671270\n",
      "epoch 1; batch 86144; loss 0.580645\n",
      "epoch:1; batch 86144; train accuracy: 0.671457\n",
      "epoch 1; batch 86272; loss 0.748422\n",
      "epoch:1; batch 86272; train accuracy: 0.671574\n",
      "epoch 1; batch 86400; loss 0.689128\n",
      "epoch:1; batch 86400; train accuracy: 0.671620\n",
      "epoch 1; batch 86528; loss 0.751748\n",
      "epoch:1; batch 86528; train accuracy: 0.671713\n",
      "epoch 1; batch 86656; loss 0.794217\n",
      "epoch:1; batch 86656; train accuracy: 0.671725\n",
      "epoch 1; batch 86784; loss 0.517616\n",
      "epoch:1; batch 86784; train accuracy: 0.671933\n",
      "epoch 1; batch 86912; loss 0.541788\n",
      "epoch:1; batch 86912; train accuracy: 0.672117\n",
      "epoch 1; batch 87040; loss 0.690029\n",
      "epoch:1; batch 87040; train accuracy: 0.672174\n",
      "epoch 1; batch 87168; loss 0.552845\n",
      "epoch:1; batch 87168; train accuracy: 0.672334\n",
      "epoch 1; batch 87296; loss 0.740588\n",
      "epoch:1; batch 87296; train accuracy: 0.672390\n",
      "epoch 1; batch 87424; loss 0.624127\n",
      "epoch:1; batch 87424; train accuracy: 0.672516\n",
      "epoch 1; batch 87552; loss 0.548741\n",
      "epoch:1; batch 87552; train accuracy: 0.672652\n",
      "epoch 1; batch 87680; loss 0.620440\n",
      "epoch:1; batch 87680; train accuracy: 0.672776\n",
      "epoch 1; batch 87808; loss 0.786313\n",
      "epoch:1; batch 87808; train accuracy: 0.672843\n",
      "epoch 1; batch 87936; loss 0.582658\n",
      "epoch:1; batch 87936; train accuracy: 0.672989\n",
      "epoch 1; batch 88064; loss 0.758089\n",
      "epoch:1; batch 88064; train accuracy: 0.673022\n",
      "epoch 1; batch 88192; loss 0.563162\n",
      "epoch:1; batch 88192; train accuracy: 0.673190\n",
      "epoch 1; batch 88320; loss 0.753028\n",
      "epoch:1; batch 88320; train accuracy: 0.673290\n",
      "epoch 1; batch 88448; loss 0.676241\n",
      "epoch:1; batch 88448; train accuracy: 0.673390\n",
      "epoch 1; batch 88576; loss 0.586043\n",
      "epoch:1; batch 88576; train accuracy: 0.673535\n",
      "epoch 1; batch 88704; loss 0.697218\n",
      "epoch:1; batch 88704; train accuracy: 0.673634\n",
      "epoch 1; batch 88832; loss 0.665205\n",
      "epoch:1; batch 88832; train accuracy: 0.673856\n",
      "epoch 1; batch 88960; loss 0.509103\n",
      "epoch:1; batch 88960; train accuracy: 0.674044\n",
      "epoch 1; batch 89088; loss 0.581447\n",
      "epoch:1; batch 89088; train accuracy: 0.674199\n",
      "epoch 1; batch 89216; loss 0.629731\n",
      "epoch:1; batch 89216; train accuracy: 0.674330\n",
      "epoch 1; batch 89344; loss 0.628767\n",
      "epoch:1; batch 89344; train accuracy: 0.674461\n",
      "epoch 1; batch 89472; loss 0.729338\n",
      "epoch:1; batch 89472; train accuracy: 0.674524\n",
      "epoch 1; batch 89600; loss 0.554903\n",
      "epoch:1; batch 89600; train accuracy: 0.674699\n",
      "epoch 1; batch 89728; loss 0.677131\n",
      "epoch:1; batch 89728; train accuracy: 0.674773\n",
      "epoch 1; batch 89856; loss 0.683520\n",
      "epoch:1; batch 89856; train accuracy: 0.674858\n",
      "epoch 1; batch 89984; loss 0.605794\n",
      "epoch:1; batch 89984; train accuracy: 0.674976\n",
      "epoch 1; batch 90112; loss 0.625295\n",
      "epoch:1; batch 90112; train accuracy: 0.675082\n",
      "epoch 1; batch 90240; loss 0.786335\n",
      "epoch:1; batch 90240; train accuracy: 0.675144\n",
      "epoch 1; batch 90368; loss 0.515613\n",
      "epoch:1; batch 90368; train accuracy: 0.675350\n",
      "epoch 1; batch 90496; loss 0.757662\n",
      "epoch:1; batch 90496; train accuracy: 0.675400\n",
      "epoch 1; batch 90624; loss 0.603983\n",
      "epoch:1; batch 90624; train accuracy: 0.675516\n",
      "epoch 1; batch 90752; loss 0.610636\n",
      "epoch:1; batch 90752; train accuracy: 0.675644\n",
      "epoch 1; batch 90880; loss 0.696361\n",
      "epoch:1; batch 90880; train accuracy: 0.675737\n",
      "epoch 1; batch 91008; loss 0.768457\n",
      "epoch:1; batch 91008; train accuracy: 0.675721\n",
      "epoch 1; batch 91136; loss 0.692150\n",
      "epoch:1; batch 91136; train accuracy: 0.675759\n",
      "epoch 1; batch 91264; loss 0.520801\n",
      "epoch:1; batch 91264; train accuracy: 0.675940\n",
      "epoch 1; batch 91392; loss 0.573652\n",
      "epoch:1; batch 91392; train accuracy: 0.676055\n",
      "epoch 1; batch 91520; loss 0.643272\n",
      "epoch:1; batch 91520; train accuracy: 0.676136\n",
      "epoch 1; batch 91648; loss 0.579965\n",
      "epoch:1; batch 91648; train accuracy: 0.676272\n",
      "epoch 1; batch 91776; loss 0.676166\n",
      "epoch:1; batch 91776; train accuracy: 0.676321\n",
      "epoch 1; batch 91904; loss 0.632045\n",
      "epoch:1; batch 91904; train accuracy: 0.676499\n",
      "epoch 1; batch 92032; loss 0.744706\n",
      "epoch:1; batch 92032; train accuracy: 0.676515\n",
      "epoch 1; batch 92160; loss 0.499575\n",
      "epoch:1; batch 92160; train accuracy: 0.676725\n",
      "epoch 1; batch 92288; loss 0.740593\n",
      "epoch:1; batch 92288; train accuracy: 0.676773\n",
      "epoch 1; batch 92416; loss 0.747782\n",
      "epoch:1; batch 92416; train accuracy: 0.676852\n",
      "epoch 1; batch 92544; loss 0.486613\n",
      "epoch:1; batch 92544; train accuracy: 0.677040\n",
      "epoch 1; batch 92672; loss 0.667320\n",
      "epoch:1; batch 92672; train accuracy: 0.677152\n",
      "epoch 1; batch 92800; loss 0.572163\n",
      "epoch:1; batch 92800; train accuracy: 0.677284\n",
      "epoch 1; batch 92928; loss 0.787812\n",
      "epoch:1; batch 92928; train accuracy: 0.677331\n",
      "epoch 1; batch 93056; loss 0.633062\n",
      "epoch:1; batch 93056; train accuracy: 0.677431\n",
      "epoch 1; batch 93184; loss 0.752215\n",
      "epoch:1; batch 93184; train accuracy: 0.677455\n",
      "epoch 1; batch 93312; loss 0.696216\n",
      "epoch:1; batch 93312; train accuracy: 0.677523\n",
      "epoch 1; batch 93440; loss 0.602280\n",
      "epoch:1; batch 93440; train accuracy: 0.677643\n",
      "epoch 1; batch 93568; loss 0.653960\n",
      "epoch:1; batch 93568; train accuracy: 0.677742\n",
      "epoch 1; batch 93696; loss 0.622111\n",
      "epoch:1; batch 93696; train accuracy: 0.677884\n",
      "epoch 1; batch 93824; loss 0.591162\n",
      "epoch:1; batch 93824; train accuracy: 0.678057\n",
      "epoch 1; batch 93952; loss 0.587685\n",
      "epoch:1; batch 93952; train accuracy: 0.678187\n",
      "epoch 1; batch 94080; loss 0.688891\n",
      "epoch:1; batch 94080; train accuracy: 0.678221\n",
      "epoch 1; batch 94208; loss 0.665290\n",
      "epoch:1; batch 94208; train accuracy: 0.678254\n",
      "epoch 1; batch 94336; loss 0.609797\n",
      "epoch:1; batch 94336; train accuracy: 0.678384\n",
      "epoch 1; batch 94464; loss 0.689453\n",
      "epoch:1; batch 94464; train accuracy: 0.678512\n",
      "epoch 1; batch 94592; loss 0.717525\n",
      "epoch:1; batch 94592; train accuracy: 0.678567\n",
      "epoch 1; batch 94720; loss 0.441871\n",
      "epoch:1; batch 94720; train accuracy: 0.678758\n",
      "epoch 1; batch 94848; loss 0.682032\n",
      "epoch:1; batch 94848; train accuracy: 0.678865\n",
      "epoch 1; batch 94976; loss 0.652648\n",
      "epoch:1; batch 94976; train accuracy: 0.678982\n",
      "epoch 1; batch 95104; loss 0.648630\n",
      "epoch:1; batch 95104; train accuracy: 0.679099\n",
      "epoch 1; batch 95232; loss 0.599584\n",
      "epoch:1; batch 95232; train accuracy: 0.679236\n",
      "epoch 1; batch 95360; loss 0.617365\n",
      "epoch:1; batch 95360; train accuracy: 0.679362\n",
      "epoch 1; batch 95488; loss 0.651400\n",
      "epoch:1; batch 95488; train accuracy: 0.679468\n",
      "epoch 1; batch 95616; loss 0.660473\n",
      "epoch:1; batch 95616; train accuracy: 0.679510\n",
      "epoch 1; batch 95744; loss 0.623261\n",
      "epoch:1; batch 95744; train accuracy: 0.679677\n",
      "epoch 1; batch 95872; loss 0.739251\n",
      "epoch:1; batch 95872; train accuracy: 0.679729\n",
      "epoch 1; batch 96000; loss 0.651855\n",
      "epoch:1; batch 96000; train accuracy: 0.679823\n",
      "epoch 1; batch 96128; loss 0.862223\n",
      "epoch:1; batch 96128; train accuracy: 0.679802\n",
      "epoch 1; batch 96256; loss 0.580842\n",
      "epoch:1; batch 96256; train accuracy: 0.679989\n",
      "epoch 1; batch 96384; loss 0.604209\n",
      "epoch:1; batch 96384; train accuracy: 0.680103\n",
      "epoch 1; batch 96512; loss 0.535708\n",
      "epoch:1; batch 96512; train accuracy: 0.680247\n",
      "epoch 1; batch 96640; loss 0.634201\n",
      "epoch:1; batch 96640; train accuracy: 0.680360\n",
      "epoch 1; batch 96768; loss 0.744329\n",
      "epoch:1; batch 96768; train accuracy: 0.680370\n",
      "epoch 1; batch 96896; loss 0.646856\n",
      "epoch:1; batch 96896; train accuracy: 0.680472\n",
      "epoch 1; batch 97024; loss 0.750583\n",
      "epoch:1; batch 97024; train accuracy: 0.680440\n",
      "epoch 1; batch 97152; loss 0.626861\n",
      "epoch:1; batch 97152; train accuracy: 0.680501\n",
      "epoch 1; batch 97280; loss 0.496401\n",
      "epoch:1; batch 97280; train accuracy: 0.680695\n",
      "epoch 1; batch 97408; loss 0.669786\n",
      "epoch:1; batch 97408; train accuracy: 0.680755\n",
      "epoch 1; batch 97536; loss 0.549456\n",
      "epoch:1; batch 97536; train accuracy: 0.680938\n",
      "epoch 1; batch 97664; loss 0.880545\n",
      "epoch:1; batch 97664; train accuracy: 0.680978\n",
      "epoch 1; batch 97792; loss 0.621067\n",
      "epoch:1; batch 97792; train accuracy: 0.681129\n",
      "epoch 1; batch 97920; loss 0.641753\n",
      "epoch:1; batch 97920; train accuracy: 0.681189\n",
      "epoch 1; batch 98048; loss 0.767815\n",
      "epoch:1; batch 98048; train accuracy: 0.681228\n",
      "epoch 1; batch 98176; loss 0.681687\n",
      "epoch:1; batch 98176; train accuracy: 0.681297\n",
      "epoch 1; batch 98304; loss 0.725264\n",
      "epoch:1; batch 98304; train accuracy: 0.681325\n",
      "epoch 1; batch 98432; loss 0.595212\n",
      "epoch:1; batch 98432; train accuracy: 0.681445\n",
      "epoch 1; batch 98560; loss 0.748080\n",
      "epoch:1; batch 98560; train accuracy: 0.681473\n",
      "epoch 1; batch 98688; loss 0.656236\n",
      "epoch:1; batch 98688; train accuracy: 0.681532\n",
      "epoch 1; batch 98816; loss 0.586904\n",
      "epoch:1; batch 98816; train accuracy: 0.681600\n",
      "epoch 1; batch 98944; loss 0.671416\n",
      "epoch:1; batch 98944; train accuracy: 0.681648\n",
      "epoch 1; batch 99072; loss 0.661332\n",
      "epoch:1; batch 99072; train accuracy: 0.681696\n",
      "epoch 1; batch 99200; loss 0.764161\n",
      "epoch:1; batch 99200; train accuracy: 0.681764\n",
      "epoch 1; batch 99328; loss 0.718639\n",
      "epoch:1; batch 99328; train accuracy: 0.681862\n",
      "epoch 1; batch 99456; loss 0.610890\n",
      "epoch:1; batch 99456; train accuracy: 0.681960\n",
      "epoch 1; batch 99584; loss 0.651231\n",
      "epoch:1; batch 99584; train accuracy: 0.682087\n",
      "epoch 1; batch 99712; loss 0.714728\n",
      "epoch:1; batch 99712; train accuracy: 0.682084\n",
      "epoch 1; batch 99840; loss 0.644262\n",
      "epoch:1; batch 99840; train accuracy: 0.682181\n",
      "epoch 1; batch 99968; loss 0.642540\n",
      "epoch:1; batch 99968; train accuracy: 0.682248\n",
      "epoch 1; batch 100096; loss 0.572366\n",
      "epoch:1; batch 100096; train accuracy: 0.682375\n",
      "epoch 1; batch 100224; loss 0.649143\n",
      "epoch:1; batch 100224; train accuracy: 0.682431\n",
      "epoch 1; batch 100352; loss 0.842262\n",
      "epoch:1; batch 100352; train accuracy: 0.682498\n",
      "epoch 1; batch 100480; loss 0.674013\n",
      "epoch:1; batch 100480; train accuracy: 0.682643\n",
      "epoch 1; batch 100608; loss 0.570213\n",
      "epoch:1; batch 100608; train accuracy: 0.682739\n",
      "epoch 1; batch 100736; loss 0.660293\n",
      "epoch:1; batch 100736; train accuracy: 0.682805\n",
      "epoch 1; batch 100864; loss 0.722899\n",
      "epoch:1; batch 100864; train accuracy: 0.682860\n",
      "epoch 1; batch 100992; loss 0.656146\n",
      "epoch:1; batch 100992; train accuracy: 0.682935\n",
      "epoch 1; batch 101120; loss 0.632786\n",
      "epoch:1; batch 101120; train accuracy: 0.683030\n",
      "epoch 1; batch 101248; loss 0.729905\n",
      "epoch:1; batch 101248; train accuracy: 0.683125\n",
      "epoch 1; batch 101376; loss 0.571356\n",
      "epoch:1; batch 101376; train accuracy: 0.683249\n",
      "epoch 1; batch 101504; loss 0.624269\n",
      "epoch:1; batch 101504; train accuracy: 0.683392\n",
      "epoch 1; batch 101632; loss 0.645806\n",
      "epoch:1; batch 101632; train accuracy: 0.683466\n",
      "epoch 1; batch 101760; loss 0.671386\n",
      "epoch:1; batch 101760; train accuracy: 0.683520\n",
      "epoch 1; batch 101888; loss 0.637892\n",
      "epoch:1; batch 101888; train accuracy: 0.683633\n",
      "epoch 1; batch 102016; loss 0.564813\n",
      "epoch:1; batch 102016; train accuracy: 0.683736\n",
      "epoch 1; batch 102144; loss 0.665591\n",
      "epoch:1; batch 102144; train accuracy: 0.683809\n",
      "epoch 1; batch 102272; loss 0.756620\n",
      "epoch:1; batch 102272; train accuracy: 0.683804\n",
      "epoch 1; batch 102400; loss 0.749037\n",
      "epoch:1; batch 102400; train accuracy: 0.683818\n",
      "epoch 1; batch 102528; loss 0.613257\n",
      "epoch:1; batch 102528; train accuracy: 0.683891\n",
      "epoch 1; batch 102656; loss 0.583063\n",
      "epoch:1; batch 102656; train accuracy: 0.684022\n",
      "epoch 1; batch 102784; loss 0.699991\n",
      "epoch:1; batch 102784; train accuracy: 0.684114\n",
      "epoch 1; batch 102912; loss 0.702637\n",
      "epoch:1; batch 102912; train accuracy: 0.684157\n",
      "epoch 1; batch 103040; loss 0.653661\n",
      "epoch:1; batch 103040; train accuracy: 0.684229\n",
      "epoch 1; batch 103168; loss 0.599302\n",
      "epoch:1; batch 103168; train accuracy: 0.684369\n",
      "epoch 1; batch 103296; loss 0.661478\n",
      "epoch:1; batch 103296; train accuracy: 0.684489\n",
      "epoch 1; batch 103424; loss 0.530356\n",
      "epoch:1; batch 103424; train accuracy: 0.684638\n",
      "epoch 1; batch 103552; loss 0.672886\n",
      "epoch:1; batch 103552; train accuracy: 0.684699\n",
      "epoch 1; batch 103680; loss 0.656941\n",
      "epoch:1; batch 103680; train accuracy: 0.684741\n",
      "epoch 1; batch 103808; loss 0.680731\n",
      "epoch:1; batch 103808; train accuracy: 0.684832\n",
      "epoch 1; batch 103936; loss 0.600361\n",
      "epoch:1; batch 103936; train accuracy: 0.684931\n",
      "epoch 1; batch 104064; loss 0.634547\n",
      "epoch:1; batch 104064; train accuracy: 0.685030\n",
      "epoch 1; batch 104192; loss 0.641203\n",
      "epoch:1; batch 104192; train accuracy: 0.685129\n",
      "epoch 1; batch 104320; loss 0.562719\n",
      "epoch:1; batch 104320; train accuracy: 0.685228\n",
      "epoch 1; batch 104448; loss 0.561552\n",
      "epoch:1; batch 104448; train accuracy: 0.685355\n",
      "epoch 1; batch 104576; loss 0.534430\n",
      "epoch:1; batch 104576; train accuracy: 0.685521\n",
      "epoch 1; batch 104704; loss 0.616964\n",
      "epoch:1; batch 104704; train accuracy: 0.685619\n",
      "epoch 1; batch 104832; loss 0.843429\n",
      "epoch:1; batch 104832; train accuracy: 0.685640\n",
      "epoch 1; batch 104960; loss 0.674479\n",
      "epoch:1; batch 104960; train accuracy: 0.685699\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 31044 ; rate: 0.295770\n",
      "y_true_label_1_num: 11972 ; rate: 0.114062\n",
      "y_true_label_2_num: 23668 ; rate: 0.225495\n",
      "y_true_label_3_num: 38276 ; rate: 0.364672\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.826658\n",
      "valid avg_precision: 0.862186\n",
      "valid avg_recall: 0.782203\n",
      "valid avg_f1: 0.819579\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 4420 ; rate: 0.295139\n",
      "y_true_label_1_num: 1736 ; rate: 0.115919\n",
      "y_true_label_2_num: 3413 ; rate: 0.227898\n",
      "y_true_label_3_num: 5407 ; rate: 0.361044\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.752938\n",
      "valid avg_precision: 0.789732\n",
      "valid avg_recall: 0.702591\n",
      "valid avg_f1: 0.742823\n",
      "epoch 2\n",
      "epoch 2; batch 128; loss 0.454104\n",
      "epoch:2; batch 128; train accuracy: 0.685882\n",
      "epoch 2; batch 256; loss 0.608751\n",
      "epoch:2; batch 256; train accuracy: 0.685970\n",
      "epoch 2; batch 384; loss 0.555622\n",
      "epoch:2; batch 384; train accuracy: 0.686124\n",
      "epoch 2; batch 512; loss 0.587186\n",
      "epoch:2; batch 512; train accuracy: 0.686182\n",
      "epoch 2; batch 640; loss 0.560636\n",
      "epoch:2; batch 640; train accuracy: 0.686297\n",
      "epoch 2; batch 768; loss 0.530379\n",
      "epoch:2; batch 768; train accuracy: 0.686422\n",
      "epoch 2; batch 896; loss 0.690606\n",
      "epoch:2; batch 896; train accuracy: 0.686499\n",
      "epoch 2; batch 1024; loss 0.614875\n",
      "epoch:2; batch 1024; train accuracy: 0.686623\n",
      "epoch 2; batch 1152; loss 0.607034\n",
      "epoch:2; batch 1152; train accuracy: 0.686699\n",
      "epoch 2; batch 1280; loss 0.614275\n",
      "epoch:2; batch 1280; train accuracy: 0.686803\n",
      "epoch 2; batch 1408; loss 0.552588\n",
      "epoch:2; batch 1408; train accuracy: 0.686898\n",
      "epoch 2; batch 1536; loss 0.599277\n",
      "epoch:2; batch 1536; train accuracy: 0.686984\n",
      "epoch 2; batch 1664; loss 0.621528\n",
      "epoch:2; batch 1664; train accuracy: 0.687012\n",
      "epoch 2; batch 1792; loss 0.617240\n",
      "epoch:2; batch 1792; train accuracy: 0.687060\n",
      "epoch 2; batch 1920; loss 0.567255\n",
      "epoch:2; batch 1920; train accuracy: 0.687210\n",
      "epoch 2; batch 2048; loss 0.659306\n",
      "epoch:2; batch 2048; train accuracy: 0.687304\n",
      "epoch 2; batch 2176; loss 0.671765\n",
      "epoch:2; batch 2176; train accuracy: 0.687323\n",
      "epoch 2; batch 2304; loss 0.534716\n",
      "epoch:2; batch 2304; train accuracy: 0.687453\n",
      "epoch 2; batch 2432; loss 0.528328\n",
      "epoch:2; batch 2432; train accuracy: 0.687630\n",
      "epoch 2; batch 2560; loss 0.582300\n",
      "epoch:2; batch 2560; train accuracy: 0.687686\n",
      "epoch 2; batch 2688; loss 0.544800\n",
      "epoch:2; batch 2688; train accuracy: 0.687788\n",
      "epoch 2; batch 2816; loss 0.544511\n",
      "epoch:2; batch 2816; train accuracy: 0.687936\n",
      "epoch 2; batch 2944; loss 0.617118\n",
      "epoch:2; batch 2944; train accuracy: 0.688019\n",
      "epoch 2; batch 3072; loss 0.521358\n",
      "epoch:2; batch 3072; train accuracy: 0.688120\n",
      "epoch 2; batch 3200; loss 0.476585\n",
      "epoch:2; batch 3200; train accuracy: 0.688286\n",
      "epoch 2; batch 3328; loss 0.579707\n",
      "epoch:2; batch 3328; train accuracy: 0.688368\n",
      "epoch 2; batch 3456; loss 0.611595\n",
      "epoch:2; batch 3456; train accuracy: 0.688487\n",
      "epoch 2; batch 3584; loss 0.602441\n",
      "epoch:2; batch 3584; train accuracy: 0.688578\n",
      "epoch 2; batch 3712; loss 0.561160\n",
      "epoch:2; batch 3712; train accuracy: 0.688669\n",
      "epoch 2; batch 3840; loss 0.408673\n",
      "epoch:2; batch 3840; train accuracy: 0.688824\n",
      "epoch 2; batch 3968; loss 0.502951\n",
      "epoch:2; batch 3968; train accuracy: 0.688905\n",
      "epoch 2; batch 4096; loss 0.584301\n",
      "epoch:2; batch 4096; train accuracy: 0.689013\n",
      "epoch 2; batch 4224; loss 0.523702\n",
      "epoch:2; batch 4224; train accuracy: 0.689158\n",
      "epoch 2; batch 4352; loss 0.450798\n",
      "epoch:2; batch 4352; train accuracy: 0.689284\n",
      "epoch 2; batch 4480; loss 0.470778\n",
      "epoch:2; batch 4480; train accuracy: 0.689455\n",
      "epoch 2; batch 4608; loss 0.559413\n",
      "epoch:2; batch 4608; train accuracy: 0.689572\n",
      "epoch 2; batch 4736; loss 0.674395\n",
      "epoch:2; batch 4736; train accuracy: 0.689670\n",
      "epoch 2; batch 4864; loss 0.605971\n",
      "epoch:2; batch 4864; train accuracy: 0.689776\n",
      "epoch 2; batch 4992; loss 0.643842\n",
      "epoch:2; batch 4992; train accuracy: 0.689846\n",
      "epoch 2; batch 5120; loss 0.676523\n",
      "epoch:2; batch 5120; train accuracy: 0.689907\n",
      "epoch 2; batch 5248; loss 0.434929\n",
      "epoch:2; batch 5248; train accuracy: 0.690068\n",
      "epoch 2; batch 5376; loss 0.468600\n",
      "epoch:2; batch 5376; train accuracy: 0.690264\n",
      "epoch 2; batch 5504; loss 0.480158\n",
      "epoch:2; batch 5504; train accuracy: 0.690406\n",
      "epoch 2; batch 5632; loss 0.533498\n",
      "epoch:2; batch 5632; train accuracy: 0.690520\n",
      "epoch 2; batch 5760; loss 0.581599\n",
      "epoch:2; batch 5760; train accuracy: 0.690571\n",
      "epoch 2; batch 5888; loss 0.451646\n",
      "epoch:2; batch 5888; train accuracy: 0.690748\n",
      "epoch 2; batch 6016; loss 0.557902\n",
      "epoch:2; batch 6016; train accuracy: 0.690834\n",
      "epoch 2; batch 6144; loss 0.564868\n",
      "epoch:2; batch 6144; train accuracy: 0.690920\n",
      "epoch 2; batch 6272; loss 0.596396\n",
      "epoch:2; batch 6272; train accuracy: 0.690997\n",
      "epoch 2; batch 6400; loss 0.515792\n",
      "epoch:2; batch 6400; train accuracy: 0.691092\n",
      "epoch 2; batch 6528; loss 0.601026\n",
      "epoch:2; batch 6528; train accuracy: 0.691169\n",
      "epoch 2; batch 6656; loss 0.430164\n",
      "epoch:2; batch 6656; train accuracy: 0.691335\n",
      "epoch 2; batch 6784; loss 0.543590\n",
      "epoch:2; batch 6784; train accuracy: 0.691464\n",
      "epoch 2; batch 6912; loss 0.579182\n",
      "epoch:2; batch 6912; train accuracy: 0.691594\n",
      "epoch 2; batch 7040; loss 0.438833\n",
      "epoch:2; batch 7040; train accuracy: 0.691786\n",
      "epoch 2; batch 7168; loss 0.531615\n",
      "epoch:2; batch 7168; train accuracy: 0.691906\n",
      "epoch 2; batch 7296; loss 0.580579\n",
      "epoch:2; batch 7296; train accuracy: 0.691990\n",
      "epoch 2; batch 7424; loss 0.562646\n",
      "epoch:2; batch 7424; train accuracy: 0.692109\n",
      "epoch 2; batch 7552; loss 0.481036\n",
      "epoch:2; batch 7552; train accuracy: 0.692255\n",
      "epoch 2; batch 7680; loss 0.422728\n",
      "epoch:2; batch 7680; train accuracy: 0.692436\n",
      "epoch 2; batch 7808; loss 0.590039\n",
      "epoch:2; batch 7808; train accuracy: 0.692546\n",
      "epoch 2; batch 7936; loss 0.558629\n",
      "epoch:2; batch 7936; train accuracy: 0.692646\n",
      "epoch 2; batch 8064; loss 0.480367\n",
      "epoch:2; batch 8064; train accuracy: 0.692817\n",
      "epoch 2; batch 8192; loss 0.455181\n",
      "epoch:2; batch 8192; train accuracy: 0.692944\n",
      "epoch 2; batch 8320; loss 0.706436\n",
      "epoch:2; batch 8320; train accuracy: 0.693008\n",
      "epoch 2; batch 8448; loss 0.530078\n",
      "epoch:2; batch 8448; train accuracy: 0.693117\n",
      "epoch 2; batch 8576; loss 0.400223\n",
      "epoch:2; batch 8576; train accuracy: 0.693296\n",
      "epoch 2; batch 8704; loss 0.571594\n",
      "epoch:2; batch 8704; train accuracy: 0.693412\n",
      "epoch 2; batch 8832; loss 0.516894\n",
      "epoch:2; batch 8832; train accuracy: 0.693564\n",
      "epoch 2; batch 8960; loss 0.394319\n",
      "epoch:2; batch 8960; train accuracy: 0.693732\n",
      "epoch 2; batch 9088; loss 0.486140\n",
      "epoch:2; batch 9088; train accuracy: 0.693848\n",
      "epoch 2; batch 9216; loss 0.605976\n",
      "epoch:2; batch 9216; train accuracy: 0.693990\n",
      "epoch 2; batch 9344; loss 0.561831\n",
      "epoch:2; batch 9344; train accuracy: 0.694088\n",
      "epoch 2; batch 9472; loss 0.462257\n",
      "epoch:2; batch 9472; train accuracy: 0.694238\n",
      "epoch 2; batch 9600; loss 0.651671\n",
      "epoch:2; batch 9600; train accuracy: 0.694352\n",
      "epoch 2; batch 9728; loss 0.694221\n",
      "epoch:2; batch 9728; train accuracy: 0.694388\n",
      "epoch 2; batch 9856; loss 0.444179\n",
      "epoch:2; batch 9856; train accuracy: 0.694520\n",
      "epoch 2; batch 9984; loss 0.478857\n",
      "epoch:2; batch 9984; train accuracy: 0.694643\n",
      "epoch 2; batch 10112; loss 0.554320\n",
      "epoch:2; batch 10112; train accuracy: 0.694765\n",
      "epoch 2; batch 10240; loss 0.478282\n",
      "epoch:2; batch 10240; train accuracy: 0.694861\n",
      "epoch 2; batch 10368; loss 0.454957\n",
      "epoch:2; batch 10368; train accuracy: 0.695026\n",
      "epoch 2; batch 10496; loss 0.497780\n",
      "epoch:2; batch 10496; train accuracy: 0.695200\n",
      "epoch 2; batch 10624; loss 0.581804\n",
      "epoch:2; batch 10624; train accuracy: 0.695295\n",
      "epoch 2; batch 10752; loss 0.617381\n",
      "epoch:2; batch 10752; train accuracy: 0.695373\n",
      "epoch 2; batch 10880; loss 0.465794\n",
      "epoch:2; batch 10880; train accuracy: 0.695537\n",
      "epoch 2; batch 11008; loss 0.468019\n",
      "epoch:2; batch 11008; train accuracy: 0.695675\n",
      "epoch 2; batch 11136; loss 0.475517\n",
      "epoch:2; batch 11136; train accuracy: 0.695829\n",
      "epoch 2; batch 11264; loss 0.494090\n",
      "epoch:2; batch 11264; train accuracy: 0.695941\n",
      "epoch 2; batch 11392; loss 0.521109\n",
      "epoch:2; batch 11392; train accuracy: 0.696086\n",
      "epoch 2; batch 11520; loss 0.542919\n",
      "epoch:2; batch 11520; train accuracy: 0.696180\n",
      "epoch 2; batch 11648; loss 0.498771\n",
      "epoch:2; batch 11648; train accuracy: 0.696324\n",
      "epoch 2; batch 11776; loss 0.538450\n",
      "epoch:2; batch 11776; train accuracy: 0.696409\n",
      "epoch 2; batch 11904; loss 0.492261\n",
      "epoch:2; batch 11904; train accuracy: 0.696528\n",
      "epoch 2; batch 12032; loss 0.614409\n",
      "epoch:2; batch 12032; train accuracy: 0.696595\n",
      "epoch 2; batch 12160; loss 0.617192\n",
      "epoch:2; batch 12160; train accuracy: 0.696679\n",
      "epoch 2; batch 12288; loss 0.506738\n",
      "epoch:2; batch 12288; train accuracy: 0.696831\n",
      "epoch 2; batch 12416; loss 0.520969\n",
      "epoch:2; batch 12416; train accuracy: 0.696948\n",
      "epoch 2; batch 12544; loss 0.497723\n",
      "epoch:2; batch 12544; train accuracy: 0.697066\n",
      "epoch 2; batch 12672; loss 0.451986\n",
      "epoch:2; batch 12672; train accuracy: 0.697225\n",
      "epoch 2; batch 12800; loss 0.435917\n",
      "epoch:2; batch 12800; train accuracy: 0.697410\n",
      "epoch 2; batch 12928; loss 0.394862\n",
      "epoch:2; batch 12928; train accuracy: 0.697560\n",
      "epoch 2; batch 13056; loss 0.529410\n",
      "epoch:2; batch 13056; train accuracy: 0.697643\n",
      "epoch 2; batch 13184; loss 0.528494\n",
      "epoch:2; batch 13184; train accuracy: 0.697750\n",
      "epoch 2; batch 13312; loss 0.524609\n",
      "epoch:2; batch 13312; train accuracy: 0.697832\n",
      "epoch 2; batch 13440; loss 0.590605\n",
      "epoch:2; batch 13440; train accuracy: 0.697914\n",
      "epoch 2; batch 13568; loss 0.550262\n",
      "epoch:2; batch 13568; train accuracy: 0.698046\n",
      "epoch 2; batch 13696; loss 0.428514\n",
      "epoch:2; batch 13696; train accuracy: 0.698228\n",
      "epoch 2; batch 13824; loss 0.531050\n",
      "epoch:2; batch 13824; train accuracy: 0.698326\n",
      "epoch 2; batch 13952; loss 0.430828\n",
      "epoch:2; batch 13952; train accuracy: 0.698466\n",
      "epoch 2; batch 14080; loss 0.484809\n",
      "epoch:2; batch 14080; train accuracy: 0.698589\n",
      "epoch 2; batch 14208; loss 0.478913\n",
      "epoch:2; batch 14208; train accuracy: 0.698745\n",
      "epoch 2; batch 14336; loss 0.527620\n",
      "epoch:2; batch 14336; train accuracy: 0.698867\n",
      "epoch 2; batch 14464; loss 0.455407\n",
      "epoch:2; batch 14464; train accuracy: 0.699022\n",
      "epoch 2; batch 14592; loss 0.422916\n",
      "epoch:2; batch 14592; train accuracy: 0.699152\n",
      "epoch 2; batch 14720; loss 0.354646\n",
      "epoch:2; batch 14720; train accuracy: 0.699323\n",
      "epoch 2; batch 14848; loss 0.412841\n",
      "epoch:2; batch 14848; train accuracy: 0.699519\n",
      "epoch 2; batch 14976; loss 0.535025\n",
      "epoch:2; batch 14976; train accuracy: 0.699623\n",
      "epoch 2; batch 15104; loss 0.475108\n",
      "epoch:2; batch 15104; train accuracy: 0.699752\n",
      "epoch 2; batch 15232; loss 0.473809\n",
      "epoch:2; batch 15232; train accuracy: 0.699872\n",
      "epoch 2; batch 15360; loss 0.525115\n",
      "epoch:2; batch 15360; train accuracy: 0.700017\n",
      "epoch 2; batch 15488; loss 0.481806\n",
      "epoch:2; batch 15488; train accuracy: 0.700153\n",
      "epoch 2; batch 15616; loss 0.563844\n",
      "epoch:2; batch 15616; train accuracy: 0.700231\n",
      "epoch 2; batch 15744; loss 0.447241\n",
      "epoch:2; batch 15744; train accuracy: 0.700341\n",
      "epoch 2; batch 15872; loss 0.533698\n",
      "epoch:2; batch 15872; train accuracy: 0.700452\n",
      "epoch 2; batch 16000; loss 0.391206\n",
      "epoch:2; batch 16000; train accuracy: 0.700637\n",
      "epoch 2; batch 16128; loss 0.500269\n",
      "epoch:2; batch 16128; train accuracy: 0.700788\n",
      "epoch 2; batch 16256; loss 0.462820\n",
      "epoch:2; batch 16256; train accuracy: 0.700922\n",
      "epoch 2; batch 16384; loss 0.455409\n",
      "epoch:2; batch 16384; train accuracy: 0.701056\n",
      "epoch 2; batch 16512; loss 0.488757\n",
      "epoch:2; batch 16512; train accuracy: 0.701108\n",
      "epoch 2; batch 16640; loss 0.446172\n",
      "epoch:2; batch 16640; train accuracy: 0.701242\n",
      "epoch 2; batch 16768; loss 0.400497\n",
      "epoch:2; batch 16768; train accuracy: 0.701424\n",
      "epoch 2; batch 16896; loss 0.577618\n",
      "epoch:2; batch 16896; train accuracy: 0.701517\n",
      "epoch 2; batch 17024; loss 0.464966\n",
      "epoch:2; batch 17024; train accuracy: 0.701658\n",
      "epoch 2; batch 17152; loss 0.423291\n",
      "epoch:2; batch 17152; train accuracy: 0.701798\n",
      "epoch 2; batch 17280; loss 0.417214\n",
      "epoch:2; batch 17280; train accuracy: 0.701955\n",
      "epoch 2; batch 17408; loss 0.441257\n",
      "epoch:2; batch 17408; train accuracy: 0.702071\n",
      "epoch 2; batch 17536; loss 0.375156\n",
      "epoch:2; batch 17536; train accuracy: 0.702227\n",
      "epoch 2; batch 17664; loss 0.456420\n",
      "epoch:2; batch 17664; train accuracy: 0.702334\n",
      "epoch 2; batch 17792; loss 0.471554\n",
      "epoch:2; batch 17792; train accuracy: 0.702449\n",
      "epoch 2; batch 17920; loss 0.482553\n",
      "epoch:2; batch 17920; train accuracy: 0.702580\n",
      "epoch 2; batch 18048; loss 0.505189\n",
      "epoch:2; batch 18048; train accuracy: 0.702678\n",
      "epoch 2; batch 18176; loss 0.473186\n",
      "epoch:2; batch 18176; train accuracy: 0.702792\n",
      "epoch 2; batch 18304; loss 0.377520\n",
      "epoch:2; batch 18304; train accuracy: 0.702930\n",
      "epoch 2; batch 18432; loss 0.364920\n",
      "epoch:2; batch 18432; train accuracy: 0.703068\n",
      "epoch 2; batch 18560; loss 0.545552\n",
      "epoch:2; batch 18560; train accuracy: 0.703174\n",
      "epoch 2; batch 18688; loss 0.350939\n",
      "epoch:2; batch 18688; train accuracy: 0.703360\n",
      "epoch 2; batch 18816; loss 0.466019\n",
      "epoch:2; batch 18816; train accuracy: 0.703464\n",
      "epoch 2; batch 18944; loss 0.516653\n",
      "epoch:2; batch 18944; train accuracy: 0.703577\n",
      "epoch 2; batch 19072; loss 0.455016\n",
      "epoch:2; batch 19072; train accuracy: 0.703722\n",
      "epoch 2; batch 19200; loss 0.499029\n",
      "epoch:2; batch 19200; train accuracy: 0.703842\n",
      "epoch 2; batch 19328; loss 0.360209\n",
      "epoch:2; batch 19328; train accuracy: 0.703986\n",
      "epoch 2; batch 19456; loss 0.383341\n",
      "epoch:2; batch 19456; train accuracy: 0.704146\n",
      "epoch 2; batch 19584; loss 0.410523\n",
      "epoch:2; batch 19584; train accuracy: 0.704297\n",
      "epoch 2; batch 19712; loss 0.481076\n",
      "epoch:2; batch 19712; train accuracy: 0.704432\n",
      "epoch 2; batch 19840; loss 0.443248\n",
      "epoch:2; batch 19840; train accuracy: 0.704591\n",
      "epoch 2; batch 19968; loss 0.438129\n",
      "epoch:2; batch 19968; train accuracy: 0.704718\n",
      "epoch 2; batch 20096; loss 0.443003\n",
      "epoch:2; batch 20096; train accuracy: 0.704828\n",
      "epoch 2; batch 20224; loss 0.403048\n",
      "epoch:2; batch 20224; train accuracy: 0.704946\n",
      "epoch 2; batch 20352; loss 0.510307\n",
      "epoch:2; batch 20352; train accuracy: 0.705040\n",
      "epoch 2; batch 20480; loss 0.418573\n",
      "epoch:2; batch 20480; train accuracy: 0.705214\n",
      "epoch 2; batch 20608; loss 0.453086\n",
      "epoch:2; batch 20608; train accuracy: 0.705339\n",
      "epoch 2; batch 20736; loss 0.414110\n",
      "epoch:2; batch 20736; train accuracy: 0.705496\n",
      "epoch 2; batch 20864; loss 0.419936\n",
      "epoch:2; batch 20864; train accuracy: 0.705605\n",
      "epoch 2; batch 20992; loss 0.526002\n",
      "epoch:2; batch 20992; train accuracy: 0.705737\n",
      "epoch 2; batch 21120; loss 0.424263\n",
      "epoch:2; batch 21120; train accuracy: 0.705893\n",
      "epoch 2; batch 21248; loss 0.576108\n",
      "epoch:2; batch 21248; train accuracy: 0.705993\n",
      "epoch 2; batch 21376; loss 0.400128\n",
      "epoch:2; batch 21376; train accuracy: 0.706117\n",
      "epoch 2; batch 21504; loss 0.493007\n",
      "epoch:2; batch 21504; train accuracy: 0.706209\n",
      "epoch 2; batch 21632; loss 0.403890\n",
      "epoch:2; batch 21632; train accuracy: 0.706372\n",
      "epoch 2; batch 21760; loss 0.474852\n",
      "epoch:2; batch 21760; train accuracy: 0.706479\n",
      "epoch 2; batch 21888; loss 0.454610\n",
      "epoch:2; batch 21888; train accuracy: 0.706633\n",
      "epoch 2; batch 22016; loss 0.455024\n",
      "epoch:2; batch 22016; train accuracy: 0.706756\n",
      "epoch 2; batch 22144; loss 0.346121\n",
      "epoch:2; batch 22144; train accuracy: 0.706933\n",
      "epoch 2; batch 22272; loss 0.491878\n",
      "epoch:2; batch 22272; train accuracy: 0.707047\n",
      "epoch 2; batch 22400; loss 0.452369\n",
      "epoch:2; batch 22400; train accuracy: 0.707169\n",
      "epoch 2; batch 22528; loss 0.433773\n",
      "epoch:2; batch 22528; train accuracy: 0.707259\n",
      "epoch 2; batch 22656; loss 0.576337\n",
      "epoch:2; batch 22656; train accuracy: 0.707309\n",
      "epoch 2; batch 22784; loss 0.505755\n",
      "epoch:2; batch 22784; train accuracy: 0.707368\n",
      "epoch 2; batch 22912; loss 0.487252\n",
      "epoch:2; batch 22912; train accuracy: 0.707473\n",
      "epoch 2; batch 23040; loss 0.531768\n",
      "epoch:2; batch 23040; train accuracy: 0.707555\n",
      "epoch 2; batch 23168; loss 0.426077\n",
      "epoch:2; batch 23168; train accuracy: 0.707667\n",
      "epoch 2; batch 23296; loss 0.431731\n",
      "epoch:2; batch 23296; train accuracy: 0.707803\n",
      "epoch 2; batch 23424; loss 0.511231\n",
      "epoch:2; batch 23424; train accuracy: 0.707892\n",
      "epoch 2; batch 23552; loss 0.457903\n",
      "epoch:2; batch 23552; train accuracy: 0.707996\n",
      "epoch 2; batch 23680; loss 0.483794\n",
      "epoch:2; batch 23680; train accuracy: 0.708100\n",
      "epoch 2; batch 23808; loss 0.490860\n",
      "epoch:2; batch 23808; train accuracy: 0.708188\n",
      "epoch 2; batch 23936; loss 0.490333\n",
      "epoch:2; batch 23936; train accuracy: 0.708307\n",
      "epoch 2; batch 24064; loss 0.570669\n",
      "epoch:2; batch 24064; train accuracy: 0.708411\n",
      "epoch 2; batch 24192; loss 0.430513\n",
      "epoch:2; batch 24192; train accuracy: 0.708529\n",
      "epoch 2; batch 24320; loss 0.406550\n",
      "epoch:2; batch 24320; train accuracy: 0.708663\n",
      "epoch 2; batch 24448; loss 0.515713\n",
      "epoch:2; batch 24448; train accuracy: 0.708782\n",
      "epoch 2; batch 24576; loss 0.491192\n",
      "epoch:2; batch 24576; train accuracy: 0.708869\n",
      "epoch 2; batch 24704; loss 0.455713\n",
      "epoch:2; batch 24704; train accuracy: 0.708948\n",
      "epoch 2; batch 24832; loss 0.557658\n",
      "epoch:2; batch 24832; train accuracy: 0.709073\n",
      "epoch 2; batch 24960; loss 0.539657\n",
      "epoch:2; batch 24960; train accuracy: 0.709183\n",
      "epoch 2; batch 25088; loss 0.393567\n",
      "epoch:2; batch 25088; train accuracy: 0.709323\n",
      "epoch 2; batch 25216; loss 0.368935\n",
      "epoch:2; batch 25216; train accuracy: 0.709501\n",
      "epoch 2; batch 25344; loss 0.506702\n",
      "epoch:2; batch 25344; train accuracy: 0.709617\n",
      "epoch 2; batch 25472; loss 0.400850\n",
      "epoch:2; batch 25472; train accuracy: 0.709734\n",
      "epoch 2; batch 25600; loss 0.480143\n",
      "epoch:2; batch 25600; train accuracy: 0.709827\n",
      "epoch 2; batch 25728; loss 0.468096\n",
      "epoch:2; batch 25728; train accuracy: 0.709989\n",
      "epoch 2; batch 25856; loss 0.544248\n",
      "epoch:2; batch 25856; train accuracy: 0.710043\n",
      "epoch 2; batch 25984; loss 0.345160\n",
      "epoch:2; batch 25984; train accuracy: 0.710197\n",
      "epoch 2; batch 26112; loss 0.494352\n",
      "epoch:2; batch 26112; train accuracy: 0.710297\n",
      "epoch 2; batch 26240; loss 0.573414\n",
      "epoch:2; batch 26240; train accuracy: 0.710305\n",
      "epoch 2; batch 26368; loss 0.482034\n",
      "epoch:2; batch 26368; train accuracy: 0.710412\n",
      "epoch 2; batch 26496; loss 0.511332\n",
      "epoch:2; batch 26496; train accuracy: 0.710481\n",
      "epoch 2; batch 26624; loss 0.338605\n",
      "epoch:2; batch 26624; train accuracy: 0.710649\n",
      "epoch 2; batch 26752; loss 0.452061\n",
      "epoch:2; batch 26752; train accuracy: 0.710801\n",
      "epoch 2; batch 26880; loss 0.368721\n",
      "epoch:2; batch 26880; train accuracy: 0.710938\n",
      "epoch 2; batch 27008; loss 0.450652\n",
      "epoch:2; batch 27008; train accuracy: 0.711021\n",
      "epoch 2; batch 27136; loss 0.427331\n",
      "epoch:2; batch 27136; train accuracy: 0.711096\n",
      "epoch 2; batch 27264; loss 0.416117\n",
      "epoch:2; batch 27264; train accuracy: 0.711225\n",
      "epoch 2; batch 27392; loss 0.400745\n",
      "epoch:2; batch 27392; train accuracy: 0.711353\n",
      "epoch 2; batch 27520; loss 0.424117\n",
      "epoch:2; batch 27520; train accuracy: 0.711489\n",
      "epoch 2; batch 27648; loss 0.440341\n",
      "epoch:2; batch 27648; train accuracy: 0.711594\n",
      "epoch 2; batch 27776; loss 0.498060\n",
      "epoch:2; batch 27776; train accuracy: 0.711721\n",
      "epoch 2; batch 27904; loss 0.487205\n",
      "epoch:2; batch 27904; train accuracy: 0.711818\n",
      "epoch 2; batch 28032; loss 0.353219\n",
      "epoch:2; batch 28032; train accuracy: 0.711953\n",
      "epoch 2; batch 28160; loss 0.541907\n",
      "epoch:2; batch 28160; train accuracy: 0.712049\n",
      "epoch 2; batch 28288; loss 0.446408\n",
      "epoch:2; batch 28288; train accuracy: 0.712176\n",
      "epoch 2; batch 28416; loss 0.318023\n",
      "epoch:2; batch 28416; train accuracy: 0.712310\n",
      "epoch 2; batch 28544; loss 0.388384\n",
      "epoch:2; batch 28544; train accuracy: 0.712428\n",
      "epoch 2; batch 28672; loss 0.397124\n",
      "epoch:2; batch 28672; train accuracy: 0.712584\n",
      "epoch 2; batch 28800; loss 0.459454\n",
      "epoch:2; batch 28800; train accuracy: 0.712732\n",
      "epoch 2; batch 28928; loss 0.451121\n",
      "epoch:2; batch 28928; train accuracy: 0.712850\n",
      "epoch 2; batch 29056; loss 0.466745\n",
      "epoch:2; batch 29056; train accuracy: 0.712945\n",
      "epoch 2; batch 29184; loss 0.502916\n",
      "epoch:2; batch 29184; train accuracy: 0.713092\n",
      "epoch 2; batch 29312; loss 0.376146\n",
      "epoch:2; batch 29312; train accuracy: 0.713224\n",
      "epoch 2; batch 29440; loss 0.430206\n",
      "epoch:2; batch 29440; train accuracy: 0.713341\n",
      "epoch 2; batch 29568; loss 0.584504\n",
      "epoch:2; batch 29568; train accuracy: 0.713368\n",
      "epoch 2; batch 29696; loss 0.443326\n",
      "epoch:2; batch 29696; train accuracy: 0.713448\n",
      "epoch 2; batch 29824; loss 0.481768\n",
      "epoch:2; batch 29824; train accuracy: 0.713534\n",
      "epoch 2; batch 29952; loss 0.354785\n",
      "epoch:2; batch 29952; train accuracy: 0.713702\n",
      "epoch 2; batch 30080; loss 0.383866\n",
      "epoch:2; batch 30080; train accuracy: 0.713818\n",
      "epoch 2; batch 30208; loss 0.430171\n",
      "epoch:2; batch 30208; train accuracy: 0.713941\n",
      "epoch 2; batch 30336; loss 0.365113\n",
      "epoch:2; batch 30336; train accuracy: 0.714071\n",
      "epoch 2; batch 30464; loss 0.506176\n",
      "epoch:2; batch 30464; train accuracy: 0.714187\n",
      "epoch 2; batch 30592; loss 0.430250\n",
      "epoch:2; batch 30592; train accuracy: 0.714294\n",
      "epoch 2; batch 30720; loss 0.481021\n",
      "epoch:2; batch 30720; train accuracy: 0.714402\n",
      "epoch 2; batch 30848; loss 0.401224\n",
      "epoch:2; batch 30848; train accuracy: 0.714560\n",
      "epoch 2; batch 30976; loss 0.330182\n",
      "epoch:2; batch 30976; train accuracy: 0.714733\n",
      "epoch 2; batch 31104; loss 0.369191\n",
      "epoch:2; batch 31104; train accuracy: 0.714869\n",
      "epoch 2; batch 31232; loss 0.420299\n",
      "epoch:2; batch 31232; train accuracy: 0.714976\n",
      "epoch 2; batch 31360; loss 0.572247\n",
      "epoch:2; batch 31360; train accuracy: 0.715060\n",
      "epoch 2; batch 31488; loss 0.316838\n",
      "epoch:2; batch 31488; train accuracy: 0.715240\n",
      "epoch 2; batch 31616; loss 0.401701\n",
      "epoch:2; batch 31616; train accuracy: 0.715353\n",
      "epoch 2; batch 31744; loss 0.356422\n",
      "epoch:2; batch 31744; train accuracy: 0.715480\n",
      "epoch 2; batch 31872; loss 0.376646\n",
      "epoch:2; batch 31872; train accuracy: 0.715637\n",
      "epoch 2; batch 32000; loss 0.438767\n",
      "epoch:2; batch 32000; train accuracy: 0.715735\n",
      "epoch 2; batch 32128; loss 0.483067\n",
      "epoch:2; batch 32128; train accuracy: 0.715825\n",
      "epoch 2; batch 32256; loss 0.450837\n",
      "epoch:2; batch 32256; train accuracy: 0.715944\n",
      "epoch 2; batch 32384; loss 0.389963\n",
      "epoch:2; batch 32384; train accuracy: 0.716056\n",
      "epoch 2; batch 32512; loss 0.392785\n",
      "epoch:2; batch 32512; train accuracy: 0.716168\n",
      "epoch 2; batch 32640; loss 0.406422\n",
      "epoch:2; batch 32640; train accuracy: 0.716294\n",
      "epoch 2; batch 32768; loss 0.454472\n",
      "epoch:2; batch 32768; train accuracy: 0.716390\n",
      "epoch 2; batch 32896; loss 0.483650\n",
      "epoch:2; batch 32896; train accuracy: 0.716472\n",
      "epoch 2; batch 33024; loss 0.338560\n",
      "epoch:2; batch 33024; train accuracy: 0.716598\n",
      "epoch 2; batch 33152; loss 0.377115\n",
      "epoch:2; batch 33152; train accuracy: 0.716723\n",
      "epoch 2; batch 33280; loss 0.355786\n",
      "epoch:2; batch 33280; train accuracy: 0.716855\n",
      "epoch 2; batch 33408; loss 0.297687\n",
      "epoch:2; batch 33408; train accuracy: 0.717052\n",
      "epoch 2; batch 33536; loss 0.462167\n",
      "epoch:2; batch 33536; train accuracy: 0.717162\n",
      "epoch 2; batch 33664; loss 0.413582\n",
      "epoch:2; batch 33664; train accuracy: 0.717271\n",
      "epoch 2; batch 33792; loss 0.298763\n",
      "epoch:2; batch 33792; train accuracy: 0.717445\n",
      "epoch 2; batch 33920; loss 0.542727\n",
      "epoch:2; batch 33920; train accuracy: 0.717540\n",
      "epoch 2; batch 34048; loss 0.451044\n",
      "epoch:2; batch 34048; train accuracy: 0.717628\n",
      "epoch 2; batch 34176; loss 0.542228\n",
      "epoch:2; batch 34176; train accuracy: 0.717708\n",
      "epoch 2; batch 34304; loss 0.531085\n",
      "epoch:2; batch 34304; train accuracy: 0.717802\n",
      "epoch 2; batch 34432; loss 0.411736\n",
      "epoch:2; batch 34432; train accuracy: 0.717889\n",
      "epoch 2; batch 34560; loss 0.557036\n",
      "epoch:2; batch 34560; train accuracy: 0.717962\n",
      "epoch 2; batch 34688; loss 0.510985\n",
      "epoch:2; batch 34688; train accuracy: 0.718034\n",
      "epoch 2; batch 34816; loss 0.444156\n",
      "epoch:2; batch 34816; train accuracy: 0.718120\n",
      "epoch 2; batch 34944; loss 0.522308\n",
      "epoch:2; batch 34944; train accuracy: 0.718221\n",
      "epoch 2; batch 35072; loss 0.313608\n",
      "epoch:2; batch 35072; train accuracy: 0.718364\n",
      "epoch 2; batch 35200; loss 0.537745\n",
      "epoch:2; batch 35200; train accuracy: 0.718443\n",
      "epoch 2; batch 35328; loss 0.354602\n",
      "epoch:2; batch 35328; train accuracy: 0.718593\n",
      "epoch 2; batch 35456; loss 0.493819\n",
      "epoch:2; batch 35456; train accuracy: 0.718686\n",
      "epoch 2; batch 35584; loss 0.419498\n",
      "epoch:2; batch 35584; train accuracy: 0.718800\n",
      "epoch 2; batch 35712; loss 0.364304\n",
      "epoch:2; batch 35712; train accuracy: 0.718928\n",
      "epoch 2; batch 35840; loss 0.220413\n",
      "epoch:2; batch 35840; train accuracy: 0.719119\n",
      "epoch 2; batch 35968; loss 0.563064\n",
      "epoch:2; batch 35968; train accuracy: 0.719176\n",
      "epoch 2; batch 36096; loss 0.488424\n",
      "epoch:2; batch 36096; train accuracy: 0.719268\n",
      "epoch 2; batch 36224; loss 0.450730\n",
      "epoch:2; batch 36224; train accuracy: 0.719373\n",
      "epoch 2; batch 36352; loss 0.434219\n",
      "epoch:2; batch 36352; train accuracy: 0.719514\n",
      "epoch 2; batch 36480; loss 0.459847\n",
      "epoch:2; batch 36480; train accuracy: 0.719591\n",
      "epoch 2; batch 36608; loss 0.390312\n",
      "epoch:2; batch 36608; train accuracy: 0.719704\n",
      "epoch 2; batch 36736; loss 0.378983\n",
      "epoch:2; batch 36736; train accuracy: 0.719816\n",
      "epoch 2; batch 36864; loss 0.454573\n",
      "epoch:2; batch 36864; train accuracy: 0.719913\n",
      "epoch 2; batch 36992; loss 0.411855\n",
      "epoch:2; batch 36992; train accuracy: 0.720018\n",
      "epoch 2; batch 37120; loss 0.370626\n",
      "epoch:2; batch 37120; train accuracy: 0.720144\n",
      "epoch 2; batch 37248; loss 0.422415\n",
      "epoch:2; batch 37248; train accuracy: 0.720262\n",
      "epoch 2; batch 37376; loss 0.338773\n",
      "epoch:2; batch 37376; train accuracy: 0.720415\n",
      "epoch 2; batch 37504; loss 0.317579\n",
      "epoch:2; batch 37504; train accuracy: 0.720575\n",
      "epoch 2; batch 37632; loss 0.477971\n",
      "epoch:2; batch 37632; train accuracy: 0.720636\n",
      "epoch 2; batch 37760; loss 0.446725\n",
      "epoch:2; batch 37760; train accuracy: 0.720733\n",
      "epoch 2; batch 37888; loss 0.366004\n",
      "epoch:2; batch 37888; train accuracy: 0.720864\n",
      "epoch 2; batch 38016; loss 0.452486\n",
      "epoch:2; batch 38016; train accuracy: 0.720974\n",
      "epoch 2; batch 38144; loss 0.361537\n",
      "epoch:2; batch 38144; train accuracy: 0.721084\n",
      "epoch 2; batch 38272; loss 0.467783\n",
      "epoch:2; batch 38272; train accuracy: 0.721173\n",
      "epoch 2; batch 38400; loss 0.347274\n",
      "epoch:2; batch 38400; train accuracy: 0.721303\n",
      "epoch 2; batch 38528; loss 0.429502\n",
      "epoch:2; batch 38528; train accuracy: 0.721419\n",
      "epoch 2; batch 38656; loss 0.468083\n",
      "epoch:2; batch 38656; train accuracy: 0.721514\n",
      "epoch 2; batch 38784; loss 0.384026\n",
      "epoch:2; batch 38784; train accuracy: 0.721623\n",
      "epoch 2; batch 38912; loss 0.417880\n",
      "epoch:2; batch 38912; train accuracy: 0.721718\n",
      "epoch 2; batch 39040; loss 0.488257\n",
      "epoch:2; batch 39040; train accuracy: 0.721792\n",
      "epoch 2; batch 39168; loss 0.331189\n",
      "epoch:2; batch 39168; train accuracy: 0.721949\n",
      "epoch 2; batch 39296; loss 0.478677\n",
      "epoch:2; batch 39296; train accuracy: 0.722022\n",
      "epoch 2; batch 39424; loss 0.588429\n",
      "epoch:2; batch 39424; train accuracy: 0.722061\n",
      "epoch 2; batch 39552; loss 0.357589\n",
      "epoch:2; batch 39552; train accuracy: 0.722189\n",
      "epoch 2; batch 39680; loss 0.414578\n",
      "epoch:2; batch 39680; train accuracy: 0.722283\n",
      "epoch 2; batch 39808; loss 0.286730\n",
      "epoch:2; batch 39808; train accuracy: 0.722439\n",
      "epoch 2; batch 39936; loss 0.385276\n",
      "epoch:2; batch 39936; train accuracy: 0.722553\n",
      "epoch 2; batch 40064; loss 0.417494\n",
      "epoch:2; batch 40064; train accuracy: 0.722660\n",
      "epoch 2; batch 40192; loss 0.565864\n",
      "epoch:2; batch 40192; train accuracy: 0.722711\n",
      "epoch 2; batch 40320; loss 0.416268\n",
      "epoch:2; batch 40320; train accuracy: 0.722804\n",
      "epoch 2; batch 40448; loss 0.394490\n",
      "epoch:2; batch 40448; train accuracy: 0.722945\n",
      "epoch 2; batch 40576; loss 0.455726\n",
      "epoch:2; batch 40576; train accuracy: 0.723044\n",
      "epoch 2; batch 40704; loss 0.404335\n",
      "epoch:2; batch 40704; train accuracy: 0.723130\n",
      "epoch 2; batch 40832; loss 0.464379\n",
      "epoch:2; batch 40832; train accuracy: 0.723208\n",
      "epoch 2; batch 40960; loss 0.470107\n",
      "epoch:2; batch 40960; train accuracy: 0.723307\n",
      "epoch 2; batch 41088; loss 0.329578\n",
      "epoch:2; batch 41088; train accuracy: 0.723468\n",
      "epoch 2; batch 41216; loss 0.485016\n",
      "epoch:2; batch 41216; train accuracy: 0.723546\n",
      "epoch 2; batch 41344; loss 0.458889\n",
      "epoch:2; batch 41344; train accuracy: 0.723623\n",
      "epoch 2; batch 41472; loss 0.283469\n",
      "epoch:2; batch 41472; train accuracy: 0.723763\n",
      "epoch 2; batch 41600; loss 0.402370\n",
      "epoch:2; batch 41600; train accuracy: 0.723888\n",
      "epoch 2; batch 41728; loss 0.446887\n",
      "epoch:2; batch 41728; train accuracy: 0.723979\n",
      "epoch 2; batch 41856; loss 0.369516\n",
      "epoch:2; batch 41856; train accuracy: 0.724090\n",
      "epoch 2; batch 41984; loss 0.402678\n",
      "epoch:2; batch 41984; train accuracy: 0.724187\n",
      "epoch 2; batch 42112; loss 0.340227\n",
      "epoch:2; batch 42112; train accuracy: 0.724346\n",
      "epoch 2; batch 42240; loss 0.302842\n",
      "epoch:2; batch 42240; train accuracy: 0.724477\n",
      "epoch 2; batch 42368; loss 0.420073\n",
      "epoch:2; batch 42368; train accuracy: 0.724567\n",
      "epoch 2; batch 42496; loss 0.446725\n",
      "epoch:2; batch 42496; train accuracy: 0.724630\n",
      "epoch 2; batch 42624; loss 0.442396\n",
      "epoch:2; batch 42624; train accuracy: 0.724706\n",
      "epoch 2; batch 42752; loss 0.490910\n",
      "epoch:2; batch 42752; train accuracy: 0.724796\n",
      "epoch 2; batch 42880; loss 0.453477\n",
      "epoch:2; batch 42880; train accuracy: 0.724885\n",
      "epoch 2; batch 43008; loss 0.439284\n",
      "epoch:2; batch 43008; train accuracy: 0.725001\n",
      "epoch 2; batch 43136; loss 0.279871\n",
      "epoch:2; batch 43136; train accuracy: 0.725151\n",
      "epoch 2; batch 43264; loss 0.452967\n",
      "epoch:2; batch 43264; train accuracy: 0.725260\n",
      "epoch 2; batch 43392; loss 0.494121\n",
      "epoch:2; batch 43392; train accuracy: 0.725342\n",
      "epoch 2; batch 43520; loss 0.365019\n",
      "epoch:2; batch 43520; train accuracy: 0.725478\n",
      "epoch 2; batch 43648; loss 0.395346\n",
      "epoch:2; batch 43648; train accuracy: 0.725594\n",
      "epoch 2; batch 43776; loss 0.448133\n",
      "epoch:2; batch 43776; train accuracy: 0.725655\n",
      "epoch 2; batch 43904; loss 0.320699\n",
      "epoch:2; batch 43904; train accuracy: 0.725790\n",
      "epoch 2; batch 44032; loss 0.358447\n",
      "epoch:2; batch 44032; train accuracy: 0.725918\n",
      "epoch 2; batch 44160; loss 0.500376\n",
      "epoch:2; batch 44160; train accuracy: 0.725979\n",
      "epoch 2; batch 44288; loss 0.439901\n",
      "epoch:2; batch 44288; train accuracy: 0.726080\n",
      "epoch 2; batch 44416; loss 0.319465\n",
      "epoch:2; batch 44416; train accuracy: 0.726194\n",
      "epoch 2; batch 44544; loss 0.318052\n",
      "epoch:2; batch 44544; train accuracy: 0.726328\n",
      "epoch 2; batch 44672; loss 0.464082\n",
      "epoch:2; batch 44672; train accuracy: 0.726389\n",
      "epoch 2; batch 44800; loss 0.371758\n",
      "epoch:2; batch 44800; train accuracy: 0.726516\n",
      "epoch 2; batch 44928; loss 0.376581\n",
      "epoch:2; batch 44928; train accuracy: 0.726636\n",
      "epoch 2; batch 45056; loss 0.436127\n",
      "epoch:2; batch 45056; train accuracy: 0.726716\n",
      "epoch 2; batch 45184; loss 0.488163\n",
      "epoch:2; batch 45184; train accuracy: 0.726789\n",
      "epoch 2; batch 45312; loss 0.500971\n",
      "epoch:2; batch 45312; train accuracy: 0.726842\n",
      "epoch 2; batch 45440; loss 0.381481\n",
      "epoch:2; batch 45440; train accuracy: 0.726955\n",
      "epoch 2; batch 45568; loss 0.277947\n",
      "epoch:2; batch 45568; train accuracy: 0.727114\n",
      "epoch 2; batch 45696; loss 0.455191\n",
      "epoch:2; batch 45696; train accuracy: 0.727213\n",
      "epoch 2; batch 45824; loss 0.566132\n",
      "epoch:2; batch 45824; train accuracy: 0.727272\n",
      "epoch 2; batch 45952; loss 0.336984\n",
      "epoch:2; batch 45952; train accuracy: 0.727397\n",
      "epoch 2; batch 46080; loss 0.289859\n",
      "epoch:2; batch 46080; train accuracy: 0.727529\n",
      "epoch 2; batch 46208; loss 0.442428\n",
      "epoch:2; batch 46208; train accuracy: 0.727621\n",
      "epoch 2; batch 46336; loss 0.331786\n",
      "epoch:2; batch 46336; train accuracy: 0.727746\n",
      "epoch 2; batch 46464; loss 0.427956\n",
      "epoch:2; batch 46464; train accuracy: 0.727844\n",
      "epoch 2; batch 46592; loss 0.264887\n",
      "epoch:2; batch 46592; train accuracy: 0.727994\n",
      "epoch 2; batch 46720; loss 0.435095\n",
      "epoch:2; batch 46720; train accuracy: 0.728072\n",
      "epoch 2; batch 46848; loss 0.434361\n",
      "epoch:2; batch 46848; train accuracy: 0.728176\n",
      "epoch 2; batch 46976; loss 0.352698\n",
      "epoch:2; batch 46976; train accuracy: 0.728320\n",
      "epoch 2; batch 47104; loss 0.412147\n",
      "epoch:2; batch 47104; train accuracy: 0.728404\n",
      "epoch 2; batch 47232; loss 0.429869\n",
      "epoch:2; batch 47232; train accuracy: 0.728494\n",
      "epoch 2; batch 47360; loss 0.520172\n",
      "epoch:2; batch 47360; train accuracy: 0.728578\n",
      "epoch 2; batch 47488; loss 0.327016\n",
      "epoch:2; batch 47488; train accuracy: 0.728714\n",
      "epoch 2; batch 47616; loss 0.556450\n",
      "epoch:2; batch 47616; train accuracy: 0.728804\n",
      "epoch 2; batch 47744; loss 0.341283\n",
      "epoch:2; batch 47744; train accuracy: 0.728920\n",
      "epoch 2; batch 47872; loss 0.387349\n",
      "epoch:2; batch 47872; train accuracy: 0.729010\n",
      "epoch 2; batch 48000; loss 0.334431\n",
      "epoch:2; batch 48000; train accuracy: 0.729119\n",
      "epoch 2; batch 48128; loss 0.500510\n",
      "epoch:2; batch 48128; train accuracy: 0.729182\n",
      "epoch 2; batch 48256; loss 0.441430\n",
      "epoch:2; batch 48256; train accuracy: 0.729265\n",
      "epoch 2; batch 48384; loss 0.459693\n",
      "epoch:2; batch 48384; train accuracy: 0.729334\n",
      "epoch 2; batch 48512; loss 0.450706\n",
      "epoch:2; batch 48512; train accuracy: 0.729384\n",
      "epoch 2; batch 48640; loss 0.306092\n",
      "epoch:2; batch 48640; train accuracy: 0.729499\n",
      "epoch 2; batch 48768; loss 0.414889\n",
      "epoch:2; batch 48768; train accuracy: 0.729607\n",
      "epoch 2; batch 48896; loss 0.348825\n",
      "epoch:2; batch 48896; train accuracy: 0.729728\n",
      "epoch 2; batch 49024; loss 0.399900\n",
      "epoch:2; batch 49024; train accuracy: 0.729829\n",
      "epoch 2; batch 49152; loss 0.481214\n",
      "epoch:2; batch 49152; train accuracy: 0.729885\n",
      "epoch 2; batch 49280; loss 0.407515\n",
      "epoch:2; batch 49280; train accuracy: 0.729999\n",
      "epoch 2; batch 49408; loss 0.430357\n",
      "epoch:2; batch 49408; train accuracy: 0.730099\n",
      "epoch 2; batch 49536; loss 0.377035\n",
      "epoch:2; batch 49536; train accuracy: 0.730187\n",
      "epoch 2; batch 49664; loss 0.316276\n",
      "epoch:2; batch 49664; train accuracy: 0.730294\n",
      "epoch 2; batch 49792; loss 0.340782\n",
      "epoch:2; batch 49792; train accuracy: 0.730420\n",
      "epoch 2; batch 49920; loss 0.401640\n",
      "epoch:2; batch 49920; train accuracy: 0.730507\n",
      "epoch 2; batch 50048; loss 0.416869\n",
      "epoch:2; batch 50048; train accuracy: 0.730620\n",
      "epoch 2; batch 50176; loss 0.491715\n",
      "epoch:2; batch 50176; train accuracy: 0.730669\n",
      "epoch 2; batch 50304; loss 0.535698\n",
      "epoch:2; batch 50304; train accuracy: 0.730742\n",
      "epoch 2; batch 50432; loss 0.318808\n",
      "epoch:2; batch 50432; train accuracy: 0.730874\n",
      "epoch 2; batch 50560; loss 0.407584\n",
      "epoch:2; batch 50560; train accuracy: 0.730986\n",
      "epoch 2; batch 50688; loss 0.500682\n",
      "epoch:2; batch 50688; train accuracy: 0.731047\n",
      "epoch 2; batch 50816; loss 0.329795\n",
      "epoch:2; batch 50816; train accuracy: 0.731165\n",
      "epoch 2; batch 50944; loss 0.440185\n",
      "epoch:2; batch 50944; train accuracy: 0.731251\n",
      "epoch 2; batch 51072; loss 0.378466\n",
      "epoch:2; batch 51072; train accuracy: 0.731369\n",
      "epoch 2; batch 51200; loss 0.373004\n",
      "epoch:2; batch 51200; train accuracy: 0.731474\n",
      "epoch 2; batch 51328; loss 0.362501\n",
      "epoch:2; batch 51328; train accuracy: 0.731579\n",
      "epoch 2; batch 51456; loss 0.326598\n",
      "epoch:2; batch 51456; train accuracy: 0.731735\n",
      "epoch 2; batch 51584; loss 0.433860\n",
      "epoch:2; batch 51584; train accuracy: 0.731813\n",
      "epoch 2; batch 51712; loss 0.421543\n",
      "epoch:2; batch 51712; train accuracy: 0.731905\n",
      "epoch 2; batch 51840; loss 0.393262\n",
      "epoch:2; batch 51840; train accuracy: 0.732015\n",
      "epoch 2; batch 51968; loss 0.390242\n",
      "epoch:2; batch 51968; train accuracy: 0.732126\n",
      "epoch 2; batch 52096; loss 0.443019\n",
      "epoch:2; batch 52096; train accuracy: 0.732204\n",
      "epoch 2; batch 52224; loss 0.477404\n",
      "epoch:2; batch 52224; train accuracy: 0.732282\n",
      "epoch 2; batch 52352; loss 0.472755\n",
      "epoch:2; batch 52352; train accuracy: 0.732373\n",
      "epoch 2; batch 52480; loss 0.371604\n",
      "epoch:2; batch 52480; train accuracy: 0.732495\n",
      "epoch 2; batch 52608; loss 0.425692\n",
      "epoch:2; batch 52608; train accuracy: 0.732585\n",
      "epoch 2; batch 52736; loss 0.381589\n",
      "epoch:2; batch 52736; train accuracy: 0.732688\n",
      "epoch 2; batch 52864; loss 0.399236\n",
      "epoch:2; batch 52864; train accuracy: 0.732766\n",
      "epoch 2; batch 52992; loss 0.446968\n",
      "epoch:2; batch 52992; train accuracy: 0.732849\n",
      "epoch 2; batch 53120; loss 0.409504\n",
      "epoch:2; batch 53120; train accuracy: 0.732920\n",
      "epoch 2; batch 53248; loss 0.301281\n",
      "epoch:2; batch 53248; train accuracy: 0.733048\n",
      "epoch 2; batch 53376; loss 0.449261\n",
      "epoch:2; batch 53376; train accuracy: 0.733143\n",
      "epoch 2; batch 53504; loss 0.288438\n",
      "epoch:2; batch 53504; train accuracy: 0.733290\n",
      "epoch 2; batch 53632; loss 0.460563\n",
      "epoch:2; batch 53632; train accuracy: 0.733379\n",
      "epoch 2; batch 53760; loss 0.520609\n",
      "epoch:2; batch 53760; train accuracy: 0.733443\n",
      "epoch 2; batch 53888; loss 0.443726\n",
      "epoch:2; batch 53888; train accuracy: 0.733557\n",
      "epoch 2; batch 54016; loss 0.420640\n",
      "epoch:2; batch 54016; train accuracy: 0.733652\n",
      "epoch 2; batch 54144; loss 0.445248\n",
      "epoch:2; batch 54144; train accuracy: 0.733728\n",
      "epoch 2; batch 54272; loss 0.399699\n",
      "epoch:2; batch 54272; train accuracy: 0.733816\n",
      "epoch 2; batch 54400; loss 0.377962\n",
      "epoch:2; batch 54400; train accuracy: 0.733898\n",
      "epoch 2; batch 54528; loss 0.348522\n",
      "epoch:2; batch 54528; train accuracy: 0.733986\n",
      "epoch 2; batch 54656; loss 0.326265\n",
      "epoch:2; batch 54656; train accuracy: 0.734099\n",
      "epoch 2; batch 54784; loss 0.362317\n",
      "epoch:2; batch 54784; train accuracy: 0.734206\n",
      "epoch 2; batch 54912; loss 0.421153\n",
      "epoch:2; batch 54912; train accuracy: 0.734294\n",
      "epoch 2; batch 55040; loss 0.282759\n",
      "epoch:2; batch 55040; train accuracy: 0.734419\n",
      "epoch 2; batch 55168; loss 0.287491\n",
      "epoch:2; batch 55168; train accuracy: 0.734537\n",
      "epoch 2; batch 55296; loss 0.314019\n",
      "epoch:2; batch 55296; train accuracy: 0.734668\n",
      "epoch 2; batch 55424; loss 0.314172\n",
      "epoch:2; batch 55424; train accuracy: 0.734799\n",
      "epoch 2; batch 55552; loss 0.262124\n",
      "epoch:2; batch 55552; train accuracy: 0.734936\n",
      "epoch 2; batch 55680; loss 0.334646\n",
      "epoch:2; batch 55680; train accuracy: 0.735072\n",
      "epoch 2; batch 55808; loss 0.370690\n",
      "epoch:2; batch 55808; train accuracy: 0.735159\n",
      "epoch 2; batch 55936; loss 0.350310\n",
      "epoch:2; batch 55936; train accuracy: 0.735276\n",
      "epoch 2; batch 56064; loss 0.365917\n",
      "epoch:2; batch 56064; train accuracy: 0.735362\n",
      "epoch 2; batch 56192; loss 0.259203\n",
      "epoch:2; batch 56192; train accuracy: 0.735486\n",
      "epoch 2; batch 56320; loss 0.495252\n",
      "epoch:2; batch 56320; train accuracy: 0.735553\n",
      "epoch 2; batch 56448; loss 0.505626\n",
      "epoch:2; batch 56448; train accuracy: 0.735626\n",
      "epoch 2; batch 56576; loss 0.334773\n",
      "epoch:2; batch 56576; train accuracy: 0.735743\n",
      "epoch 2; batch 56704; loss 0.558692\n",
      "epoch:2; batch 56704; train accuracy: 0.735792\n",
      "epoch 2; batch 56832; loss 0.380256\n",
      "epoch:2; batch 56832; train accuracy: 0.735877\n",
      "epoch 2; batch 56960; loss 0.253589\n",
      "epoch:2; batch 56960; train accuracy: 0.736012\n",
      "epoch 2; batch 57088; loss 0.555285\n",
      "epoch:2; batch 57088; train accuracy: 0.736041\n",
      "epoch 2; batch 57216; loss 0.490773\n",
      "epoch:2; batch 57216; train accuracy: 0.736132\n",
      "epoch 2; batch 57344; loss 0.442261\n",
      "epoch:2; batch 57344; train accuracy: 0.736205\n",
      "epoch 2; batch 57472; loss 0.372273\n",
      "epoch:2; batch 57472; train accuracy: 0.736314\n",
      "epoch 2; batch 57600; loss 0.393568\n",
      "epoch:2; batch 57600; train accuracy: 0.736399\n",
      "epoch 2; batch 57728; loss 0.364244\n",
      "epoch:2; batch 57728; train accuracy: 0.736533\n",
      "epoch 2; batch 57856; loss 0.379215\n",
      "epoch:2; batch 57856; train accuracy: 0.736623\n",
      "epoch 2; batch 57984; loss 0.381291\n",
      "epoch:2; batch 57984; train accuracy: 0.736713\n",
      "epoch 2; batch 58112; loss 0.609695\n",
      "epoch:2; batch 58112; train accuracy: 0.736779\n",
      "epoch 2; batch 58240; loss 0.375522\n",
      "epoch:2; batch 58240; train accuracy: 0.736851\n",
      "epoch 2; batch 58368; loss 0.462168\n",
      "epoch:2; batch 58368; train accuracy: 0.736922\n",
      "epoch 2; batch 58496; loss 0.422205\n",
      "epoch:2; batch 58496; train accuracy: 0.737042\n",
      "epoch 2; batch 58624; loss 0.396738\n",
      "epoch:2; batch 58624; train accuracy: 0.737120\n",
      "epoch 2; batch 58752; loss 0.413208\n",
      "epoch:2; batch 58752; train accuracy: 0.737179\n",
      "epoch 2; batch 58880; loss 0.315299\n",
      "epoch:2; batch 58880; train accuracy: 0.737292\n",
      "epoch 2; batch 59008; loss 0.358697\n",
      "epoch:2; batch 59008; train accuracy: 0.737382\n",
      "epoch 2; batch 59136; loss 0.423358\n",
      "epoch:2; batch 59136; train accuracy: 0.737434\n",
      "epoch 2; batch 59264; loss 0.367041\n",
      "epoch:2; batch 59264; train accuracy: 0.737529\n",
      "epoch 2; batch 59392; loss 0.432365\n",
      "epoch:2; batch 59392; train accuracy: 0.737606\n",
      "epoch 2; batch 59520; loss 0.326706\n",
      "epoch:2; batch 59520; train accuracy: 0.737725\n",
      "epoch 2; batch 59648; loss 0.373071\n",
      "epoch:2; batch 59648; train accuracy: 0.737826\n",
      "epoch 2; batch 59776; loss 0.403590\n",
      "epoch:2; batch 59776; train accuracy: 0.737902\n",
      "epoch 2; batch 59904; loss 0.479633\n",
      "epoch:2; batch 59904; train accuracy: 0.737984\n",
      "epoch 2; batch 60032; loss 0.426567\n",
      "epoch:2; batch 60032; train accuracy: 0.738084\n",
      "epoch 2; batch 60160; loss 0.526845\n",
      "epoch:2; batch 60160; train accuracy: 0.738106\n",
      "epoch 2; batch 60288; loss 0.486279\n",
      "epoch:2; batch 60288; train accuracy: 0.738157\n",
      "epoch 2; batch 60416; loss 0.390361\n",
      "epoch:2; batch 60416; train accuracy: 0.738269\n",
      "epoch 2; batch 60544; loss 0.265498\n",
      "epoch:2; batch 60544; train accuracy: 0.738387\n",
      "epoch 2; batch 60672; loss 0.395904\n",
      "epoch:2; batch 60672; train accuracy: 0.738468\n",
      "epoch 2; batch 60800; loss 0.418116\n",
      "epoch:2; batch 60800; train accuracy: 0.738556\n",
      "epoch 2; batch 60928; loss 0.419041\n",
      "epoch:2; batch 60928; train accuracy: 0.738631\n",
      "epoch 2; batch 61056; loss 0.286368\n",
      "epoch:2; batch 61056; train accuracy: 0.738748\n",
      "epoch 2; batch 61184; loss 0.395152\n",
      "epoch:2; batch 61184; train accuracy: 0.738847\n",
      "epoch 2; batch 61312; loss 0.334207\n",
      "epoch:2; batch 61312; train accuracy: 0.738940\n",
      "epoch 2; batch 61440; loss 0.454833\n",
      "epoch:2; batch 61440; train accuracy: 0.738990\n",
      "epoch 2; batch 61568; loss 0.562242\n",
      "epoch:2; batch 61568; train accuracy: 0.739023\n",
      "epoch 2; batch 61696; loss 0.413284\n",
      "epoch:2; batch 61696; train accuracy: 0.739091\n",
      "epoch 2; batch 61824; loss 0.428742\n",
      "epoch:2; batch 61824; train accuracy: 0.739136\n",
      "epoch 2; batch 61952; loss 0.342807\n",
      "epoch:2; batch 61952; train accuracy: 0.739234\n",
      "epoch 2; batch 62080; loss 0.479464\n",
      "epoch:2; batch 62080; train accuracy: 0.739290\n",
      "epoch 2; batch 62208; loss 0.391021\n",
      "epoch:2; batch 62208; train accuracy: 0.739388\n",
      "epoch 2; batch 62336; loss 0.423776\n",
      "epoch:2; batch 62336; train accuracy: 0.739480\n",
      "epoch 2; batch 62464; loss 0.287642\n",
      "epoch:2; batch 62464; train accuracy: 0.739577\n",
      "epoch 2; batch 62592; loss 0.306677\n",
      "epoch:2; batch 62592; train accuracy: 0.739681\n",
      "epoch 2; batch 62720; loss 0.276012\n",
      "epoch:2; batch 62720; train accuracy: 0.739814\n",
      "epoch 2; batch 62848; loss 0.338867\n",
      "epoch:2; batch 62848; train accuracy: 0.739917\n",
      "epoch 2; batch 62976; loss 0.306299\n",
      "epoch:2; batch 62976; train accuracy: 0.740026\n",
      "epoch 2; batch 63104; loss 0.323850\n",
      "epoch:2; batch 63104; train accuracy: 0.740129\n",
      "epoch 2; batch 63232; loss 0.247135\n",
      "epoch:2; batch 63232; train accuracy: 0.740267\n",
      "epoch 2; batch 63360; loss 0.349812\n",
      "epoch:2; batch 63360; train accuracy: 0.740375\n",
      "epoch 2; batch 63488; loss 0.461568\n",
      "epoch:2; batch 63488; train accuracy: 0.740454\n",
      "epoch 2; batch 63616; loss 0.351617\n",
      "epoch:2; batch 63616; train accuracy: 0.740527\n",
      "epoch 2; batch 63744; loss 0.416873\n",
      "epoch:2; batch 63744; train accuracy: 0.740617\n",
      "epoch 2; batch 63872; loss 0.389910\n",
      "epoch:2; batch 63872; train accuracy: 0.740725\n",
      "epoch 2; batch 64000; loss 0.478589\n",
      "epoch:2; batch 64000; train accuracy: 0.740797\n",
      "epoch 2; batch 64128; loss 0.394545\n",
      "epoch:2; batch 64128; train accuracy: 0.740886\n",
      "epoch 2; batch 64256; loss 0.495795\n",
      "epoch:2; batch 64256; train accuracy: 0.740941\n",
      "epoch 2; batch 64384; loss 0.364732\n",
      "epoch:2; batch 64384; train accuracy: 0.741042\n",
      "epoch 2; batch 64512; loss 0.448793\n",
      "epoch:2; batch 64512; train accuracy: 0.741096\n",
      "epoch 2; batch 64640; loss 0.454676\n",
      "epoch:2; batch 64640; train accuracy: 0.741162\n",
      "epoch 2; batch 64768; loss 0.396648\n",
      "epoch:2; batch 64768; train accuracy: 0.741251\n",
      "epoch 2; batch 64896; loss 0.371353\n",
      "epoch:2; batch 64896; train accuracy: 0.741346\n",
      "epoch 2; batch 65024; loss 0.352174\n",
      "epoch:2; batch 65024; train accuracy: 0.741417\n",
      "epoch 2; batch 65152; loss 0.684362\n",
      "epoch:2; batch 65152; train accuracy: 0.741429\n",
      "epoch 2; batch 65280; loss 0.340355\n",
      "epoch:2; batch 65280; train accuracy: 0.741506\n",
      "epoch 2; batch 65408; loss 0.409350\n",
      "epoch:2; batch 65408; train accuracy: 0.741595\n",
      "epoch 2; batch 65536; loss 0.483317\n",
      "epoch:2; batch 65536; train accuracy: 0.741660\n",
      "epoch 2; batch 65664; loss 0.459296\n",
      "epoch:2; batch 65664; train accuracy: 0.741742\n",
      "epoch 2; batch 65792; loss 0.361036\n",
      "epoch:2; batch 65792; train accuracy: 0.741824\n",
      "epoch 2; batch 65920; loss 0.374158\n",
      "epoch:2; batch 65920; train accuracy: 0.741936\n",
      "epoch 2; batch 66048; loss 0.432486\n",
      "epoch:2; batch 66048; train accuracy: 0.742018\n",
      "epoch 2; batch 66176; loss 0.390678\n",
      "epoch:2; batch 66176; train accuracy: 0.742100\n",
      "epoch 2; batch 66304; loss 0.299955\n",
      "epoch:2; batch 66304; train accuracy: 0.742211\n",
      "epoch 2; batch 66432; loss 0.295217\n",
      "epoch:2; batch 66432; train accuracy: 0.742328\n",
      "epoch 2; batch 66560; loss 0.391752\n",
      "epoch:2; batch 66560; train accuracy: 0.742415\n",
      "epoch 2; batch 66688; loss 0.377701\n",
      "epoch:2; batch 66688; train accuracy: 0.742485\n",
      "epoch 2; batch 66816; loss 0.450187\n",
      "epoch:2; batch 66816; train accuracy: 0.742548\n",
      "epoch 2; batch 66944; loss 0.410115\n",
      "epoch:2; batch 66944; train accuracy: 0.742641\n",
      "epoch 2; batch 67072; loss 0.316547\n",
      "epoch:2; batch 67072; train accuracy: 0.742757\n",
      "epoch 2; batch 67200; loss 0.374513\n",
      "epoch:2; batch 67200; train accuracy: 0.742855\n",
      "epoch 2; batch 67328; loss 0.354935\n",
      "epoch:2; batch 67328; train accuracy: 0.742965\n",
      "epoch 2; batch 67456; loss 0.367277\n",
      "epoch:2; batch 67456; train accuracy: 0.743069\n",
      "epoch 2; batch 67584; loss 0.433091\n",
      "epoch:2; batch 67584; train accuracy: 0.743121\n",
      "epoch 2; batch 67712; loss 0.324865\n",
      "epoch:2; batch 67712; train accuracy: 0.743218\n",
      "epoch 2; batch 67840; loss 0.350702\n",
      "epoch:2; batch 67840; train accuracy: 0.743328\n",
      "epoch 2; batch 67968; loss 0.376830\n",
      "epoch:2; batch 67968; train accuracy: 0.743385\n",
      "epoch 2; batch 68096; loss 0.485139\n",
      "epoch:2; batch 68096; train accuracy: 0.743447\n",
      "epoch 2; batch 68224; loss 0.380837\n",
      "epoch:2; batch 68224; train accuracy: 0.743533\n",
      "epoch 2; batch 68352; loss 0.280495\n",
      "epoch:2; batch 68352; train accuracy: 0.743647\n",
      "epoch 2; batch 68480; loss 0.397970\n",
      "epoch:2; batch 68480; train accuracy: 0.743698\n",
      "epoch 2; batch 68608; loss 0.319912\n",
      "epoch:2; batch 68608; train accuracy: 0.743812\n",
      "epoch 2; batch 68736; loss 0.362249\n",
      "epoch:2; batch 68736; train accuracy: 0.743892\n",
      "epoch 2; batch 68864; loss 0.404952\n",
      "epoch:2; batch 68864; train accuracy: 0.743988\n",
      "epoch 2; batch 68992; loss 0.478704\n",
      "epoch:2; batch 68992; train accuracy: 0.744016\n",
      "epoch 2; batch 69120; loss 0.443366\n",
      "epoch:2; batch 69120; train accuracy: 0.744083\n",
      "epoch 2; batch 69248; loss 0.265193\n",
      "epoch:2; batch 69248; train accuracy: 0.744202\n",
      "epoch 2; batch 69376; loss 0.323359\n",
      "epoch:2; batch 69376; train accuracy: 0.744298\n",
      "epoch 2; batch 69504; loss 0.317293\n",
      "epoch:2; batch 69504; train accuracy: 0.744377\n",
      "epoch 2; batch 69632; loss 0.416798\n",
      "epoch:2; batch 69632; train accuracy: 0.744473\n",
      "epoch 2; batch 69760; loss 0.439895\n",
      "epoch:2; batch 69760; train accuracy: 0.744540\n",
      "epoch 2; batch 69888; loss 0.421318\n",
      "epoch:2; batch 69888; train accuracy: 0.744590\n",
      "epoch 2; batch 70016; loss 0.254079\n",
      "epoch:2; batch 70016; train accuracy: 0.744702\n",
      "epoch 2; batch 70144; loss 0.432744\n",
      "epoch:2; batch 70144; train accuracy: 0.744780\n",
      "epoch 2; batch 70272; loss 0.294746\n",
      "epoch:2; batch 70272; train accuracy: 0.744887\n",
      "epoch 2; batch 70400; loss 0.313766\n",
      "epoch:2; batch 70400; train accuracy: 0.744987\n",
      "epoch 2; batch 70528; loss 0.507204\n",
      "epoch:2; batch 70528; train accuracy: 0.745037\n",
      "epoch 2; batch 70656; loss 0.401194\n",
      "epoch:2; batch 70656; train accuracy: 0.745109\n",
      "epoch 2; batch 70784; loss 0.355528\n",
      "epoch:2; batch 70784; train accuracy: 0.745203\n",
      "epoch 2; batch 70912; loss 0.491755\n",
      "epoch:2; batch 70912; train accuracy: 0.745241\n",
      "epoch 2; batch 71040; loss 0.403471\n",
      "epoch:2; batch 71040; train accuracy: 0.745324\n",
      "epoch 2; batch 71168; loss 0.345329\n",
      "epoch:2; batch 71168; train accuracy: 0.745418\n",
      "epoch 2; batch 71296; loss 0.258663\n",
      "epoch:2; batch 71296; train accuracy: 0.745541\n",
      "epoch 2; batch 71424; loss 0.578743\n",
      "epoch:2; batch 71424; train accuracy: 0.745572\n",
      "epoch 2; batch 71552; loss 0.498001\n",
      "epoch:2; batch 71552; train accuracy: 0.745638\n",
      "epoch 2; batch 71680; loss 0.323515\n",
      "epoch:2; batch 71680; train accuracy: 0.745709\n",
      "epoch 2; batch 71808; loss 0.422535\n",
      "epoch:2; batch 71808; train accuracy: 0.745780\n",
      "epoch 2; batch 71936; loss 0.308241\n",
      "epoch:2; batch 71936; train accuracy: 0.745873\n",
      "epoch 2; batch 72064; loss 0.421796\n",
      "epoch:2; batch 72064; train accuracy: 0.745950\n",
      "epoch 2; batch 72192; loss 0.351556\n",
      "epoch:2; batch 72192; train accuracy: 0.746060\n",
      "epoch 2; batch 72320; loss 0.380145\n",
      "epoch:2; batch 72320; train accuracy: 0.746136\n",
      "epoch 2; batch 72448; loss 0.327537\n",
      "epoch:2; batch 72448; train accuracy: 0.746235\n",
      "epoch 2; batch 72576; loss 0.487580\n",
      "epoch:2; batch 72576; train accuracy: 0.746282\n",
      "epoch 2; batch 72704; loss 0.341770\n",
      "epoch:2; batch 72704; train accuracy: 0.746375\n",
      "epoch 2; batch 72832; loss 0.259713\n",
      "epoch:2; batch 72832; train accuracy: 0.746496\n",
      "epoch 2; batch 72960; loss 0.375123\n",
      "epoch:2; batch 72960; train accuracy: 0.746611\n",
      "epoch 2; batch 73088; loss 0.344201\n",
      "epoch:2; batch 73088; train accuracy: 0.746698\n",
      "epoch 2; batch 73216; loss 0.464453\n",
      "epoch:2; batch 73216; train accuracy: 0.746756\n",
      "epoch 2; batch 73344; loss 0.447431\n",
      "epoch:2; batch 73344; train accuracy: 0.746803\n",
      "epoch 2; batch 73472; loss 0.406925\n",
      "epoch:2; batch 73472; train accuracy: 0.746890\n",
      "epoch 2; batch 73600; loss 0.376674\n",
      "epoch:2; batch 73600; train accuracy: 0.746965\n",
      "epoch 2; batch 73728; loss 0.472745\n",
      "epoch:2; batch 73728; train accuracy: 0.747023\n",
      "epoch 2; batch 73856; loss 0.491598\n",
      "epoch:2; batch 73856; train accuracy: 0.747109\n",
      "epoch 2; batch 73984; loss 0.370242\n",
      "epoch:2; batch 73984; train accuracy: 0.747200\n",
      "epoch 2; batch 74112; loss 0.403230\n",
      "epoch:2; batch 74112; train accuracy: 0.747269\n",
      "epoch 2; batch 74240; loss 0.306135\n",
      "epoch:2; batch 74240; train accuracy: 0.747355\n",
      "epoch 2; batch 74368; loss 0.299563\n",
      "epoch:2; batch 74368; train accuracy: 0.747479\n",
      "epoch 2; batch 74496; loss 0.356364\n",
      "epoch:2; batch 74496; train accuracy: 0.747576\n",
      "epoch 2; batch 74624; loss 0.356210\n",
      "epoch:2; batch 74624; train accuracy: 0.747667\n",
      "epoch 2; batch 74752; loss 0.436178\n",
      "epoch:2; batch 74752; train accuracy: 0.747735\n",
      "epoch 2; batch 74880; loss 0.477239\n",
      "epoch:2; batch 74880; train accuracy: 0.747781\n",
      "epoch 2; batch 75008; loss 0.372378\n",
      "epoch:2; batch 75008; train accuracy: 0.747861\n",
      "epoch 2; batch 75136; loss 0.373432\n",
      "epoch:2; batch 75136; train accuracy: 0.747940\n",
      "epoch 2; batch 75264; loss 0.433790\n",
      "epoch:2; batch 75264; train accuracy: 0.748002\n",
      "epoch 2; batch 75392; loss 0.480894\n",
      "epoch:2; batch 75392; train accuracy: 0.748048\n",
      "epoch 2; batch 75520; loss 0.415629\n",
      "epoch:2; batch 75520; train accuracy: 0.748116\n",
      "epoch 2; batch 75648; loss 0.266300\n",
      "epoch:2; batch 75648; train accuracy: 0.748223\n",
      "epoch 2; batch 75776; loss 0.458111\n",
      "epoch:2; batch 75776; train accuracy: 0.748285\n",
      "epoch 2; batch 75904; loss 0.439529\n",
      "epoch:2; batch 75904; train accuracy: 0.748352\n",
      "epoch 2; batch 76032; loss 0.345696\n",
      "epoch:2; batch 76032; train accuracy: 0.748436\n",
      "epoch 2; batch 76160; loss 0.415407\n",
      "epoch:2; batch 76160; train accuracy: 0.748515\n",
      "epoch 2; batch 76288; loss 0.406777\n",
      "epoch:2; batch 76288; train accuracy: 0.748560\n",
      "epoch 2; batch 76416; loss 0.382251\n",
      "epoch:2; batch 76416; train accuracy: 0.748627\n",
      "epoch 2; batch 76544; loss 0.299281\n",
      "epoch:2; batch 76544; train accuracy: 0.748727\n",
      "epoch 2; batch 76672; loss 0.335384\n",
      "epoch:2; batch 76672; train accuracy: 0.748783\n",
      "epoch 2; batch 76800; loss 0.370191\n",
      "epoch:2; batch 76800; train accuracy: 0.748856\n",
      "epoch 2; batch 76928; loss 0.322441\n",
      "epoch:2; batch 76928; train accuracy: 0.748955\n",
      "epoch 2; batch 77056; loss 0.313186\n",
      "epoch:2; batch 77056; train accuracy: 0.749061\n",
      "epoch 2; batch 77184; loss 0.353608\n",
      "epoch:2; batch 77184; train accuracy: 0.749122\n",
      "epoch 2; batch 77312; loss 0.267157\n",
      "epoch:2; batch 77312; train accuracy: 0.749221\n",
      "epoch 2; batch 77440; loss 0.417582\n",
      "epoch:2; batch 77440; train accuracy: 0.749315\n",
      "epoch 2; batch 77568; loss 0.437268\n",
      "epoch:2; batch 77568; train accuracy: 0.749386\n",
      "epoch 2; batch 77696; loss 0.183179\n",
      "epoch:2; batch 77696; train accuracy: 0.749518\n",
      "epoch 2; batch 77824; loss 0.375193\n",
      "epoch:2; batch 77824; train accuracy: 0.749595\n",
      "epoch 2; batch 77952; loss 0.438166\n",
      "epoch:2; batch 77952; train accuracy: 0.749677\n",
      "epoch 2; batch 78080; loss 0.310172\n",
      "epoch:2; batch 78080; train accuracy: 0.749781\n",
      "epoch 2; batch 78208; loss 0.425821\n",
      "epoch:2; batch 78208; train accuracy: 0.749831\n",
      "epoch 2; batch 78336; loss 0.312100\n",
      "epoch:2; batch 78336; train accuracy: 0.749924\n",
      "epoch 2; batch 78464; loss 0.228349\n",
      "epoch:2; batch 78464; train accuracy: 0.750038\n",
      "epoch 2; batch 78592; loss 0.434620\n",
      "epoch:2; batch 78592; train accuracy: 0.750120\n",
      "epoch 2; batch 78720; loss 0.382695\n",
      "epoch:2; batch 78720; train accuracy: 0.750191\n",
      "epoch 2; batch 78848; loss 0.271262\n",
      "epoch:2; batch 78848; train accuracy: 0.750288\n",
      "epoch 2; batch 78976; loss 0.528106\n",
      "epoch:2; batch 78976; train accuracy: 0.750337\n",
      "epoch 2; batch 79104; loss 0.461336\n",
      "epoch:2; batch 79104; train accuracy: 0.750407\n",
      "epoch 2; batch 79232; loss 0.389779\n",
      "epoch:2; batch 79232; train accuracy: 0.750478\n",
      "epoch 2; batch 79360; loss 0.339702\n",
      "epoch:2; batch 79360; train accuracy: 0.750559\n",
      "epoch 2; batch 79488; loss 0.417879\n",
      "epoch:2; batch 79488; train accuracy: 0.750629\n",
      "epoch 2; batch 79616; loss 0.313721\n",
      "epoch:2; batch 79616; train accuracy: 0.750726\n",
      "epoch 2; batch 79744; loss 0.312785\n",
      "epoch:2; batch 79744; train accuracy: 0.750818\n",
      "epoch 2; batch 79872; loss 0.336832\n",
      "epoch:2; batch 79872; train accuracy: 0.750876\n",
      "epoch 2; batch 80000; loss 0.392792\n",
      "epoch:2; batch 80000; train accuracy: 0.750935\n",
      "epoch 2; batch 80128; loss 0.356095\n",
      "epoch:2; batch 80128; train accuracy: 0.751021\n",
      "epoch 2; batch 80256; loss 0.452935\n",
      "epoch:2; batch 80256; train accuracy: 0.751085\n",
      "epoch 2; batch 80384; loss 0.405575\n",
      "epoch:2; batch 80384; train accuracy: 0.751138\n",
      "epoch 2; batch 80512; loss 0.317525\n",
      "epoch:2; batch 80512; train accuracy: 0.751240\n",
      "epoch 2; batch 80640; loss 0.406520\n",
      "epoch:2; batch 80640; train accuracy: 0.751320\n",
      "epoch 2; batch 80768; loss 0.434822\n",
      "epoch:2; batch 80768; train accuracy: 0.751384\n",
      "epoch 2; batch 80896; loss 0.582303\n",
      "epoch:2; batch 80896; train accuracy: 0.751415\n",
      "epoch 2; batch 81024; loss 0.289901\n",
      "epoch:2; batch 81024; train accuracy: 0.751511\n",
      "epoch 2; batch 81152; loss 0.412542\n",
      "epoch:2; batch 81152; train accuracy: 0.751590\n",
      "epoch 2; batch 81280; loss 0.420067\n",
      "epoch:2; batch 81280; train accuracy: 0.751654\n",
      "epoch 2; batch 81408; loss 0.403969\n",
      "epoch:2; batch 81408; train accuracy: 0.751755\n",
      "epoch 2; batch 81536; loss 0.361359\n",
      "epoch:2; batch 81536; train accuracy: 0.751818\n",
      "epoch 2; batch 81664; loss 0.291200\n",
      "epoch:2; batch 81664; train accuracy: 0.751929\n",
      "epoch 2; batch 81792; loss 0.400605\n",
      "epoch:2; batch 81792; train accuracy: 0.751987\n",
      "epoch 2; batch 81920; loss 0.288444\n",
      "epoch:2; batch 81920; train accuracy: 0.752071\n",
      "epoch 2; batch 82048; loss 0.352361\n",
      "epoch:2; batch 82048; train accuracy: 0.752134\n",
      "epoch 2; batch 82176; loss 0.358770\n",
      "epoch:2; batch 82176; train accuracy: 0.752223\n",
      "epoch 2; batch 82304; loss 0.464748\n",
      "epoch:2; batch 82304; train accuracy: 0.752307\n",
      "epoch 2; batch 82432; loss 0.445306\n",
      "epoch:2; batch 82432; train accuracy: 0.752380\n",
      "epoch 2; batch 82560; loss 0.496447\n",
      "epoch:2; batch 82560; train accuracy: 0.752405\n",
      "epoch 2; batch 82688; loss 0.367160\n",
      "epoch:2; batch 82688; train accuracy: 0.752462\n",
      "epoch 2; batch 82816; loss 0.451853\n",
      "epoch:2; batch 82816; train accuracy: 0.752508\n",
      "epoch 2; batch 82944; loss 0.549870\n",
      "epoch:2; batch 82944; train accuracy: 0.752554\n",
      "epoch 2; batch 83072; loss 0.242465\n",
      "epoch:2; batch 83072; train accuracy: 0.752654\n",
      "epoch 2; batch 83200; loss 0.422062\n",
      "epoch:2; batch 83200; train accuracy: 0.752710\n",
      "epoch 2; batch 83328; loss 0.353994\n",
      "epoch:2; batch 83328; train accuracy: 0.752810\n",
      "epoch 2; batch 83456; loss 0.499886\n",
      "epoch:2; batch 83456; train accuracy: 0.752866\n",
      "epoch 2; batch 83584; loss 0.384179\n",
      "epoch:2; batch 83584; train accuracy: 0.752954\n",
      "epoch 2; batch 83712; loss 0.341565\n",
      "epoch:2; batch 83712; train accuracy: 0.753026\n",
      "epoch 2; batch 83840; loss 0.312090\n",
      "epoch:2; batch 83840; train accuracy: 0.753130\n",
      "epoch 2; batch 83968; loss 0.538908\n",
      "epoch:2; batch 83968; train accuracy: 0.753186\n",
      "epoch 2; batch 84096; loss 0.320110\n",
      "epoch:2; batch 84096; train accuracy: 0.753285\n",
      "epoch 2; batch 84224; loss 0.396557\n",
      "epoch:2; batch 84224; train accuracy: 0.753357\n",
      "epoch 2; batch 84352; loss 0.392469\n",
      "epoch:2; batch 84352; train accuracy: 0.753439\n",
      "epoch 2; batch 84480; loss 0.365441\n",
      "epoch:2; batch 84480; train accuracy: 0.753516\n",
      "epoch 2; batch 84608; loss 0.278951\n",
      "epoch:2; batch 84608; train accuracy: 0.753603\n",
      "epoch 2; batch 84736; loss 0.416770\n",
      "epoch:2; batch 84736; train accuracy: 0.753658\n",
      "epoch 2; batch 84864; loss 0.354886\n",
      "epoch:2; batch 84864; train accuracy: 0.753730\n",
      "epoch 2; batch 84992; loss 0.453870\n",
      "epoch:2; batch 84992; train accuracy: 0.753796\n",
      "epoch 2; batch 85120; loss 0.438957\n",
      "epoch:2; batch 85120; train accuracy: 0.753856\n",
      "epoch 2; batch 85248; loss 0.287369\n",
      "epoch:2; batch 85248; train accuracy: 0.753959\n",
      "epoch 2; batch 85376; loss 0.402771\n",
      "epoch:2; batch 85376; train accuracy: 0.754024\n",
      "epoch 2; batch 85504; loss 0.314849\n",
      "epoch:2; batch 85504; train accuracy: 0.754116\n",
      "epoch 2; batch 85632; loss 0.283199\n",
      "epoch:2; batch 85632; train accuracy: 0.754208\n",
      "epoch 2; batch 85760; loss 0.483288\n",
      "epoch:2; batch 85760; train accuracy: 0.754263\n",
      "epoch 2; batch 85888; loss 0.415214\n",
      "epoch:2; batch 85888; train accuracy: 0.754323\n",
      "epoch 2; batch 86016; loss 0.464928\n",
      "epoch:2; batch 86016; train accuracy: 0.754383\n",
      "epoch 2; batch 86144; loss 0.349693\n",
      "epoch:2; batch 86144; train accuracy: 0.754464\n",
      "epoch 2; batch 86272; loss 0.361758\n",
      "epoch:2; batch 86272; train accuracy: 0.754529\n",
      "epoch 2; batch 86400; loss 0.389975\n",
      "epoch:2; batch 86400; train accuracy: 0.754625\n",
      "epoch 2; batch 86528; loss 0.448181\n",
      "epoch:2; batch 86528; train accuracy: 0.754669\n",
      "epoch 2; batch 86656; loss 0.450540\n",
      "epoch:2; batch 86656; train accuracy: 0.754728\n",
      "epoch 2; batch 86784; loss 0.471537\n",
      "epoch:2; batch 86784; train accuracy: 0.754756\n",
      "epoch 2; batch 86912; loss 0.385658\n",
      "epoch:2; batch 86912; train accuracy: 0.754821\n",
      "epoch 2; batch 87040; loss 0.399356\n",
      "epoch:2; batch 87040; train accuracy: 0.754885\n",
      "epoch 2; batch 87168; loss 0.451946\n",
      "epoch:2; batch 87168; train accuracy: 0.754945\n",
      "epoch 2; batch 87296; loss 0.326356\n",
      "epoch:2; batch 87296; train accuracy: 0.755030\n",
      "epoch 2; batch 87424; loss 0.322618\n",
      "epoch:2; batch 87424; train accuracy: 0.755115\n",
      "epoch 2; batch 87552; loss 0.358095\n",
      "epoch:2; batch 87552; train accuracy: 0.755200\n",
      "epoch 2; batch 87680; loss 0.359656\n",
      "epoch:2; batch 87680; train accuracy: 0.755274\n",
      "epoch 2; batch 87808; loss 0.467697\n",
      "epoch:2; batch 87808; train accuracy: 0.755328\n",
      "epoch 2; batch 87936; loss 0.389007\n",
      "epoch:2; batch 87936; train accuracy: 0.755412\n",
      "epoch 2; batch 88064; loss 0.297676\n",
      "epoch:2; batch 88064; train accuracy: 0.755486\n",
      "epoch 2; batch 88192; loss 0.414125\n",
      "epoch:2; batch 88192; train accuracy: 0.755545\n",
      "epoch 2; batch 88320; loss 0.446869\n",
      "epoch:2; batch 88320; train accuracy: 0.755614\n",
      "epoch 2; batch 88448; loss 0.442150\n",
      "epoch:2; batch 88448; train accuracy: 0.755656\n",
      "epoch 2; batch 88576; loss 0.353236\n",
      "epoch:2; batch 88576; train accuracy: 0.755741\n",
      "epoch 2; batch 88704; loss 0.374780\n",
      "epoch:2; batch 88704; train accuracy: 0.755804\n",
      "epoch 2; batch 88832; loss 0.437750\n",
      "epoch:2; batch 88832; train accuracy: 0.755857\n",
      "epoch 2; batch 88960; loss 0.458028\n",
      "epoch:2; batch 88960; train accuracy: 0.755904\n",
      "epoch 2; batch 89088; loss 0.315047\n",
      "epoch:2; batch 89088; train accuracy: 0.755983\n",
      "epoch 2; batch 89216; loss 0.452272\n",
      "epoch:2; batch 89216; train accuracy: 0.756025\n",
      "epoch 2; batch 89344; loss 0.312979\n",
      "epoch:2; batch 89344; train accuracy: 0.756114\n",
      "epoch 2; batch 89472; loss 0.403128\n",
      "epoch:2; batch 89472; train accuracy: 0.756177\n",
      "epoch 2; batch 89600; loss 0.337003\n",
      "epoch:2; batch 89600; train accuracy: 0.756255\n",
      "epoch 2; batch 89728; loss 0.405356\n",
      "epoch:2; batch 89728; train accuracy: 0.756313\n",
      "epoch 2; batch 89856; loss 0.303881\n",
      "epoch:2; batch 89856; train accuracy: 0.756391\n",
      "epoch 2; batch 89984; loss 0.444070\n",
      "epoch:2; batch 89984; train accuracy: 0.756443\n",
      "epoch 2; batch 90112; loss 0.393811\n",
      "epoch:2; batch 90112; train accuracy: 0.756521\n",
      "epoch 2; batch 90240; loss 0.324866\n",
      "epoch:2; batch 90240; train accuracy: 0.756593\n",
      "epoch 2; batch 90368; loss 0.313284\n",
      "epoch:2; batch 90368; train accuracy: 0.756666\n",
      "epoch 2; batch 90496; loss 0.278274\n",
      "epoch:2; batch 90496; train accuracy: 0.756759\n",
      "epoch 2; batch 90624; loss 0.445317\n",
      "epoch:2; batch 90624; train accuracy: 0.756805\n",
      "epoch 2; batch 90752; loss 0.370147\n",
      "epoch:2; batch 90752; train accuracy: 0.756883\n",
      "epoch 2; batch 90880; loss 0.257269\n",
      "epoch:2; batch 90880; train accuracy: 0.756985\n",
      "epoch 2; batch 91008; loss 0.295985\n",
      "epoch:2; batch 91008; train accuracy: 0.757078\n",
      "epoch 2; batch 91136; loss 0.403508\n",
      "epoch:2; batch 91136; train accuracy: 0.757139\n",
      "epoch 2; batch 91264; loss 0.269821\n",
      "epoch:2; batch 91264; train accuracy: 0.757242\n",
      "epoch 2; batch 91392; loss 0.331778\n",
      "epoch:2; batch 91392; train accuracy: 0.757313\n",
      "epoch 2; batch 91520; loss 0.369205\n",
      "epoch:2; batch 91520; train accuracy: 0.757410\n",
      "epoch 2; batch 91648; loss 0.317924\n",
      "epoch:2; batch 91648; train accuracy: 0.757497\n",
      "epoch 2; batch 91776; loss 0.408887\n",
      "epoch:2; batch 91776; train accuracy: 0.757553\n",
      "epoch 2; batch 91904; loss 0.279489\n",
      "epoch:2; batch 91904; train accuracy: 0.757640\n",
      "epoch 2; batch 92032; loss 0.282389\n",
      "epoch:2; batch 92032; train accuracy: 0.757736\n",
      "epoch 2; batch 92160; loss 0.326177\n",
      "epoch:2; batch 92160; train accuracy: 0.757823\n",
      "epoch 2; batch 92288; loss 0.461805\n",
      "epoch:2; batch 92288; train accuracy: 0.757853\n",
      "epoch 2; batch 92416; loss 0.328572\n",
      "epoch:2; batch 92416; train accuracy: 0.757914\n",
      "epoch 2; batch 92544; loss 0.470422\n",
      "epoch:2; batch 92544; train accuracy: 0.757985\n",
      "epoch 2; batch 92672; loss 0.468210\n",
      "epoch:2; batch 92672; train accuracy: 0.758025\n",
      "epoch 2; batch 92800; loss 0.461127\n",
      "epoch:2; batch 92800; train accuracy: 0.758065\n",
      "epoch 2; batch 92928; loss 0.396583\n",
      "epoch:2; batch 92928; train accuracy: 0.758126\n",
      "epoch 2; batch 93056; loss 0.294183\n",
      "epoch:2; batch 93056; train accuracy: 0.758211\n",
      "epoch 2; batch 93184; loss 0.391694\n",
      "epoch:2; batch 93184; train accuracy: 0.758262\n",
      "epoch 2; batch 93312; loss 0.355929\n",
      "epoch:2; batch 93312; train accuracy: 0.758327\n",
      "epoch 2; batch 93440; loss 0.193625\n",
      "epoch:2; batch 93440; train accuracy: 0.758463\n",
      "epoch 2; batch 93568; loss 0.380334\n",
      "epoch:2; batch 93568; train accuracy: 0.758518\n",
      "epoch 2; batch 93696; loss 0.409238\n",
      "epoch:2; batch 93696; train accuracy: 0.758583\n",
      "epoch 2; batch 93824; loss 0.323930\n",
      "epoch:2; batch 93824; train accuracy: 0.758658\n",
      "epoch 2; batch 93952; loss 0.357526\n",
      "epoch:2; batch 93952; train accuracy: 0.758743\n",
      "epoch 2; batch 94080; loss 0.441369\n",
      "epoch:2; batch 94080; train accuracy: 0.758822\n",
      "epoch 2; batch 94208; loss 0.422311\n",
      "epoch:2; batch 94208; train accuracy: 0.758872\n",
      "epoch 2; batch 94336; loss 0.278983\n",
      "epoch:2; batch 94336; train accuracy: 0.758962\n",
      "epoch 2; batch 94464; loss 0.268943\n",
      "epoch:2; batch 94464; train accuracy: 0.759066\n",
      "epoch 2; batch 94592; loss 0.339683\n",
      "epoch:2; batch 94592; train accuracy: 0.759105\n",
      "epoch 2; batch 94720; loss 0.404923\n",
      "epoch:2; batch 94720; train accuracy: 0.759150\n",
      "epoch 2; batch 94848; loss 0.401737\n",
      "epoch:2; batch 94848; train accuracy: 0.759204\n",
      "epoch 2; batch 94976; loss 0.329410\n",
      "epoch:2; batch 94976; train accuracy: 0.759283\n",
      "epoch 2; batch 95104; loss 0.334894\n",
      "epoch:2; batch 95104; train accuracy: 0.759367\n",
      "epoch 2; batch 95232; loss 0.325924\n",
      "epoch:2; batch 95232; train accuracy: 0.759451\n",
      "epoch 2; batch 95360; loss 0.331430\n",
      "epoch:2; batch 95360; train accuracy: 0.759525\n",
      "epoch 2; batch 95488; loss 0.352922\n",
      "epoch:2; batch 95488; train accuracy: 0.759604\n",
      "epoch 2; batch 95616; loss 0.248604\n",
      "epoch:2; batch 95616; train accuracy: 0.759697\n",
      "epoch 2; batch 95744; loss 0.394598\n",
      "epoch:2; batch 95744; train accuracy: 0.759751\n",
      "epoch 2; batch 95872; loss 0.237954\n",
      "epoch:2; batch 95872; train accuracy: 0.759859\n",
      "epoch 2; batch 96000; loss 0.308887\n",
      "epoch:2; batch 96000; train accuracy: 0.759947\n",
      "epoch 2; batch 96128; loss 0.394429\n",
      "epoch:2; batch 96128; train accuracy: 0.760001\n",
      "epoch 2; batch 96256; loss 0.325669\n",
      "epoch:2; batch 96256; train accuracy: 0.760064\n",
      "epoch 2; batch 96384; loss 0.387744\n",
      "epoch:2; batch 96384; train accuracy: 0.760132\n",
      "epoch 2; batch 96512; loss 0.252784\n",
      "epoch:2; batch 96512; train accuracy: 0.760225\n",
      "epoch 2; batch 96640; loss 0.564986\n",
      "epoch:2; batch 96640; train accuracy: 0.760243\n",
      "epoch 2; batch 96768; loss 0.286500\n",
      "epoch:2; batch 96768; train accuracy: 0.760311\n",
      "epoch 2; batch 96896; loss 0.266065\n",
      "epoch:2; batch 96896; train accuracy: 0.760413\n",
      "epoch 2; batch 97024; loss 0.418488\n",
      "epoch:2; batch 97024; train accuracy: 0.760441\n",
      "epoch 2; batch 97152; loss 0.417465\n",
      "epoch:2; batch 97152; train accuracy: 0.760499\n",
      "epoch 2; batch 97280; loss 0.372057\n",
      "epoch:2; batch 97280; train accuracy: 0.760562\n",
      "epoch 2; batch 97408; loss 0.373322\n",
      "epoch:2; batch 97408; train accuracy: 0.760619\n",
      "epoch 2; batch 97536; loss 0.295900\n",
      "epoch:2; batch 97536; train accuracy: 0.760716\n",
      "epoch 2; batch 97664; loss 0.354436\n",
      "epoch:2; batch 97664; train accuracy: 0.760788\n",
      "epoch 2; batch 97792; loss 0.337121\n",
      "epoch:2; batch 97792; train accuracy: 0.760866\n",
      "epoch 2; batch 97920; loss 0.319035\n",
      "epoch:2; batch 97920; train accuracy: 0.760938\n",
      "epoch 2; batch 98048; loss 0.307867\n",
      "epoch:2; batch 98048; train accuracy: 0.761019\n",
      "epoch 2; batch 98176; loss 0.385953\n",
      "epoch:2; batch 98176; train accuracy: 0.761086\n",
      "epoch 2; batch 98304; loss 0.354120\n",
      "epoch:2; batch 98304; train accuracy: 0.761163\n",
      "epoch 2; batch 98432; loss 0.494918\n",
      "epoch:2; batch 98432; train accuracy: 0.761200\n",
      "epoch 2; batch 98560; loss 0.323361\n",
      "epoch:2; batch 98560; train accuracy: 0.761281\n",
      "epoch 2; batch 98688; loss 0.355956\n",
      "epoch:2; batch 98688; train accuracy: 0.761343\n",
      "epoch 2; batch 98816; loss 0.323275\n",
      "epoch:2; batch 98816; train accuracy: 0.761424\n",
      "epoch 2; batch 98944; loss 0.328260\n",
      "epoch:2; batch 98944; train accuracy: 0.761496\n",
      "epoch 2; batch 99072; loss 0.312299\n",
      "epoch:2; batch 99072; train accuracy: 0.761572\n",
      "epoch 2; batch 99200; loss 0.447877\n",
      "epoch:2; batch 99200; train accuracy: 0.761604\n",
      "epoch 2; batch 99328; loss 0.319719\n",
      "epoch:2; batch 99328; train accuracy: 0.761680\n",
      "epoch 2; batch 99456; loss 0.417959\n",
      "epoch:2; batch 99456; train accuracy: 0.761746\n",
      "epoch 2; batch 99584; loss 0.372014\n",
      "epoch:2; batch 99584; train accuracy: 0.761807\n",
      "epoch 2; batch 99712; loss 0.362417\n",
      "epoch:2; batch 99712; train accuracy: 0.761897\n",
      "epoch 2; batch 99840; loss 0.395061\n",
      "epoch:2; batch 99840; train accuracy: 0.761953\n",
      "epoch 2; batch 99968; loss 0.443120\n",
      "epoch:2; batch 99968; train accuracy: 0.762029\n",
      "epoch 2; batch 100096; loss 0.392938\n",
      "epoch:2; batch 100096; train accuracy: 0.762089\n",
      "epoch 2; batch 100224; loss 0.383599\n",
      "epoch:2; batch 100224; train accuracy: 0.762150\n",
      "epoch 2; batch 100352; loss 0.275590\n",
      "epoch:2; batch 100352; train accuracy: 0.762259\n",
      "epoch 2; batch 100480; loss 0.348738\n",
      "epoch:2; batch 100480; train accuracy: 0.762330\n",
      "epoch 2; batch 100608; loss 0.325720\n",
      "epoch:2; batch 100608; train accuracy: 0.762400\n",
      "epoch 2; batch 100736; loss 0.224038\n",
      "epoch:2; batch 100736; train accuracy: 0.762504\n",
      "epoch 2; batch 100864; loss 0.328151\n",
      "epoch:2; batch 100864; train accuracy: 0.762593\n",
      "epoch 2; batch 100992; loss 0.224690\n",
      "epoch:2; batch 100992; train accuracy: 0.762692\n",
      "epoch 2; batch 101120; loss 0.314967\n",
      "epoch:2; batch 101120; train accuracy: 0.762767\n",
      "epoch 2; batch 101248; loss 0.372332\n",
      "epoch:2; batch 101248; train accuracy: 0.762827\n",
      "epoch 2; batch 101376; loss 0.341488\n",
      "epoch:2; batch 101376; train accuracy: 0.762887\n",
      "epoch 2; batch 101504; loss 0.374882\n",
      "epoch:2; batch 101504; train accuracy: 0.762956\n",
      "epoch 2; batch 101632; loss 0.392550\n",
      "epoch:2; batch 101632; train accuracy: 0.763001\n",
      "epoch 2; batch 101760; loss 0.267636\n",
      "epoch:2; batch 101760; train accuracy: 0.763095\n",
      "epoch 2; batch 101888; loss 0.330849\n",
      "epoch:2; batch 101888; train accuracy: 0.763179\n",
      "epoch 2; batch 102016; loss 0.340193\n",
      "epoch:2; batch 102016; train accuracy: 0.763253\n",
      "epoch 2; batch 102144; loss 0.471265\n",
      "epoch:2; batch 102144; train accuracy: 0.763307\n",
      "epoch 2; batch 102272; loss 0.485627\n",
      "epoch:2; batch 102272; train accuracy: 0.763338\n",
      "epoch 2; batch 102400; loss 0.486313\n",
      "epoch:2; batch 102400; train accuracy: 0.763387\n",
      "epoch 2; batch 102528; loss 0.394253\n",
      "epoch:2; batch 102528; train accuracy: 0.763451\n",
      "epoch 2; batch 102656; loss 0.227691\n",
      "epoch:2; batch 102656; train accuracy: 0.763530\n",
      "epoch 2; batch 102784; loss 0.334566\n",
      "epoch:2; batch 102784; train accuracy: 0.763589\n",
      "epoch 2; batch 102912; loss 0.289635\n",
      "epoch:2; batch 102912; train accuracy: 0.763653\n",
      "epoch 2; batch 103040; loss 0.339389\n",
      "epoch:2; batch 103040; train accuracy: 0.763721\n",
      "epoch 2; batch 103168; loss 0.275637\n",
      "epoch:2; batch 103168; train accuracy: 0.763804\n",
      "epoch 2; batch 103296; loss 0.415376\n",
      "epoch:2; batch 103296; train accuracy: 0.763853\n",
      "epoch 2; batch 103424; loss 0.359358\n",
      "epoch:2; batch 103424; train accuracy: 0.763917\n",
      "epoch 2; batch 103552; loss 0.317164\n",
      "epoch:2; batch 103552; train accuracy: 0.763985\n",
      "epoch 2; batch 103680; loss 0.307380\n",
      "epoch:2; batch 103680; train accuracy: 0.764048\n",
      "epoch 2; batch 103808; loss 0.286167\n",
      "epoch:2; batch 103808; train accuracy: 0.764145\n",
      "epoch 2; batch 103936; loss 0.415136\n",
      "epoch:2; batch 103936; train accuracy: 0.764189\n",
      "epoch 2; batch 104064; loss 0.368100\n",
      "epoch:2; batch 104064; train accuracy: 0.764252\n",
      "epoch 2; batch 104192; loss 0.435136\n",
      "epoch:2; batch 104192; train accuracy: 0.764291\n",
      "epoch 2; batch 104320; loss 0.318350\n",
      "epoch:2; batch 104320; train accuracy: 0.764359\n",
      "epoch 2; batch 104448; loss 0.332928\n",
      "epoch:2; batch 104448; train accuracy: 0.764426\n",
      "epoch 2; batch 104576; loss 0.463012\n",
      "epoch:2; batch 104576; train accuracy: 0.764461\n",
      "epoch 2; batch 104704; loss 0.334367\n",
      "epoch:2; batch 104704; train accuracy: 0.764523\n",
      "epoch 2; batch 104832; loss 0.344254\n",
      "epoch:2; batch 104832; train accuracy: 0.764595\n",
      "epoch 2; batch 104960; loss 0.354529\n",
      "epoch:2; batch 104960; train accuracy: 0.764653\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 31044 ; rate: 0.295770\n",
      "y_true_label_1_num: 11978 ; rate: 0.114120\n",
      "y_true_label_2_num: 23668 ; rate: 0.225495\n",
      "y_true_label_3_num: 38270 ; rate: 0.364615\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.940387\n",
      "valid avg_precision: 0.952470\n",
      "valid avg_recall: 0.927753\n",
      "valid avg_f1: 0.938612\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 4423 ; rate: 0.295339\n",
      "y_true_label_1_num: 1735 ; rate: 0.115852\n",
      "y_true_label_2_num: 3410 ; rate: 0.227698\n",
      "y_true_label_3_num: 5408 ; rate: 0.361111\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.716279\n",
      "valid avg_precision: 0.729260\n",
      "valid avg_recall: 0.689436\n",
      "valid avg_f1: 0.699944\n",
      "epoch 3\n",
      "epoch 3; batch 128; loss 0.300400\n",
      "epoch:3; batch 128; train accuracy: 0.764716\n",
      "epoch 3; batch 256; loss 0.230557\n",
      "epoch:3; batch 256; train accuracy: 0.764811\n",
      "epoch 3; batch 384; loss 0.335670\n",
      "epoch:3; batch 384; train accuracy: 0.764855\n",
      "epoch 3; batch 512; loss 0.269663\n",
      "epoch:3; batch 512; train accuracy: 0.764945\n",
      "epoch 3; batch 640; loss 0.273241\n",
      "epoch:3; batch 640; train accuracy: 0.765041\n",
      "epoch 3; batch 768; loss 0.242601\n",
      "epoch:3; batch 768; train accuracy: 0.765131\n",
      "epoch 3; batch 896; loss 0.341277\n",
      "epoch:3; batch 896; train accuracy: 0.765208\n",
      "epoch 3; batch 1024; loss 0.267580\n",
      "epoch:3; batch 1024; train accuracy: 0.765279\n",
      "epoch 3; batch 1152; loss 0.265180\n",
      "epoch:3; batch 1152; train accuracy: 0.765355\n",
      "epoch 3; batch 1280; loss 0.264250\n",
      "epoch:3; batch 1280; train accuracy: 0.765431\n",
      "epoch 3; batch 1408; loss 0.319534\n",
      "epoch:3; batch 1408; train accuracy: 0.765497\n",
      "epoch 3; batch 1536; loss 0.185923\n",
      "epoch:3; batch 1536; train accuracy: 0.765611\n",
      "epoch 3; batch 1664; loss 0.329259\n",
      "epoch:3; batch 1664; train accuracy: 0.765653\n",
      "epoch 3; batch 1792; loss 0.247513\n",
      "epoch:3; batch 1792; train accuracy: 0.765738\n",
      "epoch 3; batch 1920; loss 0.344040\n",
      "epoch:3; batch 1920; train accuracy: 0.765800\n",
      "epoch 3; batch 2048; loss 0.173363\n",
      "epoch:3; batch 2048; train accuracy: 0.765903\n",
      "epoch 3; batch 2176; loss 0.322389\n",
      "epoch:3; batch 2176; train accuracy: 0.765979\n",
      "epoch 3; batch 2304; loss 0.239091\n",
      "epoch:3; batch 2304; train accuracy: 0.766068\n",
      "epoch 3; batch 2432; loss 0.308417\n",
      "epoch:3; batch 2432; train accuracy: 0.766134\n",
      "epoch 3; batch 2560; loss 0.223324\n",
      "epoch:3; batch 2560; train accuracy: 0.766204\n",
      "epoch 3; batch 2688; loss 0.287352\n",
      "epoch:3; batch 2688; train accuracy: 0.766279\n",
      "epoch 3; batch 2816; loss 0.181109\n",
      "epoch:3; batch 2816; train accuracy: 0.766372\n",
      "epoch 3; batch 2944; loss 0.313474\n",
      "epoch:3; batch 2944; train accuracy: 0.766461\n",
      "epoch 3; batch 3072; loss 0.244381\n",
      "epoch:3; batch 3072; train accuracy: 0.766536\n",
      "epoch 3; batch 3200; loss 0.272195\n",
      "epoch:3; batch 3200; train accuracy: 0.766624\n",
      "epoch 3; batch 3328; loss 0.196744\n",
      "epoch:3; batch 3328; train accuracy: 0.766722\n",
      "epoch 3; batch 3456; loss 0.273563\n",
      "epoch:3; batch 3456; train accuracy: 0.766792\n",
      "epoch 3; batch 3584; loss 0.209172\n",
      "epoch:3; batch 3584; train accuracy: 0.766885\n",
      "epoch 3; batch 3712; loss 0.260146\n",
      "epoch:3; batch 3712; train accuracy: 0.766964\n",
      "epoch 3; batch 3840; loss 0.269994\n",
      "epoch:3; batch 3840; train accuracy: 0.767028\n",
      "epoch 3; batch 3968; loss 0.244394\n",
      "epoch:3; batch 3968; train accuracy: 0.767107\n",
      "epoch 3; batch 4096; loss 0.305238\n",
      "epoch:3; batch 4096; train accuracy: 0.767181\n",
      "epoch 3; batch 4224; loss 0.274441\n",
      "epoch:3; batch 4224; train accuracy: 0.767259\n",
      "epoch 3; batch 4352; loss 0.289877\n",
      "epoch:3; batch 4352; train accuracy: 0.767333\n",
      "epoch 3; batch 4480; loss 0.306774\n",
      "epoch:3; batch 4480; train accuracy: 0.767397\n",
      "epoch 3; batch 4608; loss 0.220926\n",
      "epoch:3; batch 4608; train accuracy: 0.767504\n",
      "epoch 3; batch 4736; loss 0.161053\n",
      "epoch:3; batch 4736; train accuracy: 0.767605\n",
      "epoch 3; batch 4864; loss 0.134522\n",
      "epoch:3; batch 4864; train accuracy: 0.767729\n",
      "epoch 3; batch 4992; loss 0.228257\n",
      "epoch:3; batch 4992; train accuracy: 0.767812\n",
      "epoch 3; batch 5120; loss 0.207796\n",
      "epoch:3; batch 5120; train accuracy: 0.767913\n",
      "epoch 3; batch 5248; loss 0.239877\n",
      "epoch:3; batch 5248; train accuracy: 0.767995\n",
      "epoch 3; batch 5376; loss 0.162712\n",
      "epoch:3; batch 5376; train accuracy: 0.768091\n",
      "epoch 3; batch 5504; loss 0.239297\n",
      "epoch:3; batch 5504; train accuracy: 0.768187\n",
      "epoch 3; batch 5632; loss 0.258822\n",
      "epoch:3; batch 5632; train accuracy: 0.768274\n",
      "epoch 3; batch 5760; loss 0.250660\n",
      "epoch:3; batch 5760; train accuracy: 0.768356\n",
      "epoch 3; batch 5888; loss 0.243504\n",
      "epoch:3; batch 5888; train accuracy: 0.768438\n",
      "epoch 3; batch 6016; loss 0.178810\n",
      "epoch:3; batch 6016; train accuracy: 0.768538\n",
      "epoch 3; batch 6144; loss 0.210158\n",
      "epoch:3; batch 6144; train accuracy: 0.768624\n",
      "epoch 3; batch 6272; loss 0.219583\n",
      "epoch:3; batch 6272; train accuracy: 0.768715\n",
      "epoch 3; batch 6400; loss 0.306534\n",
      "epoch:3; batch 6400; train accuracy: 0.768782\n",
      "epoch 3; batch 6528; loss 0.251707\n",
      "epoch:3; batch 6528; train accuracy: 0.768868\n",
      "epoch 3; batch 6656; loss 0.211780\n",
      "epoch:3; batch 6656; train accuracy: 0.768959\n",
      "epoch 3; batch 6784; loss 0.196035\n",
      "epoch:3; batch 6784; train accuracy: 0.769049\n",
      "epoch 3; batch 6912; loss 0.305573\n",
      "epoch:3; batch 6912; train accuracy: 0.769135\n",
      "epoch 3; batch 7040; loss 0.174999\n",
      "epoch:3; batch 7040; train accuracy: 0.769229\n",
      "epoch 3; batch 7168; loss 0.230680\n",
      "epoch:3; batch 7168; train accuracy: 0.769315\n",
      "epoch 3; batch 7296; loss 0.246188\n",
      "epoch:3; batch 7296; train accuracy: 0.769400\n",
      "epoch 3; batch 7424; loss 0.377904\n",
      "epoch:3; batch 7424; train accuracy: 0.769467\n",
      "epoch 3; batch 7552; loss 0.225037\n",
      "epoch:3; batch 7552; train accuracy: 0.769557\n",
      "epoch 3; batch 7680; loss 0.253693\n",
      "epoch:3; batch 7680; train accuracy: 0.769637\n",
      "epoch 3; batch 7808; loss 0.257659\n",
      "epoch:3; batch 7808; train accuracy: 0.769708\n",
      "epoch 3; batch 7936; loss 0.253575\n",
      "epoch:3; batch 7936; train accuracy: 0.769784\n",
      "epoch 3; batch 8064; loss 0.203304\n",
      "epoch:3; batch 8064; train accuracy: 0.769873\n",
      "epoch 3; batch 8192; loss 0.277996\n",
      "epoch:3; batch 8192; train accuracy: 0.769948\n",
      "epoch 3; batch 8320; loss 0.263635\n",
      "epoch:3; batch 8320; train accuracy: 0.770038\n",
      "epoch 3; batch 8448; loss 0.202894\n",
      "epoch:3; batch 8448; train accuracy: 0.770131\n",
      "epoch 3; batch 8576; loss 0.196329\n",
      "epoch:3; batch 8576; train accuracy: 0.770225\n",
      "epoch 3; batch 8704; loss 0.234054\n",
      "epoch:3; batch 8704; train accuracy: 0.770309\n",
      "epoch 3; batch 8832; loss 0.216325\n",
      "epoch:3; batch 8832; train accuracy: 0.770402\n",
      "epoch 3; batch 8960; loss 0.237957\n",
      "epoch:3; batch 8960; train accuracy: 0.770482\n",
      "epoch 3; batch 9088; loss 0.196425\n",
      "epoch:3; batch 9088; train accuracy: 0.770593\n",
      "epoch 3; batch 9216; loss 0.152743\n",
      "epoch:3; batch 9216; train accuracy: 0.770700\n",
      "epoch 3; batch 9344; loss 0.359877\n",
      "epoch:3; batch 9344; train accuracy: 0.770769\n",
      "epoch 3; batch 9472; loss 0.176850\n",
      "epoch:3; batch 9472; train accuracy: 0.770844\n",
      "epoch 3; batch 9600; loss 0.109324\n",
      "epoch:3; batch 9600; train accuracy: 0.770968\n",
      "epoch 3; batch 9728; loss 0.204773\n",
      "epoch:3; batch 9728; train accuracy: 0.771052\n",
      "epoch 3; batch 9856; loss 0.198649\n",
      "epoch:3; batch 9856; train accuracy: 0.771135\n",
      "epoch 3; batch 9984; loss 0.120789\n",
      "epoch:3; batch 9984; train accuracy: 0.771246\n",
      "epoch 3; batch 10112; loss 0.203069\n",
      "epoch:3; batch 10112; train accuracy: 0.771347\n",
      "epoch 3; batch 10240; loss 0.160722\n",
      "epoch:3; batch 10240; train accuracy: 0.771448\n",
      "epoch 3; batch 10368; loss 0.300639\n",
      "epoch:3; batch 10368; train accuracy: 0.771531\n",
      "epoch 3; batch 10496; loss 0.177116\n",
      "epoch:3; batch 10496; train accuracy: 0.771627\n",
      "epoch 3; batch 10624; loss 0.242955\n",
      "epoch:3; batch 10624; train accuracy: 0.771724\n",
      "epoch 3; batch 10752; loss 0.225164\n",
      "epoch:3; batch 10752; train accuracy: 0.771815\n",
      "epoch 3; batch 10880; loss 0.245218\n",
      "epoch:3; batch 10880; train accuracy: 0.771902\n",
      "epoch 3; batch 11008; loss 0.216224\n",
      "epoch:3; batch 11008; train accuracy: 0.771980\n",
      "epoch 3; batch 11136; loss 0.212845\n",
      "epoch:3; batch 11136; train accuracy: 0.772062\n",
      "epoch 3; batch 11264; loss 0.134444\n",
      "epoch:3; batch 11264; train accuracy: 0.772163\n",
      "epoch 3; batch 11392; loss 0.285263\n",
      "epoch:3; batch 11392; train accuracy: 0.772231\n",
      "epoch 3; batch 11520; loss 0.175958\n",
      "epoch:3; batch 11520; train accuracy: 0.772331\n",
      "epoch 3; batch 11648; loss 0.212807\n",
      "epoch:3; batch 11648; train accuracy: 0.772422\n",
      "epoch 3; batch 11776; loss 0.252115\n",
      "epoch:3; batch 11776; train accuracy: 0.772504\n",
      "epoch 3; batch 11904; loss 0.151383\n",
      "epoch:3; batch 11904; train accuracy: 0.772604\n",
      "epoch 3; batch 12032; loss 0.242515\n",
      "epoch:3; batch 12032; train accuracy: 0.772699\n",
      "epoch 3; batch 12160; loss 0.278428\n",
      "epoch:3; batch 12160; train accuracy: 0.772767\n",
      "epoch 3; batch 12288; loss 0.290021\n",
      "epoch:3; batch 12288; train accuracy: 0.772848\n",
      "epoch 3; batch 12416; loss 0.156692\n",
      "epoch:3; batch 12416; train accuracy: 0.772934\n",
      "epoch 3; batch 12544; loss 0.137736\n",
      "epoch:3; batch 12544; train accuracy: 0.773046\n",
      "epoch 3; batch 12672; loss 0.214974\n",
      "epoch:3; batch 12672; train accuracy: 0.773123\n",
      "epoch 3; batch 12800; loss 0.274914\n",
      "epoch:3; batch 12800; train accuracy: 0.773200\n",
      "epoch 3; batch 12928; loss 0.171032\n",
      "epoch:3; batch 12928; train accuracy: 0.773303\n",
      "epoch 3; batch 13056; loss 0.277860\n",
      "epoch:3; batch 13056; train accuracy: 0.773379\n",
      "epoch 3; batch 13184; loss 0.222409\n",
      "epoch:3; batch 13184; train accuracy: 0.773464\n",
      "epoch 3; batch 13312; loss 0.140392\n",
      "epoch:3; batch 13312; train accuracy: 0.773563\n",
      "epoch 3; batch 13440; loss 0.126104\n",
      "epoch:3; batch 13440; train accuracy: 0.773675\n",
      "epoch 3; batch 13568; loss 0.195935\n",
      "epoch:3; batch 13568; train accuracy: 0.773764\n",
      "epoch 3; batch 13696; loss 0.136892\n",
      "epoch:3; batch 13696; train accuracy: 0.773867\n",
      "epoch 3; batch 13824; loss 0.169798\n",
      "epoch:3; batch 13824; train accuracy: 0.773974\n",
      "epoch 3; batch 13952; loss 0.218535\n",
      "epoch:3; batch 13952; train accuracy: 0.774054\n",
      "epoch 3; batch 14080; loss 0.205617\n",
      "epoch:3; batch 14080; train accuracy: 0.774143\n",
      "epoch 3; batch 14208; loss 0.153259\n",
      "epoch:3; batch 14208; train accuracy: 0.774250\n",
      "epoch 3; batch 14336; loss 0.141200\n",
      "epoch:3; batch 14336; train accuracy: 0.774365\n",
      "epoch 3; batch 14464; loss 0.140154\n",
      "epoch:3; batch 14464; train accuracy: 0.774463\n",
      "epoch 3; batch 14592; loss 0.064365\n",
      "epoch:3; batch 14592; train accuracy: 0.774587\n",
      "epoch 3; batch 14720; loss 0.140328\n",
      "epoch:3; batch 14720; train accuracy: 0.774679\n",
      "epoch 3; batch 14848; loss 0.233874\n",
      "epoch:3; batch 14848; train accuracy: 0.774759\n",
      "epoch 3; batch 14976; loss 0.189357\n",
      "epoch:3; batch 14976; train accuracy: 0.774847\n",
      "epoch 3; batch 15104; loss 0.210224\n",
      "epoch:3; batch 15104; train accuracy: 0.774944\n",
      "epoch 3; batch 15232; loss 0.164271\n",
      "epoch:3; batch 15232; train accuracy: 0.775036\n",
      "epoch 3; batch 15360; loss 0.221222\n",
      "epoch:3; batch 15360; train accuracy: 0.775120\n",
      "epoch 3; batch 15488; loss 0.188063\n",
      "epoch:3; batch 15488; train accuracy: 0.775194\n",
      "epoch 3; batch 15616; loss 0.226803\n",
      "epoch:3; batch 15616; train accuracy: 0.775282\n",
      "epoch 3; batch 15744; loss 0.179285\n",
      "epoch:3; batch 15744; train accuracy: 0.775365\n",
      "epoch 3; batch 15872; loss 0.389821\n",
      "epoch:3; batch 15872; train accuracy: 0.775413\n",
      "epoch 3; batch 16000; loss 0.157211\n",
      "epoch:3; batch 16000; train accuracy: 0.775500\n",
      "epoch 3; batch 16128; loss 0.230234\n",
      "epoch:3; batch 16128; train accuracy: 0.775570\n",
      "epoch 3; batch 16256; loss 0.173498\n",
      "epoch:3; batch 16256; train accuracy: 0.775661\n",
      "epoch 3; batch 16384; loss 0.275640\n",
      "epoch:3; batch 16384; train accuracy: 0.775726\n",
      "epoch 3; batch 16512; loss 0.304630\n",
      "epoch:3; batch 16512; train accuracy: 0.775778\n",
      "epoch 3; batch 16640; loss 0.135041\n",
      "epoch:3; batch 16640; train accuracy: 0.775883\n",
      "epoch 3; batch 16768; loss 0.156360\n",
      "epoch:3; batch 16768; train accuracy: 0.775970\n",
      "epoch 3; batch 16896; loss 0.189689\n",
      "epoch:3; batch 16896; train accuracy: 0.776056\n",
      "epoch 3; batch 17024; loss 0.141872\n",
      "epoch:3; batch 17024; train accuracy: 0.776161\n",
      "epoch 3; batch 17152; loss 0.221984\n",
      "epoch:3; batch 17152; train accuracy: 0.776238\n",
      "epoch 3; batch 17280; loss 0.218240\n",
      "epoch:3; batch 17280; train accuracy: 0.776312\n",
      "epoch 3; batch 17408; loss 0.184670\n",
      "epoch:3; batch 17408; train accuracy: 0.776394\n",
      "epoch 3; batch 17536; loss 0.147572\n",
      "epoch:3; batch 17536; train accuracy: 0.776497\n",
      "epoch 3; batch 17664; loss 0.230675\n",
      "epoch:3; batch 17664; train accuracy: 0.776592\n",
      "epoch 3; batch 17792; loss 0.149909\n",
      "epoch:3; batch 17792; train accuracy: 0.776696\n",
      "epoch 3; batch 17920; loss 0.218951\n",
      "epoch:3; batch 17920; train accuracy: 0.776782\n",
      "epoch 3; batch 18048; loss 0.121858\n",
      "epoch:3; batch 18048; train accuracy: 0.776881\n",
      "epoch 3; batch 18176; loss 0.202440\n",
      "epoch:3; batch 18176; train accuracy: 0.776962\n",
      "epoch 3; batch 18304; loss 0.132261\n",
      "epoch:3; batch 18304; train accuracy: 0.777052\n",
      "epoch 3; batch 18432; loss 0.118895\n",
      "epoch:3; batch 18432; train accuracy: 0.777160\n",
      "epoch 3; batch 18560; loss 0.172194\n",
      "epoch:3; batch 18560; train accuracy: 0.777250\n",
      "epoch 3; batch 18688; loss 0.184189\n",
      "epoch:3; batch 18688; train accuracy: 0.777331\n",
      "epoch 3; batch 18816; loss 0.201260\n",
      "epoch:3; batch 18816; train accuracy: 0.777412\n",
      "epoch 3; batch 18944; loss 0.177024\n",
      "epoch:3; batch 18944; train accuracy: 0.777501\n",
      "epoch 3; batch 19072; loss 0.175000\n",
      "epoch:3; batch 19072; train accuracy: 0.777599\n",
      "epoch 3; batch 19200; loss 0.160532\n",
      "epoch:3; batch 19200; train accuracy: 0.777689\n",
      "epoch 3; batch 19328; loss 0.216924\n",
      "epoch:3; batch 19328; train accuracy: 0.777782\n",
      "epoch 3; batch 19456; loss 0.146184\n",
      "epoch:3; batch 19456; train accuracy: 0.777871\n",
      "epoch 3; batch 19584; loss 0.116102\n",
      "epoch:3; batch 19584; train accuracy: 0.777969\n",
      "epoch 3; batch 19712; loss 0.129324\n",
      "epoch:3; batch 19712; train accuracy: 0.778067\n",
      "epoch 3; batch 19840; loss 0.175514\n",
      "epoch:3; batch 19840; train accuracy: 0.778155\n",
      "epoch 3; batch 19968; loss 0.098263\n",
      "epoch:3; batch 19968; train accuracy: 0.778257\n",
      "epoch 3; batch 20096; loss 0.135901\n",
      "epoch:3; batch 20096; train accuracy: 0.778359\n",
      "epoch 3; batch 20224; loss 0.189055\n",
      "epoch:3; batch 20224; train accuracy: 0.778452\n",
      "epoch 3; batch 20352; loss 0.214674\n",
      "epoch:3; batch 20352; train accuracy: 0.778544\n",
      "epoch 3; batch 20480; loss 0.164189\n",
      "epoch:3; batch 20480; train accuracy: 0.778650\n",
      "epoch 3; batch 20608; loss 0.267319\n",
      "epoch:3; batch 20608; train accuracy: 0.778738\n",
      "epoch 3; batch 20736; loss 0.130010\n",
      "epoch:3; batch 20736; train accuracy: 0.778831\n",
      "epoch 3; batch 20864; loss 0.210898\n",
      "epoch:3; batch 20864; train accuracy: 0.778906\n",
      "epoch 3; batch 20992; loss 0.183788\n",
      "epoch:3; batch 20992; train accuracy: 0.778994\n",
      "epoch 3; batch 21120; loss 0.198880\n",
      "epoch:3; batch 21120; train accuracy: 0.779082\n",
      "epoch 3; batch 21248; loss 0.326185\n",
      "epoch:3; batch 21248; train accuracy: 0.779169\n",
      "epoch 3; batch 21376; loss 0.177278\n",
      "epoch:3; batch 21376; train accuracy: 0.779253\n",
      "epoch 3; batch 21504; loss 0.155365\n",
      "epoch:3; batch 21504; train accuracy: 0.779353\n",
      "epoch 3; batch 21632; loss 0.141368\n",
      "epoch:3; batch 21632; train accuracy: 0.779436\n",
      "epoch 3; batch 21760; loss 0.159879\n",
      "epoch:3; batch 21760; train accuracy: 0.779528\n",
      "epoch 3; batch 21888; loss 0.155889\n",
      "epoch:3; batch 21888; train accuracy: 0.779615\n",
      "epoch 3; batch 22016; loss 0.192047\n",
      "epoch:3; batch 22016; train accuracy: 0.779706\n",
      "epoch 3; batch 22144; loss 0.143854\n",
      "epoch:3; batch 22144; train accuracy: 0.779798\n",
      "epoch 3; batch 22272; loss 0.131209\n",
      "epoch:3; batch 22272; train accuracy: 0.779893\n",
      "epoch 3; batch 22400; loss 0.084951\n",
      "epoch:3; batch 22400; train accuracy: 0.780002\n",
      "epoch 3; batch 22528; loss 0.139148\n",
      "epoch:3; batch 22528; train accuracy: 0.780088\n",
      "epoch 3; batch 22656; loss 0.187329\n",
      "epoch:3; batch 22656; train accuracy: 0.780171\n",
      "epoch 3; batch 22784; loss 0.074045\n",
      "epoch:3; batch 22784; train accuracy: 0.780279\n",
      "epoch 3; batch 22912; loss 0.175703\n",
      "epoch:3; batch 22912; train accuracy: 0.780370\n",
      "epoch 3; batch 23040; loss 0.128426\n",
      "epoch:3; batch 23040; train accuracy: 0.780482\n",
      "epoch 3; batch 23168; loss 0.170978\n",
      "epoch:3; batch 23168; train accuracy: 0.780585\n",
      "epoch 3; batch 23296; loss 0.117681\n",
      "epoch:3; batch 23296; train accuracy: 0.780680\n",
      "epoch 3; batch 23424; loss 0.107304\n",
      "epoch:3; batch 23424; train accuracy: 0.780779\n",
      "epoch 3; batch 23552; loss 0.083394\n",
      "epoch:3; batch 23552; train accuracy: 0.780886\n",
      "epoch 3; batch 23680; loss 0.164620\n",
      "epoch:3; batch 23680; train accuracy: 0.780980\n",
      "epoch 3; batch 23808; loss 0.136806\n",
      "epoch:3; batch 23808; train accuracy: 0.781075\n",
      "epoch 3; batch 23936; loss 0.169266\n",
      "epoch:3; batch 23936; train accuracy: 0.781177\n",
      "epoch 3; batch 24064; loss 0.113854\n",
      "epoch:3; batch 24064; train accuracy: 0.781280\n",
      "epoch 3; batch 24192; loss 0.132216\n",
      "epoch:3; batch 24192; train accuracy: 0.781378\n",
      "epoch 3; batch 24320; loss 0.145039\n",
      "epoch:3; batch 24320; train accuracy: 0.781468\n",
      "epoch 3; batch 24448; loss 0.114426\n",
      "epoch:3; batch 24448; train accuracy: 0.781566\n",
      "epoch 3; batch 24576; loss 0.124520\n",
      "epoch:3; batch 24576; train accuracy: 0.781668\n",
      "epoch 3; batch 24704; loss 0.160511\n",
      "epoch:3; batch 24704; train accuracy: 0.781753\n",
      "epoch 3; batch 24832; loss 0.163614\n",
      "epoch:3; batch 24832; train accuracy: 0.781838\n",
      "epoch 3; batch 24960; loss 0.225799\n",
      "epoch:3; batch 24960; train accuracy: 0.781914\n",
      "epoch 3; batch 25088; loss 0.160970\n",
      "epoch:3; batch 25088; train accuracy: 0.782007\n",
      "epoch 3; batch 25216; loss 0.148316\n",
      "epoch:3; batch 25216; train accuracy: 0.782101\n",
      "epoch 3; batch 25344; loss 0.203655\n",
      "epoch:3; batch 25344; train accuracy: 0.782189\n",
      "epoch 3; batch 25472; loss 0.112778\n",
      "epoch:3; batch 25472; train accuracy: 0.782282\n",
      "epoch 3; batch 25600; loss 0.141482\n",
      "epoch:3; batch 25600; train accuracy: 0.782367\n",
      "epoch 3; batch 25728; loss 0.249771\n",
      "epoch:3; batch 25728; train accuracy: 0.782442\n",
      "epoch 3; batch 25856; loss 0.147503\n",
      "epoch:3; batch 25856; train accuracy: 0.782531\n",
      "epoch 3; batch 25984; loss 0.252889\n",
      "epoch:3; batch 25984; train accuracy: 0.782619\n",
      "epoch 3; batch 26112; loss 0.162023\n",
      "epoch:3; batch 26112; train accuracy: 0.782712\n",
      "epoch 3; batch 26240; loss 0.140150\n",
      "epoch:3; batch 26240; train accuracy: 0.782800\n",
      "epoch 3; batch 26368; loss 0.125999\n",
      "epoch:3; batch 26368; train accuracy: 0.782892\n",
      "epoch 3; batch 26496; loss 0.148487\n",
      "epoch:3; batch 26496; train accuracy: 0.782993\n",
      "epoch 3; batch 26624; loss 0.096418\n",
      "epoch:3; batch 26624; train accuracy: 0.783081\n",
      "epoch 3; batch 26752; loss 0.115787\n",
      "epoch:3; batch 26752; train accuracy: 0.783189\n",
      "epoch 3; batch 26880; loss 0.120094\n",
      "epoch:3; batch 26880; train accuracy: 0.783281\n",
      "epoch 3; batch 27008; loss 0.138525\n",
      "epoch:3; batch 27008; train accuracy: 0.783369\n",
      "epoch 3; batch 27136; loss 0.199253\n",
      "epoch:3; batch 27136; train accuracy: 0.783448\n",
      "epoch 3; batch 27264; loss 0.145478\n",
      "epoch:3; batch 27264; train accuracy: 0.783535\n",
      "epoch 3; batch 27392; loss 0.194682\n",
      "epoch:3; batch 27392; train accuracy: 0.783614\n",
      "epoch 3; batch 27520; loss 0.112193\n",
      "epoch:3; batch 27520; train accuracy: 0.783714\n",
      "epoch 3; batch 27648; loss 0.205650\n",
      "epoch:3; batch 27648; train accuracy: 0.783792\n",
      "epoch 3; batch 27776; loss 0.151052\n",
      "epoch:3; batch 27776; train accuracy: 0.783888\n",
      "epoch 3; batch 27904; loss 0.111513\n",
      "epoch:3; batch 27904; train accuracy: 0.783979\n",
      "epoch 3; batch 28032; loss 0.073132\n",
      "epoch:3; batch 28032; train accuracy: 0.784083\n",
      "epoch 3; batch 28160; loss 0.166197\n",
      "epoch:3; batch 28160; train accuracy: 0.784165\n",
      "epoch 3; batch 28288; loss 0.130997\n",
      "epoch:3; batch 28288; train accuracy: 0.784256\n",
      "epoch 3; batch 28416; loss 0.220557\n",
      "epoch:3; batch 28416; train accuracy: 0.784346\n",
      "epoch 3; batch 28544; loss 0.149961\n",
      "epoch:3; batch 28544; train accuracy: 0.784429\n",
      "epoch 3; batch 28672; loss 0.153370\n",
      "epoch:3; batch 28672; train accuracy: 0.784511\n",
      "epoch 3; batch 28800; loss 0.144951\n",
      "epoch:3; batch 28800; train accuracy: 0.784605\n",
      "epoch 3; batch 28928; loss 0.145018\n",
      "epoch:3; batch 28928; train accuracy: 0.784700\n",
      "epoch 3; batch 29056; loss 0.154684\n",
      "epoch:3; batch 29056; train accuracy: 0.784782\n",
      "epoch 3; batch 29184; loss 0.257244\n",
      "epoch:3; batch 29184; train accuracy: 0.784843\n",
      "epoch 3; batch 29312; loss 0.162632\n",
      "epoch:3; batch 29312; train accuracy: 0.784928\n",
      "epoch 3; batch 29440; loss 0.101066\n",
      "epoch:3; batch 29440; train accuracy: 0.785023\n",
      "epoch 3; batch 29568; loss 0.158780\n",
      "epoch:3; batch 29568; train accuracy: 0.785104\n",
      "epoch 3; batch 29696; loss 0.097226\n",
      "epoch:3; batch 29696; train accuracy: 0.785190\n",
      "epoch 3; batch 29824; loss 0.241101\n",
      "epoch:3; batch 29824; train accuracy: 0.785258\n",
      "epoch 3; batch 29952; loss 0.185652\n",
      "epoch:3; batch 29952; train accuracy: 0.785340\n",
      "epoch 3; batch 30080; loss 0.154407\n",
      "epoch:3; batch 30080; train accuracy: 0.785421\n",
      "epoch 3; batch 30208; loss 0.215345\n",
      "epoch:3; batch 30208; train accuracy: 0.785502\n",
      "epoch 3; batch 30336; loss 0.172534\n",
      "epoch:3; batch 30336; train accuracy: 0.785587\n",
      "epoch 3; batch 30464; loss 0.159818\n",
      "epoch:3; batch 30464; train accuracy: 0.785672\n",
      "epoch 3; batch 30592; loss 0.269357\n",
      "epoch:3; batch 30592; train accuracy: 0.785732\n",
      "epoch 3; batch 30720; loss 0.193817\n",
      "epoch:3; batch 30720; train accuracy: 0.785813\n",
      "epoch 3; batch 30848; loss 0.166768\n",
      "epoch:3; batch 30848; train accuracy: 0.785893\n",
      "epoch 3; batch 30976; loss 0.144945\n",
      "epoch:3; batch 30976; train accuracy: 0.785982\n",
      "epoch 3; batch 31104; loss 0.094718\n",
      "epoch:3; batch 31104; train accuracy: 0.786084\n",
      "epoch 3; batch 31232; loss 0.082025\n",
      "epoch:3; batch 31232; train accuracy: 0.786176\n",
      "epoch 3; batch 31360; loss 0.198094\n",
      "epoch:3; batch 31360; train accuracy: 0.786252\n",
      "epoch 3; batch 31488; loss 0.132922\n",
      "epoch:3; batch 31488; train accuracy: 0.786337\n",
      "epoch 3; batch 31616; loss 0.190093\n",
      "epoch:3; batch 31616; train accuracy: 0.786404\n",
      "epoch 3; batch 31744; loss 0.106396\n",
      "epoch:3; batch 31744; train accuracy: 0.786489\n",
      "epoch 3; batch 31872; loss 0.111664\n",
      "epoch:3; batch 31872; train accuracy: 0.786581\n",
      "epoch 3; batch 32000; loss 0.097546\n",
      "epoch:3; batch 32000; train accuracy: 0.786682\n",
      "epoch 3; batch 32128; loss 0.207106\n",
      "epoch:3; batch 32128; train accuracy: 0.786757\n",
      "epoch 3; batch 32256; loss 0.257918\n",
      "epoch:3; batch 32256; train accuracy: 0.786829\n",
      "epoch 3; batch 32384; loss 0.140959\n",
      "epoch:3; batch 32384; train accuracy: 0.786916\n",
      "epoch 3; batch 32512; loss 0.145610\n",
      "epoch:3; batch 32512; train accuracy: 0.787000\n",
      "epoch 3; batch 32640; loss 0.205784\n",
      "epoch:3; batch 32640; train accuracy: 0.787075\n",
      "epoch 3; batch 32768; loss 0.068886\n",
      "epoch:3; batch 32768; train accuracy: 0.787175\n",
      "epoch 3; batch 32896; loss 0.143041\n",
      "epoch:3; batch 32896; train accuracy: 0.787259\n",
      "epoch 3; batch 33024; loss 0.154695\n",
      "epoch:3; batch 33024; train accuracy: 0.787342\n",
      "epoch 3; batch 33152; loss 0.183510\n",
      "epoch:3; batch 33152; train accuracy: 0.787425\n",
      "epoch 3; batch 33280; loss 0.093270\n",
      "epoch:3; batch 33280; train accuracy: 0.787512\n",
      "epoch 3; batch 33408; loss 0.154397\n",
      "epoch:3; batch 33408; train accuracy: 0.787591\n",
      "epoch 3; batch 33536; loss 0.102799\n",
      "epoch:3; batch 33536; train accuracy: 0.787691\n",
      "epoch 3; batch 33664; loss 0.215682\n",
      "epoch:3; batch 33664; train accuracy: 0.787769\n",
      "epoch 3; batch 33792; loss 0.094063\n",
      "epoch:3; batch 33792; train accuracy: 0.787868\n",
      "epoch 3; batch 33920; loss 0.098842\n",
      "epoch:3; batch 33920; train accuracy: 0.787963\n",
      "epoch 3; batch 34048; loss 0.154785\n",
      "epoch:3; batch 34048; train accuracy: 0.788042\n",
      "epoch 3; batch 34176; loss 0.078162\n",
      "epoch:3; batch 34176; train accuracy: 0.788137\n",
      "epoch 3; batch 34304; loss 0.112874\n",
      "epoch:3; batch 34304; train accuracy: 0.788219\n",
      "epoch 3; batch 34432; loss 0.111535\n",
      "epoch:3; batch 34432; train accuracy: 0.788310\n",
      "epoch 3; batch 34560; loss 0.077601\n",
      "epoch:3; batch 34560; train accuracy: 0.788404\n",
      "epoch 3; batch 34688; loss 0.071303\n",
      "epoch:3; batch 34688; train accuracy: 0.788502\n",
      "epoch 3; batch 34816; loss 0.107434\n",
      "epoch:3; batch 34816; train accuracy: 0.788597\n",
      "epoch 3; batch 34944; loss 0.185386\n",
      "epoch:3; batch 34944; train accuracy: 0.788670\n",
      "epoch 3; batch 35072; loss 0.243597\n",
      "epoch:3; batch 35072; train accuracy: 0.788744\n",
      "epoch 3; batch 35200; loss 0.182762\n",
      "epoch:3; batch 35200; train accuracy: 0.788814\n",
      "epoch 3; batch 35328; loss 0.057620\n",
      "epoch:3; batch 35328; train accuracy: 0.788912\n",
      "epoch 3; batch 35456; loss 0.059370\n",
      "epoch:3; batch 35456; train accuracy: 0.789014\n",
      "epoch 3; batch 35584; loss 0.114083\n",
      "epoch:3; batch 35584; train accuracy: 0.789107\n",
      "epoch 3; batch 35712; loss 0.203359\n",
      "epoch:3; batch 35712; train accuracy: 0.789181\n",
      "epoch 3; batch 35840; loss 0.077186\n",
      "epoch:3; batch 35840; train accuracy: 0.789278\n",
      "epoch 3; batch 35968; loss 0.237963\n",
      "epoch:3; batch 35968; train accuracy: 0.789343\n",
      "epoch 3; batch 36096; loss 0.137368\n",
      "epoch:3; batch 36096; train accuracy: 0.789432\n",
      "epoch 3; batch 36224; loss 0.092991\n",
      "epoch:3; batch 36224; train accuracy: 0.789530\n",
      "epoch 3; batch 36352; loss 0.121010\n",
      "epoch:3; batch 36352; train accuracy: 0.789623\n",
      "epoch 3; batch 36480; loss 0.150452\n",
      "epoch:3; batch 36480; train accuracy: 0.789704\n",
      "epoch 3; batch 36608; loss 0.110459\n",
      "epoch:3; batch 36608; train accuracy: 0.789793\n",
      "epoch 3; batch 36736; loss 0.098594\n",
      "epoch:3; batch 36736; train accuracy: 0.789890\n",
      "epoch 3; batch 36864; loss 0.153780\n",
      "epoch:3; batch 36864; train accuracy: 0.789970\n",
      "epoch 3; batch 36992; loss 0.125232\n",
      "epoch:3; batch 36992; train accuracy: 0.790047\n",
      "epoch 3; batch 37120; loss 0.075113\n",
      "epoch:3; batch 37120; train accuracy: 0.790143\n",
      "epoch 3; batch 37248; loss 0.102657\n",
      "epoch:3; batch 37248; train accuracy: 0.790236\n",
      "epoch 3; batch 37376; loss 0.073810\n",
      "epoch:3; batch 37376; train accuracy: 0.790332\n",
      "epoch 3; batch 37504; loss 0.082907\n",
      "epoch:3; batch 37504; train accuracy: 0.790429\n",
      "epoch 3; batch 37632; loss 0.087676\n",
      "epoch:3; batch 37632; train accuracy: 0.790517\n",
      "epoch 3; batch 37760; loss 0.132440\n",
      "epoch:3; batch 37760; train accuracy: 0.790605\n",
      "epoch 3; batch 37888; loss 0.089969\n",
      "epoch:3; batch 37888; train accuracy: 0.790701\n",
      "epoch 3; batch 38016; loss 0.069405\n",
      "epoch:3; batch 38016; train accuracy: 0.790797\n",
      "epoch 3; batch 38144; loss 0.073832\n",
      "epoch:3; batch 38144; train accuracy: 0.790889\n",
      "epoch 3; batch 38272; loss 0.071798\n",
      "epoch:3; batch 38272; train accuracy: 0.790976\n",
      "epoch 3; batch 38400; loss 0.081830\n",
      "epoch:3; batch 38400; train accuracy: 0.791068\n",
      "epoch 3; batch 38528; loss 0.077069\n",
      "epoch:3; batch 38528; train accuracy: 0.791164\n",
      "epoch 3; batch 38656; loss 0.140752\n",
      "epoch:3; batch 38656; train accuracy: 0.791251\n",
      "epoch 3; batch 38784; loss 0.091794\n",
      "epoch:3; batch 38784; train accuracy: 0.791330\n",
      "epoch 3; batch 38912; loss 0.047305\n",
      "epoch:3; batch 38912; train accuracy: 0.791434\n",
      "epoch 3; batch 39040; loss 0.130727\n",
      "epoch:3; batch 39040; train accuracy: 0.791521\n",
      "epoch 3; batch 39168; loss 0.203221\n",
      "epoch:3; batch 39168; train accuracy: 0.791604\n",
      "epoch 3; batch 39296; loss 0.061053\n",
      "epoch:3; batch 39296; train accuracy: 0.791699\n",
      "epoch 3; batch 39424; loss 0.101061\n",
      "epoch:3; batch 39424; train accuracy: 0.791786\n",
      "epoch 3; batch 39552; loss 0.073362\n",
      "epoch:3; batch 39552; train accuracy: 0.791880\n",
      "epoch 3; batch 39680; loss 0.043379\n",
      "epoch:3; batch 39680; train accuracy: 0.791979\n",
      "epoch 3; batch 39808; loss 0.116576\n",
      "epoch:3; batch 39808; train accuracy: 0.792058\n",
      "epoch 3; batch 39936; loss 0.075458\n",
      "epoch:3; batch 39936; train accuracy: 0.792144\n",
      "epoch 3; batch 40064; loss 0.081351\n",
      "epoch:3; batch 40064; train accuracy: 0.792235\n",
      "epoch 3; batch 40192; loss 0.150082\n",
      "epoch:3; batch 40192; train accuracy: 0.792321\n",
      "epoch 3; batch 40320; loss 0.090575\n",
      "epoch:3; batch 40320; train accuracy: 0.792411\n",
      "epoch 3; batch 40448; loss 0.130839\n",
      "epoch:3; batch 40448; train accuracy: 0.792481\n",
      "epoch 3; batch 40576; loss 0.117245\n",
      "epoch:3; batch 40576; train accuracy: 0.792564\n",
      "epoch 3; batch 40704; loss 0.141292\n",
      "epoch:3; batch 40704; train accuracy: 0.792646\n",
      "epoch 3; batch 40832; loss 0.151339\n",
      "epoch:3; batch 40832; train accuracy: 0.792727\n",
      "epoch 3; batch 40960; loss 0.106715\n",
      "epoch:3; batch 40960; train accuracy: 0.792801\n",
      "epoch 3; batch 41088; loss 0.177344\n",
      "epoch:3; batch 41088; train accuracy: 0.792875\n",
      "epoch 3; batch 41216; loss 0.125908\n",
      "epoch:3; batch 41216; train accuracy: 0.792961\n",
      "epoch 3; batch 41344; loss 0.171902\n",
      "epoch:3; batch 41344; train accuracy: 0.793038\n",
      "epoch 3; batch 41472; loss 0.054212\n",
      "epoch:3; batch 41472; train accuracy: 0.793132\n",
      "epoch 3; batch 41600; loss 0.148541\n",
      "epoch:3; batch 41600; train accuracy: 0.793209\n",
      "epoch 3; batch 41728; loss 0.129224\n",
      "epoch:3; batch 41728; train accuracy: 0.793291\n",
      "epoch 3; batch 41856; loss 0.163126\n",
      "epoch:3; batch 41856; train accuracy: 0.793360\n",
      "epoch 3; batch 41984; loss 0.090888\n",
      "epoch:3; batch 41984; train accuracy: 0.793449\n",
      "epoch 3; batch 42112; loss 0.145847\n",
      "epoch:3; batch 42112; train accuracy: 0.793534\n",
      "epoch 3; batch 42240; loss 0.174230\n",
      "epoch:3; batch 42240; train accuracy: 0.793603\n",
      "epoch 3; batch 42368; loss 0.099577\n",
      "epoch:3; batch 42368; train accuracy: 0.793688\n",
      "epoch 3; batch 42496; loss 0.199754\n",
      "epoch:3; batch 42496; train accuracy: 0.793773\n",
      "epoch 3; batch 42624; loss 0.128195\n",
      "epoch:3; batch 42624; train accuracy: 0.793850\n",
      "epoch 3; batch 42752; loss 0.112438\n",
      "epoch:3; batch 42752; train accuracy: 0.793930\n",
      "epoch 3; batch 42880; loss 0.100545\n",
      "epoch:3; batch 42880; train accuracy: 0.794007\n",
      "epoch 3; batch 43008; loss 0.099177\n",
      "epoch:3; batch 43008; train accuracy: 0.794092\n",
      "epoch 3; batch 43136; loss 0.072030\n",
      "epoch:3; batch 43136; train accuracy: 0.794184\n",
      "epoch 3; batch 43264; loss 0.106580\n",
      "epoch:3; batch 43264; train accuracy: 0.794280\n",
      "epoch 3; batch 43392; loss 0.137227\n",
      "epoch:3; batch 43392; train accuracy: 0.794352\n",
      "epoch 3; batch 43520; loss 0.096221\n",
      "epoch:3; batch 43520; train accuracy: 0.794429\n",
      "epoch 3; batch 43648; loss 0.065489\n",
      "epoch:3; batch 43648; train accuracy: 0.794525\n",
      "epoch 3; batch 43776; loss 0.151090\n",
      "epoch:3; batch 43776; train accuracy: 0.794612\n",
      "epoch 3; batch 43904; loss 0.067345\n",
      "epoch:3; batch 43904; train accuracy: 0.794708\n",
      "epoch 3; batch 44032; loss 0.149873\n",
      "epoch:3; batch 44032; train accuracy: 0.794780\n",
      "epoch 3; batch 44160; loss 0.217387\n",
      "epoch:3; batch 44160; train accuracy: 0.794832\n",
      "epoch 3; batch 44288; loss 0.065390\n",
      "epoch:3; batch 44288; train accuracy: 0.794920\n",
      "epoch 3; batch 44416; loss 0.085101\n",
      "epoch:3; batch 44416; train accuracy: 0.795007\n",
      "epoch 3; batch 44544; loss 0.120695\n",
      "epoch:3; batch 44544; train accuracy: 0.795087\n",
      "epoch 3; batch 44672; loss 0.075541\n",
      "epoch:3; batch 44672; train accuracy: 0.795182\n",
      "epoch 3; batch 44800; loss 0.103343\n",
      "epoch:3; batch 44800; train accuracy: 0.795273\n",
      "epoch 3; batch 44928; loss 0.117483\n",
      "epoch:3; batch 44928; train accuracy: 0.795353\n",
      "epoch 3; batch 45056; loss 0.064415\n",
      "epoch:3; batch 45056; train accuracy: 0.795451\n",
      "epoch 3; batch 45184; loss 0.041343\n",
      "epoch:3; batch 45184; train accuracy: 0.795550\n",
      "epoch 3; batch 45312; loss 0.195256\n",
      "epoch:3; batch 45312; train accuracy: 0.795617\n",
      "epoch 3; batch 45440; loss 0.101084\n",
      "epoch:3; batch 45440; train accuracy: 0.795704\n",
      "epoch 3; batch 45568; loss 0.074257\n",
      "epoch:3; batch 45568; train accuracy: 0.795795\n",
      "epoch 3; batch 45696; loss 0.126991\n",
      "epoch:3; batch 45696; train accuracy: 0.795885\n",
      "epoch 3; batch 45824; loss 0.171936\n",
      "epoch:3; batch 45824; train accuracy: 0.795952\n",
      "epoch 3; batch 45952; loss 0.086675\n",
      "epoch:3; batch 45952; train accuracy: 0.796043\n",
      "epoch 3; batch 46080; loss 0.093183\n",
      "epoch:3; batch 46080; train accuracy: 0.796125\n",
      "epoch 3; batch 46208; loss 0.071562\n",
      "epoch:3; batch 46208; train accuracy: 0.796215\n",
      "epoch 3; batch 46336; loss 0.066436\n",
      "epoch:3; batch 46336; train accuracy: 0.796313\n",
      "epoch 3; batch 46464; loss 0.134043\n",
      "epoch:3; batch 46464; train accuracy: 0.796391\n",
      "epoch 3; batch 46592; loss 0.069400\n",
      "epoch:3; batch 46592; train accuracy: 0.796481\n",
      "epoch 3; batch 46720; loss 0.052192\n",
      "epoch:3; batch 46720; train accuracy: 0.796579\n",
      "epoch 3; batch 46848; loss 0.142775\n",
      "epoch:3; batch 46848; train accuracy: 0.796657\n",
      "epoch 3; batch 46976; loss 0.062168\n",
      "epoch:3; batch 46976; train accuracy: 0.796747\n",
      "epoch 3; batch 47104; loss 0.103063\n",
      "epoch:3; batch 47104; train accuracy: 0.796836\n",
      "epoch 3; batch 47232; loss 0.156256\n",
      "epoch:3; batch 47232; train accuracy: 0.796918\n",
      "epoch 3; batch 47360; loss 0.140428\n",
      "epoch:3; batch 47360; train accuracy: 0.797003\n",
      "epoch 3; batch 47488; loss 0.038171\n",
      "epoch:3; batch 47488; train accuracy: 0.797100\n",
      "epoch 3; batch 47616; loss 0.125139\n",
      "epoch:3; batch 47616; train accuracy: 0.797186\n",
      "epoch 3; batch 47744; loss 0.100563\n",
      "epoch:3; batch 47744; train accuracy: 0.797275\n",
      "epoch 3; batch 47872; loss 0.081032\n",
      "epoch:3; batch 47872; train accuracy: 0.797356\n",
      "epoch 3; batch 48000; loss 0.118364\n",
      "epoch:3; batch 48000; train accuracy: 0.797437\n",
      "epoch 3; batch 48128; loss 0.086845\n",
      "epoch:3; batch 48128; train accuracy: 0.797518\n",
      "epoch 3; batch 48256; loss 0.124059\n",
      "epoch:3; batch 48256; train accuracy: 0.797588\n",
      "epoch 3; batch 48384; loss 0.063039\n",
      "epoch:3; batch 48384; train accuracy: 0.797676\n",
      "epoch 3; batch 48512; loss 0.046451\n",
      "epoch:3; batch 48512; train accuracy: 0.797769\n",
      "epoch 3; batch 48640; loss 0.039508\n",
      "epoch:3; batch 48640; train accuracy: 0.797869\n",
      "epoch 3; batch 48768; loss 0.060412\n",
      "epoch:3; batch 48768; train accuracy: 0.797961\n",
      "epoch 3; batch 48896; loss 0.101382\n",
      "epoch:3; batch 48896; train accuracy: 0.798038\n",
      "epoch 3; batch 49024; loss 0.092474\n",
      "epoch:3; batch 49024; train accuracy: 0.798111\n",
      "epoch 3; batch 49152; loss 0.063359\n",
      "epoch:3; batch 49152; train accuracy: 0.798199\n",
      "epoch 3; batch 49280; loss 0.099632\n",
      "epoch:3; batch 49280; train accuracy: 0.798279\n",
      "epoch 3; batch 49408; loss 0.125285\n",
      "epoch:3; batch 49408; train accuracy: 0.798360\n",
      "epoch 3; batch 49536; loss 0.180623\n",
      "epoch:3; batch 49536; train accuracy: 0.798432\n",
      "epoch 3; batch 49664; loss 0.066556\n",
      "epoch:3; batch 49664; train accuracy: 0.798516\n",
      "epoch 3; batch 49792; loss 0.142363\n",
      "epoch:3; batch 49792; train accuracy: 0.798588\n",
      "epoch 3; batch 49920; loss 0.084729\n",
      "epoch:3; batch 49920; train accuracy: 0.798672\n",
      "epoch 3; batch 50048; loss 0.042343\n",
      "epoch:3; batch 50048; train accuracy: 0.798768\n",
      "epoch 3; batch 50176; loss 0.075531\n",
      "epoch:3; batch 50176; train accuracy: 0.798851\n",
      "epoch 3; batch 50304; loss 0.100226\n",
      "epoch:3; batch 50304; train accuracy: 0.798935\n",
      "epoch 3; batch 50432; loss 0.119004\n",
      "epoch:3; batch 50432; train accuracy: 0.799014\n",
      "epoch 3; batch 50560; loss 0.217733\n",
      "epoch:3; batch 50560; train accuracy: 0.799079\n",
      "epoch 3; batch 50688; loss 0.107887\n",
      "epoch:3; batch 50688; train accuracy: 0.799162\n",
      "epoch 3; batch 50816; loss 0.063587\n",
      "epoch:3; batch 50816; train accuracy: 0.799253\n",
      "epoch 3; batch 50944; loss 0.145086\n",
      "epoch:3; batch 50944; train accuracy: 0.799332\n",
      "epoch 3; batch 51072; loss 0.169229\n",
      "epoch:3; batch 51072; train accuracy: 0.799411\n",
      "epoch 3; batch 51200; loss 0.179483\n",
      "epoch:3; batch 51200; train accuracy: 0.799471\n",
      "epoch 3; batch 51328; loss 0.071286\n",
      "epoch:3; batch 51328; train accuracy: 0.799558\n",
      "epoch 3; batch 51456; loss 0.065122\n",
      "epoch:3; batch 51456; train accuracy: 0.799641\n",
      "epoch 3; batch 51584; loss 0.107139\n",
      "epoch:3; batch 51584; train accuracy: 0.799720\n",
      "epoch 3; batch 51712; loss 0.085171\n",
      "epoch:3; batch 51712; train accuracy: 0.799803\n",
      "epoch 3; batch 51840; loss 0.068597\n",
      "epoch:3; batch 51840; train accuracy: 0.799885\n",
      "epoch 3; batch 51968; loss 0.086350\n",
      "epoch:3; batch 51968; train accuracy: 0.799968\n",
      "epoch 3; batch 52096; loss 0.124553\n",
      "epoch:3; batch 52096; train accuracy: 0.800054\n",
      "epoch 3; batch 52224; loss 0.118805\n",
      "epoch:3; batch 52224; train accuracy: 0.800125\n",
      "epoch 3; batch 52352; loss 0.157688\n",
      "epoch:3; batch 52352; train accuracy: 0.800192\n",
      "epoch 3; batch 52480; loss 0.133364\n",
      "epoch:3; batch 52480; train accuracy: 0.800267\n",
      "epoch 3; batch 52608; loss 0.138979\n",
      "epoch:3; batch 52608; train accuracy: 0.800330\n",
      "epoch 3; batch 52736; loss 0.116960\n",
      "epoch:3; batch 52736; train accuracy: 0.800404\n",
      "epoch 3; batch 52864; loss 0.132154\n",
      "epoch:3; batch 52864; train accuracy: 0.800479\n",
      "epoch 3; batch 52992; loss 0.140390\n",
      "epoch:3; batch 52992; train accuracy: 0.800561\n",
      "epoch 3; batch 53120; loss 0.053226\n",
      "epoch:3; batch 53120; train accuracy: 0.800646\n",
      "epoch 3; batch 53248; loss 0.160143\n",
      "epoch:3; batch 53248; train accuracy: 0.800728\n",
      "epoch 3; batch 53376; loss 0.051213\n",
      "epoch:3; batch 53376; train accuracy: 0.800821\n",
      "epoch 3; batch 53504; loss 0.063858\n",
      "epoch:3; batch 53504; train accuracy: 0.800910\n",
      "epoch 3; batch 53632; loss 0.112431\n",
      "epoch:3; batch 53632; train accuracy: 0.800988\n",
      "epoch 3; batch 53760; loss 0.120223\n",
      "epoch:3; batch 53760; train accuracy: 0.801062\n",
      "epoch 3; batch 53888; loss 0.067067\n",
      "epoch:3; batch 53888; train accuracy: 0.801151\n",
      "epoch 3; batch 54016; loss 0.089382\n",
      "epoch:3; batch 54016; train accuracy: 0.801232\n",
      "epoch 3; batch 54144; loss 0.062316\n",
      "epoch:3; batch 54144; train accuracy: 0.801321\n",
      "epoch 3; batch 54272; loss 0.064739\n",
      "epoch:3; batch 54272; train accuracy: 0.801413\n",
      "epoch 3; batch 54400; loss 0.061358\n",
      "epoch:3; batch 54400; train accuracy: 0.801498\n",
      "epoch 3; batch 54528; loss 0.085029\n",
      "epoch:3; batch 54528; train accuracy: 0.801575\n",
      "epoch 3; batch 54656; loss 0.067069\n",
      "epoch:3; batch 54656; train accuracy: 0.801664\n",
      "epoch 3; batch 54784; loss 0.094288\n",
      "epoch:3; batch 54784; train accuracy: 0.801745\n",
      "epoch 3; batch 54912; loss 0.089005\n",
      "epoch:3; batch 54912; train accuracy: 0.801825\n",
      "epoch 3; batch 55040; loss 0.170327\n",
      "epoch:3; batch 55040; train accuracy: 0.801883\n",
      "epoch 3; batch 55168; loss 0.043615\n",
      "epoch:3; batch 55168; train accuracy: 0.801979\n",
      "epoch 3; batch 55296; loss 0.065311\n",
      "epoch:3; batch 55296; train accuracy: 0.802059\n",
      "epoch 3; batch 55424; loss 0.126021\n",
      "epoch:3; batch 55424; train accuracy: 0.802129\n",
      "epoch 3; batch 55552; loss 0.065849\n",
      "epoch:3; batch 55552; train accuracy: 0.802213\n",
      "epoch 3; batch 55680; loss 0.105158\n",
      "epoch:3; batch 55680; train accuracy: 0.802293\n",
      "epoch 3; batch 55808; loss 0.088155\n",
      "epoch:3; batch 55808; train accuracy: 0.802369\n",
      "epoch 3; batch 55936; loss 0.135683\n",
      "epoch:3; batch 55936; train accuracy: 0.802449\n",
      "epoch 3; batch 56064; loss 0.087703\n",
      "epoch:3; batch 56064; train accuracy: 0.802529\n",
      "epoch 3; batch 56192; loss 0.133508\n",
      "epoch:3; batch 56192; train accuracy: 0.802598\n",
      "epoch 3; batch 56320; loss 0.209652\n",
      "epoch:3; batch 56320; train accuracy: 0.802659\n",
      "epoch 3; batch 56448; loss 0.108485\n",
      "epoch:3; batch 56448; train accuracy: 0.802743\n",
      "epoch 3; batch 56576; loss 0.198675\n",
      "epoch:3; batch 56576; train accuracy: 0.802815\n",
      "epoch 3; batch 56704; loss 0.078559\n",
      "epoch:3; batch 56704; train accuracy: 0.802895\n",
      "epoch 3; batch 56832; loss 0.059529\n",
      "epoch:3; batch 56832; train accuracy: 0.802978\n",
      "epoch 3; batch 56960; loss 0.135843\n",
      "epoch:3; batch 56960; train accuracy: 0.803054\n",
      "epoch 3; batch 57088; loss 0.066573\n",
      "epoch:3; batch 57088; train accuracy: 0.803141\n",
      "epoch 3; batch 57216; loss 0.065142\n",
      "epoch:3; batch 57216; train accuracy: 0.803228\n",
      "epoch 3; batch 57344; loss 0.068090\n",
      "epoch:3; batch 57344; train accuracy: 0.803311\n",
      "epoch 3; batch 57472; loss 0.142427\n",
      "epoch:3; batch 57472; train accuracy: 0.803386\n",
      "epoch 3; batch 57600; loss 0.101399\n",
      "epoch:3; batch 57600; train accuracy: 0.803469\n",
      "epoch 3; batch 57728; loss 0.062690\n",
      "epoch:3; batch 57728; train accuracy: 0.803552\n",
      "epoch 3; batch 57856; loss 0.102326\n",
      "epoch:3; batch 57856; train accuracy: 0.803623\n",
      "epoch 3; batch 57984; loss 0.209314\n",
      "epoch:3; batch 57984; train accuracy: 0.803683\n",
      "epoch 3; batch 58112; loss 0.095584\n",
      "epoch:3; batch 58112; train accuracy: 0.803762\n",
      "epoch 3; batch 58240; loss 0.099235\n",
      "epoch:3; batch 58240; train accuracy: 0.803837\n",
      "epoch 3; batch 58368; loss 0.102132\n",
      "epoch:3; batch 58368; train accuracy: 0.803920\n",
      "epoch 3; batch 58496; loss 0.110291\n",
      "epoch:3; batch 58496; train accuracy: 0.803991\n",
      "epoch 3; batch 58624; loss 0.103468\n",
      "epoch:3; batch 58624; train accuracy: 0.804058\n",
      "epoch 3; batch 58752; loss 0.113147\n",
      "epoch:3; batch 58752; train accuracy: 0.804140\n",
      "epoch 3; batch 58880; loss 0.104869\n",
      "epoch:3; batch 58880; train accuracy: 0.804215\n",
      "epoch 3; batch 59008; loss 0.142270\n",
      "epoch:3; batch 59008; train accuracy: 0.804286\n",
      "epoch 3; batch 59136; loss 0.169977\n",
      "epoch:3; batch 59136; train accuracy: 0.804353\n",
      "epoch 3; batch 59264; loss 0.232560\n",
      "epoch:3; batch 59264; train accuracy: 0.804416\n",
      "epoch 3; batch 59392; loss 0.123365\n",
      "epoch:3; batch 59392; train accuracy: 0.804483\n",
      "epoch 3; batch 59520; loss 0.066614\n",
      "epoch:3; batch 59520; train accuracy: 0.804569\n",
      "epoch 3; batch 59648; loss 0.127511\n",
      "epoch:3; batch 59648; train accuracy: 0.804632\n",
      "epoch 3; batch 59776; loss 0.044128\n",
      "epoch:3; batch 59776; train accuracy: 0.804717\n",
      "epoch 3; batch 59904; loss 0.045972\n",
      "epoch:3; batch 59904; train accuracy: 0.804806\n",
      "epoch 3; batch 60032; loss 0.076034\n",
      "epoch:3; batch 60032; train accuracy: 0.804888\n",
      "epoch 3; batch 60160; loss 0.064612\n",
      "epoch:3; batch 60160; train accuracy: 0.804973\n",
      "epoch 3; batch 60288; loss 0.078933\n",
      "epoch:3; batch 60288; train accuracy: 0.805046\n",
      "epoch 3; batch 60416; loss 0.067591\n",
      "epoch:3; batch 60416; train accuracy: 0.805128\n",
      "epoch 3; batch 60544; loss 0.189791\n",
      "epoch:3; batch 60544; train accuracy: 0.805183\n",
      "epoch 3; batch 60672; loss 0.086901\n",
      "epoch:3; batch 60672; train accuracy: 0.805249\n",
      "epoch 3; batch 60800; loss 0.063373\n",
      "epoch:3; batch 60800; train accuracy: 0.805334\n",
      "epoch 3; batch 60928; loss 0.114735\n",
      "epoch:3; batch 60928; train accuracy: 0.805411\n",
      "epoch 3; batch 61056; loss 0.089262\n",
      "epoch:3; batch 61056; train accuracy: 0.805488\n",
      "epoch 3; batch 61184; loss 0.099949\n",
      "epoch:3; batch 61184; train accuracy: 0.805565\n",
      "epoch 3; batch 61312; loss 0.111396\n",
      "epoch:3; batch 61312; train accuracy: 0.805635\n",
      "epoch 3; batch 61440; loss 0.136383\n",
      "epoch:3; batch 61440; train accuracy: 0.805708\n",
      "epoch 3; batch 61568; loss 0.090721\n",
      "epoch:3; batch 61568; train accuracy: 0.805778\n",
      "epoch 3; batch 61696; loss 0.068689\n",
      "epoch:3; batch 61696; train accuracy: 0.805862\n",
      "epoch 3; batch 61824; loss 0.088491\n",
      "epoch:3; batch 61824; train accuracy: 0.805939\n",
      "epoch 3; batch 61952; loss 0.095407\n",
      "epoch:3; batch 61952; train accuracy: 0.806012\n",
      "epoch 3; batch 62080; loss 0.071063\n",
      "epoch:3; batch 62080; train accuracy: 0.806092\n",
      "epoch 3; batch 62208; loss 0.101333\n",
      "epoch:3; batch 62208; train accuracy: 0.806161\n",
      "epoch 3; batch 62336; loss 0.099114\n",
      "epoch:3; batch 62336; train accuracy: 0.806226\n",
      "epoch 3; batch 62464; loss 0.028180\n",
      "epoch:3; batch 62464; train accuracy: 0.806314\n",
      "epoch 3; batch 62592; loss 0.087574\n",
      "epoch:3; batch 62592; train accuracy: 0.806390\n",
      "epoch 3; batch 62720; loss 0.043623\n",
      "epoch:3; batch 62720; train accuracy: 0.806477\n",
      "epoch 3; batch 62848; loss 0.080137\n",
      "epoch:3; batch 62848; train accuracy: 0.806557\n",
      "epoch 3; batch 62976; loss 0.323305\n",
      "epoch:3; batch 62976; train accuracy: 0.806611\n",
      "epoch 3; batch 63104; loss 0.096400\n",
      "epoch:3; batch 63104; train accuracy: 0.806684\n",
      "epoch 3; batch 63232; loss 0.075882\n",
      "epoch:3; batch 63232; train accuracy: 0.806752\n",
      "epoch 3; batch 63360; loss 0.095307\n",
      "epoch:3; batch 63360; train accuracy: 0.806835\n",
      "epoch 3; batch 63488; loss 0.114506\n",
      "epoch:3; batch 63488; train accuracy: 0.806915\n",
      "epoch 3; batch 63616; loss 0.108742\n",
      "epoch:3; batch 63616; train accuracy: 0.806987\n",
      "epoch 3; batch 63744; loss 0.096826\n",
      "epoch:3; batch 63744; train accuracy: 0.807055\n",
      "epoch 3; batch 63872; loss 0.183460\n",
      "epoch:3; batch 63872; train accuracy: 0.807124\n",
      "epoch 3; batch 64000; loss 0.109174\n",
      "epoch:3; batch 64000; train accuracy: 0.807196\n",
      "epoch 3; batch 64128; loss 0.171130\n",
      "epoch:3; batch 64128; train accuracy: 0.807253\n",
      "epoch 3; batch 64256; loss 0.108192\n",
      "epoch:3; batch 64256; train accuracy: 0.807324\n",
      "epoch 3; batch 64384; loss 0.110512\n",
      "epoch:3; batch 64384; train accuracy: 0.807400\n",
      "epoch 3; batch 64512; loss 0.115400\n",
      "epoch:3; batch 64512; train accuracy: 0.807468\n",
      "epoch 3; batch 64640; loss 0.159794\n",
      "epoch:3; batch 64640; train accuracy: 0.807532\n",
      "epoch 3; batch 64768; loss 0.179049\n",
      "epoch:3; batch 64768; train accuracy: 0.807593\n",
      "epoch 3; batch 64896; loss 0.077571\n",
      "epoch:3; batch 64896; train accuracy: 0.807671\n",
      "epoch 3; batch 65024; loss 0.037534\n",
      "epoch:3; batch 65024; train accuracy: 0.807754\n",
      "epoch 3; batch 65152; loss 0.112491\n",
      "epoch:3; batch 65152; train accuracy: 0.807825\n",
      "epoch 3; batch 65280; loss 0.065168\n",
      "epoch:3; batch 65280; train accuracy: 0.807907\n",
      "epoch 3; batch 65408; loss 0.094266\n",
      "epoch:3; batch 65408; train accuracy: 0.807978\n",
      "epoch 3; batch 65536; loss 0.082877\n",
      "epoch:3; batch 65536; train accuracy: 0.808053\n",
      "epoch 3; batch 65664; loss 0.101817\n",
      "epoch:3; batch 65664; train accuracy: 0.808127\n",
      "epoch 3; batch 65792; loss 0.074880\n",
      "epoch:3; batch 65792; train accuracy: 0.808209\n",
      "epoch 3; batch 65920; loss 0.042066\n",
      "epoch:3; batch 65920; train accuracy: 0.808295\n",
      "epoch 3; batch 66048; loss 0.089873\n",
      "epoch:3; batch 66048; train accuracy: 0.808365\n",
      "epoch 3; batch 66176; loss 0.082149\n",
      "epoch:3; batch 66176; train accuracy: 0.808440\n",
      "epoch 3; batch 66304; loss 0.027530\n",
      "epoch:3; batch 66304; train accuracy: 0.808525\n",
      "epoch 3; batch 66432; loss 0.094594\n",
      "epoch:3; batch 66432; train accuracy: 0.808606\n",
      "epoch 3; batch 66560; loss 0.092112\n",
      "epoch:3; batch 66560; train accuracy: 0.808684\n",
      "epoch 3; batch 66688; loss 0.109215\n",
      "epoch:3; batch 66688; train accuracy: 0.808758\n",
      "epoch 3; batch 66816; loss 0.035544\n",
      "epoch:3; batch 66816; train accuracy: 0.808843\n",
      "epoch 3; batch 66944; loss 0.100854\n",
      "epoch:3; batch 66944; train accuracy: 0.808913\n",
      "epoch 3; batch 67072; loss 0.070122\n",
      "epoch:3; batch 67072; train accuracy: 0.808987\n",
      "epoch 3; batch 67200; loss 0.038586\n",
      "epoch:3; batch 67200; train accuracy: 0.809072\n",
      "epoch 3; batch 67328; loss 0.054712\n",
      "epoch:3; batch 67328; train accuracy: 0.809153\n",
      "epoch 3; batch 67456; loss 0.142350\n",
      "epoch:3; batch 67456; train accuracy: 0.809226\n",
      "epoch 3; batch 67584; loss 0.152429\n",
      "epoch:3; batch 67584; train accuracy: 0.809282\n",
      "epoch 3; batch 67712; loss 0.160213\n",
      "epoch:3; batch 67712; train accuracy: 0.809348\n",
      "epoch 3; batch 67840; loss 0.133707\n",
      "epoch:3; batch 67840; train accuracy: 0.809422\n",
      "epoch 3; batch 67968; loss 0.081757\n",
      "epoch:3; batch 67968; train accuracy: 0.809499\n",
      "epoch 3; batch 68096; loss 0.148058\n",
      "epoch:3; batch 68096; train accuracy: 0.809558\n",
      "epoch 3; batch 68224; loss 0.062044\n",
      "epoch:3; batch 68224; train accuracy: 0.809635\n",
      "epoch 3; batch 68352; loss 0.064362\n",
      "epoch:3; batch 68352; train accuracy: 0.809708\n",
      "epoch 3; batch 68480; loss 0.153841\n",
      "epoch:3; batch 68480; train accuracy: 0.809774\n",
      "epoch 3; batch 68608; loss 0.111771\n",
      "epoch:3; batch 68608; train accuracy: 0.809836\n",
      "epoch 3; batch 68736; loss 0.063987\n",
      "epoch:3; batch 68736; train accuracy: 0.809913\n",
      "epoch 3; batch 68864; loss 0.148103\n",
      "epoch:3; batch 68864; train accuracy: 0.809982\n",
      "epoch 3; batch 68992; loss 0.077065\n",
      "epoch:3; batch 68992; train accuracy: 0.810062\n",
      "epoch 3; batch 69120; loss 0.079168\n",
      "epoch:3; batch 69120; train accuracy: 0.810135\n",
      "epoch 3; batch 69248; loss 0.095108\n",
      "epoch:3; batch 69248; train accuracy: 0.810207\n",
      "epoch 3; batch 69376; loss 0.095546\n",
      "epoch:3; batch 69376; train accuracy: 0.810280\n",
      "epoch 3; batch 69504; loss 0.090635\n",
      "epoch:3; batch 69504; train accuracy: 0.810349\n",
      "epoch 3; batch 69632; loss 0.083172\n",
      "epoch:3; batch 69632; train accuracy: 0.810425\n",
      "epoch 3; batch 69760; loss 0.022465\n",
      "epoch:3; batch 69760; train accuracy: 0.810512\n",
      "epoch 3; batch 69888; loss 0.041615\n",
      "epoch:3; batch 69888; train accuracy: 0.810592\n",
      "epoch 3; batch 70016; loss 0.058607\n",
      "epoch:3; batch 70016; train accuracy: 0.810667\n",
      "epoch 3; batch 70144; loss 0.030998\n",
      "epoch:3; batch 70144; train accuracy: 0.810754\n",
      "epoch 3; batch 70272; loss 0.078233\n",
      "epoch:3; batch 70272; train accuracy: 0.810830\n",
      "epoch 3; batch 70400; loss 0.178180\n",
      "epoch:3; batch 70400; train accuracy: 0.810898\n",
      "epoch 3; batch 70528; loss 0.125388\n",
      "epoch:3; batch 70528; train accuracy: 0.810974\n",
      "epoch 3; batch 70656; loss 0.047091\n",
      "epoch:3; batch 70656; train accuracy: 0.811053\n",
      "epoch 3; batch 70784; loss 0.061474\n",
      "epoch:3; batch 70784; train accuracy: 0.811125\n",
      "epoch 3; batch 70912; loss 0.072993\n",
      "epoch:3; batch 70912; train accuracy: 0.811193\n",
      "epoch 3; batch 71040; loss 0.132713\n",
      "epoch:3; batch 71040; train accuracy: 0.811254\n",
      "epoch 3; batch 71168; loss 0.040640\n",
      "epoch:3; batch 71168; train accuracy: 0.811337\n",
      "epoch 3; batch 71296; loss 0.084273\n",
      "epoch:3; batch 71296; train accuracy: 0.811412\n",
      "epoch 3; batch 71424; loss 0.066063\n",
      "epoch:3; batch 71424; train accuracy: 0.811487\n",
      "epoch 3; batch 71552; loss 0.200200\n",
      "epoch:3; batch 71552; train accuracy: 0.811551\n",
      "epoch 3; batch 71680; loss 0.110772\n",
      "epoch:3; batch 71680; train accuracy: 0.811619\n",
      "epoch 3; batch 71808; loss 0.059789\n",
      "epoch:3; batch 71808; train accuracy: 0.811698\n",
      "epoch 3; batch 71936; loss 0.119807\n",
      "epoch:3; batch 71936; train accuracy: 0.811769\n",
      "epoch 3; batch 72064; loss 0.124377\n",
      "epoch:3; batch 72064; train accuracy: 0.811840\n",
      "epoch 3; batch 72192; loss 0.046296\n",
      "epoch:3; batch 72192; train accuracy: 0.811922\n",
      "epoch 3; batch 72320; loss 0.046180\n",
      "epoch:3; batch 72320; train accuracy: 0.812004\n",
      "epoch 3; batch 72448; loss 0.110168\n",
      "epoch:3; batch 72448; train accuracy: 0.812075\n",
      "epoch 3; batch 72576; loss 0.033540\n",
      "epoch:3; batch 72576; train accuracy: 0.812153\n",
      "epoch 3; batch 72704; loss 0.052399\n",
      "epoch:3; batch 72704; train accuracy: 0.812228\n",
      "epoch 3; batch 72832; loss 0.076111\n",
      "epoch:3; batch 72832; train accuracy: 0.812302\n",
      "epoch 3; batch 72960; loss 0.045128\n",
      "epoch:3; batch 72960; train accuracy: 0.812376\n",
      "epoch 3; batch 73088; loss 0.033840\n",
      "epoch:3; batch 73088; train accuracy: 0.812461\n",
      "epoch 3; batch 73216; loss 0.051678\n",
      "epoch:3; batch 73216; train accuracy: 0.812535\n",
      "epoch 3; batch 73344; loss 0.055359\n",
      "epoch:3; batch 73344; train accuracy: 0.812613\n",
      "epoch 3; batch 73472; loss 0.053595\n",
      "epoch:3; batch 73472; train accuracy: 0.812694\n",
      "epoch 3; batch 73600; loss 0.051659\n",
      "epoch:3; batch 73600; train accuracy: 0.812765\n",
      "epoch 3; batch 73728; loss 0.039025\n",
      "epoch:3; batch 73728; train accuracy: 0.812842\n",
      "epoch 3; batch 73856; loss 0.084422\n",
      "epoch:3; batch 73856; train accuracy: 0.812919\n",
      "epoch 3; batch 73984; loss 0.068522\n",
      "epoch:3; batch 73984; train accuracy: 0.812993\n",
      "epoch 3; batch 74112; loss 0.170226\n",
      "epoch:3; batch 74112; train accuracy: 0.813056\n",
      "epoch 3; batch 74240; loss 0.077035\n",
      "epoch:3; batch 74240; train accuracy: 0.813119\n",
      "epoch 3; batch 74368; loss 0.175045\n",
      "epoch:3; batch 74368; train accuracy: 0.813189\n",
      "epoch 3; batch 74496; loss 0.058398\n",
      "epoch:3; batch 74496; train accuracy: 0.813263\n",
      "epoch 3; batch 74624; loss 0.080251\n",
      "epoch:3; batch 74624; train accuracy: 0.813336\n",
      "epoch 3; batch 74752; loss 0.090949\n",
      "epoch:3; batch 74752; train accuracy: 0.813406\n",
      "epoch 3; batch 74880; loss 0.065430\n",
      "epoch:3; batch 74880; train accuracy: 0.813476\n",
      "epoch 3; batch 75008; loss 0.083827\n",
      "epoch:3; batch 75008; train accuracy: 0.813549\n",
      "epoch 3; batch 75136; loss 0.155706\n",
      "epoch:3; batch 75136; train accuracy: 0.813612\n",
      "epoch 3; batch 75264; loss 0.197122\n",
      "epoch:3; batch 75264; train accuracy: 0.813664\n",
      "epoch 3; batch 75392; loss 0.017284\n",
      "epoch:3; batch 75392; train accuracy: 0.813744\n",
      "epoch 3; batch 75520; loss 0.022422\n",
      "epoch:3; batch 75520; train accuracy: 0.813824\n",
      "epoch 3; batch 75648; loss 0.061554\n",
      "epoch:3; batch 75648; train accuracy: 0.813901\n",
      "epoch 3; batch 75776; loss 0.052675\n",
      "epoch:3; batch 75776; train accuracy: 0.813977\n",
      "epoch 3; batch 75904; loss 0.061913\n",
      "epoch:3; batch 75904; train accuracy: 0.814043\n",
      "epoch 3; batch 76032; loss 0.122099\n",
      "epoch:3; batch 76032; train accuracy: 0.814109\n",
      "epoch 3; batch 76160; loss 0.134852\n",
      "epoch:3; batch 76160; train accuracy: 0.814174\n",
      "epoch 3; batch 76288; loss 0.068992\n",
      "epoch:3; batch 76288; train accuracy: 0.814243\n",
      "epoch 3; batch 76416; loss 0.027859\n",
      "epoch:3; batch 76416; train accuracy: 0.814323\n",
      "epoch 3; batch 76544; loss 0.068382\n",
      "epoch:3; batch 76544; train accuracy: 0.814396\n",
      "epoch 3; batch 76672; loss 0.111386\n",
      "epoch:3; batch 76672; train accuracy: 0.814454\n",
      "epoch 3; batch 76800; loss 0.082137\n",
      "epoch:3; batch 76800; train accuracy: 0.814519\n",
      "epoch 3; batch 76928; loss 0.044810\n",
      "epoch:3; batch 76928; train accuracy: 0.814602\n",
      "epoch 3; batch 77056; loss 0.112391\n",
      "epoch:3; batch 77056; train accuracy: 0.814674\n",
      "epoch 3; batch 77184; loss 0.092516\n",
      "epoch:3; batch 77184; train accuracy: 0.814736\n",
      "epoch 3; batch 77312; loss 0.165839\n",
      "epoch:3; batch 77312; train accuracy: 0.814798\n",
      "epoch 3; batch 77440; loss 0.123201\n",
      "epoch:3; batch 77440; train accuracy: 0.814856\n",
      "epoch 3; batch 77568; loss 0.074925\n",
      "epoch:3; batch 77568; train accuracy: 0.814921\n",
      "epoch 3; batch 77696; loss 0.053932\n",
      "epoch:3; batch 77696; train accuracy: 0.814989\n",
      "epoch 3; batch 77824; loss 0.067865\n",
      "epoch:3; batch 77824; train accuracy: 0.815058\n",
      "epoch 3; batch 77952; loss 0.059894\n",
      "epoch:3; batch 77952; train accuracy: 0.815130\n",
      "epoch 3; batch 78080; loss 0.031215\n",
      "epoch:3; batch 78080; train accuracy: 0.815208\n",
      "epoch 3; batch 78208; loss 0.100798\n",
      "epoch:3; batch 78208; train accuracy: 0.815270\n",
      "epoch 3; batch 78336; loss 0.096989\n",
      "epoch:3; batch 78336; train accuracy: 0.815341\n",
      "epoch 3; batch 78464; loss 0.077083\n",
      "epoch:3; batch 78464; train accuracy: 0.815409\n",
      "epoch 3; batch 78592; loss 0.050110\n",
      "epoch:3; batch 78592; train accuracy: 0.815484\n",
      "epoch 3; batch 78720; loss 0.063961\n",
      "epoch:3; batch 78720; train accuracy: 0.815559\n",
      "epoch 3; batch 78848; loss 0.053730\n",
      "epoch:3; batch 78848; train accuracy: 0.815634\n",
      "epoch 3; batch 78976; loss 0.063955\n",
      "epoch:3; batch 78976; train accuracy: 0.815705\n",
      "epoch 3; batch 79104; loss 0.149940\n",
      "epoch:3; batch 79104; train accuracy: 0.815773\n",
      "epoch 3; batch 79232; loss 0.053018\n",
      "epoch:3; batch 79232; train accuracy: 0.815844\n",
      "epoch 3; batch 79360; loss 0.083564\n",
      "epoch:3; batch 79360; train accuracy: 0.815908\n",
      "epoch 3; batch 79488; loss 0.120462\n",
      "epoch:3; batch 79488; train accuracy: 0.815966\n",
      "epoch 3; batch 79616; loss 0.019779\n",
      "epoch:3; batch 79616; train accuracy: 0.816044\n",
      "epoch 3; batch 79744; loss 0.044539\n",
      "epoch:3; batch 79744; train accuracy: 0.816118\n",
      "epoch 3; batch 79872; loss 0.068841\n",
      "epoch:3; batch 79872; train accuracy: 0.816185\n",
      "epoch 3; batch 80000; loss 0.084965\n",
      "epoch:3; batch 80000; train accuracy: 0.816256\n",
      "epoch 3; batch 80128; loss 0.143039\n",
      "epoch:3; batch 80128; train accuracy: 0.816324\n",
      "epoch 3; batch 80256; loss 0.080165\n",
      "epoch:3; batch 80256; train accuracy: 0.816384\n",
      "epoch 3; batch 80384; loss 0.034573\n",
      "epoch:3; batch 80384; train accuracy: 0.816458\n",
      "epoch 3; batch 80512; loss 0.037800\n",
      "epoch:3; batch 80512; train accuracy: 0.816532\n",
      "epoch 3; batch 80640; loss 0.104133\n",
      "epoch:3; batch 80640; train accuracy: 0.816596\n",
      "epoch 3; batch 80768; loss 0.056474\n",
      "epoch:3; batch 80768; train accuracy: 0.816666\n",
      "epoch 3; batch 80896; loss 0.103619\n",
      "epoch:3; batch 80896; train accuracy: 0.816736\n",
      "epoch 3; batch 81024; loss 0.047104\n",
      "epoch:3; batch 81024; train accuracy: 0.816810\n",
      "epoch 3; batch 81152; loss 0.080905\n",
      "epoch:3; batch 81152; train accuracy: 0.816880\n",
      "epoch 3; batch 81280; loss 0.135744\n",
      "epoch:3; batch 81280; train accuracy: 0.816940\n",
      "epoch 3; batch 81408; loss 0.034390\n",
      "epoch:3; batch 81408; train accuracy: 0.817014\n",
      "epoch 3; batch 81536; loss 0.129576\n",
      "epoch:3; batch 81536; train accuracy: 0.817080\n",
      "epoch 3; batch 81664; loss 0.058283\n",
      "epoch:3; batch 81664; train accuracy: 0.817147\n",
      "epoch 3; batch 81792; loss 0.075395\n",
      "epoch:3; batch 81792; train accuracy: 0.817214\n",
      "epoch 3; batch 81920; loss 0.052413\n",
      "epoch:3; batch 81920; train accuracy: 0.817283\n",
      "epoch 3; batch 82048; loss 0.051255\n",
      "epoch:3; batch 82048; train accuracy: 0.817350\n",
      "epoch 3; batch 82176; loss 0.044663\n",
      "epoch:3; batch 82176; train accuracy: 0.817420\n",
      "epoch 3; batch 82304; loss 0.069799\n",
      "epoch:3; batch 82304; train accuracy: 0.817493\n",
      "epoch 3; batch 82432; loss 0.131207\n",
      "epoch:3; batch 82432; train accuracy: 0.817549\n",
      "epoch 3; batch 82560; loss 0.071673\n",
      "epoch:3; batch 82560; train accuracy: 0.817618\n",
      "epoch 3; batch 82688; loss 0.097691\n",
      "epoch:3; batch 82688; train accuracy: 0.817691\n",
      "epoch 3; batch 82816; loss 0.031655\n",
      "epoch:3; batch 82816; train accuracy: 0.817768\n",
      "epoch 3; batch 82944; loss 0.111249\n",
      "epoch:3; batch 82944; train accuracy: 0.817827\n",
      "epoch 3; batch 83072; loss 0.048836\n",
      "epoch:3; batch 83072; train accuracy: 0.817896\n",
      "epoch 3; batch 83200; loss 0.137768\n",
      "epoch:3; batch 83200; train accuracy: 0.817959\n",
      "epoch 3; batch 83328; loss 0.053848\n",
      "epoch:3; batch 83328; train accuracy: 0.818028\n",
      "epoch 3; batch 83456; loss 0.079449\n",
      "epoch:3; batch 83456; train accuracy: 0.818093\n",
      "epoch 3; batch 83584; loss 0.037088\n",
      "epoch:3; batch 83584; train accuracy: 0.818163\n",
      "epoch 3; batch 83712; loss 0.068526\n",
      "epoch:3; batch 83712; train accuracy: 0.818228\n",
      "epoch 3; batch 83840; loss 0.078376\n",
      "epoch:3; batch 83840; train accuracy: 0.818294\n",
      "epoch 3; batch 83968; loss 0.060731\n",
      "epoch:3; batch 83968; train accuracy: 0.818366\n",
      "epoch 3; batch 84096; loss 0.152781\n",
      "epoch:3; batch 84096; train accuracy: 0.818435\n",
      "epoch 3; batch 84224; loss 0.061958\n",
      "epoch:3; batch 84224; train accuracy: 0.818500\n",
      "epoch 3; batch 84352; loss 0.134998\n",
      "epoch:3; batch 84352; train accuracy: 0.818562\n",
      "epoch 3; batch 84480; loss 0.117966\n",
      "epoch:3; batch 84480; train accuracy: 0.818621\n",
      "epoch 3; batch 84608; loss 0.031796\n",
      "epoch:3; batch 84608; train accuracy: 0.818696\n",
      "epoch 3; batch 84736; loss 0.080789\n",
      "epoch:3; batch 84736; train accuracy: 0.818762\n",
      "epoch 3; batch 84864; loss 0.091337\n",
      "epoch:3; batch 84864; train accuracy: 0.818823\n",
      "epoch 3; batch 84992; loss 0.096463\n",
      "epoch:3; batch 84992; train accuracy: 0.818885\n",
      "epoch 3; batch 85120; loss 0.101632\n",
      "epoch:3; batch 85120; train accuracy: 0.818947\n",
      "epoch 3; batch 85248; loss 0.078980\n",
      "epoch:3; batch 85248; train accuracy: 0.819015\n",
      "epoch 3; batch 85376; loss 0.079430\n",
      "epoch:3; batch 85376; train accuracy: 0.819076\n",
      "epoch 3; batch 85504; loss 0.060851\n",
      "epoch:3; batch 85504; train accuracy: 0.819145\n",
      "epoch 3; batch 85632; loss 0.069108\n",
      "epoch:3; batch 85632; train accuracy: 0.819216\n",
      "epoch 3; batch 85760; loss 0.137457\n",
      "epoch:3; batch 85760; train accuracy: 0.819274\n",
      "epoch 3; batch 85888; loss 0.071070\n",
      "epoch:3; batch 85888; train accuracy: 0.819346\n",
      "epoch 3; batch 86016; loss 0.074980\n",
      "epoch:3; batch 86016; train accuracy: 0.819414\n",
      "epoch 3; batch 86144; loss 0.064048\n",
      "epoch:3; batch 86144; train accuracy: 0.819482\n",
      "epoch 3; batch 86272; loss 0.077348\n",
      "epoch:3; batch 86272; train accuracy: 0.819553\n",
      "epoch 3; batch 86400; loss 0.070881\n",
      "epoch:3; batch 86400; train accuracy: 0.819617\n",
      "epoch 3; batch 86528; loss 0.037110\n",
      "epoch:3; batch 86528; train accuracy: 0.819692\n",
      "epoch 3; batch 86656; loss 0.038429\n",
      "epoch:3; batch 86656; train accuracy: 0.819763\n",
      "epoch 3; batch 86784; loss 0.141122\n",
      "epoch:3; batch 86784; train accuracy: 0.819831\n",
      "epoch 3; batch 86912; loss 0.060626\n",
      "epoch:3; batch 86912; train accuracy: 0.819901\n",
      "epoch 3; batch 87040; loss 0.137650\n",
      "epoch:3; batch 87040; train accuracy: 0.819945\n",
      "epoch 3; batch 87168; loss 0.097049\n",
      "epoch:3; batch 87168; train accuracy: 0.820013\n",
      "epoch 3; batch 87296; loss 0.155142\n",
      "epoch:3; batch 87296; train accuracy: 0.820070\n",
      "epoch 3; batch 87424; loss 0.043618\n",
      "epoch:3; batch 87424; train accuracy: 0.820144\n",
      "epoch 3; batch 87552; loss 0.036450\n",
      "epoch:3; batch 87552; train accuracy: 0.820215\n",
      "epoch 3; batch 87680; loss 0.063905\n",
      "epoch:3; batch 87680; train accuracy: 0.820282\n",
      "epoch 3; batch 87808; loss 0.052147\n",
      "epoch:3; batch 87808; train accuracy: 0.820353\n",
      "epoch 3; batch 87936; loss 0.050236\n",
      "epoch:3; batch 87936; train accuracy: 0.820420\n",
      "epoch 3; batch 88064; loss 0.105571\n",
      "epoch:3; batch 88064; train accuracy: 0.820484\n",
      "epoch 3; batch 88192; loss 0.053778\n",
      "epoch:3; batch 88192; train accuracy: 0.820554\n",
      "epoch 3; batch 88320; loss 0.023462\n",
      "epoch:3; batch 88320; train accuracy: 0.820631\n",
      "epoch 3; batch 88448; loss 0.034032\n",
      "epoch:3; batch 88448; train accuracy: 0.820701\n",
      "epoch 3; batch 88576; loss 0.044287\n",
      "epoch:3; batch 88576; train accuracy: 0.820775\n",
      "epoch 3; batch 88704; loss 0.036652\n",
      "epoch:3; batch 88704; train accuracy: 0.820848\n",
      "epoch 3; batch 88832; loss 0.073565\n",
      "epoch:3; batch 88832; train accuracy: 0.820915\n",
      "epoch 3; batch 88960; loss 0.028963\n",
      "epoch:3; batch 88960; train accuracy: 0.820988\n",
      "epoch 3; batch 89088; loss 0.107286\n",
      "epoch:3; batch 89088; train accuracy: 0.821052\n",
      "epoch 3; batch 89216; loss 0.135726\n",
      "epoch:3; batch 89216; train accuracy: 0.821111\n",
      "epoch 3; batch 89344; loss 0.039009\n",
      "epoch:3; batch 89344; train accuracy: 0.821181\n",
      "epoch 3; batch 89472; loss 0.145684\n",
      "epoch:3; batch 89472; train accuracy: 0.821244\n",
      "epoch 3; batch 89600; loss 0.112834\n",
      "epoch:3; batch 89600; train accuracy: 0.821314\n",
      "epoch 3; batch 89728; loss 0.097483\n",
      "epoch:3; batch 89728; train accuracy: 0.821384\n",
      "epoch 3; batch 89856; loss 0.122836\n",
      "epoch:3; batch 89856; train accuracy: 0.821443\n",
      "epoch 3; batch 89984; loss 0.077596\n",
      "epoch:3; batch 89984; train accuracy: 0.821506\n",
      "epoch 3; batch 90112; loss 0.032941\n",
      "epoch:3; batch 90112; train accuracy: 0.821576\n",
      "epoch 3; batch 90240; loss 0.041458\n",
      "epoch:3; batch 90240; train accuracy: 0.821645\n",
      "epoch 3; batch 90368; loss 0.039093\n",
      "epoch:3; batch 90368; train accuracy: 0.821711\n",
      "epoch 3; batch 90496; loss 0.093473\n",
      "epoch:3; batch 90496; train accuracy: 0.821777\n",
      "epoch 3; batch 90624; loss 0.030229\n",
      "epoch:3; batch 90624; train accuracy: 0.821850\n",
      "epoch 3; batch 90752; loss 0.017139\n",
      "epoch:3; batch 90752; train accuracy: 0.821926\n",
      "epoch 3; batch 90880; loss 0.028777\n",
      "epoch:3; batch 90880; train accuracy: 0.821998\n",
      "epoch 3; batch 91008; loss 0.057255\n",
      "epoch:3; batch 91008; train accuracy: 0.822064\n",
      "epoch 3; batch 91136; loss 0.063311\n",
      "epoch:3; batch 91136; train accuracy: 0.822126\n",
      "epoch 3; batch 91264; loss 0.070027\n",
      "epoch:3; batch 91264; train accuracy: 0.822188\n",
      "epoch 3; batch 91392; loss 0.085277\n",
      "epoch:3; batch 91392; train accuracy: 0.822251\n",
      "epoch 3; batch 91520; loss 0.054524\n",
      "epoch:3; batch 91520; train accuracy: 0.822320\n",
      "epoch 3; batch 91648; loss 0.078846\n",
      "epoch:3; batch 91648; train accuracy: 0.822378\n",
      "epoch 3; batch 91776; loss 0.071846\n",
      "epoch:3; batch 91776; train accuracy: 0.822444\n",
      "epoch 3; batch 91904; loss 0.052256\n",
      "epoch:3; batch 91904; train accuracy: 0.822509\n",
      "epoch 3; batch 92032; loss 0.061348\n",
      "epoch:3; batch 92032; train accuracy: 0.822578\n",
      "epoch 3; batch 92160; loss 0.060340\n",
      "epoch:3; batch 92160; train accuracy: 0.822643\n",
      "epoch 3; batch 92288; loss 0.065928\n",
      "epoch:3; batch 92288; train accuracy: 0.822708\n",
      "epoch 3; batch 92416; loss 0.087964\n",
      "epoch:3; batch 92416; train accuracy: 0.822773\n",
      "epoch 3; batch 92544; loss 0.038682\n",
      "epoch:3; batch 92544; train accuracy: 0.822845\n",
      "epoch 3; batch 92672; loss 0.106727\n",
      "epoch:3; batch 92672; train accuracy: 0.822900\n",
      "epoch 3; batch 92800; loss 0.014969\n",
      "epoch:3; batch 92800; train accuracy: 0.822972\n",
      "epoch 3; batch 92928; loss 0.114470\n",
      "epoch:3; batch 92928; train accuracy: 0.823030\n",
      "epoch 3; batch 93056; loss 0.044147\n",
      "epoch:3; batch 93056; train accuracy: 0.823102\n",
      "epoch 3; batch 93184; loss 0.049086\n",
      "epoch:3; batch 93184; train accuracy: 0.823166\n",
      "epoch 3; batch 93312; loss 0.042415\n",
      "epoch:3; batch 93312; train accuracy: 0.823238\n",
      "epoch 3; batch 93440; loss 0.041205\n",
      "epoch:3; batch 93440; train accuracy: 0.823306\n",
      "epoch 3; batch 93568; loss 0.081008\n",
      "epoch:3; batch 93568; train accuracy: 0.823367\n",
      "epoch 3; batch 93696; loss 0.020605\n",
      "epoch:3; batch 93696; train accuracy: 0.823441\n",
      "epoch 3; batch 93824; loss 0.011561\n",
      "epoch:3; batch 93824; train accuracy: 0.823516\n",
      "epoch 3; batch 93952; loss 0.096450\n",
      "epoch:3; batch 93952; train accuracy: 0.823577\n",
      "epoch 3; batch 94080; loss 0.056009\n",
      "epoch:3; batch 94080; train accuracy: 0.823645\n",
      "epoch 3; batch 94208; loss 0.189177\n",
      "epoch:3; batch 94208; train accuracy: 0.823699\n",
      "epoch 3; batch 94336; loss 0.035975\n",
      "epoch:3; batch 94336; train accuracy: 0.823770\n",
      "epoch 3; batch 94464; loss 0.047041\n",
      "epoch:3; batch 94464; train accuracy: 0.823838\n",
      "epoch 3; batch 94592; loss 0.032997\n",
      "epoch:3; batch 94592; train accuracy: 0.823908\n",
      "epoch 3; batch 94720; loss 0.046380\n",
      "epoch:3; batch 94720; train accuracy: 0.823973\n",
      "epoch 3; batch 94848; loss 0.131734\n",
      "epoch:3; batch 94848; train accuracy: 0.824027\n",
      "epoch 3; batch 94976; loss 0.072907\n",
      "epoch:3; batch 94976; train accuracy: 0.824094\n",
      "epoch 3; batch 95104; loss 0.068489\n",
      "epoch:3; batch 95104; train accuracy: 0.824155\n",
      "epoch 3; batch 95232; loss 0.059652\n",
      "epoch:3; batch 95232; train accuracy: 0.824219\n",
      "epoch 3; batch 95360; loss 0.032187\n",
      "epoch:3; batch 95360; train accuracy: 0.824289\n",
      "epoch 3; batch 95488; loss 0.103775\n",
      "epoch:3; batch 95488; train accuracy: 0.824343\n",
      "epoch 3; batch 95616; loss 0.063840\n",
      "epoch:3; batch 95616; train accuracy: 0.824410\n",
      "epoch 3; batch 95744; loss 0.056049\n",
      "epoch:3; batch 95744; train accuracy: 0.824474\n",
      "epoch 3; batch 95872; loss 0.084521\n",
      "epoch:3; batch 95872; train accuracy: 0.824534\n",
      "epoch 3; batch 96000; loss 0.037256\n",
      "epoch:3; batch 96000; train accuracy: 0.824598\n",
      "epoch 3; batch 96128; loss 0.042963\n",
      "epoch:3; batch 96128; train accuracy: 0.824661\n",
      "epoch 3; batch 96256; loss 0.058384\n",
      "epoch:3; batch 96256; train accuracy: 0.824722\n",
      "epoch 3; batch 96384; loss 0.044052\n",
      "epoch:3; batch 96384; train accuracy: 0.824788\n",
      "epoch 3; batch 96512; loss 0.050781\n",
      "epoch:3; batch 96512; train accuracy: 0.824852\n",
      "epoch 3; batch 96640; loss 0.025057\n",
      "epoch:3; batch 96640; train accuracy: 0.824922\n",
      "epoch 3; batch 96768; loss 0.053788\n",
      "epoch:3; batch 96768; train accuracy: 0.824988\n",
      "epoch 3; batch 96896; loss 0.090101\n",
      "epoch:3; batch 96896; train accuracy: 0.825055\n",
      "epoch 3; batch 97024; loss 0.034510\n",
      "epoch:3; batch 97024; train accuracy: 0.825124\n",
      "epoch 3; batch 97152; loss 0.035849\n",
      "epoch:3; batch 97152; train accuracy: 0.825191\n",
      "epoch 3; batch 97280; loss 0.044015\n",
      "epoch:3; batch 97280; train accuracy: 0.825260\n",
      "epoch 3; batch 97408; loss 0.059439\n",
      "epoch:3; batch 97408; train accuracy: 0.825327\n",
      "epoch 3; batch 97536; loss 0.068789\n",
      "epoch:3; batch 97536; train accuracy: 0.825383\n",
      "epoch 3; batch 97664; loss 0.059727\n",
      "epoch:3; batch 97664; train accuracy: 0.825449\n",
      "epoch 3; batch 97792; loss 0.019109\n",
      "epoch:3; batch 97792; train accuracy: 0.825519\n",
      "epoch 3; batch 97920; loss 0.058759\n",
      "epoch:3; batch 97920; train accuracy: 0.825585\n",
      "epoch 3; batch 98048; loss 0.026665\n",
      "epoch:3; batch 98048; train accuracy: 0.825654\n",
      "epoch 3; batch 98176; loss 0.020509\n",
      "epoch:3; batch 98176; train accuracy: 0.825726\n",
      "epoch 3; batch 98304; loss 0.042747\n",
      "epoch:3; batch 98304; train accuracy: 0.825792\n",
      "epoch 3; batch 98432; loss 0.059876\n",
      "epoch:3; batch 98432; train accuracy: 0.825852\n",
      "epoch 3; batch 98560; loss 0.107171\n",
      "epoch:3; batch 98560; train accuracy: 0.825908\n",
      "epoch 3; batch 98688; loss 0.080534\n",
      "epoch:3; batch 98688; train accuracy: 0.825967\n",
      "epoch 3; batch 98816; loss 0.128208\n",
      "epoch:3; batch 98816; train accuracy: 0.826023\n",
      "epoch 3; batch 98944; loss 0.033853\n",
      "epoch:3; batch 98944; train accuracy: 0.826088\n",
      "epoch 3; batch 99072; loss 0.023075\n",
      "epoch:3; batch 99072; train accuracy: 0.826157\n",
      "epoch 3; batch 99200; loss 0.060986\n",
      "epoch:3; batch 99200; train accuracy: 0.826220\n",
      "epoch 3; batch 99328; loss 0.053070\n",
      "epoch:3; batch 99328; train accuracy: 0.826285\n",
      "epoch 3; batch 99456; loss 0.157459\n",
      "epoch:3; batch 99456; train accuracy: 0.826341\n",
      "epoch 3; batch 99584; loss 0.051061\n",
      "epoch:3; batch 99584; train accuracy: 0.826406\n",
      "epoch 3; batch 99712; loss 0.059338\n",
      "epoch:3; batch 99712; train accuracy: 0.826468\n",
      "epoch 3; batch 99840; loss 0.073338\n",
      "epoch:3; batch 99840; train accuracy: 0.826530\n",
      "epoch 3; batch 99968; loss 0.102524\n",
      "epoch:3; batch 99968; train accuracy: 0.826582\n",
      "epoch 3; batch 100096; loss 0.058093\n",
      "epoch:3; batch 100096; train accuracy: 0.826644\n",
      "epoch 3; batch 100224; loss 0.110580\n",
      "epoch:3; batch 100224; train accuracy: 0.826703\n",
      "epoch 3; batch 100352; loss 0.144011\n",
      "epoch:3; batch 100352; train accuracy: 0.826762\n",
      "epoch 3; batch 100480; loss 0.176122\n",
      "epoch:3; batch 100480; train accuracy: 0.826817\n",
      "epoch 3; batch 100608; loss 0.103242\n",
      "epoch:3; batch 100608; train accuracy: 0.826876\n",
      "epoch 3; batch 100736; loss 0.096852\n",
      "epoch:3; batch 100736; train accuracy: 0.826934\n",
      "epoch 3; batch 100864; loss 0.073552\n",
      "epoch:3; batch 100864; train accuracy: 0.826992\n",
      "epoch 3; batch 100992; loss 0.131175\n",
      "epoch:3; batch 100992; train accuracy: 0.827044\n",
      "epoch 3; batch 101120; loss 0.076750\n",
      "epoch:3; batch 101120; train accuracy: 0.827109\n",
      "epoch 3; batch 101248; loss 0.038066\n",
      "epoch:3; batch 101248; train accuracy: 0.827174\n",
      "epoch 3; batch 101376; loss 0.057181\n",
      "epoch:3; batch 101376; train accuracy: 0.827238\n",
      "epoch 3; batch 101504; loss 0.036071\n",
      "epoch:3; batch 101504; train accuracy: 0.827306\n",
      "epoch 3; batch 101632; loss 0.018832\n",
      "epoch:3; batch 101632; train accuracy: 0.827374\n",
      "epoch 3; batch 101760; loss 0.092766\n",
      "epoch:3; batch 101760; train accuracy: 0.827422\n",
      "epoch 3; batch 101888; loss 0.024844\n",
      "epoch:3; batch 101888; train accuracy: 0.827490\n",
      "epoch 3; batch 102016; loss 0.104679\n",
      "epoch:3; batch 102016; train accuracy: 0.827535\n",
      "epoch 3; batch 102144; loss 0.078081\n",
      "epoch:3; batch 102144; train accuracy: 0.827593\n",
      "epoch 3; batch 102272; loss 0.040394\n",
      "epoch:3; batch 102272; train accuracy: 0.827657\n",
      "epoch 3; batch 102400; loss 0.045759\n",
      "epoch:3; batch 102400; train accuracy: 0.827722\n",
      "epoch 3; batch 102528; loss 0.063456\n",
      "epoch:3; batch 102528; train accuracy: 0.827783\n",
      "epoch 3; batch 102656; loss 0.057162\n",
      "epoch:3; batch 102656; train accuracy: 0.827843\n",
      "epoch 3; batch 102784; loss 0.100407\n",
      "epoch:3; batch 102784; train accuracy: 0.827901\n",
      "epoch 3; batch 102912; loss 0.071271\n",
      "epoch:3; batch 102912; train accuracy: 0.827962\n",
      "epoch 3; batch 103040; loss 0.099562\n",
      "epoch:3; batch 103040; train accuracy: 0.828023\n",
      "epoch 3; batch 103168; loss 0.086068\n",
      "epoch:3; batch 103168; train accuracy: 0.828074\n",
      "epoch 3; batch 103296; loss 0.109300\n",
      "epoch:3; batch 103296; train accuracy: 0.828131\n",
      "epoch 3; batch 103424; loss 0.053351\n",
      "epoch:3; batch 103424; train accuracy: 0.828195\n",
      "epoch 3; batch 103552; loss 0.053599\n",
      "epoch:3; batch 103552; train accuracy: 0.828256\n",
      "epoch 3; batch 103680; loss 0.047962\n",
      "epoch:3; batch 103680; train accuracy: 0.828319\n",
      "epoch 3; batch 103808; loss 0.071074\n",
      "epoch:3; batch 103808; train accuracy: 0.828377\n",
      "epoch 3; batch 103936; loss 0.141714\n",
      "epoch:3; batch 103936; train accuracy: 0.828431\n",
      "epoch 3; batch 104064; loss 0.115115\n",
      "epoch:3; batch 104064; train accuracy: 0.828488\n",
      "epoch 3; batch 104192; loss 0.133712\n",
      "epoch:3; batch 104192; train accuracy: 0.828545\n",
      "epoch 3; batch 104320; loss 0.058109\n",
      "epoch:3; batch 104320; train accuracy: 0.828606\n",
      "epoch 3; batch 104448; loss 0.126764\n",
      "epoch:3; batch 104448; train accuracy: 0.828663\n",
      "epoch 3; batch 104576; loss 0.051834\n",
      "epoch:3; batch 104576; train accuracy: 0.828723\n",
      "epoch 3; batch 104704; loss 0.056648\n",
      "epoch:3; batch 104704; train accuracy: 0.828792\n",
      "epoch 3; batch 104832; loss 0.061156\n",
      "epoch:3; batch 104832; train accuracy: 0.828853\n",
      "epoch 3; batch 104960; loss 0.091583\n",
      "epoch:3; batch 104960; train accuracy: 0.828913\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 31041 ; rate: 0.295741\n",
      "y_true_label_1_num: 11979 ; rate: 0.114129\n",
      "y_true_label_2_num: 23669 ; rate: 0.225505\n",
      "y_true_label_3_num: 38271 ; rate: 0.364625\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.981117\n",
      "valid avg_precision: 0.982330\n",
      "valid avg_recall: 0.980412\n",
      "valid avg_f1: 0.981315\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 4424 ; rate: 0.295406\n",
      "y_true_label_1_num: 1734 ; rate: 0.115785\n",
      "y_true_label_2_num: 3413 ; rate: 0.227898\n",
      "y_true_label_3_num: 5405 ; rate: 0.360911\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.682893\n",
      "valid avg_precision: 0.702742\n",
      "valid avg_recall: 0.676950\n",
      "valid avg_f1: 0.685273\n",
      "epoch 4\n",
      "epoch 4; batch 128; loss 0.093294\n",
      "epoch:4; batch 128; train accuracy: 0.828969\n",
      "epoch 4; batch 256; loss 0.084878\n",
      "epoch:4; batch 256; train accuracy: 0.829023\n",
      "epoch 4; batch 384; loss 0.150255\n",
      "epoch:4; batch 384; train accuracy: 0.829080\n",
      "epoch 4; batch 512; loss 0.116886\n",
      "epoch:4; batch 512; train accuracy: 0.829136\n",
      "epoch 4; batch 640; loss 0.066050\n",
      "epoch:4; batch 640; train accuracy: 0.829196\n",
      "epoch 4; batch 768; loss 0.152787\n",
      "epoch:4; batch 768; train accuracy: 0.829253\n",
      "epoch 4; batch 896; loss 0.061334\n",
      "epoch:4; batch 896; train accuracy: 0.829319\n",
      "epoch 4; batch 1024; loss 0.164417\n",
      "epoch:4; batch 1024; train accuracy: 0.829369\n",
      "epoch 4; batch 1152; loss 0.062202\n",
      "epoch:4; batch 1152; train accuracy: 0.829429\n",
      "epoch 4; batch 1280; loss 0.041254\n",
      "epoch:4; batch 1280; train accuracy: 0.829491\n",
      "epoch 4; batch 1408; loss 0.086577\n",
      "epoch:4; batch 1408; train accuracy: 0.829548\n",
      "epoch 4; batch 1536; loss 0.067287\n",
      "epoch:4; batch 1536; train accuracy: 0.829607\n",
      "epoch 4; batch 1664; loss 0.074652\n",
      "epoch:4; batch 1664; train accuracy: 0.829664\n",
      "epoch 4; batch 1792; loss 0.054722\n",
      "epoch:4; batch 1792; train accuracy: 0.829726\n",
      "epoch 4; batch 1920; loss 0.101682\n",
      "epoch:4; batch 1920; train accuracy: 0.829770\n",
      "epoch 4; batch 2048; loss 0.067072\n",
      "epoch:4; batch 2048; train accuracy: 0.829832\n",
      "epoch 4; batch 2176; loss 0.019702\n",
      "epoch:4; batch 2176; train accuracy: 0.829901\n",
      "epoch 4; batch 2304; loss 0.014075\n",
      "epoch:4; batch 2304; train accuracy: 0.829969\n",
      "epoch 4; batch 2432; loss 0.033400\n",
      "epoch:4; batch 2432; train accuracy: 0.830035\n",
      "epoch 4; batch 2560; loss 0.056122\n",
      "epoch:4; batch 2560; train accuracy: 0.830094\n",
      "epoch 4; batch 2688; loss 0.076812\n",
      "epoch:4; batch 2688; train accuracy: 0.830153\n",
      "epoch 4; batch 2816; loss 0.096863\n",
      "epoch:4; batch 2816; train accuracy: 0.830202\n",
      "epoch 4; batch 2944; loss 0.019922\n",
      "epoch:4; batch 2944; train accuracy: 0.830271\n",
      "epoch 4; batch 3072; loss 0.026858\n",
      "epoch:4; batch 3072; train accuracy: 0.830339\n",
      "epoch 4; batch 3200; loss 0.029341\n",
      "epoch:4; batch 3200; train accuracy: 0.830407\n",
      "epoch 4; batch 3328; loss 0.033943\n",
      "epoch:4; batch 3328; train accuracy: 0.830469\n",
      "epoch 4; batch 3456; loss 0.010787\n",
      "epoch:4; batch 3456; train accuracy: 0.830538\n",
      "epoch 4; batch 3584; loss 0.035356\n",
      "epoch:4; batch 3584; train accuracy: 0.830603\n",
      "epoch 4; batch 3712; loss 0.134294\n",
      "epoch:4; batch 3712; train accuracy: 0.830652\n",
      "epoch 4; batch 3840; loss 0.006980\n",
      "epoch:4; batch 3840; train accuracy: 0.830720\n",
      "epoch 4; batch 3968; loss 0.034726\n",
      "epoch:4; batch 3968; train accuracy: 0.830781\n",
      "epoch 4; batch 4096; loss 0.061850\n",
      "epoch:4; batch 4096; train accuracy: 0.830840\n",
      "epoch 4; batch 4224; loss 0.050417\n",
      "epoch:4; batch 4224; train accuracy: 0.830905\n",
      "epoch 4; batch 4352; loss 0.072038\n",
      "epoch:4; batch 4352; train accuracy: 0.830960\n",
      "epoch 4; batch 4480; loss 0.047237\n",
      "epoch:4; batch 4480; train accuracy: 0.831021\n",
      "epoch 4; batch 4608; loss 0.024538\n",
      "epoch:4; batch 4608; train accuracy: 0.831089\n",
      "epoch 4; batch 4736; loss 0.022945\n",
      "epoch:4; batch 4736; train accuracy: 0.831151\n",
      "epoch 4; batch 4864; loss 0.014864\n",
      "epoch:4; batch 4864; train accuracy: 0.831218\n",
      "epoch 4; batch 4992; loss 0.079762\n",
      "epoch:4; batch 4992; train accuracy: 0.831279\n",
      "epoch 4; batch 5120; loss 0.030163\n",
      "epoch:4; batch 5120; train accuracy: 0.831344\n",
      "epoch 4; batch 5248; loss 0.020559\n",
      "epoch:4; batch 5248; train accuracy: 0.831408\n",
      "epoch 4; batch 5376; loss 0.147167\n",
      "epoch:4; batch 5376; train accuracy: 0.831469\n",
      "epoch 4; batch 5504; loss 0.022258\n",
      "epoch:4; batch 5504; train accuracy: 0.831537\n",
      "epoch 4; batch 5632; loss 0.046618\n",
      "epoch:4; batch 5632; train accuracy: 0.831601\n",
      "epoch 4; batch 5760; loss 0.027815\n",
      "epoch:4; batch 5760; train accuracy: 0.831665\n",
      "epoch 4; batch 5888; loss 0.049775\n",
      "epoch:4; batch 5888; train accuracy: 0.831723\n",
      "epoch 4; batch 6016; loss 0.060516\n",
      "epoch:4; batch 6016; train accuracy: 0.831784\n",
      "epoch 4; batch 6144; loss 0.051389\n",
      "epoch:4; batch 6144; train accuracy: 0.831844\n",
      "epoch 4; batch 6272; loss 0.019150\n",
      "epoch:4; batch 6272; train accuracy: 0.831908\n",
      "epoch 4; batch 6400; loss 0.013308\n",
      "epoch:4; batch 6400; train accuracy: 0.831975\n",
      "epoch 4; batch 6528; loss 0.065602\n",
      "epoch:4; batch 6528; train accuracy: 0.832030\n",
      "epoch 4; batch 6656; loss 0.015190\n",
      "epoch:4; batch 6656; train accuracy: 0.832097\n",
      "epoch 4; batch 6784; loss 0.082446\n",
      "epoch:4; batch 6784; train accuracy: 0.832154\n",
      "epoch 4; batch 6912; loss 0.064175\n",
      "epoch:4; batch 6912; train accuracy: 0.832208\n",
      "epoch 4; batch 7040; loss 0.033644\n",
      "epoch:4; batch 7040; train accuracy: 0.832263\n",
      "epoch 4; batch 7168; loss 0.009622\n",
      "epoch:4; batch 7168; train accuracy: 0.832329\n",
      "epoch 4; batch 7296; loss 0.022275\n",
      "epoch:4; batch 7296; train accuracy: 0.832393\n",
      "epoch 4; batch 7424; loss 0.086359\n",
      "epoch:4; batch 7424; train accuracy: 0.832453\n",
      "epoch 4; batch 7552; loss 0.075831\n",
      "epoch:4; batch 7552; train accuracy: 0.832507\n",
      "epoch 4; batch 7680; loss 0.038082\n",
      "epoch:4; batch 7680; train accuracy: 0.832568\n",
      "epoch 4; batch 7808; loss 0.019319\n",
      "epoch:4; batch 7808; train accuracy: 0.832631\n",
      "epoch 4; batch 7936; loss 0.055186\n",
      "epoch:4; batch 7936; train accuracy: 0.832691\n",
      "epoch 4; batch 8064; loss 0.044776\n",
      "epoch:4; batch 8064; train accuracy: 0.832754\n",
      "epoch 4; batch 8192; loss 0.034012\n",
      "epoch:4; batch 8192; train accuracy: 0.832811\n",
      "epoch 4; batch 8320; loss 0.071505\n",
      "epoch:4; batch 8320; train accuracy: 0.832868\n",
      "epoch 4; batch 8448; loss 0.008846\n",
      "epoch:4; batch 8448; train accuracy: 0.832934\n",
      "epoch 4; batch 8576; loss 0.114503\n",
      "epoch:4; batch 8576; train accuracy: 0.832988\n",
      "epoch 4; batch 8704; loss 0.007787\n",
      "epoch:4; batch 8704; train accuracy: 0.833054\n",
      "epoch 4; batch 8832; loss 0.040951\n",
      "epoch:4; batch 8832; train accuracy: 0.833114\n",
      "epoch 4; batch 8960; loss 0.120929\n",
      "epoch:4; batch 8960; train accuracy: 0.833165\n",
      "epoch 4; batch 9088; loss 0.025096\n",
      "epoch:4; batch 9088; train accuracy: 0.833227\n",
      "epoch 4; batch 9216; loss 0.060679\n",
      "epoch:4; batch 9216; train accuracy: 0.833287\n",
      "epoch 4; batch 9344; loss 0.070852\n",
      "epoch:4; batch 9344; train accuracy: 0.833341\n",
      "epoch 4; batch 9472; loss 0.062362\n",
      "epoch:4; batch 9472; train accuracy: 0.833397\n",
      "epoch 4; batch 9600; loss 0.008799\n",
      "epoch:4; batch 9600; train accuracy: 0.833463\n",
      "epoch 4; batch 9728; loss 0.018406\n",
      "epoch:4; batch 9728; train accuracy: 0.833528\n",
      "epoch 4; batch 9856; loss 0.018060\n",
      "epoch:4; batch 9856; train accuracy: 0.833591\n",
      "epoch 4; batch 9984; loss 0.057070\n",
      "epoch:4; batch 9984; train accuracy: 0.833653\n",
      "epoch 4; batch 10112; loss 0.031656\n",
      "epoch:4; batch 10112; train accuracy: 0.833713\n",
      "epoch 4; batch 10240; loss 0.048385\n",
      "epoch:4; batch 10240; train accuracy: 0.833769\n",
      "epoch 4; batch 10368; loss 0.108794\n",
      "epoch:4; batch 10368; train accuracy: 0.833825\n",
      "epoch 4; batch 10496; loss 0.067481\n",
      "epoch:4; batch 10496; train accuracy: 0.833884\n",
      "epoch 4; batch 10624; loss 0.056767\n",
      "epoch:4; batch 10624; train accuracy: 0.833947\n",
      "epoch 4; batch 10752; loss 0.027713\n",
      "epoch:4; batch 10752; train accuracy: 0.834009\n",
      "epoch 4; batch 10880; loss 0.041970\n",
      "epoch:4; batch 10880; train accuracy: 0.834071\n",
      "epoch 4; batch 11008; loss 0.043924\n",
      "epoch:4; batch 11008; train accuracy: 0.834127\n",
      "epoch 4; batch 11136; loss 0.025295\n",
      "epoch:4; batch 11136; train accuracy: 0.834186\n",
      "epoch 4; batch 11264; loss 0.026873\n",
      "epoch:4; batch 11264; train accuracy: 0.834248\n",
      "epoch 4; batch 11392; loss 0.073713\n",
      "epoch:4; batch 11392; train accuracy: 0.834307\n",
      "epoch 4; batch 11520; loss 0.089115\n",
      "epoch:4; batch 11520; train accuracy: 0.834360\n",
      "epoch 4; batch 11648; loss 0.015642\n",
      "epoch:4; batch 11648; train accuracy: 0.834422\n",
      "epoch 4; batch 11776; loss 0.027050\n",
      "epoch:4; batch 11776; train accuracy: 0.834486\n",
      "epoch 4; batch 11904; loss 0.059033\n",
      "epoch:4; batch 11904; train accuracy: 0.834542\n",
      "epoch 4; batch 12032; loss 0.038692\n",
      "epoch:4; batch 12032; train accuracy: 0.834601\n",
      "epoch 4; batch 12160; loss 0.043340\n",
      "epoch:4; batch 12160; train accuracy: 0.834656\n",
      "epoch 4; batch 12288; loss 0.033656\n",
      "epoch:4; batch 12288; train accuracy: 0.834718\n",
      "epoch 4; batch 12416; loss 0.052012\n",
      "epoch:4; batch 12416; train accuracy: 0.834773\n",
      "epoch 4; batch 12544; loss 0.095117\n",
      "epoch:4; batch 12544; train accuracy: 0.834820\n",
      "epoch 4; batch 12672; loss 0.054157\n",
      "epoch:4; batch 12672; train accuracy: 0.834875\n",
      "epoch 4; batch 12800; loss 0.019653\n",
      "epoch:4; batch 12800; train accuracy: 0.834936\n",
      "epoch 4; batch 12928; loss 0.018369\n",
      "epoch:4; batch 12928; train accuracy: 0.834998\n",
      "epoch 4; batch 13056; loss 0.092039\n",
      "epoch:4; batch 13056; train accuracy: 0.835050\n",
      "epoch 4; batch 13184; loss 0.050376\n",
      "epoch:4; batch 13184; train accuracy: 0.835105\n",
      "epoch 4; batch 13312; loss 0.064499\n",
      "epoch:4; batch 13312; train accuracy: 0.835160\n",
      "epoch 4; batch 13440; loss 0.018350\n",
      "epoch:4; batch 13440; train accuracy: 0.835225\n",
      "epoch 4; batch 13568; loss 0.030134\n",
      "epoch:4; batch 13568; train accuracy: 0.835280\n",
      "epoch 4; batch 13696; loss 0.134248\n",
      "epoch:4; batch 13696; train accuracy: 0.835329\n",
      "epoch 4; batch 13824; loss 0.053448\n",
      "epoch:4; batch 13824; train accuracy: 0.835390\n",
      "epoch 4; batch 13952; loss 0.055806\n",
      "epoch:4; batch 13952; train accuracy: 0.835448\n",
      "epoch 4; batch 14080; loss 0.018809\n",
      "epoch:4; batch 14080; train accuracy: 0.835509\n",
      "epoch 4; batch 14208; loss 0.047676\n",
      "epoch:4; batch 14208; train accuracy: 0.835567\n",
      "epoch 4; batch 14336; loss 0.010514\n",
      "epoch:4; batch 14336; train accuracy: 0.835631\n",
      "epoch 4; batch 14464; loss 0.045288\n",
      "epoch:4; batch 14464; train accuracy: 0.835689\n",
      "epoch 4; batch 14592; loss 0.012623\n",
      "epoch:4; batch 14592; train accuracy: 0.835752\n",
      "epoch 4; batch 14720; loss 0.004034\n",
      "epoch:4; batch 14720; train accuracy: 0.835816\n",
      "epoch 4; batch 14848; loss 0.073778\n",
      "epoch:4; batch 14848; train accuracy: 0.835877\n",
      "epoch 4; batch 14976; loss 0.050558\n",
      "epoch:4; batch 14976; train accuracy: 0.835934\n",
      "epoch 4; batch 15104; loss 0.066752\n",
      "epoch:4; batch 15104; train accuracy: 0.835989\n",
      "epoch 4; batch 15232; loss 0.035638\n",
      "epoch:4; batch 15232; train accuracy: 0.836047\n",
      "epoch 4; batch 15360; loss 0.010137\n",
      "epoch:4; batch 15360; train accuracy: 0.836110\n",
      "epoch 4; batch 15488; loss 0.047488\n",
      "epoch:4; batch 15488; train accuracy: 0.836165\n",
      "epoch 4; batch 15616; loss 0.023561\n",
      "epoch:4; batch 15616; train accuracy: 0.836225\n",
      "epoch 4; batch 15744; loss 0.049020\n",
      "epoch:4; batch 15744; train accuracy: 0.836279\n",
      "epoch 4; batch 15872; loss 0.046327\n",
      "epoch:4; batch 15872; train accuracy: 0.836337\n",
      "epoch 4; batch 16000; loss 0.054223\n",
      "epoch:4; batch 16000; train accuracy: 0.836391\n",
      "epoch 4; batch 16128; loss 0.125208\n",
      "epoch:4; batch 16128; train accuracy: 0.836445\n",
      "epoch 4; batch 16256; loss 0.130834\n",
      "epoch:4; batch 16256; train accuracy: 0.836499\n",
      "epoch 4; batch 16384; loss 0.020685\n",
      "epoch:4; batch 16384; train accuracy: 0.836562\n",
      "epoch 4; batch 16512; loss 0.071072\n",
      "epoch:4; batch 16512; train accuracy: 0.836616\n",
      "epoch 4; batch 16640; loss 0.023775\n",
      "epoch:4; batch 16640; train accuracy: 0.836677\n",
      "epoch 4; batch 16768; loss 0.037309\n",
      "epoch:4; batch 16768; train accuracy: 0.836734\n",
      "epoch 4; batch 16896; loss 0.199713\n",
      "epoch:4; batch 16896; train accuracy: 0.836778\n",
      "epoch 4; batch 17024; loss 0.013621\n",
      "epoch:4; batch 17024; train accuracy: 0.836841\n",
      "epoch 4; batch 17152; loss 0.054446\n",
      "epoch:4; batch 17152; train accuracy: 0.836901\n",
      "epoch 4; batch 17280; loss 0.026121\n",
      "epoch:4; batch 17280; train accuracy: 0.836961\n",
      "epoch 4; batch 17408; loss 0.057281\n",
      "epoch:4; batch 17408; train accuracy: 0.837018\n",
      "epoch 4; batch 17536; loss 0.052123\n",
      "epoch:4; batch 17536; train accuracy: 0.837075\n",
      "epoch 4; batch 17664; loss 0.022574\n",
      "epoch:4; batch 17664; train accuracy: 0.837134\n",
      "epoch 4; batch 17792; loss 0.031006\n",
      "epoch:4; batch 17792; train accuracy: 0.837191\n",
      "epoch 4; batch 17920; loss 0.040834\n",
      "epoch:4; batch 17920; train accuracy: 0.837248\n",
      "epoch 4; batch 18048; loss 0.032964\n",
      "epoch:4; batch 18048; train accuracy: 0.837301\n",
      "epoch 4; batch 18176; loss 0.046564\n",
      "epoch:4; batch 18176; train accuracy: 0.837361\n",
      "epoch 4; batch 18304; loss 0.024531\n",
      "epoch:4; batch 18304; train accuracy: 0.837420\n",
      "epoch 4; batch 18432; loss 0.065924\n",
      "epoch:4; batch 18432; train accuracy: 0.837480\n",
      "epoch 4; batch 18560; loss 0.033234\n",
      "epoch:4; batch 18560; train accuracy: 0.837539\n",
      "epoch 4; batch 18688; loss 0.007380\n",
      "epoch:4; batch 18688; train accuracy: 0.837601\n",
      "epoch 4; batch 18816; loss 0.049315\n",
      "epoch:4; batch 18816; train accuracy: 0.837658\n",
      "epoch 4; batch 18944; loss 0.009035\n",
      "epoch:4; batch 18944; train accuracy: 0.837720\n",
      "epoch 4; batch 19072; loss 0.013978\n",
      "epoch:4; batch 19072; train accuracy: 0.837779\n",
      "epoch 4; batch 19200; loss 0.021793\n",
      "epoch:4; batch 19200; train accuracy: 0.837838\n",
      "epoch 4; batch 19328; loss 0.010860\n",
      "epoch:4; batch 19328; train accuracy: 0.837900\n",
      "epoch 4; batch 19456; loss 0.011500\n",
      "epoch:4; batch 19456; train accuracy: 0.837962\n",
      "epoch 4; batch 19584; loss 0.062369\n",
      "epoch:4; batch 19584; train accuracy: 0.838018\n",
      "epoch 4; batch 19712; loss 0.051379\n",
      "epoch:4; batch 19712; train accuracy: 0.838077\n",
      "epoch 4; batch 19840; loss 0.089783\n",
      "epoch:4; batch 19840; train accuracy: 0.838133\n",
      "epoch 4; batch 19968; loss 0.031454\n",
      "epoch:4; batch 19968; train accuracy: 0.838189\n",
      "epoch 4; batch 20096; loss 0.009582\n",
      "epoch:4; batch 20096; train accuracy: 0.838251\n",
      "epoch 4; batch 20224; loss 0.004790\n",
      "epoch:4; batch 20224; train accuracy: 0.838313\n",
      "epoch 4; batch 20352; loss 0.035354\n",
      "epoch:4; batch 20352; train accuracy: 0.838372\n",
      "epoch 4; batch 20480; loss 0.021879\n",
      "epoch:4; batch 20480; train accuracy: 0.838430\n",
      "epoch 4; batch 20608; loss 0.032520\n",
      "epoch:4; batch 20608; train accuracy: 0.838489\n",
      "epoch 4; batch 20736; loss 0.045284\n",
      "epoch:4; batch 20736; train accuracy: 0.838545\n",
      "epoch 4; batch 20864; loss 0.057162\n",
      "epoch:4; batch 20864; train accuracy: 0.838597\n",
      "epoch 4; batch 20992; loss 0.030732\n",
      "epoch:4; batch 20992; train accuracy: 0.838656\n",
      "epoch 4; batch 21120; loss 0.016696\n",
      "epoch:4; batch 21120; train accuracy: 0.838714\n",
      "epoch 4; batch 21248; loss 0.030049\n",
      "epoch:4; batch 21248; train accuracy: 0.838770\n",
      "epoch 4; batch 21376; loss 0.025275\n",
      "epoch:4; batch 21376; train accuracy: 0.838828\n",
      "epoch 4; batch 21504; loss 0.013611\n",
      "epoch:4; batch 21504; train accuracy: 0.838886\n",
      "epoch 4; batch 21632; loss 0.037435\n",
      "epoch:4; batch 21632; train accuracy: 0.838939\n",
      "epoch 4; batch 21760; loss 0.019295\n",
      "epoch:4; batch 21760; train accuracy: 0.838997\n",
      "epoch 4; batch 21888; loss 0.015568\n",
      "epoch:4; batch 21888; train accuracy: 0.839055\n",
      "epoch 4; batch 22016; loss 0.014875\n",
      "epoch:4; batch 22016; train accuracy: 0.839114\n",
      "epoch 4; batch 22144; loss 0.020976\n",
      "epoch:4; batch 22144; train accuracy: 0.839175\n",
      "epoch 4; batch 22272; loss 0.011699\n",
      "epoch:4; batch 22272; train accuracy: 0.839236\n",
      "epoch 4; batch 22400; loss 0.056860\n",
      "epoch:4; batch 22400; train accuracy: 0.839291\n",
      "epoch 4; batch 22528; loss 0.054847\n",
      "epoch:4; batch 22528; train accuracy: 0.839346\n",
      "epoch 4; batch 22656; loss 0.021083\n",
      "epoch:4; batch 22656; train accuracy: 0.839404\n",
      "epoch 4; batch 22784; loss 0.026007\n",
      "epoch:4; batch 22784; train accuracy: 0.839462\n",
      "epoch 4; batch 22912; loss 0.074032\n",
      "epoch:4; batch 22912; train accuracy: 0.839511\n",
      "epoch 4; batch 23040; loss 0.040877\n",
      "epoch:4; batch 23040; train accuracy: 0.839566\n",
      "epoch 4; batch 23168; loss 0.014011\n",
      "epoch:4; batch 23168; train accuracy: 0.839626\n",
      "epoch 4; batch 23296; loss 0.049005\n",
      "epoch:4; batch 23296; train accuracy: 0.839684\n",
      "epoch 4; batch 23424; loss 0.039970\n",
      "epoch:4; batch 23424; train accuracy: 0.839739\n",
      "epoch 4; batch 23552; loss 0.013061\n",
      "epoch:4; batch 23552; train accuracy: 0.839796\n",
      "epoch 4; batch 23680; loss 0.027162\n",
      "epoch:4; batch 23680; train accuracy: 0.839857\n",
      "epoch 4; batch 23808; loss 0.058643\n",
      "epoch:4; batch 23808; train accuracy: 0.839909\n",
      "epoch 4; batch 23936; loss 0.028765\n",
      "epoch:4; batch 23936; train accuracy: 0.839966\n",
      "epoch 4; batch 24064; loss 0.021329\n",
      "epoch:4; batch 24064; train accuracy: 0.840024\n",
      "epoch 4; batch 24192; loss 0.030367\n",
      "epoch:4; batch 24192; train accuracy: 0.840075\n",
      "epoch 4; batch 24320; loss 0.024060\n",
      "epoch:4; batch 24320; train accuracy: 0.840133\n",
      "epoch 4; batch 24448; loss 0.014487\n",
      "epoch:4; batch 24448; train accuracy: 0.840190\n",
      "epoch 4; batch 24576; loss 0.011232\n",
      "epoch:4; batch 24576; train accuracy: 0.840247\n",
      "epoch 4; batch 24704; loss 0.014773\n",
      "epoch:4; batch 24704; train accuracy: 0.840305\n",
      "epoch 4; batch 24832; loss 0.013428\n",
      "epoch:4; batch 24832; train accuracy: 0.840365\n",
      "epoch 4; batch 24960; loss 0.025345\n",
      "epoch:4; batch 24960; train accuracy: 0.840422\n",
      "epoch 4; batch 25088; loss 0.023404\n",
      "epoch:4; batch 25088; train accuracy: 0.840479\n",
      "epoch 4; batch 25216; loss 0.006883\n",
      "epoch:4; batch 25216; train accuracy: 0.840539\n",
      "epoch 4; batch 25344; loss 0.007202\n",
      "epoch:4; batch 25344; train accuracy: 0.840599\n",
      "epoch 4; batch 25472; loss 0.026257\n",
      "epoch:4; batch 25472; train accuracy: 0.840656\n",
      "epoch 4; batch 25600; loss 0.024337\n",
      "epoch:4; batch 25600; train accuracy: 0.840713\n",
      "epoch 4; batch 25728; loss 0.005293\n",
      "epoch:4; batch 25728; train accuracy: 0.840773\n",
      "epoch 4; batch 25856; loss 0.090785\n",
      "epoch:4; batch 25856; train accuracy: 0.840830\n",
      "epoch 4; batch 25984; loss 0.031824\n",
      "epoch:4; batch 25984; train accuracy: 0.840884\n",
      "epoch 4; batch 26112; loss 0.044390\n",
      "epoch:4; batch 26112; train accuracy: 0.840935\n",
      "epoch 4; batch 26240; loss 0.009804\n",
      "epoch:4; batch 26240; train accuracy: 0.840994\n",
      "epoch 4; batch 26368; loss 0.016020\n",
      "epoch:4; batch 26368; train accuracy: 0.841051\n",
      "epoch 4; batch 26496; loss 0.034328\n",
      "epoch:4; batch 26496; train accuracy: 0.841102\n",
      "epoch 4; batch 26624; loss 0.036882\n",
      "epoch:4; batch 26624; train accuracy: 0.841159\n",
      "epoch 4; batch 26752; loss 0.139859\n",
      "epoch:4; batch 26752; train accuracy: 0.841209\n",
      "epoch 4; batch 26880; loss 0.032589\n",
      "epoch:4; batch 26880; train accuracy: 0.841266\n",
      "epoch 4; batch 27008; loss 0.062403\n",
      "epoch:4; batch 27008; train accuracy: 0.841316\n",
      "epoch 4; batch 27136; loss 0.001514\n",
      "epoch:4; batch 27136; train accuracy: 0.841376\n",
      "epoch 4; batch 27264; loss 0.012843\n",
      "epoch:4; batch 27264; train accuracy: 0.841435\n",
      "epoch 4; batch 27392; loss 0.008444\n",
      "epoch:4; batch 27392; train accuracy: 0.841492\n",
      "epoch 4; batch 27520; loss 0.046484\n",
      "epoch:4; batch 27520; train accuracy: 0.841548\n",
      "epoch 4; batch 27648; loss 0.106354\n",
      "epoch:4; batch 27648; train accuracy: 0.841598\n",
      "epoch 4; batch 27776; loss 0.009001\n",
      "epoch:4; batch 27776; train accuracy: 0.841658\n",
      "epoch 4; batch 27904; loss 0.004903\n",
      "epoch:4; batch 27904; train accuracy: 0.841717\n",
      "epoch 4; batch 28032; loss 0.010796\n",
      "epoch:4; batch 28032; train accuracy: 0.841776\n",
      "epoch 4; batch 28160; loss 0.032147\n",
      "epoch:4; batch 28160; train accuracy: 0.841829\n",
      "epoch 4; batch 28288; loss 0.071364\n",
      "epoch:4; batch 28288; train accuracy: 0.841882\n",
      "epoch 4; batch 28416; loss 0.006321\n",
      "epoch:4; batch 28416; train accuracy: 0.841941\n",
      "epoch 4; batch 28544; loss 0.030292\n",
      "epoch:4; batch 28544; train accuracy: 0.841994\n",
      "epoch 4; batch 28672; loss 0.008248\n",
      "epoch:4; batch 28672; train accuracy: 0.842053\n",
      "epoch 4; batch 28800; loss 0.018303\n",
      "epoch:4; batch 28800; train accuracy: 0.842109\n",
      "epoch 4; batch 28928; loss 0.047827\n",
      "epoch:4; batch 28928; train accuracy: 0.842162\n",
      "epoch 4; batch 29056; loss 0.088621\n",
      "epoch:4; batch 29056; train accuracy: 0.842215\n",
      "epoch 4; batch 29184; loss 0.026694\n",
      "epoch:4; batch 29184; train accuracy: 0.842271\n",
      "epoch 4; batch 29312; loss 0.011087\n",
      "epoch:4; batch 29312; train accuracy: 0.842329\n",
      "epoch 4; batch 29440; loss 0.028336\n",
      "epoch:4; batch 29440; train accuracy: 0.842382\n",
      "epoch 4; batch 29568; loss 0.034272\n",
      "epoch:4; batch 29568; train accuracy: 0.842435\n",
      "epoch 4; batch 29696; loss 0.060764\n",
      "epoch:4; batch 29696; train accuracy: 0.842482\n",
      "epoch 4; batch 29824; loss 0.029944\n",
      "epoch:4; batch 29824; train accuracy: 0.842534\n",
      "epoch 4; batch 29952; loss 0.022227\n",
      "epoch:4; batch 29952; train accuracy: 0.842590\n",
      "epoch 4; batch 30080; loss 0.003592\n",
      "epoch:4; batch 30080; train accuracy: 0.842648\n",
      "epoch 4; batch 30208; loss 0.062743\n",
      "epoch:4; batch 30208; train accuracy: 0.842701\n",
      "epoch 4; batch 30336; loss 0.008865\n",
      "epoch:4; batch 30336; train accuracy: 0.842759\n",
      "epoch 4; batch 30464; loss 0.008810\n",
      "epoch:4; batch 30464; train accuracy: 0.842818\n",
      "epoch 4; batch 30592; loss 0.044862\n",
      "epoch:4; batch 30592; train accuracy: 0.842873\n",
      "epoch 4; batch 30720; loss 0.038698\n",
      "epoch:4; batch 30720; train accuracy: 0.842925\n",
      "epoch 4; batch 30848; loss 0.018461\n",
      "epoch:4; batch 30848; train accuracy: 0.842981\n",
      "epoch 4; batch 30976; loss 0.014345\n",
      "epoch:4; batch 30976; train accuracy: 0.843036\n",
      "epoch 4; batch 31104; loss 0.004007\n",
      "epoch:4; batch 31104; train accuracy: 0.843094\n",
      "epoch 4; batch 31232; loss 0.003570\n",
      "epoch:4; batch 31232; train accuracy: 0.843152\n",
      "epoch 4; batch 31360; loss 0.018873\n",
      "epoch:4; batch 31360; train accuracy: 0.843210\n",
      "epoch 4; batch 31488; loss 0.006993\n",
      "epoch:4; batch 31488; train accuracy: 0.843268\n",
      "epoch 4; batch 31616; loss 0.007384\n",
      "epoch:4; batch 31616; train accuracy: 0.843326\n",
      "epoch 4; batch 31744; loss 0.015395\n",
      "epoch:4; batch 31744; train accuracy: 0.843381\n",
      "epoch 4; batch 31872; loss 0.045581\n",
      "epoch:4; batch 31872; train accuracy: 0.843436\n",
      "epoch 4; batch 32000; loss 0.086557\n",
      "epoch:4; batch 32000; train accuracy: 0.843482\n",
      "epoch 4; batch 32128; loss 0.024477\n",
      "epoch:4; batch 32128; train accuracy: 0.843534\n",
      "epoch 4; batch 32256; loss 0.135807\n",
      "epoch:4; batch 32256; train accuracy: 0.843583\n",
      "epoch 4; batch 32384; loss 0.019336\n",
      "epoch:4; batch 32384; train accuracy: 0.843638\n",
      "epoch 4; batch 32512; loss 0.066741\n",
      "epoch:4; batch 32512; train accuracy: 0.843690\n",
      "epoch 4; batch 32640; loss 0.094609\n",
      "epoch:4; batch 32640; train accuracy: 0.843736\n",
      "epoch 4; batch 32768; loss 0.024353\n",
      "epoch:4; batch 32768; train accuracy: 0.843790\n",
      "epoch 4; batch 32896; loss 0.013621\n",
      "epoch:4; batch 32896; train accuracy: 0.843845\n",
      "epoch 4; batch 33024; loss 0.016812\n",
      "epoch:4; batch 33024; train accuracy: 0.843899\n",
      "epoch 4; batch 33152; loss 0.003885\n",
      "epoch:4; batch 33152; train accuracy: 0.843957\n",
      "epoch 4; batch 33280; loss 0.042979\n",
      "epoch:4; batch 33280; train accuracy: 0.844006\n",
      "epoch 4; batch 33408; loss 0.015905\n",
      "epoch:4; batch 33408; train accuracy: 0.844060\n",
      "epoch 4; batch 33536; loss 0.010557\n",
      "epoch:4; batch 33536; train accuracy: 0.844117\n",
      "epoch 4; batch 33664; loss 0.018451\n",
      "epoch:4; batch 33664; train accuracy: 0.844175\n",
      "epoch 4; batch 33792; loss 0.006572\n",
      "epoch:4; batch 33792; train accuracy: 0.844232\n",
      "epoch 4; batch 33920; loss 0.068400\n",
      "epoch:4; batch 33920; train accuracy: 0.844283\n",
      "epoch 4; batch 34048; loss 0.047902\n",
      "epoch:4; batch 34048; train accuracy: 0.844335\n",
      "epoch 4; batch 34176; loss 0.030881\n",
      "epoch:4; batch 34176; train accuracy: 0.844389\n",
      "epoch 4; batch 34304; loss 0.008099\n",
      "epoch:4; batch 34304; train accuracy: 0.844446\n",
      "epoch 4; batch 34432; loss 0.009026\n",
      "epoch:4; batch 34432; train accuracy: 0.844500\n",
      "epoch 4; batch 34560; loss 0.015016\n",
      "epoch:4; batch 34560; train accuracy: 0.844554\n",
      "epoch 4; batch 34688; loss 0.009845\n",
      "epoch:4; batch 34688; train accuracy: 0.844611\n",
      "epoch 4; batch 34816; loss 0.085545\n",
      "epoch:4; batch 34816; train accuracy: 0.844657\n",
      "epoch 4; batch 34944; loss 0.004236\n",
      "epoch:4; batch 34944; train accuracy: 0.844713\n",
      "epoch 4; batch 35072; loss 0.065220\n",
      "epoch:4; batch 35072; train accuracy: 0.844764\n",
      "epoch 4; batch 35200; loss 0.071850\n",
      "epoch:4; batch 35200; train accuracy: 0.844813\n",
      "epoch 4; batch 35328; loss 0.041071\n",
      "epoch:4; batch 35328; train accuracy: 0.844861\n",
      "epoch 4; batch 35456; loss 0.019154\n",
      "epoch:4; batch 35456; train accuracy: 0.844917\n",
      "epoch 4; batch 35584; loss 0.110061\n",
      "epoch:4; batch 35584; train accuracy: 0.844960\n",
      "epoch 4; batch 35712; loss 0.015581\n",
      "epoch:4; batch 35712; train accuracy: 0.845014\n",
      "epoch 4; batch 35840; loss 0.048036\n",
      "epoch:4; batch 35840; train accuracy: 0.845064\n",
      "epoch 4; batch 35968; loss 0.052291\n",
      "epoch:4; batch 35968; train accuracy: 0.845110\n",
      "epoch 4; batch 36096; loss 0.086958\n",
      "epoch:4; batch 36096; train accuracy: 0.845155\n",
      "epoch 4; batch 36224; loss 0.093845\n",
      "epoch:4; batch 36224; train accuracy: 0.845208\n",
      "epoch 4; batch 36352; loss 0.120255\n",
      "epoch:4; batch 36352; train accuracy: 0.845256\n",
      "epoch 4; batch 36480; loss 0.119287\n",
      "epoch:4; batch 36480; train accuracy: 0.845304\n",
      "epoch 4; batch 36608; loss 0.029708\n",
      "epoch:4; batch 36608; train accuracy: 0.845352\n",
      "epoch 4; batch 36736; loss 0.014815\n",
      "epoch:4; batch 36736; train accuracy: 0.845405\n",
      "epoch 4; batch 36864; loss 0.021394\n",
      "epoch:4; batch 36864; train accuracy: 0.845459\n",
      "epoch 4; batch 36992; loss 0.078489\n",
      "epoch:4; batch 36992; train accuracy: 0.845509\n",
      "epoch 4; batch 37120; loss 0.034620\n",
      "epoch:4; batch 37120; train accuracy: 0.845560\n",
      "epoch 4; batch 37248; loss 0.029266\n",
      "epoch:4; batch 37248; train accuracy: 0.845610\n",
      "epoch 4; batch 37376; loss 0.060663\n",
      "epoch:4; batch 37376; train accuracy: 0.845658\n",
      "epoch 4; batch 37504; loss 0.036729\n",
      "epoch:4; batch 37504; train accuracy: 0.845708\n",
      "epoch 4; batch 37632; loss 0.020741\n",
      "epoch:4; batch 37632; train accuracy: 0.845758\n",
      "epoch 4; batch 37760; loss 0.035288\n",
      "epoch:4; batch 37760; train accuracy: 0.845809\n",
      "epoch 4; batch 37888; loss 0.017894\n",
      "epoch:4; batch 37888; train accuracy: 0.845862\n",
      "epoch 4; batch 38016; loss 0.063510\n",
      "epoch:4; batch 38016; train accuracy: 0.845915\n",
      "epoch 4; batch 38144; loss 0.021616\n",
      "epoch:4; batch 38144; train accuracy: 0.845968\n",
      "epoch 4; batch 38272; loss 0.025985\n",
      "epoch:4; batch 38272; train accuracy: 0.846024\n",
      "epoch 4; batch 38400; loss 0.013046\n",
      "epoch:4; batch 38400; train accuracy: 0.846080\n",
      "epoch 4; batch 38528; loss 0.007524\n",
      "epoch:4; batch 38528; train accuracy: 0.846135\n",
      "epoch 4; batch 38656; loss 0.055977\n",
      "epoch:4; batch 38656; train accuracy: 0.846183\n",
      "epoch 4; batch 38784; loss 0.092830\n",
      "epoch:4; batch 38784; train accuracy: 0.846230\n",
      "epoch 4; batch 38912; loss 0.062726\n",
      "epoch:4; batch 38912; train accuracy: 0.846280\n",
      "epoch 4; batch 39040; loss 0.040577\n",
      "epoch:4; batch 39040; train accuracy: 0.846324\n",
      "epoch 4; batch 39168; loss 0.003511\n",
      "epoch:4; batch 39168; train accuracy: 0.846380\n",
      "epoch 4; batch 39296; loss 0.038395\n",
      "epoch:4; batch 39296; train accuracy: 0.846427\n",
      "epoch 4; batch 39424; loss 0.013893\n",
      "epoch:4; batch 39424; train accuracy: 0.846482\n",
      "epoch 4; batch 39552; loss 0.078879\n",
      "epoch:4; batch 39552; train accuracy: 0.846529\n",
      "epoch 4; batch 39680; loss 0.008330\n",
      "epoch:4; batch 39680; train accuracy: 0.846584\n",
      "epoch 4; batch 39808; loss 0.002663\n",
      "epoch:4; batch 39808; train accuracy: 0.846640\n",
      "epoch 4; batch 39936; loss 0.038489\n",
      "epoch:4; batch 39936; train accuracy: 0.846684\n",
      "epoch 4; batch 40064; loss 0.017740\n",
      "epoch:4; batch 40064; train accuracy: 0.846736\n",
      "epoch 4; batch 40192; loss 0.024990\n",
      "epoch:4; batch 40192; train accuracy: 0.846789\n",
      "epoch 4; batch 40320; loss 0.063326\n",
      "epoch:4; batch 40320; train accuracy: 0.846838\n",
      "epoch 4; batch 40448; loss 0.048682\n",
      "epoch:4; batch 40448; train accuracy: 0.846885\n",
      "epoch 4; batch 40576; loss 0.025119\n",
      "epoch:4; batch 40576; train accuracy: 0.846937\n",
      "epoch 4; batch 40704; loss 0.117438\n",
      "epoch:4; batch 40704; train accuracy: 0.846978\n",
      "epoch 4; batch 40832; loss 0.024458\n",
      "epoch:4; batch 40832; train accuracy: 0.847031\n",
      "epoch 4; batch 40960; loss 0.010752\n",
      "epoch:4; batch 40960; train accuracy: 0.847086\n",
      "epoch 4; batch 41088; loss 0.036506\n",
      "epoch:4; batch 41088; train accuracy: 0.847135\n",
      "epoch 4; batch 41216; loss 0.012855\n",
      "epoch:4; batch 41216; train accuracy: 0.847187\n",
      "epoch 4; batch 41344; loss 0.021747\n",
      "epoch:4; batch 41344; train accuracy: 0.847239\n",
      "epoch 4; batch 41472; loss 0.021149\n",
      "epoch:4; batch 41472; train accuracy: 0.847291\n",
      "epoch 4; batch 41600; loss 0.038339\n",
      "epoch:4; batch 41600; train accuracy: 0.847338\n",
      "epoch 4; batch 41728; loss 0.013949\n",
      "epoch:4; batch 41728; train accuracy: 0.847390\n",
      "epoch 4; batch 41856; loss 0.030952\n",
      "epoch:4; batch 41856; train accuracy: 0.847439\n",
      "epoch 4; batch 41984; loss 0.035621\n",
      "epoch:4; batch 41984; train accuracy: 0.847488\n",
      "epoch 4; batch 42112; loss 0.071255\n",
      "epoch:4; batch 42112; train accuracy: 0.847537\n",
      "epoch 4; batch 42240; loss 0.018957\n",
      "epoch:4; batch 42240; train accuracy: 0.847589\n",
      "epoch 4; batch 42368; loss 0.018543\n",
      "epoch:4; batch 42368; train accuracy: 0.847641\n",
      "epoch 4; batch 42496; loss 0.034694\n",
      "epoch:4; batch 42496; train accuracy: 0.847693\n",
      "epoch 4; batch 42624; loss 0.020189\n",
      "epoch:4; batch 42624; train accuracy: 0.847744\n",
      "epoch 4; batch 42752; loss 0.004852\n",
      "epoch:4; batch 42752; train accuracy: 0.847799\n",
      "epoch 4; batch 42880; loss 0.036255\n",
      "epoch:4; batch 42880; train accuracy: 0.847848\n",
      "epoch 4; batch 43008; loss 0.010605\n",
      "epoch:4; batch 43008; train accuracy: 0.847902\n",
      "epoch 4; batch 43136; loss 0.020400\n",
      "epoch:4; batch 43136; train accuracy: 0.847951\n",
      "epoch 4; batch 43264; loss 0.091555\n",
      "epoch:4; batch 43264; train accuracy: 0.848002\n",
      "epoch 4; batch 43392; loss 0.027688\n",
      "epoch:4; batch 43392; train accuracy: 0.848054\n",
      "epoch 4; batch 43520; loss 0.014169\n",
      "epoch:4; batch 43520; train accuracy: 0.848108\n",
      "epoch 4; batch 43648; loss 0.004977\n",
      "epoch:4; batch 43648; train accuracy: 0.848162\n",
      "epoch 4; batch 43776; loss 0.001411\n",
      "epoch:4; batch 43776; train accuracy: 0.848217\n",
      "epoch 4; batch 43904; loss 0.023284\n",
      "epoch:4; batch 43904; train accuracy: 0.848268\n",
      "epoch 4; batch 44032; loss 0.131586\n",
      "epoch:4; batch 44032; train accuracy: 0.848317\n",
      "epoch 4; batch 44160; loss 0.055796\n",
      "epoch:4; batch 44160; train accuracy: 0.848362\n",
      "epoch 4; batch 44288; loss 0.008541\n",
      "epoch:4; batch 44288; train accuracy: 0.848416\n",
      "epoch 4; batch 44416; loss 0.006644\n",
      "epoch:4; batch 44416; train accuracy: 0.848470\n",
      "epoch 4; batch 44544; loss 0.015049\n",
      "epoch:4; batch 44544; train accuracy: 0.848522\n",
      "epoch 4; batch 44672; loss 0.006016\n",
      "epoch:4; batch 44672; train accuracy: 0.848575\n",
      "epoch 4; batch 44800; loss 0.006074\n",
      "epoch:4; batch 44800; train accuracy: 0.848629\n",
      "epoch 4; batch 44928; loss 0.048422\n",
      "epoch:4; batch 44928; train accuracy: 0.848680\n",
      "epoch 4; batch 45056; loss 0.089760\n",
      "epoch:4; batch 45056; train accuracy: 0.848726\n",
      "epoch 4; batch 45184; loss 0.012992\n",
      "epoch:4; batch 45184; train accuracy: 0.848777\n",
      "epoch 4; batch 45312; loss 0.005901\n",
      "epoch:4; batch 45312; train accuracy: 0.848831\n",
      "epoch 4; batch 45440; loss 0.040628\n",
      "epoch:4; batch 45440; train accuracy: 0.848876\n",
      "epoch 4; batch 45568; loss 0.011313\n",
      "epoch:4; batch 45568; train accuracy: 0.848927\n",
      "epoch 4; batch 45696; loss 0.024276\n",
      "epoch:4; batch 45696; train accuracy: 0.848978\n",
      "epoch 4; batch 45824; loss 0.006836\n",
      "epoch:4; batch 45824; train accuracy: 0.849031\n",
      "epoch 4; batch 45952; loss 0.047215\n",
      "epoch:4; batch 45952; train accuracy: 0.849082\n",
      "epoch 4; batch 46080; loss 0.014437\n",
      "epoch:4; batch 46080; train accuracy: 0.849133\n",
      "epoch 4; batch 46208; loss 0.010419\n",
      "epoch:4; batch 46208; train accuracy: 0.849186\n",
      "epoch 4; batch 46336; loss 0.024727\n",
      "epoch:4; batch 46336; train accuracy: 0.849237\n",
      "epoch 4; batch 46464; loss 0.014757\n",
      "epoch:4; batch 46464; train accuracy: 0.849288\n",
      "epoch 4; batch 46592; loss 0.006140\n",
      "epoch:4; batch 46592; train accuracy: 0.849341\n",
      "epoch 4; batch 46720; loss 0.096102\n",
      "epoch:4; batch 46720; train accuracy: 0.849389\n",
      "epoch 4; batch 46848; loss 0.044116\n",
      "epoch:4; batch 46848; train accuracy: 0.849439\n",
      "epoch 4; batch 46976; loss 0.024912\n",
      "epoch:4; batch 46976; train accuracy: 0.849487\n",
      "epoch 4; batch 47104; loss 0.010234\n",
      "epoch:4; batch 47104; train accuracy: 0.849540\n",
      "epoch 4; batch 47232; loss 0.134772\n",
      "epoch:4; batch 47232; train accuracy: 0.849588\n",
      "epoch 4; batch 47360; loss 0.005868\n",
      "epoch:4; batch 47360; train accuracy: 0.849641\n",
      "epoch 4; batch 47488; loss 0.020151\n",
      "epoch:4; batch 47488; train accuracy: 0.849689\n",
      "epoch 4; batch 47616; loss 0.022935\n",
      "epoch:4; batch 47616; train accuracy: 0.849736\n",
      "epoch 4; batch 47744; loss 0.023532\n",
      "epoch:4; batch 47744; train accuracy: 0.849787\n",
      "epoch 4; batch 47872; loss 0.012765\n",
      "epoch:4; batch 47872; train accuracy: 0.849840\n",
      "epoch 4; batch 48000; loss 0.026802\n",
      "epoch:4; batch 48000; train accuracy: 0.849890\n",
      "epoch 4; batch 48128; loss 0.014748\n",
      "epoch:4; batch 48128; train accuracy: 0.849943\n",
      "epoch 4; batch 48256; loss 0.011245\n",
      "epoch:4; batch 48256; train accuracy: 0.849996\n",
      "epoch 4; batch 48384; loss 0.009312\n",
      "epoch:4; batch 48384; train accuracy: 0.850048\n",
      "epoch 4; batch 48512; loss 0.004423\n",
      "epoch:4; batch 48512; train accuracy: 0.850101\n",
      "epoch 4; batch 48640; loss 0.019232\n",
      "epoch:4; batch 48640; train accuracy: 0.850149\n",
      "epoch 4; batch 48768; loss 0.027028\n",
      "epoch:4; batch 48768; train accuracy: 0.850199\n",
      "epoch 4; batch 48896; loss 0.033225\n",
      "epoch:4; batch 48896; train accuracy: 0.850246\n",
      "epoch 4; batch 49024; loss 0.015484\n",
      "epoch:4; batch 49024; train accuracy: 0.850296\n",
      "epoch 4; batch 49152; loss 0.009922\n",
      "epoch:4; batch 49152; train accuracy: 0.850346\n",
      "epoch 4; batch 49280; loss 0.004904\n",
      "epoch:4; batch 49280; train accuracy: 0.850398\n",
      "epoch 4; batch 49408; loss 0.006333\n",
      "epoch:4; batch 49408; train accuracy: 0.850451\n",
      "epoch 4; batch 49536; loss 0.005441\n",
      "epoch:4; batch 49536; train accuracy: 0.850503\n",
      "epoch 4; batch 49664; loss 0.046901\n",
      "epoch:4; batch 49664; train accuracy: 0.850550\n",
      "epoch 4; batch 49792; loss 0.062286\n",
      "epoch:4; batch 49792; train accuracy: 0.850600\n",
      "epoch 4; batch 49920; loss 0.021672\n",
      "epoch:4; batch 49920; train accuracy: 0.850647\n",
      "epoch 4; batch 50048; loss 0.011541\n",
      "epoch:4; batch 50048; train accuracy: 0.850697\n",
      "epoch 4; batch 50176; loss 0.012347\n",
      "epoch:4; batch 50176; train accuracy: 0.850749\n",
      "epoch 4; batch 50304; loss 0.006597\n",
      "epoch:4; batch 50304; train accuracy: 0.850801\n",
      "epoch 4; batch 50432; loss 0.027147\n",
      "epoch:4; batch 50432; train accuracy: 0.850851\n",
      "epoch 4; batch 50560; loss 0.009182\n",
      "epoch:4; batch 50560; train accuracy: 0.850903\n",
      "epoch 4; batch 50688; loss 0.009334\n",
      "epoch:4; batch 50688; train accuracy: 0.850953\n",
      "epoch 4; batch 50816; loss 0.007994\n",
      "epoch:4; batch 50816; train accuracy: 0.851005\n",
      "epoch 4; batch 50944; loss 0.051071\n",
      "epoch:4; batch 50944; train accuracy: 0.851049\n",
      "epoch 4; batch 51072; loss 0.039003\n",
      "epoch:4; batch 51072; train accuracy: 0.851098\n",
      "epoch 4; batch 51200; loss 0.037291\n",
      "epoch:4; batch 51200; train accuracy: 0.851145\n",
      "epoch 4; batch 51328; loss 0.007787\n",
      "epoch:4; batch 51328; train accuracy: 0.851197\n",
      "epoch 4; batch 51456; loss 0.077796\n",
      "epoch:4; batch 51456; train accuracy: 0.851243\n",
      "epoch 4; batch 51584; loss 0.042756\n",
      "epoch:4; batch 51584; train accuracy: 0.851284\n",
      "epoch 4; batch 51712; loss 0.018099\n",
      "epoch:4; batch 51712; train accuracy: 0.851333\n",
      "epoch 4; batch 51840; loss 0.004520\n",
      "epoch:4; batch 51840; train accuracy: 0.851385\n",
      "epoch 4; batch 51968; loss 0.027334\n",
      "epoch:4; batch 51968; train accuracy: 0.851434\n",
      "epoch 4; batch 52096; loss 0.011120\n",
      "epoch:4; batch 52096; train accuracy: 0.851483\n",
      "epoch 4; batch 52224; loss 0.005362\n",
      "epoch:4; batch 52224; train accuracy: 0.851535\n",
      "epoch 4; batch 52352; loss 0.049561\n",
      "epoch:4; batch 52352; train accuracy: 0.851582\n",
      "epoch 4; batch 52480; loss 0.006714\n",
      "epoch:4; batch 52480; train accuracy: 0.851633\n",
      "epoch 4; batch 52608; loss 0.003987\n",
      "epoch:4; batch 52608; train accuracy: 0.851685\n",
      "epoch 4; batch 52736; loss 0.040009\n",
      "epoch:4; batch 52736; train accuracy: 0.851731\n",
      "epoch 4; batch 52864; loss 0.043351\n",
      "epoch:4; batch 52864; train accuracy: 0.851777\n",
      "epoch 4; batch 52992; loss 0.019351\n",
      "epoch:4; batch 52992; train accuracy: 0.851826\n",
      "epoch 4; batch 53120; loss 0.034307\n",
      "epoch:4; batch 53120; train accuracy: 0.851875\n",
      "epoch 4; batch 53248; loss 0.004899\n",
      "epoch:4; batch 53248; train accuracy: 0.851927\n",
      "epoch 4; batch 53376; loss 0.004418\n",
      "epoch:4; batch 53376; train accuracy: 0.851978\n",
      "epoch 4; batch 53504; loss 0.003745\n",
      "epoch:4; batch 53504; train accuracy: 0.852029\n",
      "epoch 4; batch 53632; loss 0.031993\n",
      "epoch:4; batch 53632; train accuracy: 0.852075\n",
      "epoch 4; batch 53760; loss 0.019774\n",
      "epoch:4; batch 53760; train accuracy: 0.852124\n",
      "epoch 4; batch 53888; loss 0.016638\n",
      "epoch:4; batch 53888; train accuracy: 0.852175\n",
      "epoch 4; batch 54016; loss 0.031015\n",
      "epoch:4; batch 54016; train accuracy: 0.852221\n",
      "epoch 4; batch 54144; loss 0.042038\n",
      "epoch:4; batch 54144; train accuracy: 0.852267\n",
      "epoch 4; batch 54272; loss 0.033396\n",
      "epoch:4; batch 54272; train accuracy: 0.852313\n",
      "epoch 4; batch 54400; loss 0.019992\n",
      "epoch:4; batch 54400; train accuracy: 0.852364\n",
      "epoch 4; batch 54528; loss 0.022589\n",
      "epoch:4; batch 54528; train accuracy: 0.852413\n",
      "epoch 4; batch 54656; loss 0.054493\n",
      "epoch:4; batch 54656; train accuracy: 0.852461\n",
      "epoch 4; batch 54784; loss 0.021446\n",
      "epoch:4; batch 54784; train accuracy: 0.852509\n",
      "epoch 4; batch 54912; loss 0.017471\n",
      "epoch:4; batch 54912; train accuracy: 0.852558\n",
      "epoch 4; batch 55040; loss 0.045125\n",
      "epoch:4; batch 55040; train accuracy: 0.852603\n",
      "epoch 4; batch 55168; loss 0.055111\n",
      "epoch:4; batch 55168; train accuracy: 0.852646\n",
      "epoch 4; batch 55296; loss 0.054853\n",
      "epoch:4; batch 55296; train accuracy: 0.852686\n",
      "epoch 4; batch 55424; loss 0.025336\n",
      "epoch:4; batch 55424; train accuracy: 0.852732\n",
      "epoch 4; batch 55552; loss 0.004303\n",
      "epoch:4; batch 55552; train accuracy: 0.852783\n",
      "epoch 4; batch 55680; loss 0.019060\n",
      "epoch:4; batch 55680; train accuracy: 0.852831\n",
      "epoch 4; batch 55808; loss 0.027668\n",
      "epoch:4; batch 55808; train accuracy: 0.852879\n",
      "epoch 4; batch 55936; loss 0.033362\n",
      "epoch:4; batch 55936; train accuracy: 0.852927\n",
      "epoch 4; batch 56064; loss 0.055608\n",
      "epoch:4; batch 56064; train accuracy: 0.852972\n",
      "epoch 4; batch 56192; loss 0.011270\n",
      "epoch:4; batch 56192; train accuracy: 0.853023\n",
      "epoch 4; batch 56320; loss 0.009879\n",
      "epoch:4; batch 56320; train accuracy: 0.853074\n",
      "epoch 4; batch 56448; loss 0.024834\n",
      "epoch:4; batch 56448; train accuracy: 0.853122\n",
      "epoch 4; batch 56576; loss 0.009960\n",
      "epoch:4; batch 56576; train accuracy: 0.853170\n",
      "epoch 4; batch 56704; loss 0.009913\n",
      "epoch:4; batch 56704; train accuracy: 0.853220\n",
      "epoch 4; batch 56832; loss 0.001697\n",
      "epoch:4; batch 56832; train accuracy: 0.853271\n",
      "epoch 4; batch 56960; loss 0.003458\n",
      "epoch:4; batch 56960; train accuracy: 0.853321\n",
      "epoch 4; batch 57088; loss 0.014251\n",
      "epoch:4; batch 57088; train accuracy: 0.853369\n",
      "epoch 4; batch 57216; loss 0.005568\n",
      "epoch:4; batch 57216; train accuracy: 0.853420\n",
      "epoch 4; batch 57344; loss 0.012869\n",
      "epoch:4; batch 57344; train accuracy: 0.853470\n",
      "epoch 4; batch 57472; loss 0.008881\n",
      "epoch:4; batch 57472; train accuracy: 0.853520\n",
      "epoch 4; batch 57600; loss 0.034451\n",
      "epoch:4; batch 57600; train accuracy: 0.853565\n",
      "epoch 4; batch 57728; loss 0.005052\n",
      "epoch:4; batch 57728; train accuracy: 0.853616\n",
      "epoch 4; batch 57856; loss 0.014340\n",
      "epoch:4; batch 57856; train accuracy: 0.853666\n",
      "epoch 4; batch 57984; loss 0.015226\n",
      "epoch:4; batch 57984; train accuracy: 0.853713\n",
      "epoch 4; batch 58112; loss 0.004626\n",
      "epoch:4; batch 58112; train accuracy: 0.853764\n",
      "epoch 4; batch 58240; loss 0.009884\n",
      "epoch:4; batch 58240; train accuracy: 0.853814\n",
      "epoch 4; batch 58368; loss 0.054351\n",
      "epoch:4; batch 58368; train accuracy: 0.853859\n",
      "epoch 4; batch 58496; loss 0.013862\n",
      "epoch:4; batch 58496; train accuracy: 0.853906\n",
      "epoch 4; batch 58624; loss 0.012053\n",
      "epoch:4; batch 58624; train accuracy: 0.853953\n",
      "epoch 4; batch 58752; loss 0.033027\n",
      "epoch:4; batch 58752; train accuracy: 0.853998\n",
      "epoch 4; batch 58880; loss 0.010113\n",
      "epoch:4; batch 58880; train accuracy: 0.854048\n",
      "epoch 4; batch 59008; loss 0.002854\n",
      "epoch:4; batch 59008; train accuracy: 0.854098\n",
      "epoch 4; batch 59136; loss 0.038401\n",
      "epoch:4; batch 59136; train accuracy: 0.854143\n",
      "epoch 4; batch 59264; loss 0.068547\n",
      "epoch:4; batch 59264; train accuracy: 0.854187\n",
      "epoch 4; batch 59392; loss 0.017043\n",
      "epoch:4; batch 59392; train accuracy: 0.854234\n",
      "epoch 4; batch 59520; loss 0.087410\n",
      "epoch:4; batch 59520; train accuracy: 0.854273\n",
      "epoch 4; batch 59648; loss 0.030661\n",
      "epoch:4; batch 59648; train accuracy: 0.854321\n",
      "epoch 4; batch 59776; loss 0.010080\n",
      "epoch:4; batch 59776; train accuracy: 0.854370\n",
      "epoch 4; batch 59904; loss 0.005349\n",
      "epoch:4; batch 59904; train accuracy: 0.854420\n",
      "epoch 4; batch 60032; loss 0.041505\n",
      "epoch:4; batch 60032; train accuracy: 0.854465\n",
      "epoch 4; batch 60160; loss 0.046589\n",
      "epoch:4; batch 60160; train accuracy: 0.854509\n",
      "epoch 4; batch 60288; loss 0.081405\n",
      "epoch:4; batch 60288; train accuracy: 0.854548\n",
      "epoch 4; batch 60416; loss 0.032069\n",
      "epoch:4; batch 60416; train accuracy: 0.854592\n",
      "epoch 4; batch 60544; loss 0.028851\n",
      "epoch:4; batch 60544; train accuracy: 0.854636\n",
      "epoch 4; batch 60672; loss 0.007454\n",
      "epoch:4; batch 60672; train accuracy: 0.854686\n",
      "epoch 4; batch 60800; loss 0.036815\n",
      "epoch:4; batch 60800; train accuracy: 0.854730\n",
      "epoch 4; batch 60928; loss 0.013075\n",
      "epoch:4; batch 60928; train accuracy: 0.854780\n",
      "epoch 4; batch 61056; loss 0.014265\n",
      "epoch:4; batch 61056; train accuracy: 0.854829\n",
      "epoch 4; batch 61184; loss 0.022478\n",
      "epoch:4; batch 61184; train accuracy: 0.854876\n",
      "epoch 4; batch 61312; loss 0.015205\n",
      "epoch:4; batch 61312; train accuracy: 0.854925\n",
      "epoch 4; batch 61440; loss 0.011757\n",
      "epoch:4; batch 61440; train accuracy: 0.854975\n",
      "epoch 4; batch 61568; loss 0.098571\n",
      "epoch:4; batch 61568; train accuracy: 0.855021\n",
      "epoch 4; batch 61696; loss 0.010934\n",
      "epoch:4; batch 61696; train accuracy: 0.855068\n",
      "epoch 4; batch 61824; loss 0.015607\n",
      "epoch:4; batch 61824; train accuracy: 0.855117\n",
      "epoch 4; batch 61952; loss 0.027659\n",
      "epoch:4; batch 61952; train accuracy: 0.855161\n",
      "epoch 4; batch 62080; loss 0.015565\n",
      "epoch:4; batch 62080; train accuracy: 0.855210\n",
      "epoch 4; batch 62208; loss 0.176994\n",
      "epoch:4; batch 62208; train accuracy: 0.855249\n",
      "epoch 4; batch 62336; loss 0.005656\n",
      "epoch:4; batch 62336; train accuracy: 0.855298\n",
      "epoch 4; batch 62464; loss 0.013144\n",
      "epoch:4; batch 62464; train accuracy: 0.855344\n",
      "epoch 4; batch 62592; loss 0.001133\n",
      "epoch:4; batch 62592; train accuracy: 0.855393\n",
      "epoch 4; batch 62720; loss 0.020270\n",
      "epoch:4; batch 62720; train accuracy: 0.855440\n",
      "epoch 4; batch 62848; loss 0.005165\n",
      "epoch:4; batch 62848; train accuracy: 0.855489\n",
      "epoch 4; batch 62976; loss 0.048941\n",
      "epoch:4; batch 62976; train accuracy: 0.855535\n",
      "epoch 4; batch 63104; loss 0.013187\n",
      "epoch:4; batch 63104; train accuracy: 0.855584\n",
      "epoch 4; batch 63232; loss 0.011741\n",
      "epoch:4; batch 63232; train accuracy: 0.855630\n",
      "epoch 4; batch 63360; loss 0.013119\n",
      "epoch:4; batch 63360; train accuracy: 0.855676\n",
      "epoch 4; batch 63488; loss 0.023973\n",
      "epoch:4; batch 63488; train accuracy: 0.855720\n",
      "epoch 4; batch 63616; loss 0.007211\n",
      "epoch:4; batch 63616; train accuracy: 0.855769\n",
      "epoch 4; batch 63744; loss 0.011311\n",
      "epoch:4; batch 63744; train accuracy: 0.855815\n",
      "epoch 4; batch 63872; loss 0.029311\n",
      "epoch:4; batch 63872; train accuracy: 0.855858\n",
      "epoch 4; batch 64000; loss 0.047294\n",
      "epoch:4; batch 64000; train accuracy: 0.855904\n",
      "epoch 4; batch 64128; loss 0.049076\n",
      "epoch:4; batch 64128; train accuracy: 0.855950\n",
      "epoch 4; batch 64256; loss 0.004115\n",
      "epoch:4; batch 64256; train accuracy: 0.855999\n",
      "epoch 4; batch 64384; loss 0.023141\n",
      "epoch:4; batch 64384; train accuracy: 0.856045\n",
      "epoch 4; batch 64512; loss 0.011962\n",
      "epoch:4; batch 64512; train accuracy: 0.856091\n",
      "epoch 4; batch 64640; loss 0.002628\n",
      "epoch:4; batch 64640; train accuracy: 0.856139\n",
      "epoch 4; batch 64768; loss 0.013739\n",
      "epoch:4; batch 64768; train accuracy: 0.856185\n",
      "epoch 4; batch 64896; loss 0.039997\n",
      "epoch:4; batch 64896; train accuracy: 0.856231\n",
      "epoch 4; batch 65024; loss 0.045686\n",
      "epoch:4; batch 65024; train accuracy: 0.856274\n",
      "epoch 4; batch 65152; loss 0.041986\n",
      "epoch:4; batch 65152; train accuracy: 0.856317\n",
      "epoch 4; batch 65280; loss 0.017245\n",
      "epoch:4; batch 65280; train accuracy: 0.856363\n",
      "epoch 4; batch 65408; loss 0.008682\n",
      "epoch:4; batch 65408; train accuracy: 0.856411\n",
      "epoch 4; batch 65536; loss 0.050110\n",
      "epoch:4; batch 65536; train accuracy: 0.856454\n",
      "epoch 4; batch 65664; loss 0.007744\n",
      "epoch:4; batch 65664; train accuracy: 0.856500\n",
      "epoch 4; batch 65792; loss 0.004465\n",
      "epoch:4; batch 65792; train accuracy: 0.856548\n",
      "epoch 4; batch 65920; loss 0.022705\n",
      "epoch:4; batch 65920; train accuracy: 0.856594\n",
      "epoch 4; batch 66048; loss 0.005188\n",
      "epoch:4; batch 66048; train accuracy: 0.856642\n",
      "epoch 4; batch 66176; loss 0.034092\n",
      "epoch:4; batch 66176; train accuracy: 0.856685\n",
      "epoch 4; batch 66304; loss 0.020165\n",
      "epoch:4; batch 66304; train accuracy: 0.856731\n",
      "epoch 4; batch 66432; loss 0.063852\n",
      "epoch:4; batch 66432; train accuracy: 0.856773\n",
      "epoch 4; batch 66560; loss 0.007404\n",
      "epoch:4; batch 66560; train accuracy: 0.856822\n",
      "epoch 4; batch 66688; loss 0.015434\n",
      "epoch:4; batch 66688; train accuracy: 0.856870\n",
      "epoch 4; batch 66816; loss 0.009445\n",
      "epoch:4; batch 66816; train accuracy: 0.856918\n",
      "epoch 4; batch 66944; loss 0.084576\n",
      "epoch:4; batch 66944; train accuracy: 0.856955\n",
      "epoch 4; batch 67072; loss 0.028533\n",
      "epoch:4; batch 67072; train accuracy: 0.857000\n",
      "epoch 4; batch 67200; loss 0.007844\n",
      "epoch:4; batch 67200; train accuracy: 0.857048\n",
      "epoch 4; batch 67328; loss 0.080230\n",
      "epoch:4; batch 67328; train accuracy: 0.857091\n",
      "epoch 4; batch 67456; loss 0.045055\n",
      "epoch:4; batch 67456; train accuracy: 0.857134\n",
      "epoch 4; batch 67584; loss 0.046175\n",
      "epoch:4; batch 67584; train accuracy: 0.857179\n",
      "epoch 4; batch 67712; loss 0.037965\n",
      "epoch:4; batch 67712; train accuracy: 0.857219\n",
      "epoch 4; batch 67840; loss 0.040241\n",
      "epoch:4; batch 67840; train accuracy: 0.857264\n",
      "epoch 4; batch 67968; loss 0.004981\n",
      "epoch:4; batch 67968; train accuracy: 0.857312\n",
      "epoch 4; batch 68096; loss 0.033688\n",
      "epoch:4; batch 68096; train accuracy: 0.857354\n",
      "epoch 4; batch 68224; loss 0.080346\n",
      "epoch:4; batch 68224; train accuracy: 0.857394\n",
      "epoch 4; batch 68352; loss 0.030025\n",
      "epoch:4; batch 68352; train accuracy: 0.857436\n",
      "epoch 4; batch 68480; loss 0.006095\n",
      "epoch:4; batch 68480; train accuracy: 0.857484\n",
      "epoch 4; batch 68608; loss 0.012371\n",
      "epoch:4; batch 68608; train accuracy: 0.857529\n",
      "epoch 4; batch 68736; loss 0.032180\n",
      "epoch:4; batch 68736; train accuracy: 0.857574\n",
      "epoch 4; batch 68864; loss 0.013637\n",
      "epoch:4; batch 68864; train accuracy: 0.857621\n",
      "epoch 4; batch 68992; loss 0.071756\n",
      "epoch:4; batch 68992; train accuracy: 0.857656\n",
      "epoch 4; batch 69120; loss 0.020821\n",
      "epoch:4; batch 69120; train accuracy: 0.857701\n",
      "epoch 4; batch 69248; loss 0.003138\n",
      "epoch:4; batch 69248; train accuracy: 0.857748\n",
      "epoch 4; batch 69376; loss 0.009507\n",
      "epoch:4; batch 69376; train accuracy: 0.857795\n",
      "epoch 4; batch 69504; loss 0.141235\n",
      "epoch:4; batch 69504; train accuracy: 0.857830\n",
      "epoch 4; batch 69632; loss 0.026622\n",
      "epoch:4; batch 69632; train accuracy: 0.857874\n",
      "epoch 4; batch 69760; loss 0.006531\n",
      "epoch:4; batch 69760; train accuracy: 0.857922\n",
      "epoch 4; batch 69888; loss 0.015333\n",
      "epoch:4; batch 69888; train accuracy: 0.857969\n",
      "epoch 4; batch 70016; loss 0.074448\n",
      "epoch:4; batch 70016; train accuracy: 0.858006\n",
      "epoch 4; batch 70144; loss 0.057927\n",
      "epoch:4; batch 70144; train accuracy: 0.858048\n",
      "epoch 4; batch 70272; loss 0.037209\n",
      "epoch:4; batch 70272; train accuracy: 0.858090\n",
      "epoch 4; batch 70400; loss 0.057155\n",
      "epoch:4; batch 70400; train accuracy: 0.858129\n",
      "epoch 4; batch 70528; loss 0.039285\n",
      "epoch:4; batch 70528; train accuracy: 0.858174\n",
      "epoch 4; batch 70656; loss 0.015570\n",
      "epoch:4; batch 70656; train accuracy: 0.858218\n",
      "epoch 4; batch 70784; loss 0.026717\n",
      "epoch:4; batch 70784; train accuracy: 0.858263\n",
      "epoch 4; batch 70912; loss 0.029045\n",
      "epoch:4; batch 70912; train accuracy: 0.858307\n",
      "epoch 4; batch 71040; loss 0.064594\n",
      "epoch:4; batch 71040; train accuracy: 0.858349\n",
      "epoch 4; batch 71168; loss 0.022152\n",
      "epoch:4; batch 71168; train accuracy: 0.858393\n",
      "epoch 4; batch 71296; loss 0.004389\n",
      "epoch:4; batch 71296; train accuracy: 0.858440\n",
      "epoch 4; batch 71424; loss 0.015952\n",
      "epoch:4; batch 71424; train accuracy: 0.858485\n",
      "epoch 4; batch 71552; loss 0.008457\n",
      "epoch:4; batch 71552; train accuracy: 0.858531\n",
      "epoch 4; batch 71680; loss 0.061108\n",
      "epoch:4; batch 71680; train accuracy: 0.858570\n",
      "epoch 4; batch 71808; loss 0.016765\n",
      "epoch:4; batch 71808; train accuracy: 0.858615\n",
      "epoch 4; batch 71936; loss 0.030061\n",
      "epoch:4; batch 71936; train accuracy: 0.858656\n",
      "epoch 4; batch 72064; loss 0.076745\n",
      "epoch:4; batch 72064; train accuracy: 0.858698\n",
      "epoch 4; batch 72192; loss 0.040467\n",
      "epoch:4; batch 72192; train accuracy: 0.858742\n",
      "epoch 4; batch 72320; loss 0.037921\n",
      "epoch:4; batch 72320; train accuracy: 0.858784\n",
      "epoch 4; batch 72448; loss 0.004756\n",
      "epoch:4; batch 72448; train accuracy: 0.858830\n",
      "epoch 4; batch 72576; loss 0.008290\n",
      "epoch:4; batch 72576; train accuracy: 0.858877\n",
      "epoch 4; batch 72704; loss 0.041672\n",
      "epoch:4; batch 72704; train accuracy: 0.858921\n",
      "epoch 4; batch 72832; loss 0.067131\n",
      "epoch:4; batch 72832; train accuracy: 0.858957\n",
      "epoch 4; batch 72960; loss 0.019806\n",
      "epoch:4; batch 72960; train accuracy: 0.859001\n",
      "epoch 4; batch 73088; loss 0.045361\n",
      "epoch:4; batch 73088; train accuracy: 0.859045\n",
      "epoch 4; batch 73216; loss 0.036560\n",
      "epoch:4; batch 73216; train accuracy: 0.859084\n",
      "epoch 4; batch 73344; loss 0.011703\n",
      "epoch:4; batch 73344; train accuracy: 0.859130\n",
      "epoch 4; batch 73472; loss 0.033917\n",
      "epoch:4; batch 73472; train accuracy: 0.859169\n",
      "epoch 4; batch 73600; loss 0.045428\n",
      "epoch:4; batch 73600; train accuracy: 0.859210\n",
      "epoch 4; batch 73728; loss 0.031277\n",
      "epoch:4; batch 73728; train accuracy: 0.859254\n",
      "epoch 4; batch 73856; loss 0.066423\n",
      "epoch:4; batch 73856; train accuracy: 0.859290\n",
      "epoch 4; batch 73984; loss 0.035542\n",
      "epoch:4; batch 73984; train accuracy: 0.859334\n",
      "epoch 4; batch 74112; loss 0.014331\n",
      "epoch:4; batch 74112; train accuracy: 0.859378\n",
      "epoch 4; batch 74240; loss 0.027420\n",
      "epoch:4; batch 74240; train accuracy: 0.859421\n",
      "epoch 4; batch 74368; loss 0.061366\n",
      "epoch:4; batch 74368; train accuracy: 0.859465\n",
      "epoch 4; batch 74496; loss 0.037300\n",
      "epoch:4; batch 74496; train accuracy: 0.859506\n",
      "epoch 4; batch 74624; loss 0.052170\n",
      "epoch:4; batch 74624; train accuracy: 0.859547\n",
      "epoch 4; batch 74752; loss 0.009334\n",
      "epoch:4; batch 74752; train accuracy: 0.859593\n",
      "epoch 4; batch 74880; loss 0.051868\n",
      "epoch:4; batch 74880; train accuracy: 0.859632\n",
      "epoch 4; batch 75008; loss 0.013387\n",
      "epoch:4; batch 75008; train accuracy: 0.859675\n",
      "epoch 4; batch 75136; loss 0.031339\n",
      "epoch:4; batch 75136; train accuracy: 0.859716\n",
      "epoch 4; batch 75264; loss 0.012746\n",
      "epoch:4; batch 75264; train accuracy: 0.859762\n",
      "epoch 4; batch 75392; loss 0.004190\n",
      "epoch:4; batch 75392; train accuracy: 0.859808\n",
      "epoch 4; batch 75520; loss 0.006423\n",
      "epoch:4; batch 75520; train accuracy: 0.859854\n",
      "epoch 4; batch 75648; loss 0.026712\n",
      "epoch:4; batch 75648; train accuracy: 0.859897\n",
      "epoch 4; batch 75776; loss 0.057800\n",
      "epoch:4; batch 75776; train accuracy: 0.859938\n",
      "epoch 4; batch 75904; loss 0.038972\n",
      "epoch:4; batch 75904; train accuracy: 0.859981\n",
      "epoch 4; batch 76032; loss 0.006948\n",
      "epoch:4; batch 76032; train accuracy: 0.860027\n",
      "epoch 4; batch 76160; loss 0.018005\n",
      "epoch:4; batch 76160; train accuracy: 0.860071\n",
      "epoch 4; batch 76288; loss 0.059214\n",
      "epoch:4; batch 76288; train accuracy: 0.860111\n",
      "epoch 4; batch 76416; loss 0.007135\n",
      "epoch:4; batch 76416; train accuracy: 0.860157\n",
      "epoch 4; batch 76544; loss 0.020940\n",
      "epoch:4; batch 76544; train accuracy: 0.860200\n",
      "epoch 4; batch 76672; loss 0.019252\n",
      "epoch:4; batch 76672; train accuracy: 0.860243\n",
      "epoch 4; batch 76800; loss 0.057046\n",
      "epoch:4; batch 76800; train accuracy: 0.860286\n",
      "epoch 4; batch 76928; loss 0.020576\n",
      "epoch:4; batch 76928; train accuracy: 0.860330\n",
      "epoch 4; batch 77056; loss 0.002710\n",
      "epoch:4; batch 77056; train accuracy: 0.860375\n",
      "epoch 4; batch 77184; loss 0.041197\n",
      "epoch:4; batch 77184; train accuracy: 0.860416\n",
      "epoch 4; batch 77312; loss 0.006045\n",
      "epoch:4; batch 77312; train accuracy: 0.860461\n",
      "epoch 4; batch 77440; loss 0.011858\n",
      "epoch:4; batch 77440; train accuracy: 0.860507\n",
      "epoch 4; batch 77568; loss 0.018079\n",
      "epoch:4; batch 77568; train accuracy: 0.860550\n",
      "epoch 4; batch 77696; loss 0.009805\n",
      "epoch:4; batch 77696; train accuracy: 0.860595\n",
      "epoch 4; batch 77824; loss 0.008060\n",
      "epoch:4; batch 77824; train accuracy: 0.860641\n",
      "epoch 4; batch 77952; loss 0.077240\n",
      "epoch:4; batch 77952; train accuracy: 0.860681\n",
      "epoch 4; batch 78080; loss 0.006900\n",
      "epoch:4; batch 78080; train accuracy: 0.860726\n",
      "epoch 4; batch 78208; loss 0.003591\n",
      "epoch:4; batch 78208; train accuracy: 0.860772\n",
      "epoch 4; batch 78336; loss 0.022088\n",
      "epoch:4; batch 78336; train accuracy: 0.860814\n",
      "epoch 4; batch 78464; loss 0.000861\n",
      "epoch:4; batch 78464; train accuracy: 0.860860\n",
      "epoch 4; batch 78592; loss 0.024079\n",
      "epoch:4; batch 78592; train accuracy: 0.860902\n",
      "epoch 4; batch 78720; loss 0.021056\n",
      "epoch:4; batch 78720; train accuracy: 0.860945\n",
      "epoch 4; batch 78848; loss 0.044431\n",
      "epoch:4; batch 78848; train accuracy: 0.860983\n",
      "epoch 4; batch 78976; loss 0.057171\n",
      "epoch:4; batch 78976; train accuracy: 0.861025\n",
      "epoch 4; batch 79104; loss 0.008099\n",
      "epoch:4; batch 79104; train accuracy: 0.861071\n",
      "epoch 4; batch 79232; loss 0.032483\n",
      "epoch:4; batch 79232; train accuracy: 0.861111\n",
      "epoch 4; batch 79360; loss 0.009782\n",
      "epoch:4; batch 79360; train accuracy: 0.861153\n",
      "epoch 4; batch 79488; loss 0.031847\n",
      "epoch:4; batch 79488; train accuracy: 0.861191\n",
      "epoch 4; batch 79616; loss 0.048106\n",
      "epoch:4; batch 79616; train accuracy: 0.861233\n",
      "epoch 4; batch 79744; loss 0.015151\n",
      "epoch:4; batch 79744; train accuracy: 0.861276\n",
      "epoch 4; batch 79872; loss 0.029486\n",
      "epoch:4; batch 79872; train accuracy: 0.861318\n",
      "epoch 4; batch 80000; loss 0.002737\n",
      "epoch:4; batch 80000; train accuracy: 0.861363\n",
      "epoch 4; batch 80128; loss 0.017795\n",
      "epoch:4; batch 80128; train accuracy: 0.861405\n",
      "epoch 4; batch 80256; loss 0.033493\n",
      "epoch:4; batch 80256; train accuracy: 0.861445\n",
      "epoch 4; batch 80384; loss 0.026276\n",
      "epoch:4; batch 80384; train accuracy: 0.861485\n",
      "epoch 4; batch 80512; loss 0.202711\n",
      "epoch:4; batch 80512; train accuracy: 0.861520\n",
      "epoch 4; batch 80640; loss 0.082072\n",
      "epoch:4; batch 80640; train accuracy: 0.861562\n",
      "epoch 4; batch 80768; loss 0.004038\n",
      "epoch:4; batch 80768; train accuracy: 0.861607\n",
      "epoch 4; batch 80896; loss 0.007298\n",
      "epoch:4; batch 80896; train accuracy: 0.861652\n",
      "epoch 4; batch 81024; loss 0.013339\n",
      "epoch:4; batch 81024; train accuracy: 0.861694\n",
      "epoch 4; batch 81152; loss 0.007949\n",
      "epoch:4; batch 81152; train accuracy: 0.861738\n",
      "epoch 4; batch 81280; loss 0.003059\n",
      "epoch:4; batch 81280; train accuracy: 0.861783\n",
      "epoch 4; batch 81408; loss 0.023809\n",
      "epoch:4; batch 81408; train accuracy: 0.861825\n",
      "epoch 4; batch 81536; loss 0.009823\n",
      "epoch:4; batch 81536; train accuracy: 0.861870\n",
      "epoch 4; batch 81664; loss 0.017735\n",
      "epoch:4; batch 81664; train accuracy: 0.861914\n",
      "epoch 4; batch 81792; loss 0.004297\n",
      "epoch:4; batch 81792; train accuracy: 0.861959\n",
      "epoch 4; batch 81920; loss 0.004956\n",
      "epoch:4; batch 81920; train accuracy: 0.862004\n",
      "epoch 4; batch 82048; loss 0.013195\n",
      "epoch:4; batch 82048; train accuracy: 0.862048\n",
      "epoch 4; batch 82176; loss 0.004347\n",
      "epoch:4; batch 82176; train accuracy: 0.862092\n",
      "epoch 4; batch 82304; loss 0.018234\n",
      "epoch:4; batch 82304; train accuracy: 0.862134\n",
      "epoch 4; batch 82432; loss 0.022643\n",
      "epoch:4; batch 82432; train accuracy: 0.862176\n",
      "epoch 4; batch 82560; loss 0.005592\n",
      "epoch:4; batch 82560; train accuracy: 0.862221\n",
      "epoch 4; batch 82688; loss 0.005742\n",
      "epoch:4; batch 82688; train accuracy: 0.862265\n",
      "epoch 4; batch 82816; loss 0.029911\n",
      "epoch:4; batch 82816; train accuracy: 0.862304\n",
      "epoch 4; batch 82944; loss 0.021181\n",
      "epoch:4; batch 82944; train accuracy: 0.862346\n",
      "epoch 4; batch 83072; loss 0.002058\n",
      "epoch:4; batch 83072; train accuracy: 0.862390\n",
      "epoch 4; batch 83200; loss 0.005633\n",
      "epoch:4; batch 83200; train accuracy: 0.862435\n",
      "epoch 4; batch 83328; loss 0.037922\n",
      "epoch:4; batch 83328; train accuracy: 0.862471\n",
      "epoch 4; batch 83456; loss 0.005810\n",
      "epoch:4; batch 83456; train accuracy: 0.862516\n",
      "epoch 4; batch 83584; loss 0.005519\n",
      "epoch:4; batch 83584; train accuracy: 0.862560\n",
      "epoch 4; batch 83712; loss 0.020578\n",
      "epoch:4; batch 83712; train accuracy: 0.862601\n",
      "epoch 4; batch 83840; loss 0.041624\n",
      "epoch:4; batch 83840; train accuracy: 0.862643\n",
      "epoch 4; batch 83968; loss 0.078600\n",
      "epoch:4; batch 83968; train accuracy: 0.862680\n",
      "epoch 4; batch 84096; loss 0.010596\n",
      "epoch:4; batch 84096; train accuracy: 0.862721\n",
      "epoch 4; batch 84224; loss 0.015968\n",
      "epoch:4; batch 84224; train accuracy: 0.862763\n",
      "epoch 4; batch 84352; loss 0.007111\n",
      "epoch:4; batch 84352; train accuracy: 0.862807\n",
      "epoch 4; batch 84480; loss 0.027345\n",
      "epoch:4; batch 84480; train accuracy: 0.862848\n",
      "epoch 4; batch 84608; loss 0.015412\n",
      "epoch:4; batch 84608; train accuracy: 0.862890\n",
      "epoch 4; batch 84736; loss 0.009385\n",
      "epoch:4; batch 84736; train accuracy: 0.862931\n",
      "epoch 4; batch 84864; loss 0.005171\n",
      "epoch:4; batch 84864; train accuracy: 0.862975\n",
      "epoch 4; batch 84992; loss 0.033141\n",
      "epoch:4; batch 84992; train accuracy: 0.863011\n",
      "epoch 4; batch 85120; loss 0.007997\n",
      "epoch:4; batch 85120; train accuracy: 0.863055\n",
      "epoch 4; batch 85248; loss 0.178719\n",
      "epoch:4; batch 85248; train accuracy: 0.863091\n",
      "epoch 4; batch 85376; loss 0.010183\n",
      "epoch:4; batch 85376; train accuracy: 0.863135\n",
      "epoch 4; batch 85504; loss 0.006348\n",
      "epoch:4; batch 85504; train accuracy: 0.863179\n",
      "epoch 4; batch 85632; loss 0.003445\n",
      "epoch:4; batch 85632; train accuracy: 0.863223\n",
      "epoch 4; batch 85760; loss 0.019491\n",
      "epoch:4; batch 85760; train accuracy: 0.863261\n",
      "epoch 4; batch 85888; loss 0.035801\n",
      "epoch:4; batch 85888; train accuracy: 0.863302\n",
      "epoch 4; batch 86016; loss 0.021735\n",
      "epoch:4; batch 86016; train accuracy: 0.863344\n",
      "epoch 4; batch 86144; loss 0.007150\n",
      "epoch:4; batch 86144; train accuracy: 0.863387\n",
      "epoch 4; batch 86272; loss 0.011808\n",
      "epoch:4; batch 86272; train accuracy: 0.863431\n",
      "epoch 4; batch 86400; loss 0.101662\n",
      "epoch:4; batch 86400; train accuracy: 0.863472\n",
      "epoch 4; batch 86528; loss 0.001258\n",
      "epoch:4; batch 86528; train accuracy: 0.863515\n",
      "epoch 4; batch 86656; loss 0.003984\n",
      "epoch:4; batch 86656; train accuracy: 0.863559\n",
      "epoch 4; batch 86784; loss 0.002796\n",
      "epoch:4; batch 86784; train accuracy: 0.863602\n",
      "epoch 4; batch 86912; loss 0.091559\n",
      "epoch:4; batch 86912; train accuracy: 0.863641\n",
      "epoch 4; batch 87040; loss 0.000894\n",
      "epoch:4; batch 87040; train accuracy: 0.863684\n",
      "epoch 4; batch 87168; loss 0.002787\n",
      "epoch:4; batch 87168; train accuracy: 0.863728\n",
      "epoch 4; batch 87296; loss 0.007165\n",
      "epoch:4; batch 87296; train accuracy: 0.863771\n",
      "epoch 4; batch 87424; loss 0.012863\n",
      "epoch:4; batch 87424; train accuracy: 0.863812\n",
      "epoch 4; batch 87552; loss 0.002159\n",
      "epoch:4; batch 87552; train accuracy: 0.863855\n",
      "epoch 4; batch 87680; loss 0.011432\n",
      "epoch:4; batch 87680; train accuracy: 0.863899\n",
      "epoch 4; batch 87808; loss 0.005735\n",
      "epoch:4; batch 87808; train accuracy: 0.863942\n",
      "epoch 4; batch 87936; loss 0.009148\n",
      "epoch:4; batch 87936; train accuracy: 0.863983\n",
      "epoch 4; batch 88064; loss 0.005962\n",
      "epoch:4; batch 88064; train accuracy: 0.864026\n",
      "epoch 4; batch 88192; loss 0.011162\n",
      "epoch:4; batch 88192; train accuracy: 0.864069\n",
      "epoch 4; batch 88320; loss 0.025975\n",
      "epoch:4; batch 88320; train accuracy: 0.864110\n",
      "epoch 4; batch 88448; loss 0.033121\n",
      "epoch:4; batch 88448; train accuracy: 0.864145\n",
      "epoch 4; batch 88576; loss 0.035560\n",
      "epoch:4; batch 88576; train accuracy: 0.864183\n",
      "epoch 4; batch 88704; loss 0.013137\n",
      "epoch:4; batch 88704; train accuracy: 0.864224\n",
      "epoch 4; batch 88832; loss 0.001321\n",
      "epoch:4; batch 88832; train accuracy: 0.864267\n",
      "epoch 4; batch 88960; loss 0.016084\n",
      "epoch:4; batch 88960; train accuracy: 0.864308\n",
      "epoch 4; batch 89088; loss 0.007142\n",
      "epoch:4; batch 89088; train accuracy: 0.864351\n",
      "epoch 4; batch 89216; loss 0.005501\n",
      "epoch:4; batch 89216; train accuracy: 0.864394\n",
      "epoch 4; batch 89344; loss 0.007656\n",
      "epoch:4; batch 89344; train accuracy: 0.864437\n",
      "epoch 4; batch 89472; loss 0.015577\n",
      "epoch:4; batch 89472; train accuracy: 0.864477\n",
      "epoch 4; batch 89600; loss 0.019344\n",
      "epoch:4; batch 89600; train accuracy: 0.864517\n",
      "epoch 4; batch 89728; loss 0.027662\n",
      "epoch:4; batch 89728; train accuracy: 0.864555\n",
      "epoch 4; batch 89856; loss 0.004140\n",
      "epoch:4; batch 89856; train accuracy: 0.864598\n",
      "epoch 4; batch 89984; loss 0.008494\n",
      "epoch:4; batch 89984; train accuracy: 0.864641\n",
      "epoch 4; batch 90112; loss 0.013685\n",
      "epoch:4; batch 90112; train accuracy: 0.864681\n",
      "epoch 4; batch 90240; loss 0.001329\n",
      "epoch:4; batch 90240; train accuracy: 0.864724\n",
      "epoch 4; batch 90368; loss 0.015234\n",
      "epoch:4; batch 90368; train accuracy: 0.864764\n",
      "epoch 4; batch 90496; loss 0.011769\n",
      "epoch:4; batch 90496; train accuracy: 0.864805\n",
      "epoch 4; batch 90624; loss 0.006931\n",
      "epoch:4; batch 90624; train accuracy: 0.864845\n",
      "epoch 4; batch 90752; loss 0.061207\n",
      "epoch:4; batch 90752; train accuracy: 0.864882\n",
      "epoch 4; batch 90880; loss 0.036871\n",
      "epoch:4; batch 90880; train accuracy: 0.864923\n",
      "epoch 4; batch 91008; loss 0.027160\n",
      "epoch:4; batch 91008; train accuracy: 0.864963\n",
      "epoch 4; batch 91136; loss 0.118276\n",
      "epoch:4; batch 91136; train accuracy: 0.865003\n",
      "epoch 4; batch 91264; loss 0.007912\n",
      "epoch:4; batch 91264; train accuracy: 0.865045\n",
      "epoch 4; batch 91392; loss 0.006548\n",
      "epoch:4; batch 91392; train accuracy: 0.865088\n",
      "epoch 4; batch 91520; loss 0.029461\n",
      "epoch:4; batch 91520; train accuracy: 0.865125\n",
      "epoch 4; batch 91648; loss 0.009568\n",
      "epoch:4; batch 91648; train accuracy: 0.865168\n",
      "epoch 4; batch 91776; loss 0.021879\n",
      "epoch:4; batch 91776; train accuracy: 0.865208\n",
      "epoch 4; batch 91904; loss 0.073103\n",
      "epoch:4; batch 91904; train accuracy: 0.865248\n",
      "epoch 4; batch 92032; loss 0.002231\n",
      "epoch:4; batch 92032; train accuracy: 0.865290\n",
      "epoch 4; batch 92160; loss 0.041774\n",
      "epoch:4; batch 92160; train accuracy: 0.865330\n",
      "epoch 4; batch 92288; loss 0.003036\n",
      "epoch:4; batch 92288; train accuracy: 0.865373\n",
      "epoch 4; batch 92416; loss 0.015394\n",
      "epoch:4; batch 92416; train accuracy: 0.865412\n",
      "epoch 4; batch 92544; loss 0.007836\n",
      "epoch:4; batch 92544; train accuracy: 0.865455\n",
      "epoch 4; batch 92672; loss 0.008430\n",
      "epoch:4; batch 92672; train accuracy: 0.865497\n",
      "epoch 4; batch 92800; loss 0.047164\n",
      "epoch:4; batch 92800; train accuracy: 0.865534\n",
      "epoch 4; batch 92928; loss 0.056075\n",
      "epoch:4; batch 92928; train accuracy: 0.865572\n",
      "epoch 4; batch 93056; loss 0.032578\n",
      "epoch:4; batch 93056; train accuracy: 0.865609\n",
      "epoch 4; batch 93184; loss 0.009490\n",
      "epoch:4; batch 93184; train accuracy: 0.865651\n",
      "epoch 4; batch 93312; loss 0.004772\n",
      "epoch:4; batch 93312; train accuracy: 0.865693\n",
      "epoch 4; batch 93440; loss 0.011539\n",
      "epoch:4; batch 93440; train accuracy: 0.865735\n",
      "epoch 4; batch 93568; loss 0.009311\n",
      "epoch:4; batch 93568; train accuracy: 0.865777\n",
      "epoch 4; batch 93696; loss 0.042696\n",
      "epoch:4; batch 93696; train accuracy: 0.865812\n",
      "epoch 4; batch 93824; loss 0.009761\n",
      "epoch:4; batch 93824; train accuracy: 0.865854\n",
      "epoch 4; batch 93952; loss 0.027499\n",
      "epoch:4; batch 93952; train accuracy: 0.865894\n",
      "epoch 4; batch 94080; loss 0.014299\n",
      "epoch:4; batch 94080; train accuracy: 0.865936\n",
      "epoch 4; batch 94208; loss 0.060464\n",
      "epoch:4; batch 94208; train accuracy: 0.865970\n",
      "epoch 4; batch 94336; loss 0.006948\n",
      "epoch:4; batch 94336; train accuracy: 0.866012\n",
      "epoch 4; batch 94464; loss 0.007871\n",
      "epoch:4; batch 94464; train accuracy: 0.866054\n",
      "epoch 4; batch 94592; loss 0.013838\n",
      "epoch:4; batch 94592; train accuracy: 0.866093\n",
      "epoch 4; batch 94720; loss 0.011012\n",
      "epoch:4; batch 94720; train accuracy: 0.866135\n",
      "epoch 4; batch 94848; loss 0.011430\n",
      "epoch:4; batch 94848; train accuracy: 0.866177\n",
      "epoch 4; batch 94976; loss 0.004333\n",
      "epoch:4; batch 94976; train accuracy: 0.866219\n",
      "epoch 4; batch 95104; loss 0.065884\n",
      "epoch:4; batch 95104; train accuracy: 0.866253\n",
      "epoch 4; batch 95232; loss 0.014531\n",
      "epoch:4; batch 95232; train accuracy: 0.866293\n",
      "epoch 4; batch 95360; loss 0.003127\n",
      "epoch:4; batch 95360; train accuracy: 0.866334\n",
      "epoch 4; batch 95488; loss 0.013808\n",
      "epoch:4; batch 95488; train accuracy: 0.866374\n",
      "epoch 4; batch 95616; loss 0.008965\n",
      "epoch:4; batch 95616; train accuracy: 0.866413\n",
      "epoch 4; batch 95744; loss 0.018960\n",
      "epoch:4; batch 95744; train accuracy: 0.866452\n",
      "epoch 4; batch 95872; loss 0.003174\n",
      "epoch:4; batch 95872; train accuracy: 0.866494\n",
      "epoch 4; batch 96000; loss 0.004198\n",
      "epoch:4; batch 96000; train accuracy: 0.866535\n",
      "epoch 4; batch 96128; loss 0.009637\n",
      "epoch:4; batch 96128; train accuracy: 0.866577\n",
      "epoch 4; batch 96256; loss 0.004823\n",
      "epoch:4; batch 96256; train accuracy: 0.866618\n",
      "epoch 4; batch 96384; loss 0.012193\n",
      "epoch:4; batch 96384; train accuracy: 0.866657\n",
      "epoch 4; batch 96512; loss 0.017386\n",
      "epoch:4; batch 96512; train accuracy: 0.866696\n",
      "epoch 4; batch 96640; loss 0.016947\n",
      "epoch:4; batch 96640; train accuracy: 0.866736\n",
      "epoch 4; batch 96768; loss 0.035435\n",
      "epoch:4; batch 96768; train accuracy: 0.866772\n",
      "epoch 4; batch 96896; loss 0.002798\n",
      "epoch:4; batch 96896; train accuracy: 0.866813\n",
      "epoch 4; batch 97024; loss 0.000674\n",
      "epoch:4; batch 97024; train accuracy: 0.866855\n",
      "epoch 4; batch 97152; loss 0.027737\n",
      "epoch:4; batch 97152; train accuracy: 0.866894\n",
      "epoch 4; batch 97280; loss 0.006329\n",
      "epoch:4; batch 97280; train accuracy: 0.866935\n",
      "epoch 4; batch 97408; loss 0.004245\n",
      "epoch:4; batch 97408; train accuracy: 0.866976\n",
      "epoch 4; batch 97536; loss 0.002259\n",
      "epoch:4; batch 97536; train accuracy: 0.867018\n",
      "epoch 4; batch 97664; loss 0.083670\n",
      "epoch:4; batch 97664; train accuracy: 0.867057\n",
      "epoch 4; batch 97792; loss 0.025958\n",
      "epoch:4; batch 97792; train accuracy: 0.867093\n",
      "epoch 4; batch 97920; loss 0.005327\n",
      "epoch:4; batch 97920; train accuracy: 0.867134\n",
      "epoch 4; batch 98048; loss 0.038278\n",
      "epoch:4; batch 98048; train accuracy: 0.867173\n",
      "epoch 4; batch 98176; loss 0.005803\n",
      "epoch:4; batch 98176; train accuracy: 0.867214\n",
      "epoch 4; batch 98304; loss 0.051885\n",
      "epoch:4; batch 98304; train accuracy: 0.867250\n",
      "epoch 4; batch 98432; loss 0.014581\n",
      "epoch:4; batch 98432; train accuracy: 0.867289\n",
      "epoch 4; batch 98560; loss 0.001727\n",
      "epoch:4; batch 98560; train accuracy: 0.867330\n",
      "epoch 4; batch 98688; loss 0.029105\n",
      "epoch:4; batch 98688; train accuracy: 0.867369\n",
      "epoch 4; batch 98816; loss 0.006307\n",
      "epoch:4; batch 98816; train accuracy: 0.867410\n",
      "epoch 4; batch 98944; loss 0.019190\n",
      "epoch:4; batch 98944; train accuracy: 0.867446\n",
      "epoch 4; batch 99072; loss 0.043016\n",
      "epoch:4; batch 99072; train accuracy: 0.867485\n",
      "epoch 4; batch 99200; loss 0.020728\n",
      "epoch:4; batch 99200; train accuracy: 0.867523\n",
      "epoch 4; batch 99328; loss 0.010471\n",
      "epoch:4; batch 99328; train accuracy: 0.867564\n",
      "epoch 4; batch 99456; loss 0.006054\n",
      "epoch:4; batch 99456; train accuracy: 0.867605\n",
      "epoch 4; batch 99584; loss 0.004662\n",
      "epoch:4; batch 99584; train accuracy: 0.867646\n",
      "epoch 4; batch 99712; loss 0.002635\n",
      "epoch:4; batch 99712; train accuracy: 0.867687\n",
      "epoch 4; batch 99840; loss 0.060256\n",
      "epoch:4; batch 99840; train accuracy: 0.867723\n",
      "epoch 4; batch 99968; loss 0.008514\n",
      "epoch:4; batch 99968; train accuracy: 0.867764\n",
      "epoch 4; batch 100096; loss 0.019606\n",
      "epoch:4; batch 100096; train accuracy: 0.867800\n",
      "epoch 4; batch 100224; loss 0.047731\n",
      "epoch:4; batch 100224; train accuracy: 0.867836\n",
      "epoch 4; batch 100352; loss 0.063579\n",
      "epoch:4; batch 100352; train accuracy: 0.867864\n",
      "epoch 4; batch 100480; loss 0.011023\n",
      "epoch:4; batch 100480; train accuracy: 0.867905\n",
      "epoch 4; batch 100608; loss 0.038902\n",
      "epoch:4; batch 100608; train accuracy: 0.867941\n",
      "epoch 4; batch 100736; loss 0.010459\n",
      "epoch:4; batch 100736; train accuracy: 0.867981\n",
      "epoch 4; batch 100864; loss 0.004331\n",
      "epoch:4; batch 100864; train accuracy: 0.868022\n",
      "epoch 4; batch 100992; loss 0.008852\n",
      "epoch:4; batch 100992; train accuracy: 0.868063\n",
      "epoch 4; batch 101120; loss 0.013182\n",
      "epoch:4; batch 101120; train accuracy: 0.868103\n",
      "epoch 4; batch 101248; loss 0.028863\n",
      "epoch:4; batch 101248; train accuracy: 0.868139\n",
      "epoch 4; batch 101376; loss 0.011997\n",
      "epoch:4; batch 101376; train accuracy: 0.868177\n",
      "epoch 4; batch 101504; loss 0.021593\n",
      "epoch:4; batch 101504; train accuracy: 0.868215\n",
      "epoch 4; batch 101632; loss 0.019848\n",
      "epoch:4; batch 101632; train accuracy: 0.868256\n",
      "epoch 4; batch 101760; loss 0.016239\n",
      "epoch:4; batch 101760; train accuracy: 0.868294\n",
      "epoch 4; batch 101888; loss 0.011496\n",
      "epoch:4; batch 101888; train accuracy: 0.868334\n",
      "epoch 4; batch 102016; loss 0.025760\n",
      "epoch:4; batch 102016; train accuracy: 0.868372\n",
      "epoch 4; batch 102144; loss 0.007634\n",
      "epoch:4; batch 102144; train accuracy: 0.868413\n",
      "epoch 4; batch 102272; loss 0.055436\n",
      "epoch:4; batch 102272; train accuracy: 0.868448\n",
      "epoch 4; batch 102400; loss 0.006308\n",
      "epoch:4; batch 102400; train accuracy: 0.868489\n",
      "epoch 4; batch 102528; loss 0.024903\n",
      "epoch:4; batch 102528; train accuracy: 0.868527\n",
      "epoch 4; batch 102656; loss 0.006180\n",
      "epoch:4; batch 102656; train accuracy: 0.868567\n",
      "epoch 4; batch 102784; loss 0.003337\n",
      "epoch:4; batch 102784; train accuracy: 0.868607\n",
      "epoch 4; batch 102912; loss 0.042170\n",
      "epoch:4; batch 102912; train accuracy: 0.868643\n",
      "epoch 4; batch 103040; loss 0.021119\n",
      "epoch:4; batch 103040; train accuracy: 0.868681\n",
      "epoch 4; batch 103168; loss 0.013680\n",
      "epoch:4; batch 103168; train accuracy: 0.868718\n",
      "epoch 4; batch 103296; loss 0.010565\n",
      "epoch:4; batch 103296; train accuracy: 0.868759\n",
      "epoch 4; batch 103424; loss 0.049067\n",
      "epoch:4; batch 103424; train accuracy: 0.868796\n",
      "epoch 4; batch 103552; loss 0.008184\n",
      "epoch:4; batch 103552; train accuracy: 0.868837\n",
      "epoch 4; batch 103680; loss 0.045170\n",
      "epoch:4; batch 103680; train accuracy: 0.868872\n",
      "epoch 4; batch 103808; loss 0.003308\n",
      "epoch:4; batch 103808; train accuracy: 0.868912\n",
      "epoch 4; batch 103936; loss 0.018547\n",
      "epoch:4; batch 103936; train accuracy: 0.868950\n",
      "epoch 4; batch 104064; loss 0.007285\n",
      "epoch:4; batch 104064; train accuracy: 0.868990\n",
      "epoch 4; batch 104192; loss 0.071821\n",
      "epoch:4; batch 104192; train accuracy: 0.869025\n",
      "epoch 4; batch 104320; loss 0.008312\n",
      "epoch:4; batch 104320; train accuracy: 0.869065\n",
      "epoch 4; batch 104448; loss 0.006883\n",
      "epoch:4; batch 104448; train accuracy: 0.869105\n",
      "epoch 4; batch 104576; loss 0.016541\n",
      "epoch:4; batch 104576; train accuracy: 0.869142\n",
      "epoch 4; batch 104704; loss 0.009072\n",
      "epoch:4; batch 104704; train accuracy: 0.869182\n",
      "epoch 4; batch 104832; loss 0.011562\n",
      "epoch:4; batch 104832; train accuracy: 0.869222\n",
      "epoch 4; batch 104960; loss 0.003580\n",
      "epoch:4; batch 104960; train accuracy: 0.869262\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 31040 ; rate: 0.295732\n",
      "y_true_label_1_num: 11977 ; rate: 0.114110\n",
      "y_true_label_2_num: 23668 ; rate: 0.225495\n",
      "y_true_label_3_num: 38275 ; rate: 0.364663\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.989720\n",
      "valid avg_precision: 0.990206\n",
      "valid avg_recall: 0.989482\n",
      "valid avg_f1: 0.989832\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 4423 ; rate: 0.295339\n",
      "y_true_label_1_num: 1735 ; rate: 0.115852\n",
      "y_true_label_2_num: 3413 ; rate: 0.227898\n",
      "y_true_label_3_num: 5405 ; rate: 0.360911\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.685296\n",
      "valid avg_precision: 0.694282\n",
      "valid avg_recall: 0.682025\n",
      "valid avg_f1: 0.687633\n"
     ]
    }
   ],
   "source": [
    "#训练过程\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "for num in range(epochs):\n",
    "    print('epoch %d' % (num+1))\n",
    "    train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=10000).batch(batch_size,drop_remainder=True)\n",
    "    index = 0\n",
    "    for x,y in train:\n",
    "        index +=1\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(x)\n",
    "            #print(y.shape)\n",
    "            #print(labels_pred.shape)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            sparse_categorical_accuracy.update_state(y_true = y, y_pred= labels_pred)\n",
    "            #loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=labels_pred)\n",
    "            print(\"epoch %d; batch %d; loss %f\" % (num+1, index*batch_size, loss.numpy()))\n",
    "            print(\"epoch:%d; batch %d; train accuracy: %f\" % (num+1, index*batch_size, sparse_categorical_accuracy.result()))\n",
    "            grads = tape.gradient(loss, model.trainable_variables) #梯度\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables)) #梯度和变量\n",
    "    \n",
    "    # 模型在训练集上 的评估参数,batch_size 128\n",
    "    print('模型在训练集上的评价指标：')\n",
    "    eval_train = eval_of_experiment((train_x,train_y), batch_size ,model)\n",
    "    eval_train.eval_of_valid()\n",
    "    eval_train.result()\n",
    "    #  模型在验证集上 的评估参数，batch_size 128\n",
    "    print('模型在验证集上的评价指标：')\n",
    "    eval_valid = eval_of_experiment((valid_x,valid_y), batch_size ,model)\n",
    "    eval_valid.eval_of_valid()\n",
    "    eval_valid.result()\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7  env 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "embedding_matrix = load_obj('./data2/train_sales_embedding_matrix.pkl')\n",
    "\n",
    "model = single_attention_aspect(\n",
    "                maxlen=maxlen, \n",
    "                embedding_matrix=embedding_matrix, \n",
    "                # aspect=loc,\n",
    "                # aspect=ser,\n",
    "                # aspect=pri,\n",
    "                aspect=env,\n",
    "                embedding_dim=200,\n",
    "                aspect_len=20,\n",
    "                hidden_size=100,\n",
    "                activation=None,\n",
    "                output_size=4\n",
    "            )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "epoch 1; batch 128; loss 1.382536\n",
      "epoch:1; batch 128; train accuracy: 0.210938\n",
      "epoch 1; batch 256; loss 1.303047\n",
      "epoch:1; batch 256; train accuracy: 0.351562\n",
      "epoch 1; batch 384; loss 1.210957\n",
      "epoch:1; batch 384; train accuracy: 0.401042\n",
      "epoch 1; batch 512; loss 1.169766\n",
      "epoch:1; batch 512; train accuracy: 0.435547\n",
      "epoch 1; batch 640; loss 1.204925\n",
      "epoch:1; batch 640; train accuracy: 0.446875\n",
      "epoch 1; batch 768; loss 1.184158\n",
      "epoch:1; batch 768; train accuracy: 0.449219\n",
      "epoch 1; batch 896; loss 1.108414\n",
      "epoch:1; batch 896; train accuracy: 0.464286\n",
      "epoch 1; batch 1024; loss 1.080003\n",
      "epoch:1; batch 1024; train accuracy: 0.463867\n",
      "epoch 1; batch 1152; loss 1.211043\n",
      "epoch:1; batch 1152; train accuracy: 0.467014\n",
      "epoch 1; batch 1280; loss 1.085746\n",
      "epoch:1; batch 1280; train accuracy: 0.471094\n",
      "epoch 1; batch 1408; loss 1.236189\n",
      "epoch:1; batch 1408; train accuracy: 0.467330\n",
      "epoch 1; batch 1536; loss 1.118964\n",
      "epoch:1; batch 1536; train accuracy: 0.472005\n",
      "epoch 1; batch 1664; loss 1.211075\n",
      "epoch:1; batch 1664; train accuracy: 0.469952\n",
      "epoch 1; batch 1792; loss 1.114459\n",
      "epoch:1; batch 1792; train accuracy: 0.472656\n",
      "epoch 1; batch 1920; loss 1.225742\n",
      "epoch:1; batch 1920; train accuracy: 0.470313\n",
      "epoch 1; batch 2048; loss 1.171326\n",
      "epoch:1; batch 2048; train accuracy: 0.469727\n",
      "epoch 1; batch 2176; loss 1.165915\n",
      "epoch:1; batch 2176; train accuracy: 0.470588\n",
      "epoch 1; batch 2304; loss 1.215270\n",
      "epoch:1; batch 2304; train accuracy: 0.468316\n",
      "epoch 1; batch 2432; loss 1.079958\n",
      "epoch:1; batch 2432; train accuracy: 0.470806\n",
      "epoch 1; batch 2560; loss 1.263673\n",
      "epoch:1; batch 2560; train accuracy: 0.467578\n",
      "epoch 1; batch 2688; loss 1.173579\n",
      "epoch:1; batch 2688; train accuracy: 0.468750\n",
      "epoch 1; batch 2816; loss 1.124025\n",
      "epoch:1; batch 2816; train accuracy: 0.467685\n",
      "epoch 1; batch 2944; loss 1.184837\n",
      "epoch:1; batch 2944; train accuracy: 0.468750\n",
      "epoch 1; batch 3072; loss 1.162453\n",
      "epoch:1; batch 3072; train accuracy: 0.472005\n",
      "epoch 1; batch 3200; loss 1.123493\n",
      "epoch:1; batch 3200; train accuracy: 0.477500\n",
      "epoch 1; batch 3328; loss 1.144704\n",
      "epoch:1; batch 3328; train accuracy: 0.480168\n",
      "epoch 1; batch 3456; loss 1.082697\n",
      "epoch:1; batch 3456; train accuracy: 0.482060\n",
      "epoch 1; batch 3584; loss 1.164540\n",
      "epoch:1; batch 3584; train accuracy: 0.483259\n",
      "epoch 1; batch 3712; loss 1.107343\n",
      "epoch:1; batch 3712; train accuracy: 0.485991\n",
      "epoch 1; batch 3840; loss 1.028272\n",
      "epoch:1; batch 3840; train accuracy: 0.486979\n",
      "epoch 1; batch 3968; loss 1.080245\n",
      "epoch:1; batch 3968; train accuracy: 0.489667\n",
      "epoch 1; batch 4096; loss 1.135258\n",
      "epoch:1; batch 4096; train accuracy: 0.491455\n",
      "epoch 1; batch 4224; loss 1.193003\n",
      "epoch:1; batch 4224; train accuracy: 0.491714\n",
      "epoch 1; batch 4352; loss 1.130264\n",
      "epoch:1; batch 4352; train accuracy: 0.492417\n",
      "epoch 1; batch 4480; loss 1.070460\n",
      "epoch:1; batch 4480; train accuracy: 0.495089\n",
      "epoch 1; batch 4608; loss 1.050631\n",
      "epoch:1; batch 4608; train accuracy: 0.498264\n",
      "epoch 1; batch 4736; loss 1.043964\n",
      "epoch:1; batch 4736; train accuracy: 0.499578\n",
      "epoch 1; batch 4864; loss 1.001491\n",
      "epoch:1; batch 4864; train accuracy: 0.502467\n",
      "epoch 1; batch 4992; loss 1.019531\n",
      "epoch:1; batch 4992; train accuracy: 0.504407\n",
      "epoch 1; batch 5120; loss 0.955782\n",
      "epoch:1; batch 5120; train accuracy: 0.507031\n",
      "epoch 1; batch 5248; loss 0.987535\n",
      "epoch:1; batch 5248; train accuracy: 0.509718\n",
      "epoch 1; batch 5376; loss 1.212398\n",
      "epoch:1; batch 5376; train accuracy: 0.509673\n",
      "epoch 1; batch 5504; loss 0.962092\n",
      "epoch:1; batch 5504; train accuracy: 0.512536\n",
      "epoch 1; batch 5632; loss 1.060883\n",
      "epoch:1; batch 5632; train accuracy: 0.513317\n",
      "epoch 1; batch 5760; loss 1.005439\n",
      "epoch:1; batch 5760; train accuracy: 0.515451\n",
      "epoch 1; batch 5888; loss 0.959250\n",
      "epoch:1; batch 5888; train accuracy: 0.517663\n",
      "epoch 1; batch 6016; loss 1.039357\n",
      "epoch:1; batch 6016; train accuracy: 0.519614\n",
      "epoch 1; batch 6144; loss 0.984374\n",
      "epoch:1; batch 6144; train accuracy: 0.521322\n",
      "epoch 1; batch 6272; loss 0.970094\n",
      "epoch:1; batch 6272; train accuracy: 0.523119\n",
      "epoch 1; batch 6400; loss 0.847993\n",
      "epoch:1; batch 6400; train accuracy: 0.526250\n",
      "epoch 1; batch 6528; loss 1.044506\n",
      "epoch:1; batch 6528; train accuracy: 0.527114\n",
      "epoch 1; batch 6656; loss 0.919639\n",
      "epoch:1; batch 6656; train accuracy: 0.529748\n",
      "epoch 1; batch 6784; loss 0.964453\n",
      "epoch:1; batch 6784; train accuracy: 0.532134\n",
      "epoch 1; batch 6912; loss 0.993860\n",
      "epoch:1; batch 6912; train accuracy: 0.532841\n",
      "epoch 1; batch 7040; loss 0.900194\n",
      "epoch:1; batch 7040; train accuracy: 0.535227\n",
      "epoch 1; batch 7168; loss 0.977682\n",
      "epoch:1; batch 7168; train accuracy: 0.536272\n",
      "epoch 1; batch 7296; loss 0.938100\n",
      "epoch:1; batch 7296; train accuracy: 0.537966\n",
      "epoch 1; batch 7424; loss 0.858585\n",
      "epoch:1; batch 7424; train accuracy: 0.539871\n",
      "epoch 1; batch 7552; loss 0.786643\n",
      "epoch:1; batch 7552; train accuracy: 0.543035\n",
      "epoch 1; batch 7680; loss 0.614371\n",
      "epoch:1; batch 7680; train accuracy: 0.546875\n",
      "epoch 1; batch 7808; loss 0.939525\n",
      "epoch:1; batch 7808; train accuracy: 0.548284\n",
      "epoch 1; batch 7936; loss 1.197015\n",
      "epoch:1; batch 7936; train accuracy: 0.549143\n",
      "epoch 1; batch 8064; loss 0.946745\n",
      "epoch:1; batch 8064; train accuracy: 0.550967\n",
      "epoch 1; batch 8192; loss 0.845690\n",
      "epoch:1; batch 8192; train accuracy: 0.552612\n",
      "epoch 1; batch 8320; loss 0.865048\n",
      "epoch:1; batch 8320; train accuracy: 0.555048\n",
      "epoch 1; batch 8448; loss 0.852226\n",
      "epoch:1; batch 8448; train accuracy: 0.556937\n",
      "epoch 1; batch 8576; loss 0.919814\n",
      "epoch:1; batch 8576; train accuracy: 0.558652\n",
      "epoch 1; batch 8704; loss 0.914206\n",
      "epoch:1; batch 8704; train accuracy: 0.559628\n",
      "epoch 1; batch 8832; loss 0.889678\n",
      "epoch:1; batch 8832; train accuracy: 0.561481\n",
      "epoch 1; batch 8960; loss 0.962916\n",
      "epoch:1; batch 8960; train accuracy: 0.562500\n",
      "epoch 1; batch 9088; loss 0.943163\n",
      "epoch:1; batch 9088; train accuracy: 0.563600\n",
      "epoch 1; batch 9216; loss 1.108079\n",
      "epoch:1; batch 9216; train accuracy: 0.563911\n",
      "epoch 1; batch 9344; loss 0.977885\n",
      "epoch:1; batch 9344; train accuracy: 0.564747\n",
      "epoch 1; batch 9472; loss 0.852759\n",
      "epoch:1; batch 9472; train accuracy: 0.566301\n",
      "epoch 1; batch 9600; loss 0.819297\n",
      "epoch:1; batch 9600; train accuracy: 0.568438\n",
      "epoch 1; batch 9728; loss 0.935215\n",
      "epoch:1; batch 9728; train accuracy: 0.569593\n",
      "epoch 1; batch 9856; loss 0.879228\n",
      "epoch:1; batch 9856; train accuracy: 0.571124\n",
      "epoch 1; batch 9984; loss 0.748533\n",
      "epoch:1; batch 9984; train accuracy: 0.572917\n",
      "epoch 1; batch 10112; loss 0.861980\n",
      "epoch:1; batch 10112; train accuracy: 0.573972\n",
      "epoch 1; batch 10240; loss 0.891582\n",
      "epoch:1; batch 10240; train accuracy: 0.575098\n",
      "epoch 1; batch 10368; loss 0.896217\n",
      "epoch:1; batch 10368; train accuracy: 0.576196\n",
      "epoch 1; batch 10496; loss 0.919806\n",
      "epoch:1; batch 10496; train accuracy: 0.576982\n",
      "epoch 1; batch 10624; loss 0.805131\n",
      "epoch:1; batch 10624; train accuracy: 0.578313\n",
      "epoch 1; batch 10752; loss 0.693552\n",
      "epoch:1; batch 10752; train accuracy: 0.580264\n",
      "epoch 1; batch 10880; loss 0.875234\n",
      "epoch:1; batch 10880; train accuracy: 0.581342\n",
      "epoch 1; batch 11008; loss 0.784385\n",
      "epoch:1; batch 11008; train accuracy: 0.582849\n",
      "epoch 1; batch 11136; loss 0.716614\n",
      "epoch:1; batch 11136; train accuracy: 0.584411\n",
      "epoch 1; batch 11264; loss 0.767167\n",
      "epoch:1; batch 11264; train accuracy: 0.585849\n",
      "epoch 1; batch 11392; loss 0.880899\n",
      "epoch:1; batch 11392; train accuracy: 0.587342\n",
      "epoch 1; batch 11520; loss 0.802163\n",
      "epoch:1; batch 11520; train accuracy: 0.588802\n",
      "epoch 1; batch 11648; loss 0.856550\n",
      "epoch:1; batch 11648; train accuracy: 0.589887\n",
      "epoch 1; batch 11776; loss 0.837721\n",
      "epoch:1; batch 11776; train accuracy: 0.591033\n",
      "epoch 1; batch 11904; loss 0.898255\n",
      "epoch:1; batch 11904; train accuracy: 0.591230\n",
      "epoch 1; batch 12032; loss 0.782159\n",
      "epoch:1; batch 12032; train accuracy: 0.593251\n",
      "epoch 1; batch 12160; loss 0.915281\n",
      "epoch:1; batch 12160; train accuracy: 0.594161\n",
      "epoch 1; batch 12288; loss 0.783813\n",
      "epoch:1; batch 12288; train accuracy: 0.595133\n",
      "epoch 1; batch 12416; loss 0.740128\n",
      "epoch:1; batch 12416; train accuracy: 0.596488\n",
      "epoch 1; batch 12544; loss 0.796347\n",
      "epoch:1; batch 12544; train accuracy: 0.597417\n",
      "epoch 1; batch 12672; loss 0.919200\n",
      "epoch:1; batch 12672; train accuracy: 0.597932\n",
      "epoch 1; batch 12800; loss 0.909821\n",
      "epoch:1; batch 12800; train accuracy: 0.598516\n",
      "epoch 1; batch 12928; loss 0.732143\n",
      "epoch:1; batch 12928; train accuracy: 0.599783\n",
      "epoch 1; batch 13056; loss 0.870617\n",
      "epoch:1; batch 13056; train accuracy: 0.600567\n",
      "epoch 1; batch 13184; loss 0.836030\n",
      "epoch:1; batch 13184; train accuracy: 0.601259\n",
      "epoch 1; batch 13312; loss 0.875697\n",
      "epoch:1; batch 13312; train accuracy: 0.602088\n",
      "epoch 1; batch 13440; loss 0.791676\n",
      "epoch:1; batch 13440; train accuracy: 0.603274\n",
      "epoch 1; batch 13568; loss 0.843006\n",
      "epoch:1; batch 13568; train accuracy: 0.603995\n",
      "epoch 1; batch 13696; loss 0.946297\n",
      "epoch:1; batch 13696; train accuracy: 0.604264\n",
      "epoch 1; batch 13824; loss 0.835199\n",
      "epoch:1; batch 13824; train accuracy: 0.604818\n",
      "epoch 1; batch 13952; loss 0.901982\n",
      "epoch:1; batch 13952; train accuracy: 0.605146\n",
      "epoch 1; batch 14080; loss 0.743351\n",
      "epoch:1; batch 14080; train accuracy: 0.606463\n",
      "epoch 1; batch 14208; loss 0.844578\n",
      "epoch:1; batch 14208; train accuracy: 0.607264\n",
      "epoch 1; batch 14336; loss 0.639566\n",
      "epoch:1; batch 14336; train accuracy: 0.608747\n",
      "epoch 1; batch 14464; loss 0.749573\n",
      "epoch:1; batch 14464; train accuracy: 0.609928\n",
      "epoch 1; batch 14592; loss 0.684798\n",
      "epoch:1; batch 14592; train accuracy: 0.611431\n",
      "epoch 1; batch 14720; loss 0.713260\n",
      "epoch:1; batch 14720; train accuracy: 0.612500\n",
      "epoch 1; batch 14848; loss 0.925839\n",
      "epoch:1; batch 14848; train accuracy: 0.613012\n",
      "epoch 1; batch 14976; loss 0.805027\n",
      "epoch:1; batch 14976; train accuracy: 0.613916\n",
      "epoch 1; batch 15104; loss 0.701535\n",
      "epoch:1; batch 15104; train accuracy: 0.615201\n",
      "epoch 1; batch 15232; loss 0.711682\n",
      "epoch:1; batch 15232; train accuracy: 0.616334\n",
      "epoch 1; batch 15360; loss 0.715151\n",
      "epoch:1; batch 15360; train accuracy: 0.617513\n",
      "epoch 1; batch 15488; loss 0.773524\n",
      "epoch:1; batch 15488; train accuracy: 0.618027\n",
      "epoch 1; batch 15616; loss 0.720925\n",
      "epoch:1; batch 15616; train accuracy: 0.618852\n",
      "epoch 1; batch 15744; loss 0.653155\n",
      "epoch:1; batch 15744; train accuracy: 0.619855\n",
      "epoch 1; batch 15872; loss 0.740842\n",
      "epoch:1; batch 15872; train accuracy: 0.620716\n",
      "epoch 1; batch 16000; loss 0.682865\n",
      "epoch:1; batch 16000; train accuracy: 0.621687\n",
      "epoch 1; batch 16128; loss 0.906646\n",
      "epoch:1; batch 16128; train accuracy: 0.621652\n",
      "epoch 1; batch 16256; loss 0.753499\n",
      "epoch:1; batch 16256; train accuracy: 0.622416\n",
      "epoch 1; batch 16384; loss 0.624818\n",
      "epoch:1; batch 16384; train accuracy: 0.623779\n",
      "epoch 1; batch 16512; loss 0.794218\n",
      "epoch:1; batch 16512; train accuracy: 0.624273\n",
      "epoch 1; batch 16640; loss 0.715899\n",
      "epoch:1; batch 16640; train accuracy: 0.625120\n",
      "epoch 1; batch 16768; loss 0.794346\n",
      "epoch:1; batch 16768; train accuracy: 0.625477\n",
      "epoch 1; batch 16896; loss 0.695230\n",
      "epoch:1; batch 16896; train accuracy: 0.626065\n",
      "epoch 1; batch 17024; loss 0.658976\n",
      "epoch:1; batch 17024; train accuracy: 0.627232\n",
      "epoch 1; batch 17152; loss 0.786880\n",
      "epoch:1; batch 17152; train accuracy: 0.627798\n",
      "epoch 1; batch 17280; loss 0.576860\n",
      "epoch:1; batch 17280; train accuracy: 0.629282\n",
      "epoch 1; batch 17408; loss 0.571878\n",
      "epoch:1; batch 17408; train accuracy: 0.630687\n",
      "epoch 1; batch 17536; loss 0.754646\n",
      "epoch:1; batch 17536; train accuracy: 0.631216\n",
      "epoch 1; batch 17664; loss 0.742992\n",
      "epoch:1; batch 17664; train accuracy: 0.631963\n",
      "epoch 1; batch 17792; loss 0.717468\n",
      "epoch:1; batch 17792; train accuracy: 0.632925\n",
      "epoch 1; batch 17920; loss 0.760291\n",
      "epoch:1; batch 17920; train accuracy: 0.633538\n",
      "epoch 1; batch 18048; loss 0.944097\n",
      "epoch:1; batch 18048; train accuracy: 0.633865\n",
      "epoch 1; batch 18176; loss 0.747178\n",
      "epoch:1; batch 18176; train accuracy: 0.634738\n",
      "epoch 1; batch 18304; loss 0.730338\n",
      "epoch:1; batch 18304; train accuracy: 0.635380\n",
      "epoch 1; batch 18432; loss 0.667844\n",
      "epoch:1; batch 18432; train accuracy: 0.636122\n",
      "epoch 1; batch 18560; loss 0.678963\n",
      "epoch:1; batch 18560; train accuracy: 0.637123\n",
      "epoch 1; batch 18688; loss 0.667620\n",
      "epoch:1; batch 18688; train accuracy: 0.638003\n",
      "epoch 1; batch 18816; loss 0.719467\n",
      "epoch:1; batch 18816; train accuracy: 0.638871\n",
      "epoch 1; batch 18944; loss 0.634957\n",
      "epoch:1; batch 18944; train accuracy: 0.639728\n",
      "epoch 1; batch 19072; loss 0.703352\n",
      "epoch:1; batch 19072; train accuracy: 0.640468\n",
      "epoch 1; batch 19200; loss 0.660193\n",
      "epoch:1; batch 19200; train accuracy: 0.641354\n",
      "epoch 1; batch 19328; loss 0.683311\n",
      "epoch:1; batch 19328; train accuracy: 0.642125\n",
      "epoch 1; batch 19456; loss 0.777905\n",
      "epoch:1; batch 19456; train accuracy: 0.642784\n",
      "epoch 1; batch 19584; loss 0.801710\n",
      "epoch:1; batch 19584; train accuracy: 0.643331\n",
      "epoch 1; batch 19712; loss 0.764286\n",
      "epoch:1; batch 19712; train accuracy: 0.644075\n",
      "epoch 1; batch 19840; loss 0.742271\n",
      "epoch:1; batch 19840; train accuracy: 0.644859\n",
      "epoch 1; batch 19968; loss 0.705297\n",
      "epoch:1; batch 19968; train accuracy: 0.645533\n",
      "epoch 1; batch 20096; loss 0.834642\n",
      "epoch:1; batch 20096; train accuracy: 0.645701\n",
      "epoch 1; batch 20224; loss 0.658669\n",
      "epoch:1; batch 20224; train accuracy: 0.646509\n",
      "epoch 1; batch 20352; loss 0.759048\n",
      "epoch:1; batch 20352; train accuracy: 0.646963\n",
      "epoch 1; batch 20480; loss 0.735942\n",
      "epoch:1; batch 20480; train accuracy: 0.647070\n",
      "epoch 1; batch 20608; loss 0.783634\n",
      "epoch:1; batch 20608; train accuracy: 0.647710\n",
      "epoch 1; batch 20736; loss 0.604088\n",
      "epoch:1; batch 20736; train accuracy: 0.648534\n",
      "epoch 1; batch 20864; loss 0.645850\n",
      "epoch:1; batch 20864; train accuracy: 0.649252\n",
      "epoch 1; batch 20992; loss 0.643643\n",
      "epoch:1; batch 20992; train accuracy: 0.650010\n",
      "epoch 1; batch 21120; loss 0.561860\n",
      "epoch:1; batch 21120; train accuracy: 0.650947\n",
      "epoch 1; batch 21248; loss 0.561830\n",
      "epoch:1; batch 21248; train accuracy: 0.652061\n",
      "epoch 1; batch 21376; loss 0.685104\n",
      "epoch:1; batch 21376; train accuracy: 0.652788\n",
      "epoch 1; batch 21504; loss 0.600941\n",
      "epoch:1; batch 21504; train accuracy: 0.653553\n",
      "epoch 1; batch 21632; loss 0.650374\n",
      "epoch:1; batch 21632; train accuracy: 0.654401\n",
      "epoch 1; batch 21760; loss 0.664551\n",
      "epoch:1; batch 21760; train accuracy: 0.655101\n",
      "epoch 1; batch 21888; loss 0.667166\n",
      "epoch:1; batch 21888; train accuracy: 0.655747\n",
      "epoch 1; batch 22016; loss 0.645320\n",
      "epoch:1; batch 22016; train accuracy: 0.656295\n",
      "epoch 1; batch 22144; loss 0.656655\n",
      "epoch:1; batch 22144; train accuracy: 0.656882\n",
      "epoch 1; batch 22272; loss 0.560675\n",
      "epoch:1; batch 22272; train accuracy: 0.657642\n",
      "epoch 1; batch 22400; loss 0.597948\n",
      "epoch:1; batch 22400; train accuracy: 0.658393\n",
      "epoch 1; batch 22528; loss 0.619727\n",
      "epoch:1; batch 22528; train accuracy: 0.659135\n",
      "epoch 1; batch 22656; loss 0.566737\n",
      "epoch:1; batch 22656; train accuracy: 0.659913\n",
      "epoch 1; batch 22784; loss 0.591111\n",
      "epoch:1; batch 22784; train accuracy: 0.660595\n",
      "epoch 1; batch 22912; loss 0.572367\n",
      "epoch:1; batch 22912; train accuracy: 0.661313\n",
      "epoch 1; batch 23040; loss 0.549967\n",
      "epoch:1; batch 23040; train accuracy: 0.661979\n",
      "epoch 1; batch 23168; loss 0.696103\n",
      "epoch:1; batch 23168; train accuracy: 0.662336\n",
      "epoch 1; batch 23296; loss 0.580885\n",
      "epoch:1; batch 23296; train accuracy: 0.662989\n",
      "epoch 1; batch 23424; loss 0.608463\n",
      "epoch:1; batch 23424; train accuracy: 0.663636\n",
      "epoch 1; batch 23552; loss 0.793010\n",
      "epoch:1; batch 23552; train accuracy: 0.663935\n",
      "epoch 1; batch 23680; loss 0.598054\n",
      "epoch:1; batch 23680; train accuracy: 0.664443\n",
      "epoch 1; batch 23808; loss 0.612655\n",
      "epoch:1; batch 23808; train accuracy: 0.665113\n",
      "epoch 1; batch 23936; loss 0.711443\n",
      "epoch:1; batch 23936; train accuracy: 0.665525\n",
      "epoch 1; batch 24064; loss 0.658413\n",
      "epoch:1; batch 24064; train accuracy: 0.666140\n",
      "epoch 1; batch 24192; loss 0.709896\n",
      "epoch:1; batch 24192; train accuracy: 0.666501\n",
      "epoch 1; batch 24320; loss 0.622176\n",
      "epoch:1; batch 24320; train accuracy: 0.667105\n",
      "epoch 1; batch 24448; loss 0.611775\n",
      "epoch:1; batch 24448; train accuracy: 0.667826\n",
      "epoch 1; batch 24576; loss 0.662642\n",
      "epoch:1; batch 24576; train accuracy: 0.668416\n",
      "epoch 1; batch 24704; loss 0.593220\n",
      "epoch:1; batch 24704; train accuracy: 0.669001\n",
      "epoch 1; batch 24832; loss 0.549240\n",
      "epoch:1; batch 24832; train accuracy: 0.669499\n",
      "epoch 1; batch 24960; loss 0.808666\n",
      "epoch:1; batch 24960; train accuracy: 0.669752\n",
      "epoch 1; batch 25088; loss 0.619637\n",
      "epoch:1; batch 25088; train accuracy: 0.670241\n",
      "epoch 1; batch 25216; loss 0.672517\n",
      "epoch:1; batch 25216; train accuracy: 0.670765\n",
      "epoch 1; batch 25344; loss 0.638366\n",
      "epoch:1; batch 25344; train accuracy: 0.671204\n",
      "epoch 1; batch 25472; loss 0.527667\n",
      "epoch:1; batch 25472; train accuracy: 0.671836\n",
      "epoch 1; batch 25600; loss 0.735539\n",
      "epoch:1; batch 25600; train accuracy: 0.672109\n",
      "epoch 1; batch 25728; loss 0.742802\n",
      "epoch:1; batch 25728; train accuracy: 0.672458\n",
      "epoch 1; batch 25856; loss 0.622409\n",
      "epoch:1; batch 25856; train accuracy: 0.672997\n",
      "epoch 1; batch 25984; loss 0.809757\n",
      "epoch:1; batch 25984; train accuracy: 0.673260\n",
      "epoch 1; batch 26112; loss 0.600345\n",
      "epoch:1; batch 26112; train accuracy: 0.673828\n",
      "epoch 1; batch 26240; loss 0.572119\n",
      "epoch:1; batch 26240; train accuracy: 0.674352\n",
      "epoch 1; batch 26368; loss 0.527013\n",
      "epoch:1; batch 26368; train accuracy: 0.675023\n",
      "epoch 1; batch 26496; loss 0.606341\n",
      "epoch:1; batch 26496; train accuracy: 0.675649\n",
      "epoch 1; batch 26624; loss 0.760432\n",
      "epoch:1; batch 26624; train accuracy: 0.675932\n",
      "epoch 1; batch 26752; loss 0.757779\n",
      "epoch:1; batch 26752; train accuracy: 0.676136\n",
      "epoch 1; batch 26880; loss 0.649831\n",
      "epoch:1; batch 26880; train accuracy: 0.676525\n",
      "epoch 1; batch 27008; loss 0.707266\n",
      "epoch:1; batch 27008; train accuracy: 0.676836\n",
      "epoch 1; batch 27136; loss 0.807350\n",
      "epoch:1; batch 27136; train accuracy: 0.677034\n",
      "epoch 1; batch 27264; loss 0.783928\n",
      "epoch:1; batch 27264; train accuracy: 0.677450\n",
      "epoch 1; batch 27392; loss 0.621748\n",
      "epoch:1; batch 27392; train accuracy: 0.678008\n",
      "epoch 1; batch 27520; loss 0.682799\n",
      "epoch:1; batch 27520; train accuracy: 0.678343\n",
      "epoch 1; batch 27648; loss 0.922532\n",
      "epoch:1; batch 27648; train accuracy: 0.678530\n",
      "epoch 1; batch 27776; loss 0.667828\n",
      "epoch:1; batch 27776; train accuracy: 0.678895\n",
      "epoch 1; batch 27904; loss 0.529281\n",
      "epoch:1; batch 27904; train accuracy: 0.679652\n",
      "epoch 1; batch 28032; loss 0.653224\n",
      "epoch:1; batch 28032; train accuracy: 0.679973\n",
      "epoch 1; batch 28160; loss 0.557406\n",
      "epoch:1; batch 28160; train accuracy: 0.680540\n",
      "epoch 1; batch 28288; loss 0.578748\n",
      "epoch:1; batch 28288; train accuracy: 0.680925\n",
      "epoch 1; batch 28416; loss 0.555316\n",
      "epoch:1; batch 28416; train accuracy: 0.681482\n",
      "epoch 1; batch 28544; loss 0.516191\n",
      "epoch:1; batch 28544; train accuracy: 0.682070\n",
      "epoch 1; batch 28672; loss 0.580483\n",
      "epoch:1; batch 28672; train accuracy: 0.682617\n",
      "epoch 1; batch 28800; loss 0.791212\n",
      "epoch:1; batch 28800; train accuracy: 0.682674\n",
      "epoch 1; batch 28928; loss 0.631659\n",
      "epoch:1; batch 28928; train accuracy: 0.683179\n",
      "epoch 1; batch 29056; loss 0.696243\n",
      "epoch:1; batch 29056; train accuracy: 0.683473\n",
      "epoch 1; batch 29184; loss 0.616538\n",
      "epoch:1; batch 29184; train accuracy: 0.683936\n",
      "epoch 1; batch 29312; loss 0.845118\n",
      "epoch:1; batch 29312; train accuracy: 0.684123\n",
      "epoch 1; batch 29440; loss 0.672092\n",
      "epoch:1; batch 29440; train accuracy: 0.684443\n",
      "epoch 1; batch 29568; loss 0.602567\n",
      "epoch:1; batch 29568; train accuracy: 0.684794\n",
      "epoch 1; batch 29696; loss 0.631719\n",
      "epoch:1; batch 29696; train accuracy: 0.685109\n",
      "epoch 1; batch 29824; loss 0.624408\n",
      "epoch:1; batch 29824; train accuracy: 0.685488\n",
      "epoch 1; batch 29952; loss 0.587712\n",
      "epoch:1; batch 29952; train accuracy: 0.685897\n",
      "epoch 1; batch 30080; loss 0.602294\n",
      "epoch:1; batch 30080; train accuracy: 0.686336\n",
      "epoch 1; batch 30208; loss 0.629563\n",
      "epoch:1; batch 30208; train accuracy: 0.686706\n",
      "epoch 1; batch 30336; loss 0.589468\n",
      "epoch:1; batch 30336; train accuracy: 0.687137\n",
      "epoch 1; batch 30464; loss 0.545172\n",
      "epoch:1; batch 30464; train accuracy: 0.687533\n",
      "epoch 1; batch 30592; loss 0.722849\n",
      "epoch:1; batch 30592; train accuracy: 0.687794\n",
      "epoch 1; batch 30720; loss 0.691689\n",
      "epoch:1; batch 30720; train accuracy: 0.688119\n",
      "epoch 1; batch 30848; loss 0.569694\n",
      "epoch:1; batch 30848; train accuracy: 0.688570\n",
      "epoch 1; batch 30976; loss 0.744317\n",
      "epoch:1; batch 30976; train accuracy: 0.688856\n",
      "epoch 1; batch 31104; loss 0.612421\n",
      "epoch:1; batch 31104; train accuracy: 0.689268\n",
      "epoch 1; batch 31232; loss 0.648351\n",
      "epoch:1; batch 31232; train accuracy: 0.689549\n",
      "epoch 1; batch 31360; loss 0.537826\n",
      "epoch:1; batch 31360; train accuracy: 0.690019\n",
      "epoch 1; batch 31488; loss 0.523822\n",
      "epoch:1; batch 31488; train accuracy: 0.690454\n",
      "epoch 1; batch 31616; loss 0.712962\n",
      "epoch:1; batch 31616; train accuracy: 0.690695\n",
      "epoch 1; batch 31744; loss 0.488354\n",
      "epoch:1; batch 31744; train accuracy: 0.691343\n",
      "epoch 1; batch 31872; loss 0.566576\n",
      "epoch:1; batch 31872; train accuracy: 0.691798\n",
      "epoch 1; batch 32000; loss 0.575468\n",
      "epoch:1; batch 32000; train accuracy: 0.692187\n",
      "epoch 1; batch 32128; loss 0.709503\n",
      "epoch:1; batch 32128; train accuracy: 0.692511\n",
      "epoch 1; batch 32256; loss 0.589801\n",
      "epoch:1; batch 32256; train accuracy: 0.692894\n",
      "epoch 1; batch 32384; loss 0.812001\n",
      "epoch:1; batch 32384; train accuracy: 0.692811\n",
      "epoch 1; batch 32512; loss 0.536538\n",
      "epoch:1; batch 32512; train accuracy: 0.693375\n",
      "epoch 1; batch 32640; loss 0.472704\n",
      "epoch:1; batch 32640; train accuracy: 0.693934\n",
      "epoch 1; batch 32768; loss 0.661235\n",
      "epoch:1; batch 32768; train accuracy: 0.694153\n",
      "epoch 1; batch 32896; loss 0.613421\n",
      "epoch:1; batch 32896; train accuracy: 0.694431\n",
      "epoch 1; batch 33024; loss 0.719216\n",
      "epoch:1; batch 33024; train accuracy: 0.694646\n",
      "epoch 1; batch 33152; loss 0.558408\n",
      "epoch:1; batch 33152; train accuracy: 0.695101\n",
      "epoch 1; batch 33280; loss 0.494530\n",
      "epoch:1; batch 33280; train accuracy: 0.695643\n",
      "epoch 1; batch 33408; loss 0.511582\n",
      "epoch:1; batch 33408; train accuracy: 0.696121\n",
      "epoch 1; batch 33536; loss 0.677380\n",
      "epoch:1; batch 33536; train accuracy: 0.696356\n",
      "epoch 1; batch 33664; loss 0.492229\n",
      "epoch:1; batch 33664; train accuracy: 0.696738\n",
      "epoch 1; batch 33792; loss 0.623060\n",
      "epoch:1; batch 33792; train accuracy: 0.696999\n",
      "epoch 1; batch 33920; loss 0.737788\n",
      "epoch:1; batch 33920; train accuracy: 0.697111\n",
      "epoch 1; batch 34048; loss 0.575625\n",
      "epoch:1; batch 34048; train accuracy: 0.697427\n",
      "epoch 1; batch 34176; loss 0.673073\n",
      "epoch:1; batch 34176; train accuracy: 0.697566\n",
      "epoch 1; batch 34304; loss 0.590642\n",
      "epoch:1; batch 34304; train accuracy: 0.697907\n",
      "epoch 1; batch 34432; loss 0.691720\n",
      "epoch:1; batch 34432; train accuracy: 0.698101\n",
      "epoch 1; batch 34560; loss 0.713147\n",
      "epoch:1; batch 34560; train accuracy: 0.698322\n",
      "epoch 1; batch 34688; loss 0.673540\n",
      "epoch:1; batch 34688; train accuracy: 0.698685\n",
      "epoch 1; batch 34816; loss 0.671155\n",
      "epoch:1; batch 34816; train accuracy: 0.698817\n",
      "epoch 1; batch 34944; loss 0.572381\n",
      "epoch:1; batch 34944; train accuracy: 0.699176\n",
      "epoch 1; batch 35072; loss 0.729865\n",
      "epoch:1; batch 35072; train accuracy: 0.699361\n",
      "epoch 1; batch 35200; loss 0.639190\n",
      "epoch:1; batch 35200; train accuracy: 0.699716\n",
      "epoch 1; batch 35328; loss 0.583218\n",
      "epoch:1; batch 35328; train accuracy: 0.700068\n",
      "epoch 1; batch 35456; loss 0.616928\n",
      "epoch:1; batch 35456; train accuracy: 0.700530\n",
      "epoch 1; batch 35584; loss 0.595212\n",
      "epoch:1; batch 35584; train accuracy: 0.700849\n",
      "epoch 1; batch 35712; loss 0.654628\n",
      "epoch:1; batch 35712; train accuracy: 0.701165\n",
      "epoch 1; batch 35840; loss 0.535813\n",
      "epoch:1; batch 35840; train accuracy: 0.701507\n",
      "epoch 1; batch 35968; loss 0.581999\n",
      "epoch:1; batch 35968; train accuracy: 0.701790\n",
      "epoch 1; batch 36096; loss 0.539239\n",
      "epoch:1; batch 36096; train accuracy: 0.702128\n",
      "epoch 1; batch 36224; loss 0.581500\n",
      "epoch:1; batch 36224; train accuracy: 0.702435\n",
      "epoch 1; batch 36352; loss 0.535499\n",
      "epoch:1; batch 36352; train accuracy: 0.702850\n",
      "epoch 1; batch 36480; loss 0.592810\n",
      "epoch:1; batch 36480; train accuracy: 0.703180\n",
      "epoch 1; batch 36608; loss 0.676671\n",
      "epoch:1; batch 36608; train accuracy: 0.703344\n",
      "epoch 1; batch 36736; loss 0.563869\n",
      "epoch:1; batch 36736; train accuracy: 0.703669\n",
      "epoch 1; batch 36864; loss 0.625741\n",
      "epoch:1; batch 36864; train accuracy: 0.703993\n",
      "epoch 1; batch 36992; loss 0.563723\n",
      "epoch:1; batch 36992; train accuracy: 0.704287\n",
      "epoch 1; batch 37120; loss 0.577954\n",
      "epoch:1; batch 37120; train accuracy: 0.704661\n",
      "epoch 1; batch 37248; loss 0.614343\n",
      "epoch:1; batch 37248; train accuracy: 0.704951\n",
      "epoch 1; batch 37376; loss 0.581235\n",
      "epoch:1; batch 37376; train accuracy: 0.705265\n",
      "epoch 1; batch 37504; loss 0.506279\n",
      "epoch:1; batch 37504; train accuracy: 0.705605\n",
      "epoch 1; batch 37632; loss 0.476463\n",
      "epoch:1; batch 37632; train accuracy: 0.706048\n",
      "epoch 1; batch 37760; loss 0.636529\n",
      "epoch:1; batch 37760; train accuracy: 0.706250\n",
      "epoch 1; batch 37888; loss 0.637120\n",
      "epoch:1; batch 37888; train accuracy: 0.706451\n",
      "epoch 1; batch 38016; loss 0.612797\n",
      "epoch:1; batch 38016; train accuracy: 0.706808\n",
      "epoch 1; batch 38144; loss 0.670952\n",
      "epoch:1; batch 38144; train accuracy: 0.706979\n",
      "epoch 1; batch 38272; loss 0.627578\n",
      "epoch:1; batch 38272; train accuracy: 0.707201\n",
      "epoch 1; batch 38400; loss 0.517887\n",
      "epoch:1; batch 38400; train accuracy: 0.707630\n",
      "epoch 1; batch 38528; loss 0.680806\n",
      "epoch:1; batch 38528; train accuracy: 0.707849\n",
      "epoch 1; batch 38656; loss 0.489097\n",
      "epoch:1; batch 38656; train accuracy: 0.708247\n",
      "epoch 1; batch 38784; loss 0.574469\n",
      "epoch:1; batch 38784; train accuracy: 0.708617\n",
      "epoch 1; batch 38912; loss 0.493005\n",
      "epoch:1; batch 38912; train accuracy: 0.709010\n",
      "epoch 1; batch 39040; loss 0.743944\n",
      "epoch:1; batch 39040; train accuracy: 0.709221\n",
      "epoch 1; batch 39168; loss 0.609450\n",
      "epoch:1; batch 39168; train accuracy: 0.709584\n",
      "epoch 1; batch 39296; loss 0.701801\n",
      "epoch:1; batch 39296; train accuracy: 0.709691\n",
      "epoch 1; batch 39424; loss 0.490873\n",
      "epoch:1; batch 39424; train accuracy: 0.710024\n",
      "epoch 1; batch 39552; loss 0.597597\n",
      "epoch:1; batch 39552; train accuracy: 0.710280\n",
      "epoch 1; batch 39680; loss 0.636141\n",
      "epoch:1; batch 39680; train accuracy: 0.710484\n",
      "epoch 1; batch 39808; loss 0.580651\n",
      "epoch:1; batch 39808; train accuracy: 0.710686\n",
      "epoch 1; batch 39936; loss 0.675450\n",
      "epoch:1; batch 39936; train accuracy: 0.710762\n",
      "epoch 1; batch 40064; loss 0.619185\n",
      "epoch:1; batch 40064; train accuracy: 0.710938\n",
      "epoch 1; batch 40192; loss 0.495955\n",
      "epoch:1; batch 40192; train accuracy: 0.711211\n",
      "epoch 1; batch 40320; loss 0.604646\n",
      "epoch:1; batch 40320; train accuracy: 0.711508\n",
      "epoch 1; batch 40448; loss 0.528311\n",
      "epoch:1; batch 40448; train accuracy: 0.711852\n",
      "epoch 1; batch 40576; loss 0.597577\n",
      "epoch:1; batch 40576; train accuracy: 0.712194\n",
      "epoch 1; batch 40704; loss 0.577339\n",
      "epoch:1; batch 40704; train accuracy: 0.712461\n",
      "epoch 1; batch 40832; loss 0.603009\n",
      "epoch:1; batch 40832; train accuracy: 0.712823\n",
      "epoch 1; batch 40960; loss 0.658609\n",
      "epoch:1; batch 40960; train accuracy: 0.713037\n",
      "epoch 1; batch 41088; loss 0.547198\n",
      "epoch:1; batch 41088; train accuracy: 0.713298\n",
      "epoch 1; batch 41216; loss 0.584790\n",
      "epoch:1; batch 41216; train accuracy: 0.713558\n",
      "epoch 1; batch 41344; loss 0.604952\n",
      "epoch:1; batch 41344; train accuracy: 0.713695\n",
      "epoch 1; batch 41472; loss 0.569759\n",
      "epoch:1; batch 41472; train accuracy: 0.713855\n",
      "epoch 1; batch 41600; loss 0.493860\n",
      "epoch:1; batch 41600; train accuracy: 0.714183\n",
      "epoch 1; batch 41728; loss 0.525582\n",
      "epoch:1; batch 41728; train accuracy: 0.714532\n",
      "epoch 1; batch 41856; loss 0.642838\n",
      "epoch:1; batch 41856; train accuracy: 0.714736\n",
      "epoch 1; batch 41984; loss 0.628300\n",
      "epoch:1; batch 41984; train accuracy: 0.714915\n",
      "epoch 1; batch 42112; loss 0.642730\n",
      "epoch:1; batch 42112; train accuracy: 0.714927\n",
      "epoch 1; batch 42240; loss 0.769989\n",
      "epoch:1; batch 42240; train accuracy: 0.715033\n",
      "epoch 1; batch 42368; loss 0.528327\n",
      "epoch:1; batch 42368; train accuracy: 0.715351\n",
      "epoch 1; batch 42496; loss 0.627984\n",
      "epoch:1; batch 42496; train accuracy: 0.715479\n",
      "epoch 1; batch 42624; loss 0.723523\n",
      "epoch:1; batch 42624; train accuracy: 0.715630\n",
      "epoch 1; batch 42752; loss 0.678503\n",
      "epoch:1; batch 42752; train accuracy: 0.715826\n",
      "epoch 1; batch 42880; loss 0.634938\n",
      "epoch:1; batch 42880; train accuracy: 0.715998\n",
      "epoch 1; batch 43008; loss 0.637020\n",
      "epoch:1; batch 43008; train accuracy: 0.716239\n",
      "epoch 1; batch 43136; loss 0.606503\n",
      "epoch:1; batch 43136; train accuracy: 0.716362\n",
      "epoch 1; batch 43264; loss 0.599079\n",
      "epoch:1; batch 43264; train accuracy: 0.716554\n",
      "epoch 1; batch 43392; loss 0.715144\n",
      "epoch:1; batch 43392; train accuracy: 0.716653\n",
      "epoch 1; batch 43520; loss 0.532606\n",
      "epoch:1; batch 43520; train accuracy: 0.716912\n",
      "epoch 1; batch 43648; loss 0.679270\n",
      "epoch:1; batch 43648; train accuracy: 0.717100\n",
      "epoch 1; batch 43776; loss 0.689882\n",
      "epoch:1; batch 43776; train accuracy: 0.717174\n",
      "epoch 1; batch 43904; loss 0.550018\n",
      "epoch:1; batch 43904; train accuracy: 0.717429\n",
      "epoch 1; batch 44032; loss 0.723742\n",
      "epoch:1; batch 44032; train accuracy: 0.717501\n",
      "epoch 1; batch 44160; loss 0.544985\n",
      "epoch:1; batch 44160; train accuracy: 0.717708\n",
      "epoch 1; batch 44288; loss 0.650753\n",
      "epoch:1; batch 44288; train accuracy: 0.717802\n",
      "epoch 1; batch 44416; loss 0.563457\n",
      "epoch:1; batch 44416; train accuracy: 0.718030\n",
      "epoch 1; batch 44544; loss 0.648584\n",
      "epoch:1; batch 44544; train accuracy: 0.718211\n",
      "epoch 1; batch 44672; loss 0.622453\n",
      "epoch:1; batch 44672; train accuracy: 0.718437\n",
      "epoch 1; batch 44800; loss 0.552436\n",
      "epoch:1; batch 44800; train accuracy: 0.718661\n",
      "epoch 1; batch 44928; loss 0.494936\n",
      "epoch:1; batch 44928; train accuracy: 0.718995\n",
      "epoch 1; batch 45056; loss 0.838482\n",
      "epoch:1; batch 45056; train accuracy: 0.718994\n",
      "epoch 1; batch 45184; loss 0.518271\n",
      "epoch:1; batch 45184; train accuracy: 0.719259\n",
      "epoch 1; batch 45312; loss 0.616680\n",
      "epoch:1; batch 45312; train accuracy: 0.719456\n",
      "epoch 1; batch 45440; loss 0.619487\n",
      "epoch:1; batch 45440; train accuracy: 0.719586\n",
      "epoch 1; batch 45568; loss 0.702513\n",
      "epoch:1; batch 45568; train accuracy: 0.719650\n",
      "epoch 1; batch 45696; loss 0.755014\n",
      "epoch:1; batch 45696; train accuracy: 0.719647\n",
      "epoch 1; batch 45824; loss 0.590179\n",
      "epoch:1; batch 45824; train accuracy: 0.719950\n",
      "epoch 1; batch 45952; loss 0.549788\n",
      "epoch:1; batch 45952; train accuracy: 0.720208\n",
      "epoch 1; batch 46080; loss 0.633751\n",
      "epoch:1; batch 46080; train accuracy: 0.720464\n",
      "epoch 1; batch 46208; loss 0.611677\n",
      "epoch:1; batch 46208; train accuracy: 0.720611\n",
      "epoch 1; batch 46336; loss 0.608056\n",
      "epoch:1; batch 46336; train accuracy: 0.720779\n",
      "epoch 1; batch 46464; loss 0.485182\n",
      "epoch:1; batch 46464; train accuracy: 0.721074\n",
      "epoch 1; batch 46592; loss 0.586262\n",
      "epoch:1; batch 46592; train accuracy: 0.721326\n",
      "epoch 1; batch 46720; loss 0.611642\n",
      "epoch:1; batch 46720; train accuracy: 0.721490\n",
      "epoch 1; batch 46848; loss 0.669419\n",
      "epoch:1; batch 46848; train accuracy: 0.721568\n",
      "epoch 1; batch 46976; loss 0.731945\n",
      "epoch:1; batch 46976; train accuracy: 0.721688\n",
      "epoch 1; batch 47104; loss 0.493134\n",
      "epoch:1; batch 47104; train accuracy: 0.722041\n",
      "epoch 1; batch 47232; loss 0.549471\n",
      "epoch:1; batch 47232; train accuracy: 0.722328\n",
      "epoch 1; batch 47360; loss 0.542778\n",
      "epoch:1; batch 47360; train accuracy: 0.722656\n",
      "epoch 1; batch 47488; loss 0.539749\n",
      "epoch:1; batch 47488; train accuracy: 0.722962\n",
      "epoch 1; batch 47616; loss 0.648680\n",
      "epoch:1; batch 47616; train accuracy: 0.723076\n",
      "epoch 1; batch 47744; loss 0.500676\n",
      "epoch:1; batch 47744; train accuracy: 0.723316\n",
      "epoch 1; batch 47872; loss 0.577439\n",
      "epoch:1; batch 47872; train accuracy: 0.723492\n",
      "epoch 1; batch 48000; loss 0.596911\n",
      "epoch:1; batch 48000; train accuracy: 0.723583\n",
      "epoch 1; batch 48128; loss 0.540386\n",
      "epoch:1; batch 48128; train accuracy: 0.723841\n",
      "epoch 1; batch 48256; loss 0.642285\n",
      "epoch:1; batch 48256; train accuracy: 0.723931\n",
      "epoch 1; batch 48384; loss 0.664044\n",
      "epoch:1; batch 48384; train accuracy: 0.724020\n",
      "epoch 1; batch 48512; loss 0.522829\n",
      "epoch:1; batch 48512; train accuracy: 0.724336\n",
      "epoch 1; batch 48640; loss 0.659975\n",
      "epoch:1; batch 48640; train accuracy: 0.724465\n",
      "epoch 1; batch 48768; loss 0.649354\n",
      "epoch:1; batch 48768; train accuracy: 0.724430\n",
      "epoch 1; batch 48896; loss 0.674201\n",
      "epoch:1; batch 48896; train accuracy: 0.724579\n",
      "epoch 1; batch 49024; loss 0.582185\n",
      "epoch:1; batch 49024; train accuracy: 0.724686\n",
      "epoch 1; batch 49152; loss 0.462450\n",
      "epoch:1; batch 49152; train accuracy: 0.725016\n",
      "epoch 1; batch 49280; loss 0.503886\n",
      "epoch:1; batch 49280; train accuracy: 0.725304\n",
      "epoch 1; batch 49408; loss 0.653512\n",
      "epoch:1; batch 49408; train accuracy: 0.725368\n",
      "epoch 1; batch 49536; loss 0.662517\n",
      "epoch:1; batch 49536; train accuracy: 0.725513\n",
      "epoch 1; batch 49664; loss 0.728466\n",
      "epoch:1; batch 49664; train accuracy: 0.725475\n",
      "epoch 1; batch 49792; loss 0.433423\n",
      "epoch:1; batch 49792; train accuracy: 0.725799\n",
      "epoch 1; batch 49920; loss 0.692805\n",
      "epoch:1; batch 49920; train accuracy: 0.725962\n",
      "epoch 1; batch 50048; loss 0.662546\n",
      "epoch:1; batch 50048; train accuracy: 0.726003\n",
      "epoch 1; batch 50176; loss 0.763448\n",
      "epoch:1; batch 50176; train accuracy: 0.725925\n",
      "epoch 1; batch 50304; loss 0.766341\n",
      "epoch:1; batch 50304; train accuracy: 0.725906\n",
      "epoch 1; batch 50432; loss 0.744372\n",
      "epoch:1; batch 50432; train accuracy: 0.725928\n",
      "epoch 1; batch 50560; loss 0.546812\n",
      "epoch:1; batch 50560; train accuracy: 0.726108\n",
      "epoch 1; batch 50688; loss 0.605100\n",
      "epoch:1; batch 50688; train accuracy: 0.726227\n",
      "epoch 1; batch 50816; loss 0.582342\n",
      "epoch:1; batch 50816; train accuracy: 0.726444\n",
      "epoch 1; batch 50944; loss 0.626174\n",
      "epoch:1; batch 50944; train accuracy: 0.726543\n",
      "epoch 1; batch 51072; loss 0.620165\n",
      "epoch:1; batch 51072; train accuracy: 0.726700\n",
      "epoch 1; batch 51200; loss 0.481513\n",
      "epoch:1; batch 51200; train accuracy: 0.727031\n",
      "epoch 1; batch 51328; loss 0.681723\n",
      "epoch:1; batch 51328; train accuracy: 0.727108\n",
      "epoch 1; batch 51456; loss 0.773250\n",
      "epoch:1; batch 51456; train accuracy: 0.727068\n",
      "epoch 1; batch 51584; loss 0.563003\n",
      "epoch:1; batch 51584; train accuracy: 0.727357\n",
      "epoch 1; batch 51712; loss 0.557864\n",
      "epoch:1; batch 51712; train accuracy: 0.727587\n",
      "epoch 1; batch 51840; loss 0.486157\n",
      "epoch:1; batch 51840; train accuracy: 0.727797\n",
      "epoch 1; batch 51968; loss 0.581256\n",
      "epoch:1; batch 51968; train accuracy: 0.727967\n",
      "epoch 1; batch 52096; loss 0.624295\n",
      "epoch:1; batch 52096; train accuracy: 0.728021\n",
      "epoch 1; batch 52224; loss 0.565958\n",
      "epoch:1; batch 52224; train accuracy: 0.728152\n",
      "epoch 1; batch 52352; loss 0.514765\n",
      "epoch:1; batch 52352; train accuracy: 0.728377\n",
      "epoch 1; batch 52480; loss 0.760703\n",
      "epoch:1; batch 52480; train accuracy: 0.728411\n",
      "epoch 1; batch 52608; loss 0.519057\n",
      "epoch:1; batch 52608; train accuracy: 0.728596\n",
      "epoch 1; batch 52736; loss 0.568954\n",
      "epoch:1; batch 52736; train accuracy: 0.728762\n",
      "epoch 1; batch 52864; loss 0.483788\n",
      "epoch:1; batch 52864; train accuracy: 0.729041\n",
      "epoch 1; batch 52992; loss 0.527552\n",
      "epoch:1; batch 52992; train accuracy: 0.729242\n",
      "epoch 1; batch 53120; loss 0.625847\n",
      "epoch:1; batch 53120; train accuracy: 0.729311\n",
      "epoch 1; batch 53248; loss 0.666625\n",
      "epoch:1; batch 53248; train accuracy: 0.729304\n",
      "epoch 1; batch 53376; loss 0.622114\n",
      "epoch:1; batch 53376; train accuracy: 0.729410\n",
      "epoch 1; batch 53504; loss 0.516125\n",
      "epoch:1; batch 53504; train accuracy: 0.729646\n",
      "epoch 1; batch 53632; loss 0.465462\n",
      "epoch:1; batch 53632; train accuracy: 0.729881\n",
      "epoch 1; batch 53760; loss 0.583179\n",
      "epoch:1; batch 53760; train accuracy: 0.730004\n",
      "epoch 1; batch 53888; loss 0.755238\n",
      "epoch:1; batch 53888; train accuracy: 0.729977\n",
      "epoch 1; batch 54016; loss 0.504068\n",
      "epoch:1; batch 54016; train accuracy: 0.730228\n",
      "epoch 1; batch 54144; loss 0.560653\n",
      "epoch:1; batch 54144; train accuracy: 0.730423\n",
      "epoch 1; batch 54272; loss 0.581278\n",
      "epoch:1; batch 54272; train accuracy: 0.730579\n",
      "epoch 1; batch 54400; loss 0.663766\n",
      "epoch:1; batch 54400; train accuracy: 0.730588\n",
      "epoch 1; batch 54528; loss 0.646684\n",
      "epoch:1; batch 54528; train accuracy: 0.730597\n",
      "epoch 1; batch 54656; loss 0.504642\n",
      "epoch:1; batch 54656; train accuracy: 0.730862\n",
      "epoch 1; batch 54784; loss 0.591486\n",
      "epoch:1; batch 54784; train accuracy: 0.730943\n",
      "epoch 1; batch 54912; loss 0.529938\n",
      "epoch:1; batch 54912; train accuracy: 0.731170\n",
      "epoch 1; batch 55040; loss 0.680588\n",
      "epoch:1; batch 55040; train accuracy: 0.731250\n",
      "epoch 1; batch 55168; loss 0.511867\n",
      "epoch:1; batch 55168; train accuracy: 0.731475\n",
      "epoch 1; batch 55296; loss 0.685618\n",
      "epoch:1; batch 55296; train accuracy: 0.731518\n",
      "epoch 1; batch 55424; loss 0.509018\n",
      "epoch:1; batch 55424; train accuracy: 0.731723\n",
      "epoch 1; batch 55552; loss 0.708838\n",
      "epoch:1; batch 55552; train accuracy: 0.731729\n",
      "epoch 1; batch 55680; loss 0.534713\n",
      "epoch:1; batch 55680; train accuracy: 0.731879\n",
      "epoch 1; batch 55808; loss 0.585847\n",
      "epoch:1; batch 55808; train accuracy: 0.732081\n",
      "epoch 1; batch 55936; loss 0.687404\n",
      "epoch:1; batch 55936; train accuracy: 0.732176\n",
      "epoch 1; batch 56064; loss 0.702190\n",
      "epoch:1; batch 56064; train accuracy: 0.732199\n",
      "epoch 1; batch 56192; loss 0.718883\n",
      "epoch:1; batch 56192; train accuracy: 0.732150\n",
      "epoch 1; batch 56320; loss 0.808190\n",
      "epoch:1; batch 56320; train accuracy: 0.732031\n",
      "epoch 1; batch 56448; loss 0.462266\n",
      "epoch:1; batch 56448; train accuracy: 0.732338\n",
      "epoch 1; batch 56576; loss 0.609379\n",
      "epoch:1; batch 56576; train accuracy: 0.732431\n",
      "epoch 1; batch 56704; loss 0.606226\n",
      "epoch:1; batch 56704; train accuracy: 0.732576\n",
      "epoch 1; batch 56832; loss 0.601868\n",
      "epoch:1; batch 56832; train accuracy: 0.732580\n",
      "epoch 1; batch 56960; loss 0.546275\n",
      "epoch:1; batch 56960; train accuracy: 0.732742\n",
      "epoch 1; batch 57088; loss 0.423878\n",
      "epoch:1; batch 57088; train accuracy: 0.733026\n",
      "epoch 1; batch 57216; loss 0.588160\n",
      "epoch:1; batch 57216; train accuracy: 0.733152\n",
      "epoch 1; batch 57344; loss 0.437628\n",
      "epoch:1; batch 57344; train accuracy: 0.733364\n",
      "epoch 1; batch 57472; loss 0.599959\n",
      "epoch:1; batch 57472; train accuracy: 0.733522\n",
      "epoch 1; batch 57600; loss 0.577738\n",
      "epoch:1; batch 57600; train accuracy: 0.733663\n",
      "epoch 1; batch 57728; loss 0.680445\n",
      "epoch:1; batch 57728; train accuracy: 0.733786\n",
      "epoch 1; batch 57856; loss 0.504059\n",
      "epoch:1; batch 57856; train accuracy: 0.733995\n",
      "epoch 1; batch 57984; loss 0.454240\n",
      "epoch:1; batch 57984; train accuracy: 0.734220\n",
      "epoch 1; batch 58112; loss 0.471300\n",
      "epoch:1; batch 58112; train accuracy: 0.734444\n",
      "epoch 1; batch 58240; loss 0.613302\n",
      "epoch:1; batch 58240; train accuracy: 0.734615\n",
      "epoch 1; batch 58368; loss 0.859545\n",
      "epoch:1; batch 58368; train accuracy: 0.734529\n",
      "epoch 1; batch 58496; loss 0.602726\n",
      "epoch:1; batch 58496; train accuracy: 0.734614\n",
      "epoch 1; batch 58624; loss 0.545247\n",
      "epoch:1; batch 58624; train accuracy: 0.734682\n",
      "epoch 1; batch 58752; loss 0.627931\n",
      "epoch:1; batch 58752; train accuracy: 0.734766\n",
      "epoch 1; batch 58880; loss 0.665191\n",
      "epoch:1; batch 58880; train accuracy: 0.734834\n",
      "epoch 1; batch 59008; loss 0.522461\n",
      "epoch:1; batch 59008; train accuracy: 0.735036\n",
      "epoch 1; batch 59136; loss 0.609864\n",
      "epoch:1; batch 59136; train accuracy: 0.735102\n",
      "epoch 1; batch 59264; loss 0.628440\n",
      "epoch:1; batch 59264; train accuracy: 0.735168\n",
      "epoch 1; batch 59392; loss 0.763015\n",
      "epoch:1; batch 59392; train accuracy: 0.735166\n",
      "epoch 1; batch 59520; loss 0.523811\n",
      "epoch:1; batch 59520; train accuracy: 0.735333\n",
      "epoch 1; batch 59648; loss 0.451134\n",
      "epoch:1; batch 59648; train accuracy: 0.735599\n",
      "epoch 1; batch 59776; loss 0.472699\n",
      "epoch:1; batch 59776; train accuracy: 0.735814\n",
      "epoch 1; batch 59904; loss 0.601102\n",
      "epoch:1; batch 59904; train accuracy: 0.735944\n",
      "epoch 1; batch 60032; loss 0.593405\n",
      "epoch:1; batch 60032; train accuracy: 0.736107\n",
      "epoch 1; batch 60160; loss 0.560083\n",
      "epoch:1; batch 60160; train accuracy: 0.736287\n",
      "epoch 1; batch 60288; loss 0.462860\n",
      "epoch:1; batch 60288; train accuracy: 0.736465\n",
      "epoch 1; batch 60416; loss 0.605491\n",
      "epoch:1; batch 60416; train accuracy: 0.736560\n",
      "epoch 1; batch 60544; loss 0.665151\n",
      "epoch:1; batch 60544; train accuracy: 0.736638\n",
      "epoch 1; batch 60672; loss 0.764638\n",
      "epoch:1; batch 60672; train accuracy: 0.736650\n",
      "epoch 1; batch 60800; loss 0.535655\n",
      "epoch:1; batch 60800; train accuracy: 0.736743\n",
      "epoch 1; batch 60928; loss 0.540205\n",
      "epoch:1; batch 60928; train accuracy: 0.736968\n",
      "epoch 1; batch 61056; loss 0.606789\n",
      "epoch:1; batch 61056; train accuracy: 0.737077\n",
      "epoch 1; batch 61184; loss 0.580468\n",
      "epoch:1; batch 61184; train accuracy: 0.737284\n",
      "epoch 1; batch 61312; loss 0.560856\n",
      "epoch:1; batch 61312; train accuracy: 0.737441\n",
      "epoch 1; batch 61440; loss 0.613103\n",
      "epoch:1; batch 61440; train accuracy: 0.737516\n",
      "epoch 1; batch 61568; loss 0.602996\n",
      "epoch:1; batch 61568; train accuracy: 0.737640\n",
      "epoch 1; batch 61696; loss 0.421357\n",
      "epoch:1; batch 61696; train accuracy: 0.737892\n",
      "epoch 1; batch 61824; loss 0.479180\n",
      "epoch:1; batch 61824; train accuracy: 0.738095\n",
      "epoch 1; batch 61952; loss 0.610056\n",
      "epoch:1; batch 61952; train accuracy: 0.738249\n",
      "epoch 1; batch 62080; loss 0.758483\n",
      "epoch:1; batch 62080; train accuracy: 0.738257\n",
      "epoch 1; batch 62208; loss 0.512364\n",
      "epoch:1; batch 62208; train accuracy: 0.738458\n",
      "epoch 1; batch 62336; loss 0.514634\n",
      "epoch:1; batch 62336; train accuracy: 0.738610\n",
      "epoch 1; batch 62464; loss 0.353694\n",
      "epoch:1; batch 62464; train accuracy: 0.738906\n",
      "epoch 1; batch 62592; loss 0.514313\n",
      "epoch:1; batch 62592; train accuracy: 0.739152\n",
      "epoch 1; batch 62720; loss 0.664155\n",
      "epoch:1; batch 62720; train accuracy: 0.739206\n",
      "epoch 1; batch 62848; loss 0.566853\n",
      "epoch:1; batch 62848; train accuracy: 0.739244\n",
      "epoch 1; batch 62976; loss 0.508948\n",
      "epoch:1; batch 62976; train accuracy: 0.739361\n",
      "epoch 1; batch 63104; loss 0.472863\n",
      "epoch:1; batch 63104; train accuracy: 0.739541\n",
      "epoch 1; batch 63232; loss 0.512601\n",
      "epoch:1; batch 63232; train accuracy: 0.739689\n",
      "epoch 1; batch 63360; loss 0.650818\n",
      "epoch:1; batch 63360; train accuracy: 0.739694\n",
      "epoch 1; batch 63488; loss 0.661825\n",
      "epoch:1; batch 63488; train accuracy: 0.739778\n",
      "epoch 1; batch 63616; loss 0.508499\n",
      "epoch:1; batch 63616; train accuracy: 0.739940\n",
      "epoch 1; batch 63744; loss 0.623327\n",
      "epoch:1; batch 63744; train accuracy: 0.740070\n",
      "epoch 1; batch 63872; loss 0.592458\n",
      "epoch:1; batch 63872; train accuracy: 0.740121\n",
      "epoch 1; batch 64000; loss 0.581422\n",
      "epoch:1; batch 64000; train accuracy: 0.740234\n",
      "epoch 1; batch 64128; loss 0.631681\n",
      "epoch:1; batch 64128; train accuracy: 0.740301\n",
      "epoch 1; batch 64256; loss 0.716759\n",
      "epoch:1; batch 64256; train accuracy: 0.740289\n",
      "epoch 1; batch 64384; loss 0.665721\n",
      "epoch:1; batch 64384; train accuracy: 0.740293\n",
      "epoch 1; batch 64512; loss 0.591793\n",
      "epoch:1; batch 64512; train accuracy: 0.740374\n",
      "epoch 1; batch 64640; loss 0.660349\n",
      "epoch:1; batch 64640; train accuracy: 0.740393\n",
      "epoch 1; batch 64768; loss 0.570125\n",
      "epoch:1; batch 64768; train accuracy: 0.740505\n",
      "epoch 1; batch 64896; loss 0.477677\n",
      "epoch:1; batch 64896; train accuracy: 0.740677\n",
      "epoch 1; batch 65024; loss 0.469677\n",
      "epoch:1; batch 65024; train accuracy: 0.740896\n",
      "epoch 1; batch 65152; loss 0.577701\n",
      "epoch:1; batch 65152; train accuracy: 0.741021\n",
      "epoch 1; batch 65280; loss 0.465289\n",
      "epoch:1; batch 65280; train accuracy: 0.741207\n",
      "epoch 1; batch 65408; loss 0.611781\n",
      "epoch:1; batch 65408; train accuracy: 0.741224\n",
      "epoch 1; batch 65536; loss 0.504087\n",
      "epoch:1; batch 65536; train accuracy: 0.741287\n",
      "epoch 1; batch 65664; loss 0.598820\n",
      "epoch:1; batch 65664; train accuracy: 0.741380\n",
      "epoch 1; batch 65792; loss 0.599504\n",
      "epoch:1; batch 65792; train accuracy: 0.741412\n",
      "epoch 1; batch 65920; loss 0.618455\n",
      "epoch:1; batch 65920; train accuracy: 0.741475\n",
      "epoch 1; batch 66048; loss 0.590534\n",
      "epoch:1; batch 66048; train accuracy: 0.741582\n",
      "epoch 1; batch 66176; loss 0.629822\n",
      "epoch:1; batch 66176; train accuracy: 0.741553\n",
      "epoch 1; batch 66304; loss 0.527560\n",
      "epoch:1; batch 66304; train accuracy: 0.741705\n",
      "epoch 1; batch 66432; loss 0.535120\n",
      "epoch:1; batch 66432; train accuracy: 0.741856\n",
      "epoch 1; batch 66560; loss 0.608969\n",
      "epoch:1; batch 66560; train accuracy: 0.741887\n",
      "epoch 1; batch 66688; loss 0.632317\n",
      "epoch:1; batch 66688; train accuracy: 0.741963\n",
      "epoch 1; batch 66816; loss 0.787877\n",
      "epoch:1; batch 66816; train accuracy: 0.741933\n",
      "epoch 1; batch 66944; loss 0.443476\n",
      "epoch:1; batch 66944; train accuracy: 0.742098\n",
      "epoch 1; batch 67072; loss 0.534137\n",
      "epoch:1; batch 67072; train accuracy: 0.742188\n",
      "epoch 1; batch 67200; loss 0.560634\n",
      "epoch:1; batch 67200; train accuracy: 0.742292\n",
      "epoch 1; batch 67328; loss 0.496224\n",
      "epoch:1; batch 67328; train accuracy: 0.742425\n",
      "epoch 1; batch 67456; loss 0.639548\n",
      "epoch:1; batch 67456; train accuracy: 0.742425\n",
      "epoch 1; batch 67584; loss 0.513415\n",
      "epoch:1; batch 67584; train accuracy: 0.742602\n",
      "epoch 1; batch 67712; loss 0.565020\n",
      "epoch:1; batch 67712; train accuracy: 0.742675\n",
      "epoch 1; batch 67840; loss 0.622596\n",
      "epoch:1; batch 67840; train accuracy: 0.742748\n",
      "epoch 1; batch 67968; loss 0.495140\n",
      "epoch:1; batch 67968; train accuracy: 0.742908\n",
      "epoch 1; batch 68096; loss 0.531447\n",
      "epoch:1; batch 68096; train accuracy: 0.743069\n",
      "epoch 1; batch 68224; loss 0.705951\n",
      "epoch:1; batch 68224; train accuracy: 0.743155\n",
      "epoch 1; batch 68352; loss 0.643060\n",
      "epoch:1; batch 68352; train accuracy: 0.743197\n",
      "epoch 1; batch 68480; loss 0.554482\n",
      "epoch:1; batch 68480; train accuracy: 0.743283\n",
      "epoch 1; batch 68608; loss 0.695515\n",
      "epoch:1; batch 68608; train accuracy: 0.743295\n",
      "epoch 1; batch 68736; loss 0.584239\n",
      "epoch:1; batch 68736; train accuracy: 0.743380\n",
      "epoch 1; batch 68864; loss 0.699841\n",
      "epoch:1; batch 68864; train accuracy: 0.743378\n",
      "epoch 1; batch 68992; loss 0.570627\n",
      "epoch:1; batch 68992; train accuracy: 0.743535\n",
      "epoch 1; batch 69120; loss 0.581760\n",
      "epoch:1; batch 69120; train accuracy: 0.743562\n",
      "epoch 1; batch 69248; loss 0.539655\n",
      "epoch:1; batch 69248; train accuracy: 0.743704\n",
      "epoch 1; batch 69376; loss 0.557369\n",
      "epoch:1; batch 69376; train accuracy: 0.743860\n",
      "epoch 1; batch 69504; loss 0.507121\n",
      "epoch:1; batch 69504; train accuracy: 0.744029\n",
      "epoch 1; batch 69632; loss 0.658640\n",
      "epoch:1; batch 69632; train accuracy: 0.744069\n",
      "epoch 1; batch 69760; loss 0.544684\n",
      "epoch:1; batch 69760; train accuracy: 0.744209\n",
      "epoch 1; batch 69888; loss 0.545467\n",
      "epoch:1; batch 69888; train accuracy: 0.744319\n",
      "epoch 1; batch 70016; loss 0.536832\n",
      "epoch:1; batch 70016; train accuracy: 0.744458\n",
      "epoch 1; batch 70144; loss 0.579563\n",
      "epoch:1; batch 70144; train accuracy: 0.744568\n",
      "epoch 1; batch 70272; loss 0.472530\n",
      "epoch:1; batch 70272; train accuracy: 0.744721\n",
      "epoch 1; batch 70400; loss 0.553849\n",
      "epoch:1; batch 70400; train accuracy: 0.744787\n",
      "epoch 1; batch 70528; loss 0.715833\n",
      "epoch:1; batch 70528; train accuracy: 0.744825\n",
      "epoch 1; batch 70656; loss 0.490513\n",
      "epoch:1; batch 70656; train accuracy: 0.744905\n",
      "epoch 1; batch 70784; loss 0.602314\n",
      "epoch:1; batch 70784; train accuracy: 0.744985\n",
      "epoch 1; batch 70912; loss 0.525525\n",
      "epoch:1; batch 70912; train accuracy: 0.745078\n",
      "epoch 1; batch 71040; loss 0.666162\n",
      "epoch:1; batch 71040; train accuracy: 0.745144\n",
      "epoch 1; batch 71168; loss 0.825292\n",
      "epoch:1; batch 71168; train accuracy: 0.745096\n",
      "epoch 1; batch 71296; loss 0.480170\n",
      "epoch:1; batch 71296; train accuracy: 0.745273\n",
      "epoch 1; batch 71424; loss 0.613246\n",
      "epoch:1; batch 71424; train accuracy: 0.745324\n",
      "epoch 1; batch 71552; loss 0.575988\n",
      "epoch:1; batch 71552; train accuracy: 0.745430\n",
      "epoch 1; batch 71680; loss 0.534948\n",
      "epoch:1; batch 71680; train accuracy: 0.745536\n",
      "epoch 1; batch 71808; loss 0.556463\n",
      "epoch:1; batch 71808; train accuracy: 0.745599\n",
      "epoch 1; batch 71936; loss 0.622855\n",
      "epoch:1; batch 71936; train accuracy: 0.745691\n",
      "epoch 1; batch 72064; loss 0.706422\n",
      "epoch:1; batch 72064; train accuracy: 0.745670\n",
      "epoch 1; batch 72192; loss 0.592679\n",
      "epoch:1; batch 72192; train accuracy: 0.745817\n",
      "epoch 1; batch 72320; loss 0.573372\n",
      "epoch:1; batch 72320; train accuracy: 0.745838\n",
      "epoch 1; batch 72448; loss 0.487114\n",
      "epoch:1; batch 72448; train accuracy: 0.745997\n",
      "epoch 1; batch 72576; loss 0.684425\n",
      "epoch:1; batch 72576; train accuracy: 0.746059\n",
      "epoch 1; batch 72704; loss 0.600503\n",
      "epoch:1; batch 72704; train accuracy: 0.746066\n",
      "epoch 1; batch 72832; loss 0.600356\n",
      "epoch:1; batch 72832; train accuracy: 0.746142\n",
      "epoch 1; batch 72960; loss 0.706261\n",
      "epoch:1; batch 72960; train accuracy: 0.746162\n",
      "epoch 1; batch 73088; loss 0.529763\n",
      "epoch:1; batch 73088; train accuracy: 0.746292\n",
      "epoch 1; batch 73216; loss 0.566084\n",
      "epoch:1; batch 73216; train accuracy: 0.746367\n",
      "epoch 1; batch 73344; loss 0.710702\n",
      "epoch:1; batch 73344; train accuracy: 0.746319\n",
      "epoch 1; batch 73472; loss 0.596288\n",
      "epoch:1; batch 73472; train accuracy: 0.746448\n",
      "epoch 1; batch 73600; loss 0.534951\n",
      "epoch:1; batch 73600; train accuracy: 0.746535\n",
      "epoch 1; batch 73728; loss 0.599950\n",
      "epoch:1; batch 73728; train accuracy: 0.746555\n",
      "epoch 1; batch 73856; loss 0.674054\n",
      "epoch:1; batch 73856; train accuracy: 0.746574\n",
      "epoch 1; batch 73984; loss 0.554853\n",
      "epoch:1; batch 73984; train accuracy: 0.746688\n",
      "epoch 1; batch 74112; loss 0.438962\n",
      "epoch:1; batch 74112; train accuracy: 0.746870\n",
      "epoch 1; batch 74240; loss 0.420391\n",
      "epoch:1; batch 74240; train accuracy: 0.747010\n",
      "epoch 1; batch 74368; loss 0.413371\n",
      "epoch:1; batch 74368; train accuracy: 0.747217\n",
      "epoch 1; batch 74496; loss 0.645005\n",
      "epoch:1; batch 74496; train accuracy: 0.747262\n",
      "epoch 1; batch 74624; loss 0.550089\n",
      "epoch:1; batch 74624; train accuracy: 0.747360\n",
      "epoch 1; batch 74752; loss 0.536413\n",
      "epoch:1; batch 74752; train accuracy: 0.747498\n",
      "epoch 1; batch 74880; loss 0.555961\n",
      "epoch:1; batch 74880; train accuracy: 0.747556\n",
      "epoch 1; batch 75008; loss 0.593379\n",
      "epoch:1; batch 75008; train accuracy: 0.747707\n",
      "epoch 1; batch 75136; loss 0.456518\n",
      "epoch:1; batch 75136; train accuracy: 0.747804\n",
      "epoch 1; batch 75264; loss 0.540751\n",
      "epoch:1; batch 75264; train accuracy: 0.747861\n",
      "epoch 1; batch 75392; loss 0.530233\n",
      "epoch:1; batch 75392; train accuracy: 0.748024\n",
      "epoch 1; batch 75520; loss 0.414145\n",
      "epoch:1; batch 75520; train accuracy: 0.748199\n",
      "epoch 1; batch 75648; loss 0.678118\n",
      "epoch:1; batch 75648; train accuracy: 0.748202\n",
      "epoch 1; batch 75776; loss 0.594886\n",
      "epoch:1; batch 75776; train accuracy: 0.748298\n",
      "epoch 1; batch 75904; loss 0.612863\n",
      "epoch:1; batch 75904; train accuracy: 0.748340\n",
      "epoch 1; batch 76032; loss 0.570560\n",
      "epoch:1; batch 76032; train accuracy: 0.748395\n",
      "epoch 1; batch 76160; loss 0.645430\n",
      "epoch:1; batch 76160; train accuracy: 0.748503\n",
      "epoch 1; batch 76288; loss 0.506182\n",
      "epoch:1; batch 76288; train accuracy: 0.748676\n",
      "epoch 1; batch 76416; loss 0.522411\n",
      "epoch:1; batch 76416; train accuracy: 0.748770\n",
      "epoch 1; batch 76544; loss 0.472043\n",
      "epoch:1; batch 76544; train accuracy: 0.748876\n",
      "epoch 1; batch 76672; loss 0.529974\n",
      "epoch:1; batch 76672; train accuracy: 0.748983\n",
      "epoch 1; batch 76800; loss 0.503618\n",
      "epoch:1; batch 76800; train accuracy: 0.749128\n",
      "epoch 1; batch 76928; loss 0.490293\n",
      "epoch:1; batch 76928; train accuracy: 0.749272\n",
      "epoch 1; batch 77056; loss 0.688460\n",
      "epoch:1; batch 77056; train accuracy: 0.749221\n",
      "epoch 1; batch 77184; loss 0.534809\n",
      "epoch:1; batch 77184; train accuracy: 0.749300\n",
      "epoch 1; batch 77312; loss 0.497674\n",
      "epoch:1; batch 77312; train accuracy: 0.749392\n",
      "epoch 1; batch 77440; loss 0.600299\n",
      "epoch:1; batch 77440; train accuracy: 0.749458\n",
      "epoch 1; batch 77568; loss 0.598226\n",
      "epoch:1; batch 77568; train accuracy: 0.749471\n",
      "epoch 1; batch 77696; loss 0.543404\n",
      "epoch:1; batch 77696; train accuracy: 0.749627\n",
      "epoch 1; batch 77824; loss 0.544553\n",
      "epoch:1; batch 77824; train accuracy: 0.749704\n",
      "epoch 1; batch 77952; loss 0.489062\n",
      "epoch:1; batch 77952; train accuracy: 0.749795\n",
      "epoch 1; batch 78080; loss 0.495365\n",
      "epoch:1; batch 78080; train accuracy: 0.749898\n",
      "epoch 1; batch 78208; loss 0.577949\n",
      "epoch:1; batch 78208; train accuracy: 0.749974\n",
      "epoch 1; batch 78336; loss 0.513848\n",
      "epoch:1; batch 78336; train accuracy: 0.750089\n",
      "epoch 1; batch 78464; loss 0.710659\n",
      "epoch:1; batch 78464; train accuracy: 0.750038\n",
      "epoch 1; batch 78592; loss 0.420649\n",
      "epoch:1; batch 78592; train accuracy: 0.750191\n",
      "epoch 1; batch 78720; loss 0.582225\n",
      "epoch:1; batch 78720; train accuracy: 0.750229\n",
      "epoch 1; batch 78848; loss 0.540377\n",
      "epoch:1; batch 78848; train accuracy: 0.750355\n",
      "epoch 1; batch 78976; loss 0.643726\n",
      "epoch:1; batch 78976; train accuracy: 0.750392\n",
      "epoch 1; batch 79104; loss 0.372847\n",
      "epoch:1; batch 79104; train accuracy: 0.750607\n",
      "epoch 1; batch 79232; loss 0.606355\n",
      "epoch:1; batch 79232; train accuracy: 0.750669\n",
      "epoch 1; batch 79360; loss 0.599566\n",
      "epoch:1; batch 79360; train accuracy: 0.750706\n",
      "epoch 1; batch 79488; loss 0.696501\n",
      "epoch:1; batch 79488; train accuracy: 0.750692\n",
      "epoch 1; batch 79616; loss 0.680153\n",
      "epoch:1; batch 79616; train accuracy: 0.750615\n",
      "epoch 1; batch 79744; loss 0.691518\n",
      "epoch:1; batch 79744; train accuracy: 0.750589\n",
      "epoch 1; batch 79872; loss 0.647155\n",
      "epoch:1; batch 79872; train accuracy: 0.750588\n",
      "epoch 1; batch 80000; loss 0.573023\n",
      "epoch:1; batch 80000; train accuracy: 0.750713\n",
      "epoch 1; batch 80128; loss 0.613547\n",
      "epoch:1; batch 80128; train accuracy: 0.750786\n",
      "epoch 1; batch 80256; loss 0.556154\n",
      "epoch:1; batch 80256; train accuracy: 0.750947\n",
      "epoch 1; batch 80384; loss 0.614132\n",
      "epoch:1; batch 80384; train accuracy: 0.751020\n",
      "epoch 1; batch 80512; loss 0.647520\n",
      "epoch:1; batch 80512; train accuracy: 0.751056\n",
      "epoch 1; batch 80640; loss 0.594627\n",
      "epoch:1; batch 80640; train accuracy: 0.751178\n",
      "epoch 1; batch 80768; loss 0.398716\n",
      "epoch:1; batch 80768; train accuracy: 0.751374\n",
      "epoch 1; batch 80896; loss 0.556075\n",
      "epoch:1; batch 80896; train accuracy: 0.751471\n",
      "epoch 1; batch 81024; loss 0.632782\n",
      "epoch:1; batch 81024; train accuracy: 0.751580\n",
      "epoch 1; batch 81152; loss 0.589825\n",
      "epoch:1; batch 81152; train accuracy: 0.751590\n",
      "epoch 1; batch 81280; loss 0.558259\n",
      "epoch:1; batch 81280; train accuracy: 0.751649\n",
      "epoch 1; batch 81408; loss 0.545473\n",
      "epoch:1; batch 81408; train accuracy: 0.751707\n",
      "epoch 1; batch 81536; loss 0.527331\n",
      "epoch:1; batch 81536; train accuracy: 0.751754\n",
      "epoch 1; batch 81664; loss 0.550299\n",
      "epoch:1; batch 81664; train accuracy: 0.751849\n",
      "epoch 1; batch 81792; loss 0.723927\n",
      "epoch:1; batch 81792; train accuracy: 0.751834\n",
      "epoch 1; batch 81920; loss 0.536111\n",
      "epoch:1; batch 81920; train accuracy: 0.751880\n",
      "epoch 1; batch 82048; loss 0.642539\n",
      "epoch:1; batch 82048; train accuracy: 0.751950\n",
      "epoch 1; batch 82176; loss 0.540049\n",
      "epoch:1; batch 82176; train accuracy: 0.752044\n",
      "epoch 1; batch 82304; loss 0.456728\n",
      "epoch:1; batch 82304; train accuracy: 0.752126\n",
      "epoch 1; batch 82432; loss 0.598766\n",
      "epoch:1; batch 82432; train accuracy: 0.752172\n",
      "epoch 1; batch 82560; loss 0.468883\n",
      "epoch:1; batch 82560; train accuracy: 0.752338\n",
      "epoch 1; batch 82688; loss 0.493988\n",
      "epoch:1; batch 82688; train accuracy: 0.752491\n",
      "epoch 1; batch 82816; loss 0.574330\n",
      "epoch:1; batch 82816; train accuracy: 0.752572\n",
      "epoch 1; batch 82944; loss 0.510477\n",
      "epoch:1; batch 82944; train accuracy: 0.752652\n",
      "epoch 1; batch 83072; loss 0.463940\n",
      "epoch:1; batch 83072; train accuracy: 0.752769\n",
      "epoch 1; batch 83200; loss 0.486307\n",
      "epoch:1; batch 83200; train accuracy: 0.752885\n",
      "epoch 1; batch 83328; loss 0.566428\n",
      "epoch:1; batch 83328; train accuracy: 0.752988\n",
      "epoch 1; batch 83456; loss 0.470976\n",
      "epoch:1; batch 83456; train accuracy: 0.753067\n",
      "epoch 1; batch 83584; loss 0.478460\n",
      "epoch:1; batch 83584; train accuracy: 0.753159\n",
      "epoch 1; batch 83712; loss 0.574136\n",
      "epoch:1; batch 83712; train accuracy: 0.753225\n",
      "epoch 1; batch 83840; loss 0.463879\n",
      "epoch:1; batch 83840; train accuracy: 0.753340\n",
      "epoch 1; batch 83968; loss 0.629322\n",
      "epoch:1; batch 83968; train accuracy: 0.753382\n",
      "epoch 1; batch 84096; loss 0.522496\n",
      "epoch:1; batch 84096; train accuracy: 0.753484\n",
      "epoch 1; batch 84224; loss 0.571117\n",
      "epoch:1; batch 84224; train accuracy: 0.753550\n",
      "epoch 1; batch 84352; loss 0.547434\n",
      "epoch:1; batch 84352; train accuracy: 0.753663\n",
      "epoch 1; batch 84480; loss 0.591292\n",
      "epoch:1; batch 84480; train accuracy: 0.753776\n",
      "epoch 1; batch 84608; loss 0.713107\n",
      "epoch:1; batch 84608; train accuracy: 0.753829\n",
      "epoch 1; batch 84736; loss 0.751900\n",
      "epoch:1; batch 84736; train accuracy: 0.753765\n",
      "epoch 1; batch 84864; loss 0.616864\n",
      "epoch:1; batch 84864; train accuracy: 0.753783\n",
      "epoch 1; batch 84992; loss 0.579855\n",
      "epoch:1; batch 84992; train accuracy: 0.753859\n",
      "epoch 1; batch 85120; loss 0.605980\n",
      "epoch:1; batch 85120; train accuracy: 0.753889\n",
      "epoch 1; batch 85248; loss 0.649014\n",
      "epoch:1; batch 85248; train accuracy: 0.753930\n",
      "epoch 1; batch 85376; loss 0.564298\n",
      "epoch:1; batch 85376; train accuracy: 0.754041\n",
      "epoch 1; batch 85504; loss 0.591013\n",
      "epoch:1; batch 85504; train accuracy: 0.754140\n",
      "epoch 1; batch 85632; loss 0.512067\n",
      "epoch:1; batch 85632; train accuracy: 0.754262\n",
      "epoch 1; batch 85760; loss 0.543431\n",
      "epoch:1; batch 85760; train accuracy: 0.754326\n",
      "epoch 1; batch 85888; loss 0.457147\n",
      "epoch:1; batch 85888; train accuracy: 0.754448\n",
      "epoch 1; batch 86016; loss 0.506296\n",
      "epoch:1; batch 86016; train accuracy: 0.754499\n",
      "epoch 1; batch 86144; loss 0.631444\n",
      "epoch:1; batch 86144; train accuracy: 0.754527\n",
      "epoch 1; batch 86272; loss 0.617922\n",
      "epoch:1; batch 86272; train accuracy: 0.754602\n",
      "epoch 1; batch 86400; loss 0.484818\n",
      "epoch:1; batch 86400; train accuracy: 0.754722\n",
      "epoch 1; batch 86528; loss 0.587695\n",
      "epoch:1; batch 86528; train accuracy: 0.754785\n",
      "epoch 1; batch 86656; loss 0.562961\n",
      "epoch:1; batch 86656; train accuracy: 0.754870\n",
      "epoch 1; batch 86784; loss 0.572483\n",
      "epoch:1; batch 86784; train accuracy: 0.754943\n",
      "epoch 1; batch 86912; loss 0.606664\n",
      "epoch:1; batch 86912; train accuracy: 0.754982\n",
      "epoch 1; batch 87040; loss 0.516338\n",
      "epoch:1; batch 87040; train accuracy: 0.755090\n",
      "epoch 1; batch 87168; loss 0.507654\n",
      "epoch:1; batch 87168; train accuracy: 0.755162\n",
      "epoch 1; batch 87296; loss 0.568793\n",
      "epoch:1; batch 87296; train accuracy: 0.755224\n",
      "epoch 1; batch 87424; loss 0.580349\n",
      "epoch:1; batch 87424; train accuracy: 0.755307\n",
      "epoch 1; batch 87552; loss 0.545209\n",
      "epoch:1; batch 87552; train accuracy: 0.755357\n",
      "epoch 1; batch 87680; loss 0.740327\n",
      "epoch:1; batch 87680; train accuracy: 0.755326\n",
      "epoch 1; batch 87808; loss 0.677446\n",
      "epoch:1; batch 87808; train accuracy: 0.755341\n",
      "epoch 1; batch 87936; loss 0.464376\n",
      "epoch:1; batch 87936; train accuracy: 0.755459\n",
      "epoch 1; batch 88064; loss 0.572508\n",
      "epoch:1; batch 88064; train accuracy: 0.755462\n",
      "epoch 1; batch 88192; loss 0.607459\n",
      "epoch:1; batch 88192; train accuracy: 0.755522\n",
      "epoch 1; batch 88320; loss 0.532527\n",
      "epoch:1; batch 88320; train accuracy: 0.755627\n",
      "epoch 1; batch 88448; loss 0.476668\n",
      "epoch:1; batch 88448; train accuracy: 0.755710\n",
      "epoch 1; batch 88576; loss 0.561729\n",
      "epoch:1; batch 88576; train accuracy: 0.755803\n",
      "epoch 1; batch 88704; loss 0.589139\n",
      "epoch:1; batch 88704; train accuracy: 0.755840\n",
      "epoch 1; batch 88832; loss 0.680262\n",
      "epoch:1; batch 88832; train accuracy: 0.755854\n",
      "epoch 1; batch 88960; loss 0.617002\n",
      "epoch:1; batch 88960; train accuracy: 0.755902\n",
      "epoch 1; batch 89088; loss 0.511503\n",
      "epoch:1; batch 89088; train accuracy: 0.756005\n",
      "epoch 1; batch 89216; loss 0.649462\n",
      "epoch:1; batch 89216; train accuracy: 0.756086\n",
      "epoch 1; batch 89344; loss 0.507181\n",
      "epoch:1; batch 89344; train accuracy: 0.756223\n",
      "epoch 1; batch 89472; loss 0.535306\n",
      "epoch:1; batch 89472; train accuracy: 0.756259\n",
      "epoch 1; batch 89600; loss 0.547340\n",
      "epoch:1; batch 89600; train accuracy: 0.756306\n",
      "epoch 1; batch 89728; loss 0.393452\n",
      "epoch:1; batch 89728; train accuracy: 0.756464\n",
      "epoch 1; batch 89856; loss 0.495189\n",
      "epoch:1; batch 89856; train accuracy: 0.756544\n",
      "epoch 1; batch 89984; loss 0.514411\n",
      "epoch:1; batch 89984; train accuracy: 0.756568\n",
      "epoch 1; batch 90112; loss 0.647316\n",
      "epoch:1; batch 90112; train accuracy: 0.756603\n",
      "epoch 1; batch 90240; loss 0.585269\n",
      "epoch:1; batch 90240; train accuracy: 0.756649\n",
      "epoch 1; batch 90368; loss 0.626340\n",
      "epoch:1; batch 90368; train accuracy: 0.756662\n",
      "epoch 1; batch 90496; loss 0.690169\n",
      "epoch:1; batch 90496; train accuracy: 0.756630\n",
      "epoch 1; batch 90624; loss 0.583537\n",
      "epoch:1; batch 90624; train accuracy: 0.756676\n",
      "epoch 1; batch 90752; loss 0.578797\n",
      "epoch:1; batch 90752; train accuracy: 0.756711\n",
      "epoch 1; batch 90880; loss 0.515943\n",
      "epoch:1; batch 90880; train accuracy: 0.756844\n",
      "epoch 1; batch 91008; loss 0.580992\n",
      "epoch:1; batch 91008; train accuracy: 0.756878\n",
      "epoch 1; batch 91136; loss 0.481191\n",
      "epoch:1; batch 91136; train accuracy: 0.756957\n",
      "epoch 1; batch 91264; loss 0.553392\n",
      "epoch:1; batch 91264; train accuracy: 0.757024\n",
      "epoch 1; batch 91392; loss 0.391114\n",
      "epoch:1; batch 91392; train accuracy: 0.757178\n",
      "epoch 1; batch 91520; loss 0.573407\n",
      "epoch:1; batch 91520; train accuracy: 0.757179\n",
      "epoch 1; batch 91648; loss 0.544823\n",
      "epoch:1; batch 91648; train accuracy: 0.757300\n",
      "epoch 1; batch 91776; loss 0.480441\n",
      "epoch:1; batch 91776; train accuracy: 0.757388\n",
      "epoch 1; batch 91904; loss 0.639276\n",
      "epoch:1; batch 91904; train accuracy: 0.757399\n",
      "epoch 1; batch 92032; loss 0.433132\n",
      "epoch:1; batch 92032; train accuracy: 0.757530\n",
      "epoch 1; batch 92160; loss 0.582554\n",
      "epoch:1; batch 92160; train accuracy: 0.757595\n",
      "epoch 1; batch 92288; loss 0.529079\n",
      "epoch:1; batch 92288; train accuracy: 0.757661\n",
      "epoch 1; batch 92416; loss 0.529191\n",
      "epoch:1; batch 92416; train accuracy: 0.757748\n",
      "epoch 1; batch 92544; loss 0.464844\n",
      "epoch:1; batch 92544; train accuracy: 0.757888\n",
      "epoch 1; batch 92672; loss 0.465590\n",
      "epoch:1; batch 92672; train accuracy: 0.758007\n",
      "epoch 1; batch 92800; loss 0.604289\n",
      "epoch:1; batch 92800; train accuracy: 0.758071\n",
      "epoch 1; batch 92928; loss 0.527237\n",
      "epoch:1; batch 92928; train accuracy: 0.758092\n",
      "epoch 1; batch 93056; loss 0.721522\n",
      "epoch:1; batch 93056; train accuracy: 0.758081\n",
      "epoch 1; batch 93184; loss 0.625835\n",
      "epoch:1; batch 93184; train accuracy: 0.758145\n",
      "epoch 1; batch 93312; loss 0.696121\n",
      "epoch:1; batch 93312; train accuracy: 0.758145\n",
      "epoch 1; batch 93440; loss 0.559187\n",
      "epoch:1; batch 93440; train accuracy: 0.758166\n",
      "epoch 1; batch 93568; loss 0.587474\n",
      "epoch:1; batch 93568; train accuracy: 0.758229\n",
      "epoch 1; batch 93696; loss 0.457293\n",
      "epoch:1; batch 93696; train accuracy: 0.758335\n",
      "epoch 1; batch 93824; loss 0.491951\n",
      "epoch:1; batch 93824; train accuracy: 0.758409\n",
      "epoch 1; batch 93952; loss 0.533589\n",
      "epoch:1; batch 93952; train accuracy: 0.758483\n",
      "epoch 1; batch 94080; loss 0.495848\n",
      "epoch:1; batch 94080; train accuracy: 0.758599\n",
      "epoch 1; batch 94208; loss 0.569311\n",
      "epoch:1; batch 94208; train accuracy: 0.758662\n",
      "epoch 1; batch 94336; loss 0.485945\n",
      "epoch:1; batch 94336; train accuracy: 0.758777\n",
      "epoch 1; batch 94464; loss 0.618003\n",
      "epoch:1; batch 94464; train accuracy: 0.758744\n",
      "epoch 1; batch 94592; loss 0.604617\n",
      "epoch:1; batch 94592; train accuracy: 0.758732\n",
      "epoch 1; batch 94720; loss 0.511487\n",
      "epoch:1; batch 94720; train accuracy: 0.758815\n",
      "epoch 1; batch 94848; loss 0.446541\n",
      "epoch:1; batch 94848; train accuracy: 0.758930\n",
      "epoch 1; batch 94976; loss 0.571158\n",
      "epoch:1; batch 94976; train accuracy: 0.758971\n",
      "epoch 1; batch 95104; loss 0.717838\n",
      "epoch:1; batch 95104; train accuracy: 0.758896\n",
      "epoch 1; batch 95232; loss 0.546198\n",
      "epoch:1; batch 95232; train accuracy: 0.758957\n",
      "epoch 1; batch 95360; loss 0.604263\n",
      "epoch:1; batch 95360; train accuracy: 0.759008\n",
      "epoch 1; batch 95488; loss 0.529851\n",
      "epoch:1; batch 95488; train accuracy: 0.759069\n",
      "epoch 1; batch 95616; loss 0.445970\n",
      "epoch:1; batch 95616; train accuracy: 0.759193\n",
      "epoch 1; batch 95744; loss 0.560918\n",
      "epoch:1; batch 95744; train accuracy: 0.759243\n",
      "epoch 1; batch 95872; loss 0.635499\n",
      "epoch:1; batch 95872; train accuracy: 0.759252\n",
      "epoch 1; batch 96000; loss 0.606875\n",
      "epoch:1; batch 96000; train accuracy: 0.759260\n",
      "epoch 1; batch 96128; loss 0.448110\n",
      "epoch:1; batch 96128; train accuracy: 0.759352\n",
      "epoch 1; batch 96256; loss 0.627608\n",
      "epoch:1; batch 96256; train accuracy: 0.759381\n",
      "epoch 1; batch 96384; loss 0.482664\n",
      "epoch:1; batch 96384; train accuracy: 0.759473\n",
      "epoch 1; batch 96512; loss 0.716194\n",
      "epoch:1; batch 96512; train accuracy: 0.759470\n",
      "epoch 1; batch 96640; loss 0.456174\n",
      "epoch:1; batch 96640; train accuracy: 0.759592\n",
      "epoch 1; batch 96768; loss 0.362648\n",
      "epoch:1; batch 96768; train accuracy: 0.759776\n",
      "epoch 1; batch 96896; loss 0.615271\n",
      "epoch:1; batch 96896; train accuracy: 0.759835\n",
      "epoch 1; batch 97024; loss 0.569337\n",
      "epoch:1; batch 97024; train accuracy: 0.759884\n",
      "epoch 1; batch 97152; loss 0.488048\n",
      "epoch:1; batch 97152; train accuracy: 0.759933\n",
      "epoch 1; batch 97280; loss 0.645342\n",
      "epoch:1; batch 97280; train accuracy: 0.759940\n",
      "epoch 1; batch 97408; loss 0.590207\n",
      "epoch:1; batch 97408; train accuracy: 0.759979\n",
      "epoch 1; batch 97536; loss 0.675960\n",
      "epoch:1; batch 97536; train accuracy: 0.759976\n",
      "epoch 1; batch 97664; loss 0.567938\n",
      "epoch:1; batch 97664; train accuracy: 0.760014\n",
      "epoch 1; batch 97792; loss 0.512473\n",
      "epoch:1; batch 97792; train accuracy: 0.760113\n",
      "epoch 1; batch 97920; loss 0.722582\n",
      "epoch:1; batch 97920; train accuracy: 0.760090\n",
      "epoch 1; batch 98048; loss 0.530407\n",
      "epoch:1; batch 98048; train accuracy: 0.760158\n",
      "epoch 1; batch 98176; loss 0.666982\n",
      "epoch:1; batch 98176; train accuracy: 0.760186\n",
      "epoch 1; batch 98304; loss 0.588982\n",
      "epoch:1; batch 98304; train accuracy: 0.760193\n",
      "epoch 1; batch 98432; loss 0.473720\n",
      "epoch:1; batch 98432; train accuracy: 0.760281\n",
      "epoch 1; batch 98560; loss 0.632752\n",
      "epoch:1; batch 98560; train accuracy: 0.760288\n",
      "epoch 1; batch 98688; loss 0.439724\n",
      "epoch:1; batch 98688; train accuracy: 0.760396\n",
      "epoch 1; batch 98816; loss 0.425018\n",
      "epoch:1; batch 98816; train accuracy: 0.760504\n",
      "epoch 1; batch 98944; loss 0.515274\n",
      "epoch:1; batch 98944; train accuracy: 0.760572\n",
      "epoch 1; batch 99072; loss 0.447011\n",
      "epoch:1; batch 99072; train accuracy: 0.760679\n",
      "epoch 1; batch 99200; loss 0.422942\n",
      "epoch:1; batch 99200; train accuracy: 0.760806\n",
      "epoch 1; batch 99328; loss 0.906724\n",
      "epoch:1; batch 99328; train accuracy: 0.760722\n",
      "epoch 1; batch 99456; loss 0.520719\n",
      "epoch:1; batch 99456; train accuracy: 0.760819\n",
      "epoch 1; batch 99584; loss 0.533559\n",
      "epoch:1; batch 99584; train accuracy: 0.760895\n",
      "epoch 1; batch 99712; loss 0.461783\n",
      "epoch:1; batch 99712; train accuracy: 0.760992\n",
      "epoch 1; batch 99840; loss 0.558300\n",
      "epoch:1; batch 99840; train accuracy: 0.761058\n",
      "epoch 1; batch 99968; loss 0.572121\n",
      "epoch:1; batch 99968; train accuracy: 0.761094\n",
      "epoch 1; batch 100096; loss 0.516064\n",
      "epoch:1; batch 100096; train accuracy: 0.761139\n",
      "epoch 1; batch 100224; loss 0.473190\n",
      "epoch:1; batch 100224; train accuracy: 0.761245\n",
      "epoch 1; batch 100352; loss 0.603020\n",
      "epoch:1; batch 100352; train accuracy: 0.761260\n",
      "epoch 1; batch 100480; loss 0.461930\n",
      "epoch:1; batch 100480; train accuracy: 0.761365\n",
      "epoch 1; batch 100608; loss 0.502859\n",
      "epoch:1; batch 100608; train accuracy: 0.761431\n",
      "epoch 1; batch 100736; loss 0.661892\n",
      "epoch:1; batch 100736; train accuracy: 0.761426\n",
      "epoch 1; batch 100864; loss 0.650789\n",
      "epoch:1; batch 100864; train accuracy: 0.761401\n",
      "epoch 1; batch 100992; loss 0.506308\n",
      "epoch:1; batch 100992; train accuracy: 0.761486\n",
      "epoch 1; batch 101120; loss 0.546309\n",
      "epoch:1; batch 101120; train accuracy: 0.761541\n",
      "epoch 1; batch 101248; loss 0.522573\n",
      "epoch:1; batch 101248; train accuracy: 0.761615\n",
      "epoch 1; batch 101376; loss 0.523014\n",
      "epoch:1; batch 101376; train accuracy: 0.761669\n",
      "epoch 1; batch 101504; loss 0.592161\n",
      "epoch:1; batch 101504; train accuracy: 0.761694\n",
      "epoch 1; batch 101632; loss 0.489609\n",
      "epoch:1; batch 101632; train accuracy: 0.761738\n",
      "epoch 1; batch 101760; loss 0.696463\n",
      "epoch:1; batch 101760; train accuracy: 0.761724\n",
      "epoch 1; batch 101888; loss 0.546119\n",
      "epoch:1; batch 101888; train accuracy: 0.761787\n",
      "epoch 1; batch 102016; loss 0.566074\n",
      "epoch:1; batch 102016; train accuracy: 0.761841\n",
      "epoch 1; batch 102144; loss 0.569313\n",
      "epoch:1; batch 102144; train accuracy: 0.761905\n",
      "epoch 1; batch 102272; loss 0.461208\n",
      "epoch:1; batch 102272; train accuracy: 0.761997\n",
      "epoch 1; batch 102400; loss 0.525945\n",
      "epoch:1; batch 102400; train accuracy: 0.762021\n",
      "epoch 1; batch 102528; loss 0.629550\n",
      "epoch:1; batch 102528; train accuracy: 0.762055\n",
      "epoch 1; batch 102656; loss 0.583985\n",
      "epoch:1; batch 102656; train accuracy: 0.762079\n",
      "epoch 1; batch 102784; loss 0.532857\n",
      "epoch:1; batch 102784; train accuracy: 0.762093\n",
      "epoch 1; batch 102912; loss 0.508819\n",
      "epoch:1; batch 102912; train accuracy: 0.762185\n",
      "epoch 1; batch 103040; loss 0.536130\n",
      "epoch:1; batch 103040; train accuracy: 0.762257\n",
      "epoch 1; batch 103168; loss 0.489464\n",
      "epoch:1; batch 103168; train accuracy: 0.762320\n",
      "epoch 1; batch 103296; loss 0.617394\n",
      "epoch:1; batch 103296; train accuracy: 0.762343\n",
      "epoch 1; batch 103424; loss 0.539790\n",
      "epoch:1; batch 103424; train accuracy: 0.762396\n",
      "epoch 1; batch 103552; loss 0.533760\n",
      "epoch:1; batch 103552; train accuracy: 0.762467\n",
      "epoch 1; batch 103680; loss 0.561779\n",
      "epoch:1; batch 103680; train accuracy: 0.762539\n",
      "epoch 1; batch 103808; loss 0.517499\n",
      "epoch:1; batch 103808; train accuracy: 0.762581\n",
      "epoch 1; batch 103936; loss 0.556744\n",
      "epoch:1; batch 103936; train accuracy: 0.762662\n",
      "epoch 1; batch 104064; loss 0.608673\n",
      "epoch:1; batch 104064; train accuracy: 0.762675\n",
      "epoch 1; batch 104192; loss 0.524026\n",
      "epoch:1; batch 104192; train accuracy: 0.762726\n",
      "epoch 1; batch 104320; loss 0.505057\n",
      "epoch:1; batch 104320; train accuracy: 0.762749\n",
      "epoch 1; batch 104448; loss 0.841901\n",
      "epoch:1; batch 104448; train accuracy: 0.762647\n",
      "epoch 1; batch 104576; loss 0.617351\n",
      "epoch:1; batch 104576; train accuracy: 0.762680\n",
      "epoch 1; batch 104704; loss 0.600540\n",
      "epoch:1; batch 104704; train accuracy: 0.762693\n",
      "epoch 1; batch 104832; loss 0.497553\n",
      "epoch:1; batch 104832; train accuracy: 0.762763\n",
      "epoch 1; batch 104960; loss 0.554217\n",
      "epoch:1; batch 104960; train accuracy: 0.762805\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 37148 ; rate: 0.353925\n",
      "y_true_label_1_num: 8749 ; rate: 0.083356\n",
      "y_true_label_2_num: 10202 ; rate: 0.097199\n",
      "y_true_label_3_num: 48861 ; rate: 0.465520\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.864072\n",
      "valid avg_precision: 0.904107\n",
      "valid avg_recall: 0.802868\n",
      "valid avg_f1: 0.824856\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 5395 ; rate: 0.360243\n",
      "y_true_label_1_num: 1213 ; rate: 0.080996\n",
      "y_true_label_2_num: 1415 ; rate: 0.094485\n",
      "y_true_label_3_num: 6953 ; rate: 0.464276\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.807492\n",
      "valid avg_precision: 0.850224\n",
      "valid avg_recall: 0.740785\n",
      "valid avg_f1: 0.766896\n",
      "epoch 2\n",
      "epoch 2; batch 128; loss 0.437326\n",
      "epoch:2; batch 128; train accuracy: 0.762913\n",
      "epoch 2; batch 256; loss 0.556758\n",
      "epoch:2; batch 256; train accuracy: 0.762945\n",
      "epoch 2; batch 384; loss 0.594034\n",
      "epoch:2; batch 384; train accuracy: 0.762901\n",
      "epoch 2; batch 512; loss 0.531149\n",
      "epoch:2; batch 512; train accuracy: 0.762980\n",
      "epoch 2; batch 640; loss 0.390651\n",
      "epoch:2; batch 640; train accuracy: 0.763097\n",
      "epoch 2; batch 768; loss 0.420148\n",
      "epoch:2; batch 768; train accuracy: 0.763223\n",
      "epoch 2; batch 896; loss 0.520646\n",
      "epoch:2; batch 896; train accuracy: 0.763273\n",
      "epoch 2; batch 1024; loss 0.351471\n",
      "epoch:2; batch 1024; train accuracy: 0.763417\n",
      "epoch 2; batch 1152; loss 0.321005\n",
      "epoch:2; batch 1152; train accuracy: 0.763533\n",
      "epoch 2; batch 1280; loss 0.312048\n",
      "epoch:2; batch 1280; train accuracy: 0.763677\n",
      "epoch 2; batch 1408; loss 0.504695\n",
      "epoch:2; batch 1408; train accuracy: 0.763773\n",
      "epoch 2; batch 1536; loss 0.463895\n",
      "epoch:2; batch 1536; train accuracy: 0.763841\n",
      "epoch 2; batch 1664; loss 0.384498\n",
      "epoch:2; batch 1664; train accuracy: 0.763927\n",
      "epoch 2; batch 1792; loss 0.667786\n",
      "epoch:2; batch 1792; train accuracy: 0.763929\n",
      "epoch 2; batch 1920; loss 0.536468\n",
      "epoch:2; batch 1920; train accuracy: 0.763997\n",
      "epoch 2; batch 2048; loss 0.426645\n",
      "epoch:2; batch 2048; train accuracy: 0.764046\n",
      "epoch 2; batch 2176; loss 0.490153\n",
      "epoch:2; batch 2176; train accuracy: 0.764104\n",
      "epoch 2; batch 2304; loss 0.462950\n",
      "epoch:2; batch 2304; train accuracy: 0.764199\n",
      "epoch 2; batch 2432; loss 0.373205\n",
      "epoch:2; batch 2432; train accuracy: 0.764349\n",
      "epoch 2; batch 2560; loss 0.502649\n",
      "epoch:2; batch 2560; train accuracy: 0.764416\n",
      "epoch 2; batch 2688; loss 0.417592\n",
      "epoch:2; batch 2688; train accuracy: 0.764501\n",
      "epoch 2; batch 2816; loss 0.460477\n",
      "epoch:2; batch 2816; train accuracy: 0.764604\n",
      "epoch 2; batch 2944; loss 0.587209\n",
      "epoch:2; batch 2944; train accuracy: 0.764624\n",
      "epoch 2; batch 3072; loss 0.518581\n",
      "epoch:2; batch 3072; train accuracy: 0.764709\n",
      "epoch 2; batch 3200; loss 0.458409\n",
      "epoch:2; batch 3200; train accuracy: 0.764784\n",
      "epoch 2; batch 3328; loss 0.420226\n",
      "epoch:2; batch 3328; train accuracy: 0.764895\n",
      "epoch 2; batch 3456; loss 0.481230\n",
      "epoch:2; batch 3456; train accuracy: 0.764989\n",
      "epoch 2; batch 3584; loss 0.339889\n",
      "epoch:2; batch 3584; train accuracy: 0.765155\n",
      "epoch 2; batch 3712; loss 0.528445\n",
      "epoch:2; batch 3712; train accuracy: 0.765183\n",
      "epoch 2; batch 3840; loss 0.472885\n",
      "epoch:2; batch 3840; train accuracy: 0.765267\n",
      "epoch 2; batch 3968; loss 0.458309\n",
      "epoch:2; batch 3968; train accuracy: 0.765368\n",
      "epoch 2; batch 4096; loss 0.470364\n",
      "epoch:2; batch 4096; train accuracy: 0.765432\n",
      "epoch 2; batch 4224; loss 0.557021\n",
      "epoch:2; batch 4224; train accuracy: 0.765478\n",
      "epoch 2; batch 4352; loss 0.503437\n",
      "epoch:2; batch 4352; train accuracy: 0.765561\n",
      "epoch 2; batch 4480; loss 0.435306\n",
      "epoch:2; batch 4480; train accuracy: 0.765652\n",
      "epoch 2; batch 4608; loss 0.426081\n",
      "epoch:2; batch 4608; train accuracy: 0.765725\n",
      "epoch 2; batch 4736; loss 0.634470\n",
      "epoch:2; batch 4736; train accuracy: 0.765771\n",
      "epoch 2; batch 4864; loss 0.429173\n",
      "epoch:2; batch 4864; train accuracy: 0.765825\n",
      "epoch 2; batch 4992; loss 0.349799\n",
      "epoch:2; batch 4992; train accuracy: 0.765952\n",
      "epoch 2; batch 5120; loss 0.604843\n",
      "epoch:2; batch 5120; train accuracy: 0.765979\n",
      "epoch 2; batch 5248; loss 0.352909\n",
      "epoch:2; batch 5248; train accuracy: 0.766115\n",
      "epoch 2; batch 5376; loss 0.541717\n",
      "epoch:2; batch 5376; train accuracy: 0.766142\n",
      "epoch 2; batch 5504; loss 0.418745\n",
      "epoch:2; batch 5504; train accuracy: 0.766222\n",
      "epoch 2; batch 5632; loss 0.423037\n",
      "epoch:2; batch 5632; train accuracy: 0.766303\n",
      "epoch 2; batch 5760; loss 0.457671\n",
      "epoch:2; batch 5760; train accuracy: 0.766339\n",
      "epoch 2; batch 5888; loss 0.429153\n",
      "epoch:2; batch 5888; train accuracy: 0.766410\n",
      "epoch 2; batch 6016; loss 0.342299\n",
      "epoch:2; batch 6016; train accuracy: 0.766544\n",
      "epoch 2; batch 6144; loss 0.336792\n",
      "epoch:2; batch 6144; train accuracy: 0.766687\n",
      "epoch 2; batch 6272; loss 0.454529\n",
      "epoch:2; batch 6272; train accuracy: 0.766749\n",
      "epoch 2; batch 6400; loss 0.417628\n",
      "epoch:2; batch 6400; train accuracy: 0.766801\n",
      "epoch 2; batch 6528; loss 0.361786\n",
      "epoch:2; batch 6528; train accuracy: 0.766917\n",
      "epoch 2; batch 6656; loss 0.470730\n",
      "epoch:2; batch 6656; train accuracy: 0.766996\n",
      "epoch 2; batch 6784; loss 0.434094\n",
      "epoch:2; batch 6784; train accuracy: 0.767093\n",
      "epoch 2; batch 6912; loss 0.511397\n",
      "epoch:2; batch 6912; train accuracy: 0.767136\n",
      "epoch 2; batch 7040; loss 0.459867\n",
      "epoch:2; batch 7040; train accuracy: 0.767205\n",
      "epoch 2; batch 7168; loss 0.384812\n",
      "epoch:2; batch 7168; train accuracy: 0.767328\n",
      "epoch 2; batch 7296; loss 0.478374\n",
      "epoch:2; batch 7296; train accuracy: 0.767433\n",
      "epoch 2; batch 7424; loss 0.514412\n",
      "epoch:2; batch 7424; train accuracy: 0.767485\n",
      "epoch 2; batch 7552; loss 0.418669\n",
      "epoch:2; batch 7552; train accuracy: 0.767571\n",
      "epoch 2; batch 7680; loss 0.292478\n",
      "epoch:2; batch 7680; train accuracy: 0.767720\n",
      "epoch 2; batch 7808; loss 0.340626\n",
      "epoch:2; batch 7808; train accuracy: 0.767806\n",
      "epoch 2; batch 7936; loss 0.345420\n",
      "epoch:2; batch 7936; train accuracy: 0.767910\n",
      "epoch 2; batch 8064; loss 0.457169\n",
      "epoch:2; batch 8064; train accuracy: 0.767952\n",
      "epoch 2; batch 8192; loss 0.494508\n",
      "epoch:2; batch 8192; train accuracy: 0.768011\n",
      "epoch 2; batch 8320; loss 0.343618\n",
      "epoch:2; batch 8320; train accuracy: 0.768141\n",
      "epoch 2; batch 8448; loss 0.461165\n",
      "epoch:2; batch 8448; train accuracy: 0.768217\n",
      "epoch 2; batch 8576; loss 0.394119\n",
      "epoch:2; batch 8576; train accuracy: 0.768311\n",
      "epoch 2; batch 8704; loss 0.425381\n",
      "epoch:2; batch 8704; train accuracy: 0.768388\n",
      "epoch 2; batch 8832; loss 0.436471\n",
      "epoch:2; batch 8832; train accuracy: 0.768472\n",
      "epoch 2; batch 8960; loss 0.499089\n",
      "epoch:2; batch 8960; train accuracy: 0.768548\n",
      "epoch 2; batch 9088; loss 0.473910\n",
      "epoch:2; batch 9088; train accuracy: 0.768606\n",
      "epoch 2; batch 9216; loss 0.398125\n",
      "epoch:2; batch 9216; train accuracy: 0.768734\n",
      "epoch 2; batch 9344; loss 0.291701\n",
      "epoch:2; batch 9344; train accuracy: 0.768879\n",
      "epoch 2; batch 9472; loss 0.447649\n",
      "epoch:2; batch 9472; train accuracy: 0.768955\n",
      "epoch 2; batch 9600; loss 0.430209\n",
      "epoch:2; batch 9600; train accuracy: 0.769021\n",
      "epoch 2; batch 9728; loss 0.490807\n",
      "epoch:2; batch 9728; train accuracy: 0.769034\n",
      "epoch 2; batch 9856; loss 0.462183\n",
      "epoch:2; batch 9856; train accuracy: 0.769100\n",
      "epoch 2; batch 9984; loss 0.433532\n",
      "epoch:2; batch 9984; train accuracy: 0.769140\n",
      "epoch 2; batch 10112; loss 0.407041\n",
      "epoch:2; batch 10112; train accuracy: 0.769249\n",
      "epoch 2; batch 10240; loss 0.356751\n",
      "epoch:2; batch 10240; train accuracy: 0.769384\n",
      "epoch 2; batch 10368; loss 0.430621\n",
      "epoch:2; batch 10368; train accuracy: 0.769475\n",
      "epoch 2; batch 10496; loss 0.520428\n",
      "epoch:2; batch 10496; train accuracy: 0.769549\n",
      "epoch 2; batch 10624; loss 0.498734\n",
      "epoch:2; batch 10624; train accuracy: 0.769605\n",
      "epoch 2; batch 10752; loss 0.545072\n",
      "epoch:2; batch 10752; train accuracy: 0.769635\n",
      "epoch 2; batch 10880; loss 0.476515\n",
      "epoch:2; batch 10880; train accuracy: 0.769691\n",
      "epoch 2; batch 11008; loss 0.351130\n",
      "epoch:2; batch 11008; train accuracy: 0.769799\n",
      "epoch 2; batch 11136; loss 0.399627\n",
      "epoch:2; batch 11136; train accuracy: 0.769897\n",
      "epoch 2; batch 11264; loss 0.455817\n",
      "epoch:2; batch 11264; train accuracy: 0.769961\n",
      "epoch 2; batch 11392; loss 0.504389\n",
      "epoch:2; batch 11392; train accuracy: 0.770025\n",
      "epoch 2; batch 11520; loss 0.396888\n",
      "epoch:2; batch 11520; train accuracy: 0.770098\n",
      "epoch 2; batch 11648; loss 0.398653\n",
      "epoch:2; batch 11648; train accuracy: 0.770204\n",
      "epoch 2; batch 11776; loss 0.360636\n",
      "epoch:2; batch 11776; train accuracy: 0.770311\n",
      "epoch 2; batch 11904; loss 0.506632\n",
      "epoch:2; batch 11904; train accuracy: 0.770383\n",
      "epoch 2; batch 12032; loss 0.484194\n",
      "epoch:2; batch 12032; train accuracy: 0.770412\n",
      "epoch 2; batch 12160; loss 0.460174\n",
      "epoch:2; batch 12160; train accuracy: 0.770441\n",
      "epoch 2; batch 12288; loss 0.486724\n",
      "epoch:2; batch 12288; train accuracy: 0.770486\n",
      "epoch 2; batch 12416; loss 0.496746\n",
      "epoch:2; batch 12416; train accuracy: 0.770532\n",
      "epoch 2; batch 12544; loss 0.405600\n",
      "epoch:2; batch 12544; train accuracy: 0.770621\n",
      "epoch 2; batch 12672; loss 0.422733\n",
      "epoch:2; batch 12672; train accuracy: 0.770700\n",
      "epoch 2; batch 12800; loss 0.481463\n",
      "epoch:2; batch 12800; train accuracy: 0.770746\n",
      "epoch 2; batch 12928; loss 0.319013\n",
      "epoch:2; batch 12928; train accuracy: 0.770850\n",
      "epoch 2; batch 13056; loss 0.335369\n",
      "epoch:2; batch 13056; train accuracy: 0.770929\n",
      "epoch 2; batch 13184; loss 0.407051\n",
      "epoch:2; batch 13184; train accuracy: 0.771000\n",
      "epoch 2; batch 13312; loss 0.459586\n",
      "epoch:2; batch 13312; train accuracy: 0.771079\n",
      "epoch 2; batch 13440; loss 0.429991\n",
      "epoch:2; batch 13440; train accuracy: 0.771132\n",
      "epoch 2; batch 13568; loss 0.454846\n",
      "epoch:2; batch 13568; train accuracy: 0.771168\n",
      "epoch 2; batch 13696; loss 0.342390\n",
      "epoch:2; batch 13696; train accuracy: 0.771255\n",
      "epoch 2; batch 13824; loss 0.348615\n",
      "epoch:2; batch 13824; train accuracy: 0.771392\n",
      "epoch 2; batch 13952; loss 0.346003\n",
      "epoch:2; batch 13952; train accuracy: 0.771503\n",
      "epoch 2; batch 14080; loss 0.426267\n",
      "epoch:2; batch 14080; train accuracy: 0.771606\n",
      "epoch 2; batch 14208; loss 0.274145\n",
      "epoch:2; batch 14208; train accuracy: 0.771759\n",
      "epoch 2; batch 14336; loss 0.451709\n",
      "epoch:2; batch 14336; train accuracy: 0.771820\n",
      "epoch 2; batch 14464; loss 0.356657\n",
      "epoch:2; batch 14464; train accuracy: 0.771922\n",
      "epoch 2; batch 14592; loss 0.356087\n",
      "epoch:2; batch 14592; train accuracy: 0.772057\n",
      "epoch 2; batch 14720; loss 0.346136\n",
      "epoch:2; batch 14720; train accuracy: 0.772184\n",
      "epoch 2; batch 14848; loss 0.386129\n",
      "epoch:2; batch 14848; train accuracy: 0.772302\n",
      "epoch 2; batch 14976; loss 0.422403\n",
      "epoch:2; batch 14976; train accuracy: 0.772395\n",
      "epoch 2; batch 15104; loss 0.325380\n",
      "epoch:2; batch 15104; train accuracy: 0.772521\n",
      "epoch 2; batch 15232; loss 0.421273\n",
      "epoch:2; batch 15232; train accuracy: 0.772614\n",
      "epoch 2; batch 15360; loss 0.486420\n",
      "epoch:2; batch 15360; train accuracy: 0.772665\n",
      "epoch 2; batch 15488; loss 0.323970\n",
      "epoch:2; batch 15488; train accuracy: 0.772807\n",
      "epoch 2; batch 15616; loss 0.355786\n",
      "epoch:2; batch 15616; train accuracy: 0.772907\n",
      "epoch 2; batch 15744; loss 0.319749\n",
      "epoch:2; batch 15744; train accuracy: 0.773032\n",
      "epoch 2; batch 15872; loss 0.415546\n",
      "epoch:2; batch 15872; train accuracy: 0.773115\n",
      "epoch 2; batch 16000; loss 0.358850\n",
      "epoch:2; batch 16000; train accuracy: 0.773189\n",
      "epoch 2; batch 16128; loss 0.400182\n",
      "epoch:2; batch 16128; train accuracy: 0.773264\n",
      "epoch 2; batch 16256; loss 0.327642\n",
      "epoch:2; batch 16256; train accuracy: 0.773380\n",
      "epoch 2; batch 16384; loss 0.378882\n",
      "epoch:2; batch 16384; train accuracy: 0.773479\n",
      "epoch 2; batch 16512; loss 0.425336\n",
      "epoch:2; batch 16512; train accuracy: 0.773586\n",
      "epoch 2; batch 16640; loss 0.357524\n",
      "epoch:2; batch 16640; train accuracy: 0.773692\n",
      "epoch 2; batch 16768; loss 0.423728\n",
      "epoch:2; batch 16768; train accuracy: 0.773774\n",
      "epoch 2; batch 16896; loss 0.394360\n",
      "epoch:2; batch 16896; train accuracy: 0.773831\n",
      "epoch 2; batch 17024; loss 0.341389\n",
      "epoch:2; batch 17024; train accuracy: 0.773938\n",
      "epoch 2; batch 17152; loss 0.357131\n",
      "epoch:2; batch 17152; train accuracy: 0.774068\n",
      "epoch 2; batch 17280; loss 0.412424\n",
      "epoch:2; batch 17280; train accuracy: 0.774174\n",
      "epoch 2; batch 17408; loss 0.244544\n",
      "epoch:2; batch 17408; train accuracy: 0.774328\n",
      "epoch 2; batch 17536; loss 0.272529\n",
      "epoch:2; batch 17536; train accuracy: 0.774450\n",
      "epoch 2; batch 17664; loss 0.343454\n",
      "epoch:2; batch 17664; train accuracy: 0.774530\n",
      "epoch 2; batch 17792; loss 0.505213\n",
      "epoch:2; batch 17792; train accuracy: 0.774602\n",
      "epoch 2; batch 17920; loss 0.399234\n",
      "epoch:2; batch 17920; train accuracy: 0.774666\n",
      "epoch 2; batch 18048; loss 0.377891\n",
      "epoch:2; batch 18048; train accuracy: 0.774779\n",
      "epoch 2; batch 18176; loss 0.508072\n",
      "epoch:2; batch 18176; train accuracy: 0.774842\n",
      "epoch 2; batch 18304; loss 0.366098\n",
      "epoch:2; batch 18304; train accuracy: 0.774914\n",
      "epoch 2; batch 18432; loss 0.364522\n",
      "epoch:2; batch 18432; train accuracy: 0.774994\n",
      "epoch 2; batch 18560; loss 0.399750\n",
      "epoch:2; batch 18560; train accuracy: 0.775049\n",
      "epoch 2; batch 18688; loss 0.347541\n",
      "epoch:2; batch 18688; train accuracy: 0.775168\n",
      "epoch 2; batch 18816; loss 0.420253\n",
      "epoch:2; batch 18816; train accuracy: 0.775255\n",
      "epoch 2; batch 18944; loss 0.471534\n",
      "epoch:2; batch 18944; train accuracy: 0.775278\n",
      "epoch 2; batch 19072; loss 0.375772\n",
      "epoch:2; batch 19072; train accuracy: 0.775356\n",
      "epoch 2; batch 19200; loss 0.320733\n",
      "epoch:2; batch 19200; train accuracy: 0.775459\n",
      "epoch 2; batch 19328; loss 0.336058\n",
      "epoch:2; batch 19328; train accuracy: 0.775554\n",
      "epoch 2; batch 19456; loss 0.322827\n",
      "epoch:2; batch 19456; train accuracy: 0.775688\n",
      "epoch 2; batch 19584; loss 0.399519\n",
      "epoch:2; batch 19584; train accuracy: 0.775766\n",
      "epoch 2; batch 19712; loss 0.377371\n",
      "epoch:2; batch 19712; train accuracy: 0.775868\n",
      "epoch 2; batch 19840; loss 0.539173\n",
      "epoch:2; batch 19840; train accuracy: 0.775930\n",
      "epoch 2; batch 19968; loss 0.445267\n",
      "epoch:2; batch 19968; train accuracy: 0.776007\n",
      "epoch 2; batch 20096; loss 0.385318\n",
      "epoch:2; batch 20096; train accuracy: 0.776076\n",
      "epoch 2; batch 20224; loss 0.350503\n",
      "epoch:2; batch 20224; train accuracy: 0.776169\n",
      "epoch 2; batch 20352; loss 0.367862\n",
      "epoch:2; batch 20352; train accuracy: 0.776286\n",
      "epoch 2; batch 20480; loss 0.299261\n",
      "epoch:2; batch 20480; train accuracy: 0.776411\n",
      "epoch 2; batch 20608; loss 0.363332\n",
      "epoch:2; batch 20608; train accuracy: 0.776496\n",
      "epoch 2; batch 20736; loss 0.364114\n",
      "epoch:2; batch 20736; train accuracy: 0.776596\n",
      "epoch 2; batch 20864; loss 0.289895\n",
      "epoch:2; batch 20864; train accuracy: 0.776712\n",
      "epoch 2; batch 20992; loss 0.350645\n",
      "epoch:2; batch 20992; train accuracy: 0.776788\n",
      "epoch 2; batch 21120; loss 0.328353\n",
      "epoch:2; batch 21120; train accuracy: 0.776888\n",
      "epoch 2; batch 21248; loss 0.401205\n",
      "epoch:2; batch 21248; train accuracy: 0.776979\n",
      "epoch 2; batch 21376; loss 0.293757\n",
      "epoch:2; batch 21376; train accuracy: 0.777110\n",
      "epoch 2; batch 21504; loss 0.615639\n",
      "epoch:2; batch 21504; train accuracy: 0.777154\n",
      "epoch 2; batch 21632; loss 0.380088\n",
      "epoch:2; batch 21632; train accuracy: 0.777237\n",
      "epoch 2; batch 21760; loss 0.378771\n",
      "epoch:2; batch 21760; train accuracy: 0.777296\n",
      "epoch 2; batch 21888; loss 0.372186\n",
      "epoch:2; batch 21888; train accuracy: 0.777387\n",
      "epoch 2; batch 22016; loss 0.345862\n",
      "epoch:2; batch 22016; train accuracy: 0.777517\n",
      "epoch 2; batch 22144; loss 0.417502\n",
      "epoch:2; batch 22144; train accuracy: 0.777584\n",
      "epoch 2; batch 22272; loss 0.289723\n",
      "epoch:2; batch 22272; train accuracy: 0.777705\n",
      "epoch 2; batch 22400; loss 0.405504\n",
      "epoch:2; batch 22400; train accuracy: 0.777756\n",
      "epoch 2; batch 22528; loss 0.430388\n",
      "epoch:2; batch 22528; train accuracy: 0.777822\n",
      "epoch 2; batch 22656; loss 0.410774\n",
      "epoch:2; batch 22656; train accuracy: 0.777904\n",
      "epoch 2; batch 22784; loss 0.371530\n",
      "epoch:2; batch 22784; train accuracy: 0.777986\n",
      "epoch 2; batch 22912; loss 0.330826\n",
      "epoch:2; batch 22912; train accuracy: 0.778067\n",
      "epoch 2; batch 23040; loss 0.439793\n",
      "epoch:2; batch 23040; train accuracy: 0.778156\n",
      "epoch 2; batch 23168; loss 0.439661\n",
      "epoch:2; batch 23168; train accuracy: 0.778191\n",
      "epoch 2; batch 23296; loss 0.286234\n",
      "epoch:2; batch 23296; train accuracy: 0.778318\n",
      "epoch 2; batch 23424; loss 0.356293\n",
      "epoch:2; batch 23424; train accuracy: 0.778423\n",
      "epoch 2; batch 23552; loss 0.317986\n",
      "epoch:2; batch 23552; train accuracy: 0.778527\n",
      "epoch 2; batch 23680; loss 0.365235\n",
      "epoch:2; batch 23680; train accuracy: 0.778638\n",
      "epoch 2; batch 23808; loss 0.349404\n",
      "epoch:2; batch 23808; train accuracy: 0.778742\n",
      "epoch 2; batch 23936; loss 0.463629\n",
      "epoch:2; batch 23936; train accuracy: 0.778783\n",
      "epoch 2; batch 24064; loss 0.263817\n",
      "epoch:2; batch 24064; train accuracy: 0.778909\n",
      "epoch 2; batch 24192; loss 0.320515\n",
      "epoch:2; batch 24192; train accuracy: 0.778997\n",
      "epoch 2; batch 24320; loss 0.331699\n",
      "epoch:2; batch 24320; train accuracy: 0.779107\n",
      "epoch 2; batch 24448; loss 0.383413\n",
      "epoch:2; batch 24448; train accuracy: 0.779171\n",
      "epoch 2; batch 24576; loss 0.342839\n",
      "epoch:2; batch 24576; train accuracy: 0.779281\n",
      "epoch 2; batch 24704; loss 0.348018\n",
      "epoch:2; batch 24704; train accuracy: 0.779399\n",
      "epoch 2; batch 24832; loss 0.578887\n",
      "epoch:2; batch 24832; train accuracy: 0.779432\n",
      "epoch 2; batch 24960; loss 0.410642\n",
      "epoch:2; batch 24960; train accuracy: 0.779495\n",
      "epoch 2; batch 25088; loss 0.431055\n",
      "epoch:2; batch 25088; train accuracy: 0.779566\n",
      "epoch 2; batch 25216; loss 0.329156\n",
      "epoch:2; batch 25216; train accuracy: 0.779668\n",
      "epoch 2; batch 25344; loss 0.372642\n",
      "epoch:2; batch 25344; train accuracy: 0.779738\n",
      "epoch 2; batch 25472; loss 0.374695\n",
      "epoch:2; batch 25472; train accuracy: 0.779801\n",
      "epoch 2; batch 25600; loss 0.326917\n",
      "epoch:2; batch 25600; train accuracy: 0.779925\n",
      "epoch 2; batch 25728; loss 0.373976\n",
      "epoch:2; batch 25728; train accuracy: 0.780010\n",
      "epoch 2; batch 25856; loss 0.355016\n",
      "epoch:2; batch 25856; train accuracy: 0.780126\n",
      "epoch 2; batch 25984; loss 0.346938\n",
      "epoch:2; batch 25984; train accuracy: 0.780242\n",
      "epoch 2; batch 26112; loss 0.367007\n",
      "epoch:2; batch 26112; train accuracy: 0.780350\n",
      "epoch 2; batch 26240; loss 0.358666\n",
      "epoch:2; batch 26240; train accuracy: 0.780419\n",
      "epoch 2; batch 26368; loss 0.309787\n",
      "epoch:2; batch 26368; train accuracy: 0.780511\n",
      "epoch 2; batch 26496; loss 0.357179\n",
      "epoch:2; batch 26496; train accuracy: 0.780596\n",
      "epoch 2; batch 26624; loss 0.241837\n",
      "epoch:2; batch 26624; train accuracy: 0.780726\n",
      "epoch 2; batch 26752; loss 0.372853\n",
      "epoch:2; batch 26752; train accuracy: 0.780779\n",
      "epoch 2; batch 26880; loss 0.395459\n",
      "epoch:2; batch 26880; train accuracy: 0.780833\n",
      "epoch 2; batch 27008; loss 0.335838\n",
      "epoch:2; batch 27008; train accuracy: 0.780917\n",
      "epoch 2; batch 27136; loss 0.365796\n",
      "epoch:2; batch 27136; train accuracy: 0.780993\n",
      "epoch 2; batch 27264; loss 0.449205\n",
      "epoch:2; batch 27264; train accuracy: 0.781046\n",
      "epoch 2; batch 27392; loss 0.471834\n",
      "epoch:2; batch 27392; train accuracy: 0.781091\n",
      "epoch 2; batch 27520; loss 0.386193\n",
      "epoch:2; batch 27520; train accuracy: 0.781190\n",
      "epoch 2; batch 27648; loss 0.358471\n",
      "epoch:2; batch 27648; train accuracy: 0.781288\n",
      "epoch 2; batch 27776; loss 0.379875\n",
      "epoch:2; batch 27776; train accuracy: 0.781363\n",
      "epoch 2; batch 27904; loss 0.423818\n",
      "epoch:2; batch 27904; train accuracy: 0.781446\n",
      "epoch 2; batch 28032; loss 0.259231\n",
      "epoch:2; batch 28032; train accuracy: 0.781558\n",
      "epoch 2; batch 28160; loss 0.330060\n",
      "epoch:2; batch 28160; train accuracy: 0.781648\n",
      "epoch 2; batch 28288; loss 0.316144\n",
      "epoch:2; batch 28288; train accuracy: 0.781738\n",
      "epoch 2; batch 28416; loss 0.282147\n",
      "epoch:2; batch 28416; train accuracy: 0.781857\n",
      "epoch 2; batch 28544; loss 0.380570\n",
      "epoch:2; batch 28544; train accuracy: 0.781939\n",
      "epoch 2; batch 28672; loss 0.341247\n",
      "epoch:2; batch 28672; train accuracy: 0.782006\n",
      "epoch 2; batch 28800; loss 0.298051\n",
      "epoch:2; batch 28800; train accuracy: 0.782102\n",
      "epoch 2; batch 28928; loss 0.285957\n",
      "epoch:2; batch 28928; train accuracy: 0.782206\n",
      "epoch 2; batch 29056; loss 0.312308\n",
      "epoch:2; batch 29056; train accuracy: 0.782310\n",
      "epoch 2; batch 29184; loss 0.249522\n",
      "epoch:2; batch 29184; train accuracy: 0.782443\n",
      "epoch 2; batch 29312; loss 0.230734\n",
      "epoch:2; batch 29312; train accuracy: 0.782546\n",
      "epoch 2; batch 29440; loss 0.373705\n",
      "epoch:2; batch 29440; train accuracy: 0.782619\n",
      "epoch 2; batch 29568; loss 0.449235\n",
      "epoch:2; batch 29568; train accuracy: 0.782692\n",
      "epoch 2; batch 29696; loss 0.315361\n",
      "epoch:2; batch 29696; train accuracy: 0.782817\n",
      "epoch 2; batch 29824; loss 0.286874\n",
      "epoch:2; batch 29824; train accuracy: 0.782949\n",
      "epoch 2; batch 29952; loss 0.362553\n",
      "epoch:2; batch 29952; train accuracy: 0.783044\n",
      "epoch 2; batch 30080; loss 0.418691\n",
      "epoch:2; batch 30080; train accuracy: 0.783116\n",
      "epoch 2; batch 30208; loss 0.306173\n",
      "epoch:2; batch 30208; train accuracy: 0.783203\n",
      "epoch 2; batch 30336; loss 0.235507\n",
      "epoch:2; batch 30336; train accuracy: 0.783334\n",
      "epoch 2; batch 30464; loss 0.474060\n",
      "epoch:2; batch 30464; train accuracy: 0.783391\n",
      "epoch 2; batch 30592; loss 0.373151\n",
      "epoch:2; batch 30592; train accuracy: 0.783471\n",
      "epoch 2; batch 30720; loss 0.282295\n",
      "epoch:2; batch 30720; train accuracy: 0.783586\n",
      "epoch 2; batch 30848; loss 0.313474\n",
      "epoch:2; batch 30848; train accuracy: 0.783665\n",
      "epoch 2; batch 30976; loss 0.335728\n",
      "epoch:2; batch 30976; train accuracy: 0.783744\n",
      "epoch 2; batch 31104; loss 0.375626\n",
      "epoch:2; batch 31104; train accuracy: 0.783815\n",
      "epoch 2; batch 31232; loss 0.308874\n",
      "epoch:2; batch 31232; train accuracy: 0.783886\n",
      "epoch 2; batch 31360; loss 0.351873\n",
      "epoch:2; batch 31360; train accuracy: 0.783986\n",
      "epoch 2; batch 31488; loss 0.366051\n",
      "epoch:2; batch 31488; train accuracy: 0.784035\n",
      "epoch 2; batch 31616; loss 0.400129\n",
      "epoch:2; batch 31616; train accuracy: 0.784128\n",
      "epoch 2; batch 31744; loss 0.356682\n",
      "epoch:2; batch 31744; train accuracy: 0.784205\n",
      "epoch 2; batch 31872; loss 0.475562\n",
      "epoch:2; batch 31872; train accuracy: 0.784232\n",
      "epoch 2; batch 32000; loss 0.287829\n",
      "epoch:2; batch 32000; train accuracy: 0.784331\n",
      "epoch 2; batch 32128; loss 0.400009\n",
      "epoch:2; batch 32128; train accuracy: 0.784372\n",
      "epoch 2; batch 32256; loss 0.352842\n",
      "epoch:2; batch 32256; train accuracy: 0.784442\n",
      "epoch 2; batch 32384; loss 0.264291\n",
      "epoch:2; batch 32384; train accuracy: 0.784556\n",
      "epoch 2; batch 32512; loss 0.381999\n",
      "epoch:2; batch 32512; train accuracy: 0.784640\n",
      "epoch 2; batch 32640; loss 0.339060\n",
      "epoch:2; batch 32640; train accuracy: 0.784738\n",
      "epoch 2; batch 32768; loss 0.363740\n",
      "epoch:2; batch 32768; train accuracy: 0.784829\n",
      "epoch 2; batch 32896; loss 0.331301\n",
      "epoch:2; batch 32896; train accuracy: 0.784906\n",
      "epoch 2; batch 33024; loss 0.355110\n",
      "epoch:2; batch 33024; train accuracy: 0.785011\n",
      "epoch 2; batch 33152; loss 0.429403\n",
      "epoch:2; batch 33152; train accuracy: 0.785051\n",
      "epoch 2; batch 33280; loss 0.431861\n",
      "epoch:2; batch 33280; train accuracy: 0.785120\n",
      "epoch 2; batch 33408; loss 0.426231\n",
      "epoch:2; batch 33408; train accuracy: 0.785167\n",
      "epoch 2; batch 33536; loss 0.344021\n",
      "epoch:2; batch 33536; train accuracy: 0.785243\n",
      "epoch 2; batch 33664; loss 0.323572\n",
      "epoch:2; batch 33664; train accuracy: 0.785362\n",
      "epoch 2; batch 33792; loss 0.318351\n",
      "epoch:2; batch 33792; train accuracy: 0.785459\n",
      "epoch 2; batch 33920; loss 0.394342\n",
      "epoch:2; batch 33920; train accuracy: 0.785513\n",
      "epoch 2; batch 34048; loss 0.412157\n",
      "epoch:2; batch 34048; train accuracy: 0.785559\n",
      "epoch 2; batch 34176; loss 0.395910\n",
      "epoch:2; batch 34176; train accuracy: 0.785627\n",
      "epoch 2; batch 34304; loss 0.349741\n",
      "epoch:2; batch 34304; train accuracy: 0.785709\n",
      "epoch 2; batch 34432; loss 0.443810\n",
      "epoch:2; batch 34432; train accuracy: 0.785784\n",
      "epoch 2; batch 34560; loss 0.322504\n",
      "epoch:2; batch 34560; train accuracy: 0.785859\n",
      "epoch 2; batch 34688; loss 0.375950\n",
      "epoch:2; batch 34688; train accuracy: 0.785933\n",
      "epoch 2; batch 34816; loss 0.407452\n",
      "epoch:2; batch 34816; train accuracy: 0.785986\n",
      "epoch 2; batch 34944; loss 0.285662\n",
      "epoch:2; batch 34944; train accuracy: 0.786110\n",
      "epoch 2; batch 35072; loss 0.340452\n",
      "epoch:2; batch 35072; train accuracy: 0.786170\n",
      "epoch 2; batch 35200; loss 0.361005\n",
      "epoch:2; batch 35200; train accuracy: 0.786251\n",
      "epoch 2; batch 35328; loss 0.294516\n",
      "epoch:2; batch 35328; train accuracy: 0.786361\n",
      "epoch 2; batch 35456; loss 0.279361\n",
      "epoch:2; batch 35456; train accuracy: 0.786484\n",
      "epoch 2; batch 35584; loss 0.337650\n",
      "epoch:2; batch 35584; train accuracy: 0.786565\n",
      "epoch 2; batch 35712; loss 0.425845\n",
      "epoch:2; batch 35712; train accuracy: 0.786610\n",
      "epoch 2; batch 35840; loss 0.376776\n",
      "epoch:2; batch 35840; train accuracy: 0.786690\n",
      "epoch 2; batch 35968; loss 0.282417\n",
      "epoch:2; batch 35968; train accuracy: 0.786799\n",
      "epoch 2; batch 36096; loss 0.280876\n",
      "epoch:2; batch 36096; train accuracy: 0.786886\n",
      "epoch 2; batch 36224; loss 0.397799\n",
      "epoch:2; batch 36224; train accuracy: 0.786959\n",
      "epoch 2; batch 36352; loss 0.462509\n",
      "epoch:2; batch 36352; train accuracy: 0.787024\n",
      "epoch 2; batch 36480; loss 0.286598\n",
      "epoch:2; batch 36480; train accuracy: 0.787132\n",
      "epoch 2; batch 36608; loss 0.404113\n",
      "epoch:2; batch 36608; train accuracy: 0.787191\n",
      "epoch 2; batch 36736; loss 0.416956\n",
      "epoch:2; batch 36736; train accuracy: 0.787277\n",
      "epoch 2; batch 36864; loss 0.354756\n",
      "epoch:2; batch 36864; train accuracy: 0.787342\n",
      "epoch 2; batch 36992; loss 0.230353\n",
      "epoch:2; batch 36992; train accuracy: 0.787477\n",
      "epoch 2; batch 37120; loss 0.352659\n",
      "epoch:2; batch 37120; train accuracy: 0.787549\n",
      "epoch 2; batch 37248; loss 0.317434\n",
      "epoch:2; batch 37248; train accuracy: 0.787628\n",
      "epoch 2; batch 37376; loss 0.286912\n",
      "epoch:2; batch 37376; train accuracy: 0.787735\n",
      "epoch 2; batch 37504; loss 0.453544\n",
      "epoch:2; batch 37504; train accuracy: 0.787764\n",
      "epoch 2; batch 37632; loss 0.371097\n",
      "epoch:2; batch 37632; train accuracy: 0.787842\n",
      "epoch 2; batch 37760; loss 0.276868\n",
      "epoch:2; batch 37760; train accuracy: 0.787941\n",
      "epoch 2; batch 37888; loss 0.369528\n",
      "epoch:2; batch 37888; train accuracy: 0.788033\n",
      "epoch 2; batch 38016; loss 0.360793\n",
      "epoch:2; batch 38016; train accuracy: 0.788132\n",
      "epoch 2; batch 38144; loss 0.191617\n",
      "epoch:2; batch 38144; train accuracy: 0.788259\n",
      "epoch 2; batch 38272; loss 0.246034\n",
      "epoch:2; batch 38272; train accuracy: 0.788357\n",
      "epoch 2; batch 38400; loss 0.299094\n",
      "epoch:2; batch 38400; train accuracy: 0.788435\n",
      "epoch 2; batch 38528; loss 0.377588\n",
      "epoch:2; batch 38528; train accuracy: 0.788512\n",
      "epoch 2; batch 38656; loss 0.344660\n",
      "epoch:2; batch 38656; train accuracy: 0.788589\n",
      "epoch 2; batch 38784; loss 0.415426\n",
      "epoch:2; batch 38784; train accuracy: 0.788666\n",
      "epoch 2; batch 38912; loss 0.464077\n",
      "epoch:2; batch 38912; train accuracy: 0.788694\n",
      "epoch 2; batch 39040; loss 0.312608\n",
      "epoch:2; batch 39040; train accuracy: 0.788785\n",
      "epoch 2; batch 39168; loss 0.320329\n",
      "epoch:2; batch 39168; train accuracy: 0.788861\n",
      "epoch 2; batch 39296; loss 0.320229\n",
      "epoch:2; batch 39296; train accuracy: 0.788938\n",
      "epoch 2; batch 39424; loss 0.396606\n",
      "epoch:2; batch 39424; train accuracy: 0.788966\n",
      "epoch 2; batch 39552; loss 0.276459\n",
      "epoch:2; batch 39552; train accuracy: 0.789083\n",
      "epoch 2; batch 39680; loss 0.396478\n",
      "epoch:2; batch 39680; train accuracy: 0.789159\n",
      "epoch 2; batch 39808; loss 0.330637\n",
      "epoch:2; batch 39808; train accuracy: 0.789263\n",
      "epoch 2; batch 39936; loss 0.389308\n",
      "epoch:2; batch 39936; train accuracy: 0.789318\n",
      "epoch 2; batch 40064; loss 0.349768\n",
      "epoch:2; batch 40064; train accuracy: 0.789400\n",
      "epoch 2; batch 40192; loss 0.303566\n",
      "epoch:2; batch 40192; train accuracy: 0.789483\n",
      "epoch 2; batch 40320; loss 0.422469\n",
      "epoch:2; batch 40320; train accuracy: 0.789544\n",
      "epoch 2; batch 40448; loss 0.347923\n",
      "epoch:2; batch 40448; train accuracy: 0.789578\n",
      "epoch 2; batch 40576; loss 0.329627\n",
      "epoch:2; batch 40576; train accuracy: 0.789660\n",
      "epoch 2; batch 40704; loss 0.299787\n",
      "epoch:2; batch 40704; train accuracy: 0.789728\n",
      "epoch 2; batch 40832; loss 0.389432\n",
      "epoch:2; batch 40832; train accuracy: 0.789810\n",
      "epoch 2; batch 40960; loss 0.406310\n",
      "epoch:2; batch 40960; train accuracy: 0.789892\n",
      "epoch 2; batch 41088; loss 0.269945\n",
      "epoch:2; batch 41088; train accuracy: 0.789973\n",
      "epoch 2; batch 41216; loss 0.331491\n",
      "epoch:2; batch 41216; train accuracy: 0.790048\n",
      "epoch 2; batch 41344; loss 0.437671\n",
      "epoch:2; batch 41344; train accuracy: 0.790074\n",
      "epoch 2; batch 41472; loss 0.252798\n",
      "epoch:2; batch 41472; train accuracy: 0.790169\n",
      "epoch 2; batch 41600; loss 0.382146\n",
      "epoch:2; batch 41600; train accuracy: 0.790229\n",
      "epoch 2; batch 41728; loss 0.394178\n",
      "epoch:2; batch 41728; train accuracy: 0.790269\n",
      "epoch 2; batch 41856; loss 0.277619\n",
      "epoch:2; batch 41856; train accuracy: 0.790370\n",
      "epoch 2; batch 41984; loss 0.291669\n",
      "epoch:2; batch 41984; train accuracy: 0.790451\n",
      "epoch 2; batch 42112; loss 0.368132\n",
      "epoch:2; batch 42112; train accuracy: 0.790518\n",
      "epoch 2; batch 42240; loss 0.306244\n",
      "epoch:2; batch 42240; train accuracy: 0.790618\n",
      "epoch 2; batch 42368; loss 0.315612\n",
      "epoch:2; batch 42368; train accuracy: 0.790712\n",
      "epoch 2; batch 42496; loss 0.399125\n",
      "epoch:2; batch 42496; train accuracy: 0.790765\n",
      "epoch 2; batch 42624; loss 0.345032\n",
      "epoch:2; batch 42624; train accuracy: 0.790851\n",
      "epoch 2; batch 42752; loss 0.307360\n",
      "epoch:2; batch 42752; train accuracy: 0.790938\n",
      "epoch 2; batch 42880; loss 0.346203\n",
      "epoch:2; batch 42880; train accuracy: 0.791011\n",
      "epoch 2; batch 43008; loss 0.374954\n",
      "epoch:2; batch 43008; train accuracy: 0.791090\n",
      "epoch 2; batch 43136; loss 0.321456\n",
      "epoch:2; batch 43136; train accuracy: 0.791203\n",
      "epoch 2; batch 43264; loss 0.487862\n",
      "epoch:2; batch 43264; train accuracy: 0.791215\n",
      "epoch 2; batch 43392; loss 0.246280\n",
      "epoch:2; batch 43392; train accuracy: 0.791321\n",
      "epoch 2; batch 43520; loss 0.411207\n",
      "epoch:2; batch 43520; train accuracy: 0.791386\n",
      "epoch 2; batch 43648; loss 0.452664\n",
      "epoch:2; batch 43648; train accuracy: 0.791451\n",
      "epoch 2; batch 43776; loss 0.341943\n",
      "epoch:2; batch 43776; train accuracy: 0.791543\n",
      "epoch 2; batch 43904; loss 0.353682\n",
      "epoch:2; batch 43904; train accuracy: 0.791615\n",
      "epoch 2; batch 44032; loss 0.368694\n",
      "epoch:2; batch 44032; train accuracy: 0.791694\n",
      "epoch 2; batch 44160; loss 0.411284\n",
      "epoch:2; batch 44160; train accuracy: 0.791765\n",
      "epoch 2; batch 44288; loss 0.413336\n",
      "epoch:2; batch 44288; train accuracy: 0.791803\n",
      "epoch 2; batch 44416; loss 0.308604\n",
      "epoch:2; batch 44416; train accuracy: 0.791908\n",
      "epoch 2; batch 44544; loss 0.288994\n",
      "epoch:2; batch 44544; train accuracy: 0.791992\n",
      "epoch 2; batch 44672; loss 0.249284\n",
      "epoch:2; batch 44672; train accuracy: 0.792103\n",
      "epoch 2; batch 44800; loss 0.265941\n",
      "epoch:2; batch 44800; train accuracy: 0.792194\n",
      "epoch 2; batch 44928; loss 0.298250\n",
      "epoch:2; batch 44928; train accuracy: 0.792278\n",
      "epoch 2; batch 45056; loss 0.416806\n",
      "epoch:2; batch 45056; train accuracy: 0.792342\n",
      "epoch 2; batch 45184; loss 0.358901\n",
      "epoch:2; batch 45184; train accuracy: 0.792399\n",
      "epoch 2; batch 45312; loss 0.328924\n",
      "epoch:2; batch 45312; train accuracy: 0.792496\n",
      "epoch 2; batch 45440; loss 0.149999\n",
      "epoch:2; batch 45440; train accuracy: 0.792646\n",
      "epoch 2; batch 45568; loss 0.283465\n",
      "epoch:2; batch 45568; train accuracy: 0.792776\n",
      "epoch 2; batch 45696; loss 0.286611\n",
      "epoch:2; batch 45696; train accuracy: 0.792886\n",
      "epoch 2; batch 45824; loss 0.354386\n",
      "epoch:2; batch 45824; train accuracy: 0.792942\n",
      "epoch 2; batch 45952; loss 0.367480\n",
      "epoch:2; batch 45952; train accuracy: 0.793005\n",
      "epoch 2; batch 46080; loss 0.428629\n",
      "epoch:2; batch 46080; train accuracy: 0.793068\n",
      "epoch 2; batch 46208; loss 0.282367\n",
      "epoch:2; batch 46208; train accuracy: 0.793144\n",
      "epoch 2; batch 46336; loss 0.267317\n",
      "epoch:2; batch 46336; train accuracy: 0.793227\n",
      "epoch 2; batch 46464; loss 0.233136\n",
      "epoch:2; batch 46464; train accuracy: 0.793329\n",
      "epoch 2; batch 46592; loss 0.297230\n",
      "epoch:2; batch 46592; train accuracy: 0.793391\n",
      "epoch 2; batch 46720; loss 0.295852\n",
      "epoch:2; batch 46720; train accuracy: 0.793460\n",
      "epoch 2; batch 46848; loss 0.413686\n",
      "epoch:2; batch 46848; train accuracy: 0.793522\n",
      "epoch 2; batch 46976; loss 0.317772\n",
      "epoch:2; batch 46976; train accuracy: 0.793591\n",
      "epoch 2; batch 47104; loss 0.476806\n",
      "epoch:2; batch 47104; train accuracy: 0.793607\n",
      "epoch 2; batch 47232; loss 0.289847\n",
      "epoch:2; batch 47232; train accuracy: 0.793662\n",
      "epoch 2; batch 47360; loss 0.322475\n",
      "epoch:2; batch 47360; train accuracy: 0.793737\n",
      "epoch 2; batch 47488; loss 0.250234\n",
      "epoch:2; batch 47488; train accuracy: 0.793831\n",
      "epoch 2; batch 47616; loss 0.381165\n",
      "epoch:2; batch 47616; train accuracy: 0.793880\n",
      "epoch 2; batch 47744; loss 0.305146\n",
      "epoch:2; batch 47744; train accuracy: 0.793961\n",
      "epoch 2; batch 47872; loss 0.316621\n",
      "epoch:2; batch 47872; train accuracy: 0.794055\n",
      "epoch 2; batch 48000; loss 0.274252\n",
      "epoch:2; batch 48000; train accuracy: 0.794136\n",
      "epoch 2; batch 48128; loss 0.279055\n",
      "epoch:2; batch 48128; train accuracy: 0.794229\n",
      "epoch 2; batch 48256; loss 0.257325\n",
      "epoch:2; batch 48256; train accuracy: 0.794317\n",
      "epoch 2; batch 48384; loss 0.280092\n",
      "epoch:2; batch 48384; train accuracy: 0.794423\n",
      "epoch 2; batch 48512; loss 0.418168\n",
      "epoch:2; batch 48512; train accuracy: 0.794451\n",
      "epoch 2; batch 48640; loss 0.255714\n",
      "epoch:2; batch 48640; train accuracy: 0.794544\n",
      "epoch 2; batch 48768; loss 0.351193\n",
      "epoch:2; batch 48768; train accuracy: 0.794605\n",
      "epoch 2; batch 48896; loss 0.230365\n",
      "epoch:2; batch 48896; train accuracy: 0.794711\n",
      "epoch 2; batch 49024; loss 0.251306\n",
      "epoch:2; batch 49024; train accuracy: 0.794816\n",
      "epoch 2; batch 49152; loss 0.605041\n",
      "epoch:2; batch 49152; train accuracy: 0.794812\n",
      "epoch 2; batch 49280; loss 0.417684\n",
      "epoch:2; batch 49280; train accuracy: 0.794865\n",
      "epoch 2; batch 49408; loss 0.369720\n",
      "epoch:2; batch 49408; train accuracy: 0.794925\n",
      "epoch 2; batch 49536; loss 0.293129\n",
      "epoch:2; batch 49536; train accuracy: 0.795004\n",
      "epoch 2; batch 49664; loss 0.224964\n",
      "epoch:2; batch 49664; train accuracy: 0.795129\n",
      "epoch 2; batch 49792; loss 0.501728\n",
      "epoch:2; batch 49792; train accuracy: 0.795150\n",
      "epoch 2; batch 49920; loss 0.353900\n",
      "epoch:2; batch 49920; train accuracy: 0.795222\n",
      "epoch 2; batch 50048; loss 0.408674\n",
      "epoch:2; batch 50048; train accuracy: 0.795275\n",
      "epoch 2; batch 50176; loss 0.357974\n",
      "epoch:2; batch 50176; train accuracy: 0.795354\n",
      "epoch 2; batch 50304; loss 0.346486\n",
      "epoch:2; batch 50304; train accuracy: 0.795407\n",
      "epoch 2; batch 50432; loss 0.433562\n",
      "epoch:2; batch 50432; train accuracy: 0.795440\n",
      "epoch 2; batch 50560; loss 0.353594\n",
      "epoch:2; batch 50560; train accuracy: 0.795486\n",
      "epoch 2; batch 50688; loss 0.364394\n",
      "epoch:2; batch 50688; train accuracy: 0.795539\n",
      "epoch 2; batch 50816; loss 0.296667\n",
      "epoch:2; batch 50816; train accuracy: 0.795630\n",
      "epoch 2; batch 50944; loss 0.310993\n",
      "epoch:2; batch 50944; train accuracy: 0.795701\n",
      "epoch 2; batch 51072; loss 0.373958\n",
      "epoch:2; batch 51072; train accuracy: 0.795747\n",
      "epoch 2; batch 51200; loss 0.358750\n",
      "epoch:2; batch 51200; train accuracy: 0.795812\n",
      "epoch 2; batch 51328; loss 0.339286\n",
      "epoch:2; batch 51328; train accuracy: 0.795870\n",
      "epoch 2; batch 51456; loss 0.363037\n",
      "epoch:2; batch 51456; train accuracy: 0.795948\n",
      "epoch 2; batch 51584; loss 0.224839\n",
      "epoch:2; batch 51584; train accuracy: 0.796051\n",
      "epoch 2; batch 51712; loss 0.476096\n",
      "epoch:2; batch 51712; train accuracy: 0.796084\n",
      "epoch 2; batch 51840; loss 0.360114\n",
      "epoch:2; batch 51840; train accuracy: 0.796122\n",
      "epoch 2; batch 51968; loss 0.325444\n",
      "epoch:2; batch 51968; train accuracy: 0.796187\n",
      "epoch 2; batch 52096; loss 0.319726\n",
      "epoch:2; batch 52096; train accuracy: 0.796277\n",
      "epoch 2; batch 52224; loss 0.421130\n",
      "epoch:2; batch 52224; train accuracy: 0.796315\n",
      "epoch 2; batch 52352; loss 0.393293\n",
      "epoch:2; batch 52352; train accuracy: 0.796373\n",
      "epoch 2; batch 52480; loss 0.195086\n",
      "epoch:2; batch 52480; train accuracy: 0.796513\n",
      "epoch 2; batch 52608; loss 0.350485\n",
      "epoch:2; batch 52608; train accuracy: 0.796577\n",
      "epoch 2; batch 52736; loss 0.278338\n",
      "epoch:2; batch 52736; train accuracy: 0.796659\n",
      "epoch 2; batch 52864; loss 0.291347\n",
      "epoch:2; batch 52864; train accuracy: 0.796736\n",
      "epoch 2; batch 52992; loss 0.357622\n",
      "epoch:2; batch 52992; train accuracy: 0.796805\n",
      "epoch 2; batch 53120; loss 0.342696\n",
      "epoch:2; batch 53120; train accuracy: 0.796881\n",
      "epoch 2; batch 53248; loss 0.357538\n",
      "epoch:2; batch 53248; train accuracy: 0.796932\n",
      "epoch 2; batch 53376; loss 0.485178\n",
      "epoch:2; batch 53376; train accuracy: 0.796951\n",
      "epoch 2; batch 53504; loss 0.435694\n",
      "epoch:2; batch 53504; train accuracy: 0.796970\n",
      "epoch 2; batch 53632; loss 0.223483\n",
      "epoch:2; batch 53632; train accuracy: 0.797058\n",
      "epoch 2; batch 53760; loss 0.316140\n",
      "epoch:2; batch 53760; train accuracy: 0.797146\n",
      "epoch 2; batch 53888; loss 0.283551\n",
      "epoch:2; batch 53888; train accuracy: 0.797240\n",
      "epoch 2; batch 54016; loss 0.324842\n",
      "epoch:2; batch 54016; train accuracy: 0.797303\n",
      "epoch 2; batch 54144; loss 0.556873\n",
      "epoch:2; batch 54144; train accuracy: 0.797334\n",
      "epoch 2; batch 54272; loss 0.349744\n",
      "epoch:2; batch 54272; train accuracy: 0.797396\n",
      "epoch 2; batch 54400; loss 0.371484\n",
      "epoch:2; batch 54400; train accuracy: 0.797446\n",
      "epoch 2; batch 54528; loss 0.333697\n",
      "epoch:2; batch 54528; train accuracy: 0.797515\n",
      "epoch 2; batch 54656; loss 0.361636\n",
      "epoch:2; batch 54656; train accuracy: 0.797577\n",
      "epoch 2; batch 54784; loss 0.222030\n",
      "epoch:2; batch 54784; train accuracy: 0.797676\n",
      "epoch 2; batch 54912; loss 0.315016\n",
      "epoch:2; batch 54912; train accuracy: 0.797763\n",
      "epoch 2; batch 55040; loss 0.472502\n",
      "epoch:2; batch 55040; train accuracy: 0.797800\n",
      "epoch 2; batch 55168; loss 0.354567\n",
      "epoch:2; batch 55168; train accuracy: 0.797874\n",
      "epoch 2; batch 55296; loss 0.259470\n",
      "epoch:2; batch 55296; train accuracy: 0.797973\n",
      "epoch 2; batch 55424; loss 0.286150\n",
      "epoch:2; batch 55424; train accuracy: 0.798028\n",
      "epoch 2; batch 55552; loss 0.322931\n",
      "epoch:2; batch 55552; train accuracy: 0.798102\n",
      "epoch 2; batch 55680; loss 0.274946\n",
      "epoch:2; batch 55680; train accuracy: 0.798176\n",
      "epoch 2; batch 55808; loss 0.304215\n",
      "epoch:2; batch 55808; train accuracy: 0.798237\n",
      "epoch 2; batch 55936; loss 0.427674\n",
      "epoch:2; batch 55936; train accuracy: 0.798273\n",
      "epoch 2; batch 56064; loss 0.437210\n",
      "epoch:2; batch 56064; train accuracy: 0.798310\n",
      "epoch 2; batch 56192; loss 0.252503\n",
      "epoch:2; batch 56192; train accuracy: 0.798383\n",
      "epoch 2; batch 56320; loss 0.291025\n",
      "epoch:2; batch 56320; train accuracy: 0.798469\n",
      "epoch 2; batch 56448; loss 0.381822\n",
      "epoch:2; batch 56448; train accuracy: 0.798548\n",
      "epoch 2; batch 56576; loss 0.349369\n",
      "epoch:2; batch 56576; train accuracy: 0.798615\n",
      "epoch 2; batch 56704; loss 0.454999\n",
      "epoch:2; batch 56704; train accuracy: 0.798656\n",
      "epoch 2; batch 56832; loss 0.349998\n",
      "epoch:2; batch 56832; train accuracy: 0.798729\n",
      "epoch 2; batch 56960; loss 0.342077\n",
      "epoch:2; batch 56960; train accuracy: 0.798790\n",
      "epoch 2; batch 57088; loss 0.338309\n",
      "epoch:2; batch 57088; train accuracy: 0.798874\n",
      "epoch 2; batch 57216; loss 0.397007\n",
      "epoch:2; batch 57216; train accuracy: 0.798928\n",
      "epoch 2; batch 57344; loss 0.461964\n",
      "epoch:2; batch 57344; train accuracy: 0.798970\n",
      "epoch 2; batch 57472; loss 0.373608\n",
      "epoch:2; batch 57472; train accuracy: 0.799036\n",
      "epoch 2; batch 57600; loss 0.420252\n",
      "epoch:2; batch 57600; train accuracy: 0.799090\n",
      "epoch 2; batch 57728; loss 0.372302\n",
      "epoch:2; batch 57728; train accuracy: 0.799125\n",
      "epoch 2; batch 57856; loss 0.285500\n",
      "epoch:2; batch 57856; train accuracy: 0.799178\n",
      "epoch 2; batch 57984; loss 0.377965\n",
      "epoch:2; batch 57984; train accuracy: 0.799226\n",
      "epoch 2; batch 58112; loss 0.359296\n",
      "epoch:2; batch 58112; train accuracy: 0.799309\n",
      "epoch 2; batch 58240; loss 0.367448\n",
      "epoch:2; batch 58240; train accuracy: 0.799369\n",
      "epoch 2; batch 58368; loss 0.338537\n",
      "epoch:2; batch 58368; train accuracy: 0.799434\n",
      "epoch 2; batch 58496; loss 0.394342\n",
      "epoch:2; batch 58496; train accuracy: 0.799475\n",
      "epoch 2; batch 58624; loss 0.249386\n",
      "epoch:2; batch 58624; train accuracy: 0.799559\n",
      "epoch 2; batch 58752; loss 0.340999\n",
      "epoch:2; batch 58752; train accuracy: 0.799612\n",
      "epoch 2; batch 58880; loss 0.304883\n",
      "epoch:2; batch 58880; train accuracy: 0.799683\n",
      "epoch 2; batch 59008; loss 0.276136\n",
      "epoch:2; batch 59008; train accuracy: 0.799760\n",
      "epoch 2; batch 59136; loss 0.283986\n",
      "epoch:2; batch 59136; train accuracy: 0.799831\n",
      "epoch 2; batch 59264; loss 0.355052\n",
      "epoch:2; batch 59264; train accuracy: 0.799895\n",
      "epoch 2; batch 59392; loss 0.330937\n",
      "epoch:2; batch 59392; train accuracy: 0.799966\n",
      "epoch 2; batch 59520; loss 0.269511\n",
      "epoch:2; batch 59520; train accuracy: 0.800043\n",
      "epoch 2; batch 59648; loss 0.360090\n",
      "epoch:2; batch 59648; train accuracy: 0.800107\n",
      "epoch 2; batch 59776; loss 0.309844\n",
      "epoch:2; batch 59776; train accuracy: 0.800183\n",
      "epoch 2; batch 59904; loss 0.334125\n",
      "epoch:2; batch 59904; train accuracy: 0.800229\n",
      "epoch 2; batch 60032; loss 0.334989\n",
      "epoch:2; batch 60032; train accuracy: 0.800287\n",
      "epoch 2; batch 60160; loss 0.248881\n",
      "epoch:2; batch 60160; train accuracy: 0.800375\n",
      "epoch 2; batch 60288; loss 0.387673\n",
      "epoch:2; batch 60288; train accuracy: 0.800409\n",
      "epoch 2; batch 60416; loss 0.385581\n",
      "epoch:2; batch 60416; train accuracy: 0.800479\n",
      "epoch 2; batch 60544; loss 0.396076\n",
      "epoch:2; batch 60544; train accuracy: 0.800530\n",
      "epoch 2; batch 60672; loss 0.443950\n",
      "epoch:2; batch 60672; train accuracy: 0.800558\n",
      "epoch 2; batch 60800; loss 0.286920\n",
      "epoch:2; batch 60800; train accuracy: 0.800646\n",
      "epoch 2; batch 60928; loss 0.319856\n",
      "epoch:2; batch 60928; train accuracy: 0.800727\n",
      "epoch 2; batch 61056; loss 0.374775\n",
      "epoch:2; batch 61056; train accuracy: 0.800790\n",
      "epoch 2; batch 61184; loss 0.393364\n",
      "epoch:2; batch 61184; train accuracy: 0.800823\n",
      "epoch 2; batch 61312; loss 0.299026\n",
      "epoch:2; batch 61312; train accuracy: 0.800917\n",
      "epoch 2; batch 61440; loss 0.337413\n",
      "epoch:2; batch 61440; train accuracy: 0.800986\n",
      "epoch 2; batch 61568; loss 0.363542\n",
      "epoch:2; batch 61568; train accuracy: 0.801060\n",
      "epoch 2; batch 61696; loss 0.285175\n",
      "epoch:2; batch 61696; train accuracy: 0.801123\n",
      "epoch 2; batch 61824; loss 0.374770\n",
      "epoch:2; batch 61824; train accuracy: 0.801198\n",
      "epoch 2; batch 61952; loss 0.376762\n",
      "epoch:2; batch 61952; train accuracy: 0.801243\n",
      "epoch 2; batch 62080; loss 0.409441\n",
      "epoch:2; batch 62080; train accuracy: 0.801275\n",
      "epoch 2; batch 62208; loss 0.487998\n",
      "epoch:2; batch 62208; train accuracy: 0.801302\n",
      "epoch 2; batch 62336; loss 0.386853\n",
      "epoch:2; batch 62336; train accuracy: 0.801340\n",
      "epoch 2; batch 62464; loss 0.283221\n",
      "epoch:2; batch 62464; train accuracy: 0.801402\n",
      "epoch 2; batch 62592; loss 0.378169\n",
      "epoch:2; batch 62592; train accuracy: 0.801447\n",
      "epoch 2; batch 62720; loss 0.323450\n",
      "epoch:2; batch 62720; train accuracy: 0.801509\n",
      "epoch 2; batch 62848; loss 0.364155\n",
      "epoch:2; batch 62848; train accuracy: 0.801571\n",
      "epoch 2; batch 62976; loss 0.301087\n",
      "epoch:2; batch 62976; train accuracy: 0.801639\n",
      "epoch 2; batch 63104; loss 0.311047\n",
      "epoch:2; batch 63104; train accuracy: 0.801701\n",
      "epoch 2; batch 63232; loss 0.340416\n",
      "epoch:2; batch 63232; train accuracy: 0.801744\n",
      "epoch 2; batch 63360; loss 0.250346\n",
      "epoch:2; batch 63360; train accuracy: 0.801824\n",
      "epoch 2; batch 63488; loss 0.444888\n",
      "epoch:2; batch 63488; train accuracy: 0.801856\n",
      "epoch 2; batch 63616; loss 0.384046\n",
      "epoch:2; batch 63616; train accuracy: 0.801923\n",
      "epoch 2; batch 63744; loss 0.382311\n",
      "epoch:2; batch 63744; train accuracy: 0.801949\n",
      "epoch 2; batch 63872; loss 0.346849\n",
      "epoch:2; batch 63872; train accuracy: 0.801998\n",
      "epoch 2; batch 64000; loss 0.403362\n",
      "epoch:2; batch 64000; train accuracy: 0.802030\n",
      "epoch 2; batch 64128; loss 0.416161\n",
      "epoch:2; batch 64128; train accuracy: 0.802044\n",
      "epoch 2; batch 64256; loss 0.287937\n",
      "epoch:2; batch 64256; train accuracy: 0.802123\n",
      "epoch 2; batch 64384; loss 0.291874\n",
      "epoch:2; batch 64384; train accuracy: 0.802196\n",
      "epoch 2; batch 64512; loss 0.401309\n",
      "epoch:2; batch 64512; train accuracy: 0.802245\n",
      "epoch 2; batch 64640; loss 0.343627\n",
      "epoch:2; batch 64640; train accuracy: 0.802299\n",
      "epoch 2; batch 64768; loss 0.535389\n",
      "epoch:2; batch 64768; train accuracy: 0.802337\n",
      "epoch 2; batch 64896; loss 0.349669\n",
      "epoch:2; batch 64896; train accuracy: 0.802403\n",
      "epoch 2; batch 65024; loss 0.251628\n",
      "epoch:2; batch 65024; train accuracy: 0.802481\n",
      "epoch 2; batch 65152; loss 0.384152\n",
      "epoch:2; batch 65152; train accuracy: 0.802518\n",
      "epoch 2; batch 65280; loss 0.362695\n",
      "epoch:2; batch 65280; train accuracy: 0.802567\n",
      "epoch 2; batch 65408; loss 0.308360\n",
      "epoch:2; batch 65408; train accuracy: 0.802627\n",
      "epoch 2; batch 65536; loss 0.324870\n",
      "epoch:2; batch 65536; train accuracy: 0.802670\n",
      "epoch 2; batch 65664; loss 0.353272\n",
      "epoch:2; batch 65664; train accuracy: 0.802701\n",
      "epoch 2; batch 65792; loss 0.488536\n",
      "epoch:2; batch 65792; train accuracy: 0.802685\n",
      "epoch 2; batch 65920; loss 0.311485\n",
      "epoch:2; batch 65920; train accuracy: 0.802774\n",
      "epoch 2; batch 66048; loss 0.401443\n",
      "epoch:2; batch 66048; train accuracy: 0.802822\n",
      "epoch 2; batch 66176; loss 0.404759\n",
      "epoch:2; batch 66176; train accuracy: 0.802876\n",
      "epoch 2; batch 66304; loss 0.246890\n",
      "epoch:2; batch 66304; train accuracy: 0.802988\n",
      "epoch 2; batch 66432; loss 0.400141\n",
      "epoch:2; batch 66432; train accuracy: 0.803042\n",
      "epoch 2; batch 66560; loss 0.333570\n",
      "epoch:2; batch 66560; train accuracy: 0.803078\n",
      "epoch 2; batch 66688; loss 0.227283\n",
      "epoch:2; batch 66688; train accuracy: 0.803161\n",
      "epoch 2; batch 66816; loss 0.297517\n",
      "epoch:2; batch 66816; train accuracy: 0.803220\n",
      "epoch 2; batch 66944; loss 0.355585\n",
      "epoch:2; batch 66944; train accuracy: 0.803262\n",
      "epoch 2; batch 67072; loss 0.246674\n",
      "epoch:2; batch 67072; train accuracy: 0.803356\n",
      "epoch 2; batch 67200; loss 0.237753\n",
      "epoch:2; batch 67200; train accuracy: 0.803450\n",
      "epoch 2; batch 67328; loss 0.383197\n",
      "epoch:2; batch 67328; train accuracy: 0.803480\n",
      "epoch 2; batch 67456; loss 0.330983\n",
      "epoch:2; batch 67456; train accuracy: 0.803551\n",
      "epoch 2; batch 67584; loss 0.323993\n",
      "epoch:2; batch 67584; train accuracy: 0.803621\n",
      "epoch 2; batch 67712; loss 0.469139\n",
      "epoch:2; batch 67712; train accuracy: 0.803633\n",
      "epoch 2; batch 67840; loss 0.330592\n",
      "epoch:2; batch 67840; train accuracy: 0.803692\n",
      "epoch 2; batch 67968; loss 0.262242\n",
      "epoch:2; batch 67968; train accuracy: 0.803768\n",
      "epoch 2; batch 68096; loss 0.219016\n",
      "epoch:2; batch 68096; train accuracy: 0.803850\n",
      "epoch 2; batch 68224; loss 0.358021\n",
      "epoch:2; batch 68224; train accuracy: 0.803902\n",
      "epoch 2; batch 68352; loss 0.396408\n",
      "epoch:2; batch 68352; train accuracy: 0.803943\n",
      "epoch 2; batch 68480; loss 0.250339\n",
      "epoch:2; batch 68480; train accuracy: 0.804030\n",
      "epoch 2; batch 68608; loss 0.471219\n",
      "epoch:2; batch 68608; train accuracy: 0.804054\n",
      "epoch 2; batch 68736; loss 0.295359\n",
      "epoch:2; batch 68736; train accuracy: 0.804112\n",
      "epoch 2; batch 68864; loss 0.350321\n",
      "epoch:2; batch 68864; train accuracy: 0.804170\n",
      "epoch 2; batch 68992; loss 0.331808\n",
      "epoch:2; batch 68992; train accuracy: 0.804233\n",
      "epoch 2; batch 69120; loss 0.497253\n",
      "epoch:2; batch 69120; train accuracy: 0.804257\n",
      "epoch 2; batch 69248; loss 0.315850\n",
      "epoch:2; batch 69248; train accuracy: 0.804314\n",
      "epoch 2; batch 69376; loss 0.301254\n",
      "epoch:2; batch 69376; train accuracy: 0.804389\n",
      "epoch 2; batch 69504; loss 0.393749\n",
      "epoch:2; batch 69504; train accuracy: 0.804430\n",
      "epoch 2; batch 69632; loss 0.339851\n",
      "epoch:2; batch 69632; train accuracy: 0.804476\n",
      "epoch 2; batch 69760; loss 0.409806\n",
      "epoch:2; batch 69760; train accuracy: 0.804527\n",
      "epoch 2; batch 69888; loss 0.333823\n",
      "epoch:2; batch 69888; train accuracy: 0.804602\n",
      "epoch 2; batch 70016; loss 0.484207\n",
      "epoch:2; batch 70016; train accuracy: 0.804625\n",
      "epoch 2; batch 70144; loss 0.322591\n",
      "epoch:2; batch 70144; train accuracy: 0.804676\n",
      "epoch 2; batch 70272; loss 0.362920\n",
      "epoch:2; batch 70272; train accuracy: 0.804716\n",
      "epoch 2; batch 70400; loss 0.376033\n",
      "epoch:2; batch 70400; train accuracy: 0.804762\n",
      "epoch 2; batch 70528; loss 0.342466\n",
      "epoch:2; batch 70528; train accuracy: 0.804807\n",
      "epoch 2; batch 70656; loss 0.300205\n",
      "epoch:2; batch 70656; train accuracy: 0.804853\n",
      "epoch 2; batch 70784; loss 0.232913\n",
      "epoch:2; batch 70784; train accuracy: 0.804926\n",
      "epoch 2; batch 70912; loss 0.311176\n",
      "epoch:2; batch 70912; train accuracy: 0.804983\n",
      "epoch 2; batch 71040; loss 0.212196\n",
      "epoch:2; batch 71040; train accuracy: 0.805062\n",
      "epoch 2; batch 71168; loss 0.278904\n",
      "epoch:2; batch 71168; train accuracy: 0.805096\n",
      "epoch 2; batch 71296; loss 0.398400\n",
      "epoch:2; batch 71296; train accuracy: 0.805130\n",
      "epoch 2; batch 71424; loss 0.311447\n",
      "epoch:2; batch 71424; train accuracy: 0.805198\n",
      "epoch 2; batch 71552; loss 0.361861\n",
      "epoch:2; batch 71552; train accuracy: 0.805214\n",
      "epoch 2; batch 71680; loss 0.232078\n",
      "epoch:2; batch 71680; train accuracy: 0.805299\n",
      "epoch 2; batch 71808; loss 0.252182\n",
      "epoch:2; batch 71808; train accuracy: 0.805378\n",
      "epoch 2; batch 71936; loss 0.353895\n",
      "epoch:2; batch 71936; train accuracy: 0.805428\n",
      "epoch 2; batch 72064; loss 0.468900\n",
      "epoch:2; batch 72064; train accuracy: 0.805450\n",
      "epoch 2; batch 72192; loss 0.322829\n",
      "epoch:2; batch 72192; train accuracy: 0.805506\n",
      "epoch 2; batch 72320; loss 0.278137\n",
      "epoch:2; batch 72320; train accuracy: 0.805579\n",
      "epoch 2; batch 72448; loss 0.268749\n",
      "epoch:2; batch 72448; train accuracy: 0.805634\n",
      "epoch 2; batch 72576; loss 0.242133\n",
      "epoch:2; batch 72576; train accuracy: 0.805718\n",
      "epoch 2; batch 72704; loss 0.556545\n",
      "epoch:2; batch 72704; train accuracy: 0.805718\n",
      "epoch 2; batch 72832; loss 0.303314\n",
      "epoch:2; batch 72832; train accuracy: 0.805796\n",
      "epoch 2; batch 72960; loss 0.314909\n",
      "epoch:2; batch 72960; train accuracy: 0.805834\n",
      "epoch 2; batch 73088; loss 0.247622\n",
      "epoch:2; batch 73088; train accuracy: 0.805940\n",
      "epoch 2; batch 73216; loss 0.255561\n",
      "epoch:2; batch 73216; train accuracy: 0.806035\n",
      "epoch 2; batch 73344; loss 0.422634\n",
      "epoch:2; batch 73344; train accuracy: 0.806050\n",
      "epoch 2; batch 73472; loss 0.436653\n",
      "epoch:2; batch 73472; train accuracy: 0.806128\n",
      "epoch 2; batch 73600; loss 0.329792\n",
      "epoch:2; batch 73600; train accuracy: 0.806205\n",
      "epoch 2; batch 73728; loss 0.292607\n",
      "epoch:2; batch 73728; train accuracy: 0.806277\n",
      "epoch 2; batch 73856; loss 0.338406\n",
      "epoch:2; batch 73856; train accuracy: 0.806326\n",
      "epoch 2; batch 73984; loss 0.302800\n",
      "epoch:2; batch 73984; train accuracy: 0.806375\n",
      "epoch 2; batch 74112; loss 0.272631\n",
      "epoch:2; batch 74112; train accuracy: 0.806441\n",
      "epoch 2; batch 74240; loss 0.225547\n",
      "epoch:2; batch 74240; train accuracy: 0.806535\n",
      "epoch 2; batch 74368; loss 0.378165\n",
      "epoch:2; batch 74368; train accuracy: 0.806567\n",
      "epoch 2; batch 74496; loss 0.291059\n",
      "epoch:2; batch 74496; train accuracy: 0.806643\n",
      "epoch 2; batch 74624; loss 0.311778\n",
      "epoch:2; batch 74624; train accuracy: 0.806703\n",
      "epoch 2; batch 74752; loss 0.236178\n",
      "epoch:2; batch 74752; train accuracy: 0.806796\n",
      "epoch 2; batch 74880; loss 0.272339\n",
      "epoch:2; batch 74880; train accuracy: 0.806856\n",
      "epoch 2; batch 75008; loss 0.314910\n",
      "epoch:2; batch 75008; train accuracy: 0.806921\n",
      "epoch 2; batch 75136; loss 0.229034\n",
      "epoch:2; batch 75136; train accuracy: 0.806997\n",
      "epoch 2; batch 75264; loss 0.321537\n",
      "epoch:2; batch 75264; train accuracy: 0.807046\n",
      "epoch 2; batch 75392; loss 0.316688\n",
      "epoch:2; batch 75392; train accuracy: 0.807094\n",
      "epoch 2; batch 75520; loss 0.307798\n",
      "epoch:2; batch 75520; train accuracy: 0.807148\n",
      "epoch 2; batch 75648; loss 0.348930\n",
      "epoch:2; batch 75648; train accuracy: 0.807190\n",
      "epoch 2; batch 75776; loss 0.160481\n",
      "epoch:2; batch 75776; train accuracy: 0.807271\n",
      "epoch 2; batch 75904; loss 0.318404\n",
      "epoch:2; batch 75904; train accuracy: 0.807336\n",
      "epoch 2; batch 76032; loss 0.268682\n",
      "epoch:2; batch 76032; train accuracy: 0.807406\n",
      "epoch 2; batch 76160; loss 0.259327\n",
      "epoch:2; batch 76160; train accuracy: 0.807481\n",
      "epoch 2; batch 76288; loss 0.292266\n",
      "epoch:2; batch 76288; train accuracy: 0.807540\n",
      "epoch 2; batch 76416; loss 0.323423\n",
      "epoch:2; batch 76416; train accuracy: 0.807577\n",
      "epoch 2; batch 76544; loss 0.241213\n",
      "epoch:2; batch 76544; train accuracy: 0.807635\n",
      "epoch 2; batch 76672; loss 0.280440\n",
      "epoch:2; batch 76672; train accuracy: 0.807677\n",
      "epoch 2; batch 76800; loss 0.429453\n",
      "epoch:2; batch 76800; train accuracy: 0.807730\n",
      "epoch 2; batch 76928; loss 0.305730\n",
      "epoch:2; batch 76928; train accuracy: 0.807772\n",
      "epoch 2; batch 77056; loss 0.330672\n",
      "epoch:2; batch 77056; train accuracy: 0.807814\n",
      "epoch 2; batch 77184; loss 0.278015\n",
      "epoch:2; batch 77184; train accuracy: 0.807877\n",
      "epoch 2; batch 77312; loss 0.403458\n",
      "epoch:2; batch 77312; train accuracy: 0.807941\n",
      "epoch 2; batch 77440; loss 0.323218\n",
      "epoch:2; batch 77440; train accuracy: 0.807988\n",
      "epoch 2; batch 77568; loss 0.368894\n",
      "epoch:2; batch 77568; train accuracy: 0.808040\n",
      "epoch 2; batch 77696; loss 0.299741\n",
      "epoch:2; batch 77696; train accuracy: 0.808087\n",
      "epoch 2; batch 77824; loss 0.383362\n",
      "epoch:2; batch 77824; train accuracy: 0.808118\n",
      "epoch 2; batch 77952; loss 0.321118\n",
      "epoch:2; batch 77952; train accuracy: 0.808165\n",
      "epoch 2; batch 78080; loss 0.359256\n",
      "epoch:2; batch 78080; train accuracy: 0.808189\n",
      "epoch 2; batch 78208; loss 0.227160\n",
      "epoch:2; batch 78208; train accuracy: 0.808280\n",
      "epoch 2; batch 78336; loss 0.347281\n",
      "epoch:2; batch 78336; train accuracy: 0.808337\n",
      "epoch 2; batch 78464; loss 0.392323\n",
      "epoch:2; batch 78464; train accuracy: 0.808384\n",
      "epoch 2; batch 78592; loss 0.423952\n",
      "epoch:2; batch 78592; train accuracy: 0.808425\n",
      "epoch 2; batch 78720; loss 0.330507\n",
      "epoch:2; batch 78720; train accuracy: 0.808460\n",
      "epoch 2; batch 78848; loss 0.295468\n",
      "epoch:2; batch 78848; train accuracy: 0.808534\n",
      "epoch 2; batch 78976; loss 0.306125\n",
      "epoch:2; batch 78976; train accuracy: 0.808596\n",
      "epoch 2; batch 79104; loss 0.273919\n",
      "epoch:2; batch 79104; train accuracy: 0.808659\n",
      "epoch 2; batch 79232; loss 0.282575\n",
      "epoch:2; batch 79232; train accuracy: 0.808732\n",
      "epoch 2; batch 79360; loss 0.421301\n",
      "epoch:2; batch 79360; train accuracy: 0.808767\n",
      "epoch 2; batch 79488; loss 0.300704\n",
      "epoch:2; batch 79488; train accuracy: 0.808824\n",
      "epoch 2; batch 79616; loss 0.302137\n",
      "epoch:2; batch 79616; train accuracy: 0.808881\n",
      "epoch 2; batch 79744; loss 0.287488\n",
      "epoch:2; batch 79744; train accuracy: 0.808948\n",
      "epoch 2; batch 79872; loss 0.316935\n",
      "epoch:2; batch 79872; train accuracy: 0.809000\n",
      "epoch 2; batch 80000; loss 0.243278\n",
      "epoch:2; batch 80000; train accuracy: 0.809067\n",
      "epoch 2; batch 80128; loss 0.410420\n",
      "epoch:2; batch 80128; train accuracy: 0.809123\n",
      "epoch 2; batch 80256; loss 0.379368\n",
      "epoch:2; batch 80256; train accuracy: 0.809169\n",
      "epoch 2; batch 80384; loss 0.339766\n",
      "epoch:2; batch 80384; train accuracy: 0.809203\n",
      "epoch 2; batch 80512; loss 0.333738\n",
      "epoch:2; batch 80512; train accuracy: 0.809265\n",
      "epoch 2; batch 80640; loss 0.379353\n",
      "epoch:2; batch 80640; train accuracy: 0.809305\n",
      "epoch 2; batch 80768; loss 0.287542\n",
      "epoch:2; batch 80768; train accuracy: 0.809372\n",
      "epoch 2; batch 80896; loss 0.406285\n",
      "epoch:2; batch 80896; train accuracy: 0.809395\n",
      "epoch 2; batch 81024; loss 0.323875\n",
      "epoch:2; batch 81024; train accuracy: 0.809446\n",
      "epoch 2; batch 81152; loss 0.347379\n",
      "epoch:2; batch 81152; train accuracy: 0.809523\n",
      "epoch 2; batch 81280; loss 0.295375\n",
      "epoch:2; batch 81280; train accuracy: 0.809595\n",
      "epoch 2; batch 81408; loss 0.354889\n",
      "epoch:2; batch 81408; train accuracy: 0.809629\n",
      "epoch 2; batch 81536; loss 0.205407\n",
      "epoch:2; batch 81536; train accuracy: 0.809706\n",
      "epoch 2; batch 81664; loss 0.279128\n",
      "epoch:2; batch 81664; train accuracy: 0.809757\n",
      "epoch 2; batch 81792; loss 0.301341\n",
      "epoch:2; batch 81792; train accuracy: 0.809817\n",
      "epoch 2; batch 81920; loss 0.289295\n",
      "epoch:2; batch 81920; train accuracy: 0.809883\n",
      "epoch 2; batch 82048; loss 0.366146\n",
      "epoch:2; batch 82048; train accuracy: 0.809917\n",
      "epoch 2; batch 82176; loss 0.268449\n",
      "epoch:2; batch 82176; train accuracy: 0.809983\n",
      "epoch 2; batch 82304; loss 0.218599\n",
      "epoch:2; batch 82304; train accuracy: 0.810070\n",
      "epoch 2; batch 82432; loss 0.283763\n",
      "epoch:2; batch 82432; train accuracy: 0.810125\n",
      "epoch 2; batch 82560; loss 0.251417\n",
      "epoch:2; batch 82560; train accuracy: 0.810191\n",
      "epoch 2; batch 82688; loss 0.220179\n",
      "epoch:2; batch 82688; train accuracy: 0.810278\n",
      "epoch 2; batch 82816; loss 0.264156\n",
      "epoch:2; batch 82816; train accuracy: 0.810349\n",
      "epoch 2; batch 82944; loss 0.327813\n",
      "epoch:2; batch 82944; train accuracy: 0.810393\n",
      "epoch 2; batch 83072; loss 0.464842\n",
      "epoch:2; batch 83072; train accuracy: 0.810410\n",
      "epoch 2; batch 83200; loss 0.243230\n",
      "epoch:2; batch 83200; train accuracy: 0.810475\n",
      "epoch 2; batch 83328; loss 0.405173\n",
      "epoch:2; batch 83328; train accuracy: 0.810498\n",
      "epoch 2; batch 83456; loss 0.328080\n",
      "epoch:2; batch 83456; train accuracy: 0.810536\n",
      "epoch 2; batch 83584; loss 0.386612\n",
      "epoch:2; batch 83584; train accuracy: 0.810575\n",
      "epoch 2; batch 83712; loss 0.310510\n",
      "epoch:2; batch 83712; train accuracy: 0.810618\n",
      "epoch 2; batch 83840; loss 0.223551\n",
      "epoch:2; batch 83840; train accuracy: 0.810694\n",
      "epoch 2; batch 83968; loss 0.282508\n",
      "epoch:2; batch 83968; train accuracy: 0.810753\n",
      "epoch 2; batch 84096; loss 0.342828\n",
      "epoch:2; batch 84096; train accuracy: 0.810786\n",
      "epoch 2; batch 84224; loss 0.253654\n",
      "epoch:2; batch 84224; train accuracy: 0.810856\n",
      "epoch 2; batch 84352; loss 0.222387\n",
      "epoch:2; batch 84352; train accuracy: 0.810931\n",
      "epoch 2; batch 84480; loss 0.444600\n",
      "epoch:2; batch 84480; train accuracy: 0.810964\n",
      "epoch 2; batch 84608; loss 0.240480\n",
      "epoch:2; batch 84608; train accuracy: 0.811049\n",
      "epoch 2; batch 84736; loss 0.306974\n",
      "epoch:2; batch 84736; train accuracy: 0.811129\n",
      "epoch 2; batch 84864; loss 0.200839\n",
      "epoch:2; batch 84864; train accuracy: 0.811215\n",
      "epoch 2; batch 84992; loss 0.230376\n",
      "epoch:2; batch 84992; train accuracy: 0.811279\n",
      "epoch 2; batch 85120; loss 0.345723\n",
      "epoch:2; batch 85120; train accuracy: 0.811316\n",
      "epoch 2; batch 85248; loss 0.183275\n",
      "epoch:2; batch 85248; train accuracy: 0.811401\n",
      "epoch 2; batch 85376; loss 0.336378\n",
      "epoch:2; batch 85376; train accuracy: 0.811465\n",
      "epoch 2; batch 85504; loss 0.284085\n",
      "epoch:2; batch 85504; train accuracy: 0.811513\n",
      "epoch 2; batch 85632; loss 0.215807\n",
      "epoch:2; batch 85632; train accuracy: 0.811582\n",
      "epoch 2; batch 85760; loss 0.426061\n",
      "epoch:2; batch 85760; train accuracy: 0.811619\n",
      "epoch 2; batch 85888; loss 0.212023\n",
      "epoch:2; batch 85888; train accuracy: 0.811683\n",
      "epoch 2; batch 86016; loss 0.414735\n",
      "epoch:2; batch 86016; train accuracy: 0.811704\n",
      "epoch 2; batch 86144; loss 0.281909\n",
      "epoch:2; batch 86144; train accuracy: 0.811757\n",
      "epoch 2; batch 86272; loss 0.380773\n",
      "epoch:2; batch 86272; train accuracy: 0.811794\n",
      "epoch 2; batch 86400; loss 0.362473\n",
      "epoch:2; batch 86400; train accuracy: 0.811831\n",
      "epoch 2; batch 86528; loss 0.326555\n",
      "epoch:2; batch 86528; train accuracy: 0.811884\n",
      "epoch 2; batch 86656; loss 0.310341\n",
      "epoch:2; batch 86656; train accuracy: 0.811952\n",
      "epoch 2; batch 86784; loss 0.325942\n",
      "epoch:2; batch 86784; train accuracy: 0.812010\n",
      "epoch 2; batch 86912; loss 0.239461\n",
      "epoch:2; batch 86912; train accuracy: 0.812067\n",
      "epoch 2; batch 87040; loss 0.286090\n",
      "epoch:2; batch 87040; train accuracy: 0.812125\n",
      "epoch 2; batch 87168; loss 0.276255\n",
      "epoch:2; batch 87168; train accuracy: 0.812188\n",
      "epoch 2; batch 87296; loss 0.181233\n",
      "epoch:2; batch 87296; train accuracy: 0.812282\n",
      "epoch 2; batch 87424; loss 0.279937\n",
      "epoch:2; batch 87424; train accuracy: 0.812349\n",
      "epoch 2; batch 87552; loss 0.368912\n",
      "epoch:2; batch 87552; train accuracy: 0.812396\n",
      "epoch 2; batch 87680; loss 0.494159\n",
      "epoch:2; batch 87680; train accuracy: 0.812412\n",
      "epoch 2; batch 87808; loss 0.377324\n",
      "epoch:2; batch 87808; train accuracy: 0.812443\n",
      "epoch 2; batch 87936; loss 0.384759\n",
      "epoch:2; batch 87936; train accuracy: 0.812479\n",
      "epoch 2; batch 88064; loss 0.342173\n",
      "epoch:2; batch 88064; train accuracy: 0.812510\n",
      "epoch 2; batch 88192; loss 0.344740\n",
      "epoch:2; batch 88192; train accuracy: 0.812567\n",
      "epoch 2; batch 88320; loss 0.421522\n",
      "epoch:2; batch 88320; train accuracy: 0.812593\n",
      "epoch 2; batch 88448; loss 0.326622\n",
      "epoch:2; batch 88448; train accuracy: 0.812629\n",
      "epoch 2; batch 88576; loss 0.353980\n",
      "epoch:2; batch 88576; train accuracy: 0.812676\n",
      "epoch 2; batch 88704; loss 0.296278\n",
      "epoch:2; batch 88704; train accuracy: 0.812738\n",
      "epoch 2; batch 88832; loss 0.344520\n",
      "epoch:2; batch 88832; train accuracy: 0.812789\n",
      "epoch 2; batch 88960; loss 0.274263\n",
      "epoch:2; batch 88960; train accuracy: 0.812840\n",
      "epoch 2; batch 89088; loss 0.381903\n",
      "epoch:2; batch 89088; train accuracy: 0.812876\n",
      "epoch 2; batch 89216; loss 0.328397\n",
      "epoch:2; batch 89216; train accuracy: 0.812922\n",
      "epoch 2; batch 89344; loss 0.213564\n",
      "epoch:2; batch 89344; train accuracy: 0.812994\n",
      "epoch 2; batch 89472; loss 0.399536\n",
      "epoch:2; batch 89472; train accuracy: 0.813030\n",
      "epoch 2; batch 89600; loss 0.377163\n",
      "epoch:2; batch 89600; train accuracy: 0.813055\n",
      "epoch 2; batch 89728; loss 0.305465\n",
      "epoch:2; batch 89728; train accuracy: 0.813127\n",
      "epoch 2; batch 89856; loss 0.197365\n",
      "epoch:2; batch 89856; train accuracy: 0.813203\n",
      "epoch 2; batch 89984; loss 0.254790\n",
      "epoch:2; batch 89984; train accuracy: 0.813269\n",
      "epoch 2; batch 90112; loss 0.321753\n",
      "epoch:2; batch 90112; train accuracy: 0.813320\n",
      "epoch 2; batch 90240; loss 0.259468\n",
      "epoch:2; batch 90240; train accuracy: 0.813371\n",
      "epoch 2; batch 90368; loss 0.270594\n",
      "epoch:2; batch 90368; train accuracy: 0.813442\n",
      "epoch 2; batch 90496; loss 0.440241\n",
      "epoch:2; batch 90496; train accuracy: 0.813472\n",
      "epoch 2; batch 90624; loss 0.326084\n",
      "epoch:2; batch 90624; train accuracy: 0.813528\n",
      "epoch 2; batch 90752; loss 0.375753\n",
      "epoch:2; batch 90752; train accuracy: 0.813563\n",
      "epoch 2; batch 90880; loss 0.216904\n",
      "epoch:2; batch 90880; train accuracy: 0.813628\n",
      "epoch 2; batch 91008; loss 0.300307\n",
      "epoch:2; batch 91008; train accuracy: 0.813694\n",
      "epoch 2; batch 91136; loss 0.225402\n",
      "epoch:2; batch 91136; train accuracy: 0.813760\n",
      "epoch 2; batch 91264; loss 0.348722\n",
      "epoch:2; batch 91264; train accuracy: 0.813810\n",
      "epoch 2; batch 91392; loss 0.331399\n",
      "epoch:2; batch 91392; train accuracy: 0.813865\n",
      "epoch 2; batch 91520; loss 0.279923\n",
      "epoch:2; batch 91520; train accuracy: 0.813915\n",
      "epoch 2; batch 91648; loss 0.332980\n",
      "epoch:2; batch 91648; train accuracy: 0.813955\n",
      "epoch 2; batch 91776; loss 0.494112\n",
      "epoch:2; batch 91776; train accuracy: 0.813964\n",
      "epoch 2; batch 91904; loss 0.322616\n",
      "epoch:2; batch 91904; train accuracy: 0.813999\n",
      "epoch 2; batch 92032; loss 0.332554\n",
      "epoch:2; batch 92032; train accuracy: 0.814033\n",
      "epoch 2; batch 92160; loss 0.314645\n",
      "epoch:2; batch 92160; train accuracy: 0.814073\n",
      "epoch 2; batch 92288; loss 0.260146\n",
      "epoch:2; batch 92288; train accuracy: 0.814143\n",
      "epoch 2; batch 92416; loss 0.296054\n",
      "epoch:2; batch 92416; train accuracy: 0.814182\n",
      "epoch 2; batch 92544; loss 0.368692\n",
      "epoch:2; batch 92544; train accuracy: 0.814216\n",
      "epoch 2; batch 92672; loss 0.190800\n",
      "epoch:2; batch 92672; train accuracy: 0.814301\n",
      "epoch 2; batch 92800; loss 0.275265\n",
      "epoch:2; batch 92800; train accuracy: 0.814371\n",
      "epoch 2; batch 92928; loss 0.208264\n",
      "epoch:2; batch 92928; train accuracy: 0.814440\n",
      "epoch 2; batch 93056; loss 0.482302\n",
      "epoch:2; batch 93056; train accuracy: 0.814444\n",
      "epoch 2; batch 93184; loss 0.288590\n",
      "epoch:2; batch 93184; train accuracy: 0.814499\n",
      "epoch 2; batch 93312; loss 0.238328\n",
      "epoch:2; batch 93312; train accuracy: 0.814568\n",
      "epoch 2; batch 93440; loss 0.416114\n",
      "epoch:2; batch 93440; train accuracy: 0.814597\n",
      "epoch 2; batch 93568; loss 0.332462\n",
      "epoch:2; batch 93568; train accuracy: 0.814621\n",
      "epoch 2; batch 93696; loss 0.342237\n",
      "epoch:2; batch 93696; train accuracy: 0.814670\n",
      "epoch 2; batch 93824; loss 0.235083\n",
      "epoch:2; batch 93824; train accuracy: 0.814739\n",
      "epoch 2; batch 93952; loss 0.304704\n",
      "epoch:2; batch 93952; train accuracy: 0.814798\n",
      "epoch 2; batch 94080; loss 0.344761\n",
      "epoch:2; batch 94080; train accuracy: 0.814846\n",
      "epoch 2; batch 94208; loss 0.266181\n",
      "epoch:2; batch 94208; train accuracy: 0.814920\n",
      "epoch 2; batch 94336; loss 0.262769\n",
      "epoch:2; batch 94336; train accuracy: 0.814984\n",
      "epoch 2; batch 94464; loss 0.296273\n",
      "epoch:2; batch 94464; train accuracy: 0.815042\n",
      "epoch 2; batch 94592; loss 0.321086\n",
      "epoch:2; batch 94592; train accuracy: 0.815071\n",
      "epoch 2; batch 94720; loss 0.338626\n",
      "epoch:2; batch 94720; train accuracy: 0.815089\n",
      "epoch 2; batch 94848; loss 0.372707\n",
      "epoch:2; batch 94848; train accuracy: 0.815123\n",
      "epoch 2; batch 94976; loss 0.345813\n",
      "epoch:2; batch 94976; train accuracy: 0.815151\n",
      "epoch 2; batch 95104; loss 0.288487\n",
      "epoch:2; batch 95104; train accuracy: 0.815199\n",
      "epoch 2; batch 95232; loss 0.275600\n",
      "epoch:2; batch 95232; train accuracy: 0.815257\n",
      "epoch 2; batch 95360; loss 0.344683\n",
      "epoch:2; batch 95360; train accuracy: 0.815310\n",
      "epoch 2; batch 95488; loss 0.234410\n",
      "epoch:2; batch 95488; train accuracy: 0.815374\n",
      "epoch 2; batch 95616; loss 0.406766\n",
      "epoch:2; batch 95616; train accuracy: 0.815397\n",
      "epoch 2; batch 95744; loss 0.306169\n",
      "epoch:2; batch 95744; train accuracy: 0.815440\n",
      "epoch 2; batch 95872; loss 0.377106\n",
      "epoch:2; batch 95872; train accuracy: 0.815483\n",
      "epoch 2; batch 96000; loss 0.289536\n",
      "epoch:2; batch 96000; train accuracy: 0.815535\n",
      "epoch 2; batch 96128; loss 0.339962\n",
      "epoch:2; batch 96128; train accuracy: 0.815588\n",
      "epoch 2; batch 96256; loss 0.281035\n",
      "epoch:2; batch 96256; train accuracy: 0.815636\n",
      "epoch 2; batch 96384; loss 0.222367\n",
      "epoch:2; batch 96384; train accuracy: 0.815713\n",
      "epoch 2; batch 96512; loss 0.196703\n",
      "epoch:2; batch 96512; train accuracy: 0.815786\n",
      "epoch 2; batch 96640; loss 0.304088\n",
      "epoch:2; batch 96640; train accuracy: 0.815818\n",
      "epoch 2; batch 96768; loss 0.308095\n",
      "epoch:2; batch 96768; train accuracy: 0.815876\n",
      "epoch 2; batch 96896; loss 0.332634\n",
      "epoch:2; batch 96896; train accuracy: 0.815923\n",
      "epoch 2; batch 97024; loss 0.269975\n",
      "epoch:2; batch 97024; train accuracy: 0.815980\n",
      "epoch 2; batch 97152; loss 0.427718\n",
      "epoch:2; batch 97152; train accuracy: 0.816008\n",
      "epoch 2; batch 97280; loss 0.291360\n",
      "epoch:2; batch 97280; train accuracy: 0.816065\n",
      "epoch 2; batch 97408; loss 0.304356\n",
      "epoch:2; batch 97408; train accuracy: 0.816097\n",
      "epoch 2; batch 97536; loss 0.367688\n",
      "epoch:2; batch 97536; train accuracy: 0.816140\n",
      "epoch 2; batch 97664; loss 0.238097\n",
      "epoch:2; batch 97664; train accuracy: 0.816201\n",
      "epoch 2; batch 97792; loss 0.278299\n",
      "epoch:2; batch 97792; train accuracy: 0.816253\n",
      "epoch 2; batch 97920; loss 0.407036\n",
      "epoch:2; batch 97920; train accuracy: 0.816281\n",
      "epoch 2; batch 98048; loss 0.247561\n",
      "epoch:2; batch 98048; train accuracy: 0.816342\n",
      "epoch 2; batch 98176; loss 0.263828\n",
      "epoch:2; batch 98176; train accuracy: 0.816399\n",
      "epoch 2; batch 98304; loss 0.423304\n",
      "epoch:2; batch 98304; train accuracy: 0.816421\n",
      "epoch 2; batch 98432; loss 0.268726\n",
      "epoch:2; batch 98432; train accuracy: 0.816473\n",
      "epoch 2; batch 98560; loss 0.277405\n",
      "epoch:2; batch 98560; train accuracy: 0.816544\n",
      "epoch 2; batch 98688; loss 0.372776\n",
      "epoch:2; batch 98688; train accuracy: 0.816566\n",
      "epoch 2; batch 98816; loss 0.509671\n",
      "epoch:2; batch 98816; train accuracy: 0.816568\n",
      "epoch 2; batch 98944; loss 0.270090\n",
      "epoch:2; batch 98944; train accuracy: 0.816624\n",
      "epoch 2; batch 99072; loss 0.370858\n",
      "epoch:2; batch 99072; train accuracy: 0.816651\n",
      "epoch 2; batch 99200; loss 0.334066\n",
      "epoch:2; batch 99200; train accuracy: 0.816707\n",
      "epoch 2; batch 99328; loss 0.342902\n",
      "epoch:2; batch 99328; train accuracy: 0.816759\n",
      "epoch 2; batch 99456; loss 0.265640\n",
      "epoch:2; batch 99456; train accuracy: 0.816820\n",
      "epoch 2; batch 99584; loss 0.353766\n",
      "epoch:2; batch 99584; train accuracy: 0.816836\n",
      "epoch 2; batch 99712; loss 0.290148\n",
      "epoch:2; batch 99712; train accuracy: 0.816873\n",
      "epoch 2; batch 99840; loss 0.385875\n",
      "epoch:2; batch 99840; train accuracy: 0.816904\n",
      "epoch 2; batch 99968; loss 0.245446\n",
      "epoch:2; batch 99968; train accuracy: 0.816955\n",
      "epoch 2; batch 100096; loss 0.274755\n",
      "epoch:2; batch 100096; train accuracy: 0.817006\n",
      "epoch 2; batch 100224; loss 0.248501\n",
      "epoch:2; batch 100224; train accuracy: 0.817071\n",
      "epoch 2; batch 100352; loss 0.273039\n",
      "epoch:2; batch 100352; train accuracy: 0.817127\n",
      "epoch 2; batch 100480; loss 0.257475\n",
      "epoch:2; batch 100480; train accuracy: 0.817178\n",
      "epoch 2; batch 100608; loss 0.296404\n",
      "epoch:2; batch 100608; train accuracy: 0.817219\n",
      "epoch 2; batch 100736; loss 0.243248\n",
      "epoch:2; batch 100736; train accuracy: 0.817293\n",
      "epoch 2; batch 100864; loss 0.272232\n",
      "epoch:2; batch 100864; train accuracy: 0.817358\n",
      "epoch 2; batch 100992; loss 0.344841\n",
      "epoch:2; batch 100992; train accuracy: 0.817399\n",
      "epoch 2; batch 101120; loss 0.415373\n",
      "epoch:2; batch 101120; train accuracy: 0.817411\n",
      "epoch 2; batch 101248; loss 0.304231\n",
      "epoch:2; batch 101248; train accuracy: 0.817456\n",
      "epoch 2; batch 101376; loss 0.128463\n",
      "epoch:2; batch 101376; train accuracy: 0.817545\n",
      "epoch 2; batch 101504; loss 0.297990\n",
      "epoch:2; batch 101504; train accuracy: 0.817595\n",
      "epoch 2; batch 101632; loss 0.309422\n",
      "epoch:2; batch 101632; train accuracy: 0.817645\n",
      "epoch 2; batch 101760; loss 0.430235\n",
      "epoch:2; batch 101760; train accuracy: 0.817666\n",
      "epoch 2; batch 101888; loss 0.286501\n",
      "epoch:2; batch 101888; train accuracy: 0.817716\n",
      "epoch 2; batch 102016; loss 0.215988\n",
      "epoch:2; batch 102016; train accuracy: 0.817786\n",
      "epoch 2; batch 102144; loss 0.416521\n",
      "epoch:2; batch 102144; train accuracy: 0.817802\n",
      "epoch 2; batch 102272; loss 0.352586\n",
      "epoch:2; batch 102272; train accuracy: 0.817842\n",
      "epoch 2; batch 102400; loss 0.389463\n",
      "epoch:2; batch 102400; train accuracy: 0.817858\n",
      "epoch 2; batch 102528; loss 0.226043\n",
      "epoch:2; batch 102528; train accuracy: 0.817922\n",
      "epoch 2; batch 102656; loss 0.311145\n",
      "epoch:2; batch 102656; train accuracy: 0.817962\n",
      "epoch 2; batch 102784; loss 0.354447\n",
      "epoch:2; batch 102784; train accuracy: 0.817992\n",
      "epoch 2; batch 102912; loss 0.241577\n",
      "epoch:2; batch 102912; train accuracy: 0.818052\n",
      "epoch 2; batch 103040; loss 0.326571\n",
      "epoch:2; batch 103040; train accuracy: 0.818091\n",
      "epoch 2; batch 103168; loss 0.302001\n",
      "epoch:2; batch 103168; train accuracy: 0.818141\n",
      "epoch 2; batch 103296; loss 0.232086\n",
      "epoch:2; batch 103296; train accuracy: 0.818190\n",
      "epoch 2; batch 103424; loss 0.233716\n",
      "epoch:2; batch 103424; train accuracy: 0.818254\n",
      "epoch 2; batch 103552; loss 0.270932\n",
      "epoch:2; batch 103552; train accuracy: 0.818298\n",
      "epoch 2; batch 103680; loss 0.381222\n",
      "epoch:2; batch 103680; train accuracy: 0.818333\n",
      "epoch 2; batch 103808; loss 0.198090\n",
      "epoch:2; batch 103808; train accuracy: 0.818401\n",
      "epoch 2; batch 103936; loss 0.284399\n",
      "epoch:2; batch 103936; train accuracy: 0.818455\n",
      "epoch 2; batch 104064; loss 0.197138\n",
      "epoch:2; batch 104064; train accuracy: 0.818528\n",
      "epoch 2; batch 104192; loss 0.339348\n",
      "epoch:2; batch 104192; train accuracy: 0.818586\n",
      "epoch 2; batch 104320; loss 0.234392\n",
      "epoch:2; batch 104320; train accuracy: 0.818650\n",
      "epoch 2; batch 104448; loss 0.280214\n",
      "epoch:2; batch 104448; train accuracy: 0.818698\n",
      "epoch 2; batch 104576; loss 0.267456\n",
      "epoch:2; batch 104576; train accuracy: 0.818747\n",
      "epoch 2; batch 104704; loss 0.307848\n",
      "epoch:2; batch 104704; train accuracy: 0.818786\n",
      "epoch 2; batch 104832; loss 0.384323\n",
      "epoch:2; batch 104832; train accuracy: 0.818816\n",
      "epoch 2; batch 104960; loss 0.268427\n",
      "epoch:2; batch 104960; train accuracy: 0.818874\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 37152 ; rate: 0.353963\n",
      "y_true_label_1_num: 8748 ; rate: 0.083346\n",
      "y_true_label_2_num: 10205 ; rate: 0.097228\n",
      "y_true_label_3_num: 48855 ; rate: 0.465463\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.950819\n",
      "valid avg_precision: 0.960911\n",
      "valid avg_recall: 0.939682\n",
      "valid avg_f1: 0.946030\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 5388 ; rate: 0.359776\n",
      "y_true_label_1_num: 1214 ; rate: 0.081063\n",
      "y_true_label_2_num: 1418 ; rate: 0.094685\n",
      "y_true_label_3_num: 6956 ; rate: 0.464476\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.793536\n",
      "valid avg_precision: 0.796439\n",
      "valid avg_recall: 0.772837\n",
      "valid avg_f1: 0.774864\n",
      "epoch 3\n",
      "epoch 3; batch 128; loss 0.256255\n",
      "epoch:3; batch 128; train accuracy: 0.818922\n",
      "epoch 3; batch 256; loss 0.284410\n",
      "epoch:3; batch 256; train accuracy: 0.818976\n",
      "epoch 3; batch 384; loss 0.292570\n",
      "epoch:3; batch 384; train accuracy: 0.819019\n",
      "epoch 3; batch 512; loss 0.169255\n",
      "epoch:3; batch 512; train accuracy: 0.819096\n",
      "epoch 3; batch 640; loss 0.233738\n",
      "epoch:3; batch 640; train accuracy: 0.819158\n",
      "epoch 3; batch 768; loss 0.204139\n",
      "epoch:3; batch 768; train accuracy: 0.819230\n",
      "epoch 3; batch 896; loss 0.256239\n",
      "epoch:3; batch 896; train accuracy: 0.819288\n",
      "epoch 3; batch 1024; loss 0.330768\n",
      "epoch:3; batch 1024; train accuracy: 0.819345\n",
      "epoch 3; batch 1152; loss 0.180596\n",
      "epoch:3; batch 1152; train accuracy: 0.819427\n",
      "epoch 3; batch 1280; loss 0.109268\n",
      "epoch:3; batch 1280; train accuracy: 0.819526\n",
      "epoch 3; batch 1408; loss 0.292273\n",
      "epoch:3; batch 1408; train accuracy: 0.819570\n",
      "epoch 3; batch 1536; loss 0.289013\n",
      "epoch:3; batch 1536; train accuracy: 0.819594\n",
      "epoch 3; batch 1664; loss 0.139491\n",
      "epoch:3; batch 1664; train accuracy: 0.819665\n",
      "epoch 3; batch 1792; loss 0.208921\n",
      "epoch:3; batch 1792; train accuracy: 0.819736\n",
      "epoch 3; batch 1920; loss 0.269871\n",
      "epoch:3; batch 1920; train accuracy: 0.819779\n",
      "epoch 3; batch 2048; loss 0.178886\n",
      "epoch:3; batch 2048; train accuracy: 0.819860\n",
      "epoch 3; batch 2176; loss 0.109107\n",
      "epoch:3; batch 2176; train accuracy: 0.819940\n",
      "epoch 3; batch 2304; loss 0.351017\n",
      "epoch:3; batch 2304; train accuracy: 0.819997\n",
      "epoch 3; batch 2432; loss 0.186304\n",
      "epoch:3; batch 2432; train accuracy: 0.820054\n",
      "epoch 3; batch 2560; loss 0.168080\n",
      "epoch:3; batch 2560; train accuracy: 0.820120\n",
      "epoch 3; batch 2688; loss 0.241822\n",
      "epoch:3; batch 2688; train accuracy: 0.820162\n",
      "epoch 3; batch 2816; loss 0.350768\n",
      "epoch:3; batch 2816; train accuracy: 0.820195\n",
      "epoch 3; batch 2944; loss 0.278795\n",
      "epoch:3; batch 2944; train accuracy: 0.820251\n",
      "epoch 3; batch 3072; loss 0.197759\n",
      "epoch:3; batch 3072; train accuracy: 0.820308\n",
      "epoch 3; batch 3200; loss 0.099474\n",
      "epoch:3; batch 3200; train accuracy: 0.820402\n",
      "epoch 3; batch 3328; loss 0.196734\n",
      "epoch:3; batch 3328; train accuracy: 0.820477\n",
      "epoch 3; batch 3456; loss 0.236873\n",
      "epoch:3; batch 3456; train accuracy: 0.820537\n",
      "epoch 3; batch 3584; loss 0.231025\n",
      "epoch:3; batch 3584; train accuracy: 0.820589\n",
      "epoch 3; batch 3712; loss 0.214685\n",
      "epoch:3; batch 3712; train accuracy: 0.820650\n",
      "epoch 3; batch 3840; loss 0.167681\n",
      "epoch:3; batch 3840; train accuracy: 0.820729\n",
      "epoch 3; batch 3968; loss 0.228318\n",
      "epoch:3; batch 3968; train accuracy: 0.820803\n",
      "epoch 3; batch 4096; loss 0.223167\n",
      "epoch:3; batch 4096; train accuracy: 0.820878\n",
      "epoch 3; batch 4224; loss 0.278801\n",
      "epoch:3; batch 4224; train accuracy: 0.820943\n",
      "epoch 3; batch 4352; loss 0.239420\n",
      "epoch:3; batch 4352; train accuracy: 0.821008\n",
      "epoch 3; batch 4480; loss 0.239012\n",
      "epoch:3; batch 4480; train accuracy: 0.821068\n",
      "epoch 3; batch 4608; loss 0.201610\n",
      "epoch:3; batch 4608; train accuracy: 0.821138\n",
      "epoch 3; batch 4736; loss 0.216535\n",
      "epoch:3; batch 4736; train accuracy: 0.821202\n",
      "epoch 3; batch 4864; loss 0.192456\n",
      "epoch:3; batch 4864; train accuracy: 0.821267\n",
      "epoch 3; batch 4992; loss 0.142524\n",
      "epoch:3; batch 4992; train accuracy: 0.821336\n",
      "epoch 3; batch 5120; loss 0.133425\n",
      "epoch:3; batch 5120; train accuracy: 0.821405\n",
      "epoch 3; batch 5248; loss 0.095519\n",
      "epoch:3; batch 5248; train accuracy: 0.821493\n",
      "epoch 3; batch 5376; loss 0.270000\n",
      "epoch:3; batch 5376; train accuracy: 0.821543\n",
      "epoch 3; batch 5504; loss 0.180042\n",
      "epoch:3; batch 5504; train accuracy: 0.821598\n",
      "epoch 3; batch 5632; loss 0.099851\n",
      "epoch:3; batch 5632; train accuracy: 0.821676\n",
      "epoch 3; batch 5760; loss 0.130014\n",
      "epoch:3; batch 5760; train accuracy: 0.821759\n",
      "epoch 3; batch 5888; loss 0.127346\n",
      "epoch:3; batch 5888; train accuracy: 0.821846\n",
      "epoch 3; batch 6016; loss 0.157986\n",
      "epoch:3; batch 6016; train accuracy: 0.821919\n",
      "epoch 3; batch 6144; loss 0.229335\n",
      "epoch:3; batch 6144; train accuracy: 0.821969\n",
      "epoch 3; batch 6272; loss 0.052604\n",
      "epoch:3; batch 6272; train accuracy: 0.822066\n",
      "epoch 3; batch 6400; loss 0.250696\n",
      "epoch:3; batch 6400; train accuracy: 0.822106\n",
      "epoch 3; batch 6528; loss 0.135563\n",
      "epoch:3; batch 6528; train accuracy: 0.822188\n",
      "epoch 3; batch 6656; loss 0.157972\n",
      "epoch:3; batch 6656; train accuracy: 0.822252\n",
      "epoch 3; batch 6784; loss 0.175473\n",
      "epoch:3; batch 6784; train accuracy: 0.822315\n",
      "epoch 3; batch 6912; loss 0.142289\n",
      "epoch:3; batch 6912; train accuracy: 0.822402\n",
      "epoch 3; batch 7040; loss 0.154236\n",
      "epoch:3; batch 7040; train accuracy: 0.822488\n",
      "epoch 3; batch 7168; loss 0.089398\n",
      "epoch:3; batch 7168; train accuracy: 0.822583\n",
      "epoch 3; batch 7296; loss 0.126275\n",
      "epoch:3; batch 7296; train accuracy: 0.822656\n",
      "epoch 3; batch 7424; loss 0.175811\n",
      "epoch:3; batch 7424; train accuracy: 0.822723\n",
      "epoch 3; batch 7552; loss 0.194936\n",
      "epoch:3; batch 7552; train accuracy: 0.822791\n",
      "epoch 3; batch 7680; loss 0.181514\n",
      "epoch:3; batch 7680; train accuracy: 0.822849\n",
      "epoch 3; batch 7808; loss 0.200920\n",
      "epoch:3; batch 7808; train accuracy: 0.822921\n",
      "epoch 3; batch 7936; loss 0.084461\n",
      "epoch:3; batch 7936; train accuracy: 0.823007\n",
      "epoch 3; batch 8064; loss 0.240964\n",
      "epoch:3; batch 8064; train accuracy: 0.823056\n",
      "epoch 3; batch 8192; loss 0.225644\n",
      "epoch:3; batch 8192; train accuracy: 0.823105\n",
      "epoch 3; batch 8320; loss 0.171117\n",
      "epoch:3; batch 8320; train accuracy: 0.823172\n",
      "epoch 3; batch 8448; loss 0.087358\n",
      "epoch:3; batch 8448; train accuracy: 0.823257\n",
      "epoch 3; batch 8576; loss 0.146630\n",
      "epoch:3; batch 8576; train accuracy: 0.823338\n",
      "epoch 3; batch 8704; loss 0.090645\n",
      "epoch:3; batch 8704; train accuracy: 0.823418\n",
      "epoch 3; batch 8832; loss 0.098199\n",
      "epoch:3; batch 8832; train accuracy: 0.823503\n",
      "epoch 3; batch 8960; loss 0.126504\n",
      "epoch:3; batch 8960; train accuracy: 0.823584\n",
      "epoch 3; batch 9088; loss 0.098712\n",
      "epoch:3; batch 9088; train accuracy: 0.823655\n",
      "epoch 3; batch 9216; loss 0.112649\n",
      "epoch:3; batch 9216; train accuracy: 0.823740\n",
      "epoch 3; batch 9344; loss 0.131157\n",
      "epoch:3; batch 9344; train accuracy: 0.823811\n",
      "epoch 3; batch 9472; loss 0.101685\n",
      "epoch:3; batch 9472; train accuracy: 0.823900\n",
      "epoch 3; batch 9600; loss 0.091193\n",
      "epoch:3; batch 9600; train accuracy: 0.823989\n",
      "epoch 3; batch 9728; loss 0.139090\n",
      "epoch:3; batch 9728; train accuracy: 0.824059\n",
      "epoch 3; batch 9856; loss 0.129190\n",
      "epoch:3; batch 9856; train accuracy: 0.824130\n",
      "epoch 3; batch 9984; loss 0.145840\n",
      "epoch:3; batch 9984; train accuracy: 0.824201\n",
      "epoch 3; batch 10112; loss 0.197772\n",
      "epoch:3; batch 10112; train accuracy: 0.824280\n",
      "epoch 3; batch 10240; loss 0.156016\n",
      "epoch:3; batch 10240; train accuracy: 0.824341\n",
      "epoch 3; batch 10368; loss 0.130860\n",
      "epoch:3; batch 10368; train accuracy: 0.824416\n",
      "epoch 3; batch 10496; loss 0.147928\n",
      "epoch:3; batch 10496; train accuracy: 0.824477\n",
      "epoch 3; batch 10624; loss 0.048944\n",
      "epoch:3; batch 10624; train accuracy: 0.824570\n",
      "epoch 3; batch 10752; loss 0.134401\n",
      "epoch:3; batch 10752; train accuracy: 0.824649\n",
      "epoch 3; batch 10880; loss 0.102246\n",
      "epoch:3; batch 10880; train accuracy: 0.824733\n",
      "epoch 3; batch 11008; loss 0.141804\n",
      "epoch:3; batch 11008; train accuracy: 0.824794\n",
      "epoch 3; batch 11136; loss 0.116329\n",
      "epoch:3; batch 11136; train accuracy: 0.824868\n",
      "epoch 3; batch 11264; loss 0.107318\n",
      "epoch:3; batch 11264; train accuracy: 0.824951\n",
      "epoch 3; batch 11392; loss 0.129414\n",
      "epoch:3; batch 11392; train accuracy: 0.825034\n",
      "epoch 3; batch 11520; loss 0.156307\n",
      "epoch:3; batch 11520; train accuracy: 0.825099\n",
      "epoch 3; batch 11648; loss 0.091333\n",
      "epoch:3; batch 11648; train accuracy: 0.825187\n",
      "epoch 3; batch 11776; loss 0.157229\n",
      "epoch:3; batch 11776; train accuracy: 0.825247\n",
      "epoch 3; batch 11904; loss 0.127793\n",
      "epoch:3; batch 11904; train accuracy: 0.825321\n",
      "epoch 3; batch 12032; loss 0.189666\n",
      "epoch:3; batch 12032; train accuracy: 0.825368\n",
      "epoch 3; batch 12160; loss 0.115123\n",
      "epoch:3; batch 12160; train accuracy: 0.825441\n",
      "epoch 3; batch 12288; loss 0.155014\n",
      "epoch:3; batch 12288; train accuracy: 0.825519\n",
      "epoch 3; batch 12416; loss 0.168895\n",
      "epoch:3; batch 12416; train accuracy: 0.825579\n",
      "epoch 3; batch 12544; loss 0.185832\n",
      "epoch:3; batch 12544; train accuracy: 0.825635\n",
      "epoch 3; batch 12672; loss 0.144870\n",
      "epoch:3; batch 12672; train accuracy: 0.825708\n",
      "epoch 3; batch 12800; loss 0.085029\n",
      "epoch:3; batch 12800; train accuracy: 0.825786\n",
      "epoch 3; batch 12928; loss 0.102166\n",
      "epoch:3; batch 12928; train accuracy: 0.825859\n",
      "epoch 3; batch 13056; loss 0.133194\n",
      "epoch:3; batch 13056; train accuracy: 0.825936\n",
      "epoch 3; batch 13184; loss 0.123680\n",
      "epoch:3; batch 13184; train accuracy: 0.826018\n",
      "epoch 3; batch 13312; loss 0.273707\n",
      "epoch:3; batch 13312; train accuracy: 0.826078\n",
      "epoch 3; batch 13440; loss 0.101784\n",
      "epoch:3; batch 13440; train accuracy: 0.826160\n",
      "epoch 3; batch 13568; loss 0.191561\n",
      "epoch:3; batch 13568; train accuracy: 0.826210\n",
      "epoch 3; batch 13696; loss 0.110063\n",
      "epoch:3; batch 13696; train accuracy: 0.826292\n",
      "epoch 3; batch 13824; loss 0.170521\n",
      "epoch:3; batch 13824; train accuracy: 0.826364\n",
      "epoch 3; batch 13952; loss 0.155980\n",
      "epoch:3; batch 13952; train accuracy: 0.826432\n",
      "epoch 3; batch 14080; loss 0.126000\n",
      "epoch:3; batch 14080; train accuracy: 0.826509\n",
      "epoch 3; batch 14208; loss 0.090015\n",
      "epoch:3; batch 14208; train accuracy: 0.826581\n",
      "epoch 3; batch 14336; loss 0.249729\n",
      "epoch:3; batch 14336; train accuracy: 0.826640\n",
      "epoch 3; batch 14464; loss 0.168716\n",
      "epoch:3; batch 14464; train accuracy: 0.826708\n",
      "epoch 3; batch 14592; loss 0.147253\n",
      "epoch:3; batch 14592; train accuracy: 0.826780\n",
      "epoch 3; batch 14720; loss 0.183392\n",
      "epoch:3; batch 14720; train accuracy: 0.826847\n",
      "epoch 3; batch 14848; loss 0.133260\n",
      "epoch:3; batch 14848; train accuracy: 0.826919\n",
      "epoch 3; batch 14976; loss 0.079078\n",
      "epoch:3; batch 14976; train accuracy: 0.827013\n",
      "epoch 3; batch 15104; loss 0.096116\n",
      "epoch:3; batch 15104; train accuracy: 0.827094\n",
      "epoch 3; batch 15232; loss 0.093702\n",
      "epoch:3; batch 15232; train accuracy: 0.827179\n",
      "epoch 3; batch 15360; loss 0.138749\n",
      "epoch:3; batch 15360; train accuracy: 0.827246\n",
      "epoch 3; batch 15488; loss 0.136579\n",
      "epoch:3; batch 15488; train accuracy: 0.827322\n",
      "epoch 3; batch 15616; loss 0.189813\n",
      "epoch:3; batch 15616; train accuracy: 0.827389\n",
      "epoch 3; batch 15744; loss 0.101639\n",
      "epoch:3; batch 15744; train accuracy: 0.827465\n",
      "epoch 3; batch 15872; loss 0.162043\n",
      "epoch:3; batch 15872; train accuracy: 0.827532\n",
      "epoch 3; batch 16000; loss 0.089581\n",
      "epoch:3; batch 16000; train accuracy: 0.827607\n",
      "epoch 3; batch 16128; loss 0.070231\n",
      "epoch:3; batch 16128; train accuracy: 0.827691\n",
      "epoch 3; batch 16256; loss 0.079989\n",
      "epoch:3; batch 16256; train accuracy: 0.827767\n",
      "epoch 3; batch 16384; loss 0.106422\n",
      "epoch:3; batch 16384; train accuracy: 0.827847\n",
      "epoch 3; batch 16512; loss 0.126088\n",
      "epoch:3; batch 16512; train accuracy: 0.827917\n",
      "epoch 3; batch 16640; loss 0.145656\n",
      "epoch:3; batch 16640; train accuracy: 0.827984\n",
      "epoch 3; batch 16768; loss 0.199576\n",
      "epoch:3; batch 16768; train accuracy: 0.828054\n",
      "epoch 3; batch 16896; loss 0.132059\n",
      "epoch:3; batch 16896; train accuracy: 0.828129\n",
      "epoch 3; batch 17024; loss 0.157272\n",
      "epoch:3; batch 17024; train accuracy: 0.828209\n",
      "epoch 3; batch 17152; loss 0.133587\n",
      "epoch:3; batch 17152; train accuracy: 0.828275\n",
      "epoch 3; batch 17280; loss 0.092855\n",
      "epoch:3; batch 17280; train accuracy: 0.828349\n",
      "epoch 3; batch 17408; loss 0.081202\n",
      "epoch:3; batch 17408; train accuracy: 0.828420\n",
      "epoch 3; batch 17536; loss 0.114489\n",
      "epoch:3; batch 17536; train accuracy: 0.828499\n",
      "epoch 3; batch 17664; loss 0.173412\n",
      "epoch:3; batch 17664; train accuracy: 0.828578\n",
      "epoch 3; batch 17792; loss 0.096114\n",
      "epoch:3; batch 17792; train accuracy: 0.828648\n",
      "epoch 3; batch 17920; loss 0.156566\n",
      "epoch:3; batch 17920; train accuracy: 0.828713\n",
      "epoch 3; batch 18048; loss 0.063715\n",
      "epoch:3; batch 18048; train accuracy: 0.828801\n",
      "epoch 3; batch 18176; loss 0.121060\n",
      "epoch:3; batch 18176; train accuracy: 0.828870\n",
      "epoch 3; batch 18304; loss 0.172896\n",
      "epoch:3; batch 18304; train accuracy: 0.828927\n",
      "epoch 3; batch 18432; loss 0.121912\n",
      "epoch:3; batch 18432; train accuracy: 0.829014\n",
      "epoch 3; batch 18560; loss 0.221233\n",
      "epoch:3; batch 18560; train accuracy: 0.829062\n",
      "epoch 3; batch 18688; loss 0.167973\n",
      "epoch:3; batch 18688; train accuracy: 0.829131\n",
      "epoch 3; batch 18816; loss 0.283148\n",
      "epoch:3; batch 18816; train accuracy: 0.829183\n",
      "epoch 3; batch 18944; loss 0.066932\n",
      "epoch:3; batch 18944; train accuracy: 0.829265\n",
      "epoch 3; batch 19072; loss 0.321609\n",
      "epoch:3; batch 19072; train accuracy: 0.829300\n",
      "epoch 3; batch 19200; loss 0.109843\n",
      "epoch:3; batch 19200; train accuracy: 0.829378\n",
      "epoch 3; batch 19328; loss 0.099513\n",
      "epoch:3; batch 19328; train accuracy: 0.829447\n",
      "epoch 3; batch 19456; loss 0.118034\n",
      "epoch:3; batch 19456; train accuracy: 0.829507\n",
      "epoch 3; batch 19584; loss 0.104328\n",
      "epoch:3; batch 19584; train accuracy: 0.829572\n",
      "epoch 3; batch 19712; loss 0.169652\n",
      "epoch:3; batch 19712; train accuracy: 0.829627\n",
      "epoch 3; batch 19840; loss 0.121237\n",
      "epoch:3; batch 19840; train accuracy: 0.829692\n",
      "epoch 3; batch 19968; loss 0.156236\n",
      "epoch:3; batch 19968; train accuracy: 0.829756\n",
      "epoch 3; batch 20096; loss 0.078915\n",
      "epoch:3; batch 20096; train accuracy: 0.829842\n",
      "epoch 3; batch 20224; loss 0.148497\n",
      "epoch:3; batch 20224; train accuracy: 0.829898\n",
      "epoch 3; batch 20352; loss 0.163452\n",
      "epoch:3; batch 20352; train accuracy: 0.829962\n",
      "epoch 3; batch 20480; loss 0.136448\n",
      "epoch:3; batch 20480; train accuracy: 0.830026\n",
      "epoch 3; batch 20608; loss 0.157538\n",
      "epoch:3; batch 20608; train accuracy: 0.830090\n",
      "epoch 3; batch 20736; loss 0.117049\n",
      "epoch:3; batch 20736; train accuracy: 0.830141\n",
      "epoch 3; batch 20864; loss 0.120341\n",
      "epoch:3; batch 20864; train accuracy: 0.830205\n",
      "epoch 3; batch 20992; loss 0.103274\n",
      "epoch:3; batch 20992; train accuracy: 0.830282\n",
      "epoch 3; batch 21120; loss 0.117837\n",
      "epoch:3; batch 21120; train accuracy: 0.830350\n",
      "epoch 3; batch 21248; loss 0.157211\n",
      "epoch:3; batch 21248; train accuracy: 0.830409\n",
      "epoch 3; batch 21376; loss 0.103802\n",
      "epoch:3; batch 21376; train accuracy: 0.830481\n",
      "epoch 3; batch 21504; loss 0.070509\n",
      "epoch:3; batch 21504; train accuracy: 0.830562\n",
      "epoch 3; batch 21632; loss 0.148806\n",
      "epoch:3; batch 21632; train accuracy: 0.830630\n",
      "epoch 3; batch 21760; loss 0.119411\n",
      "epoch:3; batch 21760; train accuracy: 0.830693\n",
      "epoch 3; batch 21888; loss 0.086004\n",
      "epoch:3; batch 21888; train accuracy: 0.830765\n",
      "epoch 3; batch 22016; loss 0.137954\n",
      "epoch:3; batch 22016; train accuracy: 0.830824\n",
      "epoch 3; batch 22144; loss 0.075137\n",
      "epoch:3; batch 22144; train accuracy: 0.830904\n",
      "epoch 3; batch 22272; loss 0.113497\n",
      "epoch:3; batch 22272; train accuracy: 0.830976\n",
      "epoch 3; batch 22400; loss 0.105684\n",
      "epoch:3; batch 22400; train accuracy: 0.831043\n",
      "epoch 3; batch 22528; loss 0.095051\n",
      "epoch:3; batch 22528; train accuracy: 0.831111\n",
      "epoch 3; batch 22656; loss 0.137152\n",
      "epoch:3; batch 22656; train accuracy: 0.831178\n",
      "epoch 3; batch 22784; loss 0.208334\n",
      "epoch:3; batch 22784; train accuracy: 0.831241\n",
      "epoch 3; batch 22912; loss 0.077145\n",
      "epoch:3; batch 22912; train accuracy: 0.831316\n",
      "epoch 3; batch 23040; loss 0.140017\n",
      "epoch:3; batch 23040; train accuracy: 0.831379\n",
      "epoch 3; batch 23168; loss 0.103360\n",
      "epoch:3; batch 23168; train accuracy: 0.831454\n",
      "epoch 3; batch 23296; loss 0.102870\n",
      "epoch:3; batch 23296; train accuracy: 0.831530\n",
      "epoch 3; batch 23424; loss 0.150771\n",
      "epoch:3; batch 23424; train accuracy: 0.831601\n",
      "epoch 3; batch 23552; loss 0.087122\n",
      "epoch:3; batch 23552; train accuracy: 0.831671\n",
      "epoch 3; batch 23680; loss 0.092671\n",
      "epoch:3; batch 23680; train accuracy: 0.831742\n",
      "epoch 3; batch 23808; loss 0.164305\n",
      "epoch:3; batch 23808; train accuracy: 0.831800\n",
      "epoch 3; batch 23936; loss 0.134037\n",
      "epoch:3; batch 23936; train accuracy: 0.831867\n",
      "epoch 3; batch 24064; loss 0.090069\n",
      "epoch:3; batch 24064; train accuracy: 0.831937\n",
      "epoch 3; batch 24192; loss 0.195086\n",
      "epoch:3; batch 24192; train accuracy: 0.831986\n",
      "epoch 3; batch 24320; loss 0.084166\n",
      "epoch:3; batch 24320; train accuracy: 0.832057\n",
      "epoch 3; batch 24448; loss 0.065308\n",
      "epoch:3; batch 24448; train accuracy: 0.832140\n",
      "epoch 3; batch 24576; loss 0.104539\n",
      "epoch:3; batch 24576; train accuracy: 0.832215\n",
      "epoch 3; batch 24704; loss 0.118751\n",
      "epoch:3; batch 24704; train accuracy: 0.832289\n",
      "epoch 3; batch 24832; loss 0.170717\n",
      "epoch:3; batch 24832; train accuracy: 0.832338\n",
      "epoch 3; batch 24960; loss 0.133917\n",
      "epoch:3; batch 24960; train accuracy: 0.832400\n",
      "epoch 3; batch 25088; loss 0.110946\n",
      "epoch:3; batch 25088; train accuracy: 0.832470\n",
      "epoch 3; batch 25216; loss 0.080225\n",
      "epoch:3; batch 25216; train accuracy: 0.832548\n",
      "epoch 3; batch 25344; loss 0.079799\n",
      "epoch:3; batch 25344; train accuracy: 0.832626\n",
      "epoch 3; batch 25472; loss 0.052833\n",
      "epoch:3; batch 25472; train accuracy: 0.832713\n",
      "epoch 3; batch 25600; loss 0.125610\n",
      "epoch:3; batch 25600; train accuracy: 0.832774\n",
      "epoch 3; batch 25728; loss 0.053355\n",
      "epoch:3; batch 25728; train accuracy: 0.832861\n",
      "epoch 3; batch 25856; loss 0.083857\n",
      "epoch:3; batch 25856; train accuracy: 0.832939\n",
      "epoch 3; batch 25984; loss 0.120871\n",
      "epoch:3; batch 25984; train accuracy: 0.833008\n",
      "epoch 3; batch 26112; loss 0.097463\n",
      "epoch:3; batch 26112; train accuracy: 0.833078\n",
      "epoch 3; batch 26240; loss 0.078968\n",
      "epoch:3; batch 26240; train accuracy: 0.833151\n",
      "epoch 3; batch 26368; loss 0.120430\n",
      "epoch:3; batch 26368; train accuracy: 0.833216\n",
      "epoch 3; batch 26496; loss 0.228059\n",
      "epoch:3; batch 26496; train accuracy: 0.833268\n",
      "epoch 3; batch 26624; loss 0.048434\n",
      "epoch:3; batch 26624; train accuracy: 0.833350\n",
      "epoch 3; batch 26752; loss 0.175864\n",
      "epoch:3; batch 26752; train accuracy: 0.833415\n",
      "epoch 3; batch 26880; loss 0.065389\n",
      "epoch:3; batch 26880; train accuracy: 0.833492\n",
      "epoch 3; batch 27008; loss 0.132225\n",
      "epoch:3; batch 27008; train accuracy: 0.833536\n",
      "epoch 3; batch 27136; loss 0.099244\n",
      "epoch:3; batch 27136; train accuracy: 0.833601\n",
      "epoch 3; batch 27264; loss 0.074254\n",
      "epoch:3; batch 27264; train accuracy: 0.833678\n",
      "epoch 3; batch 27392; loss 0.094899\n",
      "epoch:3; batch 27392; train accuracy: 0.833755\n",
      "epoch 3; batch 27520; loss 0.138564\n",
      "epoch:3; batch 27520; train accuracy: 0.833815\n",
      "epoch 3; batch 27648; loss 0.274586\n",
      "epoch:3; batch 27648; train accuracy: 0.833858\n",
      "epoch 3; batch 27776; loss 0.063071\n",
      "epoch:3; batch 27776; train accuracy: 0.833939\n",
      "epoch 3; batch 27904; loss 0.079401\n",
      "epoch:3; batch 27904; train accuracy: 0.834012\n",
      "epoch 3; batch 28032; loss 0.038674\n",
      "epoch:3; batch 28032; train accuracy: 0.834097\n",
      "epoch 3; batch 28160; loss 0.121488\n",
      "epoch:3; batch 28160; train accuracy: 0.834157\n",
      "epoch 3; batch 28288; loss 0.144575\n",
      "epoch:3; batch 28288; train accuracy: 0.834229\n",
      "epoch 3; batch 28416; loss 0.164430\n",
      "epoch:3; batch 28416; train accuracy: 0.834293\n",
      "epoch 3; batch 28544; loss 0.089334\n",
      "epoch:3; batch 28544; train accuracy: 0.834369\n",
      "epoch 3; batch 28672; loss 0.070321\n",
      "epoch:3; batch 28672; train accuracy: 0.834437\n",
      "epoch 3; batch 28800; loss 0.142234\n",
      "epoch:3; batch 28800; train accuracy: 0.834505\n",
      "epoch 3; batch 28928; loss 0.035972\n",
      "epoch:3; batch 28928; train accuracy: 0.834589\n",
      "epoch 3; batch 29056; loss 0.141101\n",
      "epoch:3; batch 29056; train accuracy: 0.834649\n",
      "epoch 3; batch 29184; loss 0.065854\n",
      "epoch:3; batch 29184; train accuracy: 0.834716\n",
      "epoch 3; batch 29312; loss 0.088640\n",
      "epoch:3; batch 29312; train accuracy: 0.834792\n",
      "epoch 3; batch 29440; loss 0.068320\n",
      "epoch:3; batch 29440; train accuracy: 0.834872\n",
      "epoch 3; batch 29568; loss 0.116052\n",
      "epoch:3; batch 29568; train accuracy: 0.834940\n",
      "epoch 3; batch 29696; loss 0.168449\n",
      "epoch:3; batch 29696; train accuracy: 0.834994\n",
      "epoch 3; batch 29824; loss 0.088091\n",
      "epoch:3; batch 29824; train accuracy: 0.835070\n",
      "epoch 3; batch 29952; loss 0.089909\n",
      "epoch:3; batch 29952; train accuracy: 0.835145\n",
      "epoch 3; batch 30080; loss 0.119368\n",
      "epoch:3; batch 30080; train accuracy: 0.835204\n",
      "epoch 3; batch 30208; loss 0.086452\n",
      "epoch:3; batch 30208; train accuracy: 0.835280\n",
      "epoch 3; batch 30336; loss 0.107465\n",
      "epoch:3; batch 30336; train accuracy: 0.835346\n",
      "epoch 3; batch 30464; loss 0.080208\n",
      "epoch:3; batch 30464; train accuracy: 0.835422\n",
      "epoch 3; batch 30592; loss 0.043830\n",
      "epoch:3; batch 30592; train accuracy: 0.835501\n",
      "epoch 3; batch 30720; loss 0.086872\n",
      "epoch:3; batch 30720; train accuracy: 0.835568\n",
      "epoch 3; batch 30848; loss 0.160158\n",
      "epoch:3; batch 30848; train accuracy: 0.835626\n",
      "epoch 3; batch 30976; loss 0.119166\n",
      "epoch:3; batch 30976; train accuracy: 0.835697\n",
      "epoch 3; batch 31104; loss 0.077205\n",
      "epoch:3; batch 31104; train accuracy: 0.835772\n",
      "epoch 3; batch 31232; loss 0.070278\n",
      "epoch:3; batch 31232; train accuracy: 0.835838\n",
      "epoch 3; batch 31360; loss 0.092022\n",
      "epoch:3; batch 31360; train accuracy: 0.835904\n",
      "epoch 3; batch 31488; loss 0.080777\n",
      "epoch:3; batch 31488; train accuracy: 0.835987\n",
      "epoch 3; batch 31616; loss 0.119216\n",
      "epoch:3; batch 31616; train accuracy: 0.836058\n",
      "epoch 3; batch 31744; loss 0.128949\n",
      "epoch:3; batch 31744; train accuracy: 0.836124\n",
      "epoch 3; batch 31872; loss 0.111535\n",
      "epoch:3; batch 31872; train accuracy: 0.836186\n",
      "epoch 3; batch 32000; loss 0.113406\n",
      "epoch:3; batch 32000; train accuracy: 0.836256\n",
      "epoch 3; batch 32128; loss 0.073415\n",
      "epoch:3; batch 32128; train accuracy: 0.836326\n",
      "epoch 3; batch 32256; loss 0.140624\n",
      "epoch:3; batch 32256; train accuracy: 0.836379\n",
      "epoch 3; batch 32384; loss 0.165831\n",
      "epoch:3; batch 32384; train accuracy: 0.836437\n",
      "epoch 3; batch 32512; loss 0.062204\n",
      "epoch:3; batch 32512; train accuracy: 0.836507\n",
      "epoch 3; batch 32640; loss 0.073909\n",
      "epoch:3; batch 32640; train accuracy: 0.836581\n",
      "epoch 3; batch 32768; loss 0.061021\n",
      "epoch:3; batch 32768; train accuracy: 0.836659\n",
      "epoch 3; batch 32896; loss 0.045252\n",
      "epoch:3; batch 32896; train accuracy: 0.836741\n",
      "epoch 3; batch 33024; loss 0.136708\n",
      "epoch:3; batch 33024; train accuracy: 0.836785\n",
      "epoch 3; batch 33152; loss 0.106409\n",
      "epoch:3; batch 33152; train accuracy: 0.836847\n",
      "epoch 3; batch 33280; loss 0.049670\n",
      "epoch:3; batch 33280; train accuracy: 0.836924\n",
      "epoch 3; batch 33408; loss 0.090770\n",
      "epoch:3; batch 33408; train accuracy: 0.836994\n",
      "epoch 3; batch 33536; loss 0.090580\n",
      "epoch:3; batch 33536; train accuracy: 0.837071\n",
      "epoch 3; batch 33664; loss 0.077981\n",
      "epoch:3; batch 33664; train accuracy: 0.837128\n",
      "epoch 3; batch 33792; loss 0.137588\n",
      "epoch:3; batch 33792; train accuracy: 0.837189\n",
      "epoch 3; batch 33920; loss 0.066614\n",
      "epoch:3; batch 33920; train accuracy: 0.837258\n",
      "epoch 3; batch 34048; loss 0.065456\n",
      "epoch:3; batch 34048; train accuracy: 0.837327\n",
      "epoch 3; batch 34176; loss 0.131771\n",
      "epoch:3; batch 34176; train accuracy: 0.837380\n",
      "epoch 3; batch 34304; loss 0.104941\n",
      "epoch:3; batch 34304; train accuracy: 0.837448\n",
      "epoch 3; batch 34432; loss 0.152682\n",
      "epoch:3; batch 34432; train accuracy: 0.837497\n",
      "epoch 3; batch 34560; loss 0.131748\n",
      "epoch:3; batch 34560; train accuracy: 0.837557\n",
      "epoch 3; batch 34688; loss 0.142294\n",
      "epoch:3; batch 34688; train accuracy: 0.837614\n",
      "epoch 3; batch 34816; loss 0.175699\n",
      "epoch:3; batch 34816; train accuracy: 0.837666\n",
      "epoch 3; batch 34944; loss 0.137223\n",
      "epoch:3; batch 34944; train accuracy: 0.837730\n",
      "epoch 3; batch 35072; loss 0.119766\n",
      "epoch:3; batch 35072; train accuracy: 0.837799\n",
      "epoch 3; batch 35200; loss 0.117722\n",
      "epoch:3; batch 35200; train accuracy: 0.837855\n",
      "epoch 3; batch 35328; loss 0.085449\n",
      "epoch:3; batch 35328; train accuracy: 0.837923\n",
      "epoch 3; batch 35456; loss 0.198512\n",
      "epoch:3; batch 35456; train accuracy: 0.837983\n",
      "epoch 3; batch 35584; loss 0.092459\n",
      "epoch:3; batch 35584; train accuracy: 0.838043\n",
      "epoch 3; batch 35712; loss 0.061853\n",
      "epoch:3; batch 35712; train accuracy: 0.838116\n",
      "epoch 3; batch 35840; loss 0.204286\n",
      "epoch:3; batch 35840; train accuracy: 0.838155\n",
      "epoch 3; batch 35968; loss 0.156824\n",
      "epoch:3; batch 35968; train accuracy: 0.838207\n",
      "epoch 3; batch 36096; loss 0.090559\n",
      "epoch:3; batch 36096; train accuracy: 0.838279\n",
      "epoch 3; batch 36224; loss 0.108350\n",
      "epoch:3; batch 36224; train accuracy: 0.838343\n",
      "epoch 3; batch 36352; loss 0.093819\n",
      "epoch:3; batch 36352; train accuracy: 0.838414\n",
      "epoch 3; batch 36480; loss 0.169663\n",
      "epoch:3; batch 36480; train accuracy: 0.838486\n",
      "epoch 3; batch 36608; loss 0.100541\n",
      "epoch:3; batch 36608; train accuracy: 0.838558\n",
      "epoch 3; batch 36736; loss 0.120749\n",
      "epoch:3; batch 36736; train accuracy: 0.838617\n",
      "epoch 3; batch 36864; loss 0.059716\n",
      "epoch:3; batch 36864; train accuracy: 0.838689\n",
      "epoch 3; batch 36992; loss 0.154789\n",
      "epoch:3; batch 36992; train accuracy: 0.838748\n",
      "epoch 3; batch 37120; loss 0.105761\n",
      "epoch:3; batch 37120; train accuracy: 0.838816\n",
      "epoch 3; batch 37248; loss 0.101679\n",
      "epoch:3; batch 37248; train accuracy: 0.838883\n",
      "epoch 3; batch 37376; loss 0.084499\n",
      "epoch:3; batch 37376; train accuracy: 0.838958\n",
      "epoch 3; batch 37504; loss 0.071109\n",
      "epoch:3; batch 37504; train accuracy: 0.839033\n",
      "epoch 3; batch 37632; loss 0.117360\n",
      "epoch:3; batch 37632; train accuracy: 0.839084\n",
      "epoch 3; batch 37760; loss 0.070735\n",
      "epoch:3; batch 37760; train accuracy: 0.839155\n",
      "epoch 3; batch 37888; loss 0.074772\n",
      "epoch:3; batch 37888; train accuracy: 0.839226\n",
      "epoch 3; batch 38016; loss 0.138719\n",
      "epoch:3; batch 38016; train accuracy: 0.839269\n",
      "epoch 3; batch 38144; loss 0.037792\n",
      "epoch:3; batch 38144; train accuracy: 0.839344\n",
      "epoch 3; batch 38272; loss 0.231850\n",
      "epoch:3; batch 38272; train accuracy: 0.839386\n",
      "epoch 3; batch 38400; loss 0.042303\n",
      "epoch:3; batch 38400; train accuracy: 0.839461\n",
      "epoch 3; batch 38528; loss 0.071907\n",
      "epoch:3; batch 38528; train accuracy: 0.839528\n",
      "epoch 3; batch 38656; loss 0.159579\n",
      "epoch:3; batch 38656; train accuracy: 0.839586\n",
      "epoch 3; batch 38784; loss 0.128542\n",
      "epoch:3; batch 38784; train accuracy: 0.839641\n",
      "epoch 3; batch 38912; loss 0.087318\n",
      "epoch:3; batch 38912; train accuracy: 0.839703\n",
      "epoch 3; batch 39040; loss 0.081895\n",
      "epoch:3; batch 39040; train accuracy: 0.839773\n",
      "epoch 3; batch 39168; loss 0.131602\n",
      "epoch:3; batch 39168; train accuracy: 0.839832\n",
      "epoch 3; batch 39296; loss 0.108455\n",
      "epoch:3; batch 39296; train accuracy: 0.839898\n",
      "epoch 3; batch 39424; loss 0.086946\n",
      "epoch:3; batch 39424; train accuracy: 0.839956\n",
      "epoch 3; batch 39552; loss 0.066210\n",
      "epoch:3; batch 39552; train accuracy: 0.840030\n",
      "epoch 3; batch 39680; loss 0.166401\n",
      "epoch:3; batch 39680; train accuracy: 0.840084\n",
      "epoch 3; batch 39808; loss 0.204818\n",
      "epoch:3; batch 39808; train accuracy: 0.840134\n",
      "epoch 3; batch 39936; loss 0.149629\n",
      "epoch:3; batch 39936; train accuracy: 0.840188\n",
      "epoch 3; batch 40064; loss 0.109971\n",
      "epoch:3; batch 40064; train accuracy: 0.840242\n",
      "epoch 3; batch 40192; loss 0.099143\n",
      "epoch:3; batch 40192; train accuracy: 0.840292\n",
      "epoch 3; batch 40320; loss 0.065921\n",
      "epoch:3; batch 40320; train accuracy: 0.840361\n",
      "epoch 3; batch 40448; loss 0.172769\n",
      "epoch:3; batch 40448; train accuracy: 0.840415\n",
      "epoch 3; batch 40576; loss 0.086076\n",
      "epoch:3; batch 40576; train accuracy: 0.840481\n",
      "epoch 3; batch 40704; loss 0.057062\n",
      "epoch:3; batch 40704; train accuracy: 0.840550\n",
      "epoch 3; batch 40832; loss 0.130084\n",
      "epoch:3; batch 40832; train accuracy: 0.840615\n",
      "epoch 3; batch 40960; loss 0.195710\n",
      "epoch:3; batch 40960; train accuracy: 0.840673\n",
      "epoch 3; batch 41088; loss 0.092993\n",
      "epoch:3; batch 41088; train accuracy: 0.840730\n",
      "epoch 3; batch 41216; loss 0.125249\n",
      "epoch:3; batch 41216; train accuracy: 0.840791\n",
      "epoch 3; batch 41344; loss 0.142246\n",
      "epoch:3; batch 41344; train accuracy: 0.840849\n",
      "epoch 3; batch 41472; loss 0.148673\n",
      "epoch:3; batch 41472; train accuracy: 0.840898\n",
      "epoch 3; batch 41600; loss 0.148896\n",
      "epoch:3; batch 41600; train accuracy: 0.840951\n",
      "epoch 3; batch 41728; loss 0.047456\n",
      "epoch:3; batch 41728; train accuracy: 0.841024\n",
      "epoch 3; batch 41856; loss 0.081882\n",
      "epoch:3; batch 41856; train accuracy: 0.841093\n",
      "epoch 3; batch 41984; loss 0.060696\n",
      "epoch:3; batch 41984; train accuracy: 0.841162\n",
      "epoch 3; batch 42112; loss 0.045430\n",
      "epoch:3; batch 42112; train accuracy: 0.841234\n",
      "epoch 3; batch 42240; loss 0.150762\n",
      "epoch:3; batch 42240; train accuracy: 0.841291\n",
      "epoch 3; batch 42368; loss 0.079299\n",
      "epoch:3; batch 42368; train accuracy: 0.841356\n",
      "epoch 3; batch 42496; loss 0.142794\n",
      "epoch:3; batch 42496; train accuracy: 0.841417\n",
      "epoch 3; batch 42624; loss 0.071571\n",
      "epoch:3; batch 42624; train accuracy: 0.841489\n",
      "epoch 3; batch 42752; loss 0.115462\n",
      "epoch:3; batch 42752; train accuracy: 0.841542\n",
      "epoch 3; batch 42880; loss 0.076127\n",
      "epoch:3; batch 42880; train accuracy: 0.841606\n",
      "epoch 3; batch 43008; loss 0.093430\n",
      "epoch:3; batch 43008; train accuracy: 0.841670\n",
      "epoch 3; batch 43136; loss 0.078878\n",
      "epoch:3; batch 43136; train accuracy: 0.841743\n",
      "epoch 3; batch 43264; loss 0.065322\n",
      "epoch:3; batch 43264; train accuracy: 0.841815\n",
      "epoch 3; batch 43392; loss 0.053914\n",
      "epoch:3; batch 43392; train accuracy: 0.841887\n",
      "epoch 3; batch 43520; loss 0.078655\n",
      "epoch:3; batch 43520; train accuracy: 0.841951\n",
      "epoch 3; batch 43648; loss 0.103418\n",
      "epoch:3; batch 43648; train accuracy: 0.842011\n",
      "epoch 3; batch 43776; loss 0.124633\n",
      "epoch:3; batch 43776; train accuracy: 0.842059\n",
      "epoch 3; batch 43904; loss 0.081552\n",
      "epoch:3; batch 43904; train accuracy: 0.842119\n",
      "epoch 3; batch 44032; loss 0.193349\n",
      "epoch:3; batch 44032; train accuracy: 0.842171\n",
      "epoch 3; batch 44160; loss 0.175551\n",
      "epoch:3; batch 44160; train accuracy: 0.842219\n",
      "epoch 3; batch 44288; loss 0.124769\n",
      "epoch:3; batch 44288; train accuracy: 0.842283\n",
      "epoch 3; batch 44416; loss 0.076794\n",
      "epoch:3; batch 44416; train accuracy: 0.842342\n",
      "epoch 3; batch 44544; loss 0.064180\n",
      "epoch:3; batch 44544; train accuracy: 0.842410\n",
      "epoch 3; batch 44672; loss 0.100846\n",
      "epoch:3; batch 44672; train accuracy: 0.842473\n",
      "epoch 3; batch 44800; loss 0.035229\n",
      "epoch:3; batch 44800; train accuracy: 0.842545\n",
      "epoch 3; batch 44928; loss 0.062034\n",
      "epoch:3; batch 44928; train accuracy: 0.842612\n",
      "epoch 3; batch 45056; loss 0.110569\n",
      "epoch:3; batch 45056; train accuracy: 0.842668\n",
      "epoch 3; batch 45184; loss 0.140947\n",
      "epoch:3; batch 45184; train accuracy: 0.842723\n",
      "epoch 3; batch 45312; loss 0.066587\n",
      "epoch:3; batch 45312; train accuracy: 0.842786\n",
      "epoch 3; batch 45440; loss 0.136427\n",
      "epoch:3; batch 45440; train accuracy: 0.842842\n",
      "epoch 3; batch 45568; loss 0.061085\n",
      "epoch:3; batch 45568; train accuracy: 0.842905\n",
      "epoch 3; batch 45696; loss 0.073896\n",
      "epoch:3; batch 45696; train accuracy: 0.842968\n",
      "epoch 3; batch 45824; loss 0.139159\n",
      "epoch:3; batch 45824; train accuracy: 0.843015\n",
      "epoch 3; batch 45952; loss 0.117762\n",
      "epoch:3; batch 45952; train accuracy: 0.843082\n",
      "epoch 3; batch 46080; loss 0.055558\n",
      "epoch:3; batch 46080; train accuracy: 0.843148\n",
      "epoch 3; batch 46208; loss 0.041766\n",
      "epoch:3; batch 46208; train accuracy: 0.843219\n",
      "epoch 3; batch 46336; loss 0.100717\n",
      "epoch:3; batch 46336; train accuracy: 0.843282\n",
      "epoch 3; batch 46464; loss 0.046535\n",
      "epoch:3; batch 46464; train accuracy: 0.843356\n",
      "epoch 3; batch 46592; loss 0.083878\n",
      "epoch:3; batch 46592; train accuracy: 0.843415\n",
      "epoch 3; batch 46720; loss 0.121500\n",
      "epoch:3; batch 46720; train accuracy: 0.843469\n",
      "epoch 3; batch 46848; loss 0.087253\n",
      "epoch:3; batch 46848; train accuracy: 0.843528\n",
      "epoch 3; batch 46976; loss 0.088698\n",
      "epoch:3; batch 46976; train accuracy: 0.843583\n",
      "epoch 3; batch 47104; loss 0.084213\n",
      "epoch:3; batch 47104; train accuracy: 0.843649\n",
      "epoch 3; batch 47232; loss 0.046236\n",
      "epoch:3; batch 47232; train accuracy: 0.843719\n",
      "epoch 3; batch 47360; loss 0.210169\n",
      "epoch:3; batch 47360; train accuracy: 0.843758\n",
      "epoch 3; batch 47488; loss 0.185443\n",
      "epoch:3; batch 47488; train accuracy: 0.843800\n",
      "epoch 3; batch 47616; loss 0.155031\n",
      "epoch:3; batch 47616; train accuracy: 0.843839\n",
      "epoch 3; batch 47744; loss 0.066384\n",
      "epoch:3; batch 47744; train accuracy: 0.843909\n",
      "epoch 3; batch 47872; loss 0.102106\n",
      "epoch:3; batch 47872; train accuracy: 0.843975\n",
      "epoch 3; batch 48000; loss 0.058680\n",
      "epoch:3; batch 48000; train accuracy: 0.844045\n",
      "epoch 3; batch 48128; loss 0.079496\n",
      "epoch:3; batch 48128; train accuracy: 0.844106\n",
      "epoch 3; batch 48256; loss 0.076965\n",
      "epoch:3; batch 48256; train accuracy: 0.844168\n",
      "epoch 3; batch 48384; loss 0.115657\n",
      "epoch:3; batch 48384; train accuracy: 0.844226\n",
      "epoch 3; batch 48512; loss 0.065861\n",
      "epoch:3; batch 48512; train accuracy: 0.844296\n",
      "epoch 3; batch 48640; loss 0.076952\n",
      "epoch:3; batch 48640; train accuracy: 0.844353\n",
      "epoch 3; batch 48768; loss 0.122569\n",
      "epoch:3; batch 48768; train accuracy: 0.844411\n",
      "epoch 3; batch 48896; loss 0.165711\n",
      "epoch:3; batch 48896; train accuracy: 0.844457\n",
      "epoch 3; batch 49024; loss 0.174689\n",
      "epoch:3; batch 49024; train accuracy: 0.844507\n",
      "epoch 3; batch 49152; loss 0.071960\n",
      "epoch:3; batch 49152; train accuracy: 0.844564\n",
      "epoch 3; batch 49280; loss 0.066770\n",
      "epoch:3; batch 49280; train accuracy: 0.844637\n",
      "epoch 3; batch 49408; loss 0.101264\n",
      "epoch:3; batch 49408; train accuracy: 0.844691\n",
      "epoch 3; batch 49536; loss 0.147395\n",
      "epoch:3; batch 49536; train accuracy: 0.844733\n",
      "epoch 3; batch 49664; loss 0.077934\n",
      "epoch:3; batch 49664; train accuracy: 0.844794\n",
      "epoch 3; batch 49792; loss 0.192549\n",
      "epoch:3; batch 49792; train accuracy: 0.844844\n",
      "epoch 3; batch 49920; loss 0.151614\n",
      "epoch:3; batch 49920; train accuracy: 0.844901\n",
      "epoch 3; batch 50048; loss 0.215023\n",
      "epoch:3; batch 50048; train accuracy: 0.844939\n",
      "epoch 3; batch 50176; loss 0.086609\n",
      "epoch:3; batch 50176; train accuracy: 0.844996\n",
      "epoch 3; batch 50304; loss 0.095266\n",
      "epoch:3; batch 50304; train accuracy: 0.845060\n",
      "epoch 3; batch 50432; loss 0.238991\n",
      "epoch:3; batch 50432; train accuracy: 0.845110\n",
      "epoch 3; batch 50560; loss 0.126238\n",
      "epoch:3; batch 50560; train accuracy: 0.845163\n",
      "epoch 3; batch 50688; loss 0.198244\n",
      "epoch:3; batch 50688; train accuracy: 0.845212\n",
      "epoch 3; batch 50816; loss 0.163902\n",
      "epoch:3; batch 50816; train accuracy: 0.845261\n",
      "epoch 3; batch 50944; loss 0.102312\n",
      "epoch:3; batch 50944; train accuracy: 0.845322\n",
      "epoch 3; batch 51072; loss 0.058477\n",
      "epoch:3; batch 51072; train accuracy: 0.845386\n",
      "epoch 3; batch 51200; loss 0.056401\n",
      "epoch:3; batch 51200; train accuracy: 0.845458\n",
      "epoch 3; batch 51328; loss 0.095745\n",
      "epoch:3; batch 51328; train accuracy: 0.845522\n",
      "epoch 3; batch 51456; loss 0.122431\n",
      "epoch:3; batch 51456; train accuracy: 0.845575\n",
      "epoch 3; batch 51584; loss 0.111475\n",
      "epoch:3; batch 51584; train accuracy: 0.845635\n",
      "epoch 3; batch 51712; loss 0.090982\n",
      "epoch:3; batch 51712; train accuracy: 0.845695\n",
      "epoch 3; batch 51840; loss 0.196965\n",
      "epoch:3; batch 51840; train accuracy: 0.845748\n",
      "epoch 3; batch 51968; loss 0.100640\n",
      "epoch:3; batch 51968; train accuracy: 0.845812\n",
      "epoch 3; batch 52096; loss 0.141508\n",
      "epoch:3; batch 52096; train accuracy: 0.845868\n",
      "epoch 3; batch 52224; loss 0.120962\n",
      "epoch:3; batch 52224; train accuracy: 0.845932\n",
      "epoch 3; batch 52352; loss 0.109867\n",
      "epoch:3; batch 52352; train accuracy: 0.845988\n",
      "epoch 3; batch 52480; loss 0.092216\n",
      "epoch:3; batch 52480; train accuracy: 0.846044\n",
      "epoch 3; batch 52608; loss 0.077557\n",
      "epoch:3; batch 52608; train accuracy: 0.846104\n",
      "epoch 3; batch 52736; loss 0.078004\n",
      "epoch:3; batch 52736; train accuracy: 0.846164\n",
      "epoch 3; batch 52864; loss 0.082818\n",
      "epoch:3; batch 52864; train accuracy: 0.846224\n",
      "epoch 3; batch 52992; loss 0.095707\n",
      "epoch:3; batch 52992; train accuracy: 0.846272\n",
      "epoch 3; batch 53120; loss 0.039931\n",
      "epoch:3; batch 53120; train accuracy: 0.846343\n",
      "epoch 3; batch 53248; loss 0.068890\n",
      "epoch:3; batch 53248; train accuracy: 0.846402\n",
      "epoch 3; batch 53376; loss 0.096822\n",
      "epoch:3; batch 53376; train accuracy: 0.846462\n",
      "epoch 3; batch 53504; loss 0.070087\n",
      "epoch:3; batch 53504; train accuracy: 0.846521\n",
      "epoch 3; batch 53632; loss 0.097437\n",
      "epoch:3; batch 53632; train accuracy: 0.846581\n",
      "epoch 3; batch 53760; loss 0.054723\n",
      "epoch:3; batch 53760; train accuracy: 0.846647\n",
      "epoch 3; batch 53888; loss 0.100098\n",
      "epoch:3; batch 53888; train accuracy: 0.846703\n",
      "epoch 3; batch 54016; loss 0.079233\n",
      "epoch:3; batch 54016; train accuracy: 0.846762\n",
      "epoch 3; batch 54144; loss 0.084170\n",
      "epoch:3; batch 54144; train accuracy: 0.846829\n",
      "epoch 3; batch 54272; loss 0.135058\n",
      "epoch:3; batch 54272; train accuracy: 0.846884\n",
      "epoch 3; batch 54400; loss 0.057318\n",
      "epoch:3; batch 54400; train accuracy: 0.846951\n",
      "epoch 3; batch 54528; loss 0.123211\n",
      "epoch:3; batch 54528; train accuracy: 0.847013\n",
      "epoch 3; batch 54656; loss 0.066170\n",
      "epoch:3; batch 54656; train accuracy: 0.847072\n",
      "epoch 3; batch 54784; loss 0.050376\n",
      "epoch:3; batch 54784; train accuracy: 0.847135\n",
      "epoch 3; batch 54912; loss 0.058562\n",
      "epoch:3; batch 54912; train accuracy: 0.847201\n",
      "epoch 3; batch 55040; loss 0.220190\n",
      "epoch:3; batch 55040; train accuracy: 0.847241\n",
      "epoch 3; batch 55168; loss 0.039826\n",
      "epoch:3; batch 55168; train accuracy: 0.847307\n",
      "epoch 3; batch 55296; loss 0.081188\n",
      "epoch:3; batch 55296; train accuracy: 0.847370\n",
      "epoch 3; batch 55424; loss 0.045539\n",
      "epoch:3; batch 55424; train accuracy: 0.847436\n",
      "epoch 3; batch 55552; loss 0.022707\n",
      "epoch:3; batch 55552; train accuracy: 0.847509\n",
      "epoch 3; batch 55680; loss 0.157732\n",
      "epoch:3; batch 55680; train accuracy: 0.847549\n",
      "epoch 3; batch 55808; loss 0.070883\n",
      "epoch:3; batch 55808; train accuracy: 0.847611\n",
      "epoch 3; batch 55936; loss 0.079365\n",
      "epoch:3; batch 55936; train accuracy: 0.847677\n",
      "epoch 3; batch 56064; loss 0.137466\n",
      "epoch:3; batch 56064; train accuracy: 0.847724\n",
      "epoch 3; batch 56192; loss 0.059241\n",
      "epoch:3; batch 56192; train accuracy: 0.847786\n",
      "epoch 3; batch 56320; loss 0.105447\n",
      "epoch:3; batch 56320; train accuracy: 0.847829\n",
      "epoch 3; batch 56448; loss 0.046992\n",
      "epoch:3; batch 56448; train accuracy: 0.847895\n",
      "epoch 3; batch 56576; loss 0.067939\n",
      "epoch:3; batch 56576; train accuracy: 0.847956\n",
      "epoch 3; batch 56704; loss 0.074912\n",
      "epoch:3; batch 56704; train accuracy: 0.848022\n",
      "epoch 3; batch 56832; loss 0.191399\n",
      "epoch:3; batch 56832; train accuracy: 0.848084\n",
      "epoch 3; batch 56960; loss 0.142925\n",
      "epoch:3; batch 56960; train accuracy: 0.848123\n",
      "epoch 3; batch 57088; loss 0.095641\n",
      "epoch:3; batch 57088; train accuracy: 0.848181\n",
      "epoch 3; batch 57216; loss 0.133369\n",
      "epoch:3; batch 57216; train accuracy: 0.848235\n",
      "epoch 3; batch 57344; loss 0.123340\n",
      "epoch:3; batch 57344; train accuracy: 0.848292\n",
      "epoch 3; batch 57472; loss 0.116519\n",
      "epoch:3; batch 57472; train accuracy: 0.848350\n",
      "epoch 3; batch 57600; loss 0.127993\n",
      "epoch:3; batch 57600; train accuracy: 0.848389\n",
      "epoch 3; batch 57728; loss 0.084672\n",
      "epoch:3; batch 57728; train accuracy: 0.848443\n",
      "epoch 3; batch 57856; loss 0.066624\n",
      "epoch:3; batch 57856; train accuracy: 0.848500\n",
      "epoch 3; batch 57984; loss 0.106210\n",
      "epoch:3; batch 57984; train accuracy: 0.848550\n",
      "epoch 3; batch 58112; loss 0.101487\n",
      "epoch:3; batch 58112; train accuracy: 0.848608\n",
      "epoch 3; batch 58240; loss 0.102343\n",
      "epoch:3; batch 58240; train accuracy: 0.848661\n",
      "epoch 3; batch 58368; loss 0.091804\n",
      "epoch:3; batch 58368; train accuracy: 0.848715\n",
      "epoch 3; batch 58496; loss 0.088103\n",
      "epoch:3; batch 58496; train accuracy: 0.848768\n",
      "epoch 3; batch 58624; loss 0.060960\n",
      "epoch:3; batch 58624; train accuracy: 0.848833\n",
      "epoch 3; batch 58752; loss 0.091762\n",
      "epoch:3; batch 58752; train accuracy: 0.848894\n",
      "epoch 3; batch 58880; loss 0.131529\n",
      "epoch:3; batch 58880; train accuracy: 0.848940\n",
      "epoch 3; batch 59008; loss 0.153940\n",
      "epoch:3; batch 59008; train accuracy: 0.848978\n",
      "epoch 3; batch 59136; loss 0.092756\n",
      "epoch:3; batch 59136; train accuracy: 0.849031\n",
      "epoch 3; batch 59264; loss 0.087146\n",
      "epoch:3; batch 59264; train accuracy: 0.849085\n",
      "epoch 3; batch 59392; loss 0.080139\n",
      "epoch:3; batch 59392; train accuracy: 0.849142\n",
      "epoch 3; batch 59520; loss 0.075894\n",
      "epoch:3; batch 59520; train accuracy: 0.849198\n",
      "epoch 3; batch 59648; loss 0.197831\n",
      "epoch:3; batch 59648; train accuracy: 0.849244\n",
      "epoch 3; batch 59776; loss 0.101324\n",
      "epoch:3; batch 59776; train accuracy: 0.849293\n",
      "epoch 3; batch 59904; loss 0.093083\n",
      "epoch:3; batch 59904; train accuracy: 0.849354\n",
      "epoch 3; batch 60032; loss 0.015255\n",
      "epoch:3; batch 60032; train accuracy: 0.849425\n",
      "epoch 3; batch 60160; loss 0.121085\n",
      "epoch:3; batch 60160; train accuracy: 0.849474\n",
      "epoch 3; batch 60288; loss 0.049932\n",
      "epoch:3; batch 60288; train accuracy: 0.849538\n",
      "epoch 3; batch 60416; loss 0.114660\n",
      "epoch:3; batch 60416; train accuracy: 0.849591\n",
      "epoch 3; batch 60544; loss 0.161710\n",
      "epoch:3; batch 60544; train accuracy: 0.849636\n",
      "epoch 3; batch 60672; loss 0.063872\n",
      "epoch:3; batch 60672; train accuracy: 0.849696\n",
      "epoch 3; batch 60800; loss 0.102267\n",
      "epoch:3; batch 60800; train accuracy: 0.849749\n",
      "epoch 3; batch 60928; loss 0.062652\n",
      "epoch:3; batch 60928; train accuracy: 0.849805\n",
      "epoch 3; batch 61056; loss 0.043214\n",
      "epoch:3; batch 61056; train accuracy: 0.849872\n",
      "epoch 3; batch 61184; loss 0.047819\n",
      "epoch:3; batch 61184; train accuracy: 0.849936\n",
      "epoch 3; batch 61312; loss 0.088168\n",
      "epoch:3; batch 61312; train accuracy: 0.849996\n",
      "epoch 3; batch 61440; loss 0.027329\n",
      "epoch:3; batch 61440; train accuracy: 0.850059\n",
      "epoch 3; batch 61568; loss 0.077188\n",
      "epoch:3; batch 61568; train accuracy: 0.850119\n",
      "epoch 3; batch 61696; loss 0.103615\n",
      "epoch:3; batch 61696; train accuracy: 0.850171\n",
      "epoch 3; batch 61824; loss 0.063004\n",
      "epoch:3; batch 61824; train accuracy: 0.850227\n",
      "epoch 3; batch 61952; loss 0.089227\n",
      "epoch:3; batch 61952; train accuracy: 0.850282\n",
      "epoch 3; batch 62080; loss 0.042020\n",
      "epoch:3; batch 62080; train accuracy: 0.850349\n",
      "epoch 3; batch 62208; loss 0.061135\n",
      "epoch:3; batch 62208; train accuracy: 0.850416\n",
      "epoch 3; batch 62336; loss 0.040126\n",
      "epoch:3; batch 62336; train accuracy: 0.850479\n",
      "epoch 3; batch 62464; loss 0.101861\n",
      "epoch:3; batch 62464; train accuracy: 0.850531\n",
      "epoch 3; batch 62592; loss 0.030412\n",
      "epoch:3; batch 62592; train accuracy: 0.850597\n",
      "epoch 3; batch 62720; loss 0.034358\n",
      "epoch:3; batch 62720; train accuracy: 0.850668\n",
      "epoch 3; batch 62848; loss 0.033747\n",
      "epoch:3; batch 62848; train accuracy: 0.850730\n",
      "epoch 3; batch 62976; loss 0.057212\n",
      "epoch:3; batch 62976; train accuracy: 0.850789\n",
      "epoch 3; batch 63104; loss 0.093661\n",
      "epoch:3; batch 63104; train accuracy: 0.850848\n",
      "epoch 3; batch 63232; loss 0.086019\n",
      "epoch:3; batch 63232; train accuracy: 0.850907\n",
      "epoch 3; batch 63360; loss 0.043120\n",
      "epoch:3; batch 63360; train accuracy: 0.850970\n",
      "epoch 3; batch 63488; loss 0.067901\n",
      "epoch:3; batch 63488; train accuracy: 0.851025\n",
      "epoch 3; batch 63616; loss 0.136351\n",
      "epoch:3; batch 63616; train accuracy: 0.851069\n",
      "epoch 3; batch 63744; loss 0.035826\n",
      "epoch:3; batch 63744; train accuracy: 0.851135\n",
      "epoch 3; batch 63872; loss 0.081834\n",
      "epoch:3; batch 63872; train accuracy: 0.851190\n",
      "epoch 3; batch 64000; loss 0.076214\n",
      "epoch:3; batch 64000; train accuracy: 0.851245\n",
      "epoch 3; batch 64128; loss 0.056990\n",
      "epoch:3; batch 64128; train accuracy: 0.851311\n",
      "epoch 3; batch 64256; loss 0.117783\n",
      "epoch:3; batch 64256; train accuracy: 0.851362\n",
      "epoch 3; batch 64384; loss 0.029384\n",
      "epoch:3; batch 64384; train accuracy: 0.851428\n",
      "epoch 3; batch 64512; loss 0.070565\n",
      "epoch:3; batch 64512; train accuracy: 0.851482\n",
      "epoch 3; batch 64640; loss 0.020831\n",
      "epoch:3; batch 64640; train accuracy: 0.851548\n",
      "epoch 3; batch 64768; loss 0.120749\n",
      "epoch:3; batch 64768; train accuracy: 0.851599\n",
      "epoch 3; batch 64896; loss 0.057859\n",
      "epoch:3; batch 64896; train accuracy: 0.851653\n",
      "epoch 3; batch 65024; loss 0.035281\n",
      "epoch:3; batch 65024; train accuracy: 0.851715\n",
      "epoch 3; batch 65152; loss 0.129478\n",
      "epoch:3; batch 65152; train accuracy: 0.851759\n",
      "epoch 3; batch 65280; loss 0.094709\n",
      "epoch:3; batch 65280; train accuracy: 0.851813\n",
      "epoch 3; batch 65408; loss 0.034320\n",
      "epoch:3; batch 65408; train accuracy: 0.851871\n",
      "epoch 3; batch 65536; loss 0.161103\n",
      "epoch:3; batch 65536; train accuracy: 0.851918\n",
      "epoch 3; batch 65664; loss 0.033831\n",
      "epoch:3; batch 65664; train accuracy: 0.851983\n",
      "epoch 3; batch 65792; loss 0.085009\n",
      "epoch:3; batch 65792; train accuracy: 0.852034\n",
      "epoch 3; batch 65920; loss 0.114335\n",
      "epoch:3; batch 65920; train accuracy: 0.852085\n",
      "epoch 3; batch 66048; loss 0.201363\n",
      "epoch:3; batch 66048; train accuracy: 0.852124\n",
      "epoch 3; batch 66176; loss 0.033528\n",
      "epoch:3; batch 66176; train accuracy: 0.852185\n",
      "epoch 3; batch 66304; loss 0.170788\n",
      "epoch:3; batch 66304; train accuracy: 0.852239\n",
      "epoch 3; batch 66432; loss 0.025420\n",
      "epoch:3; batch 66432; train accuracy: 0.852304\n",
      "epoch 3; batch 66560; loss 0.196609\n",
      "epoch:3; batch 66560; train accuracy: 0.852351\n",
      "epoch 3; batch 66688; loss 0.135677\n",
      "epoch:3; batch 66688; train accuracy: 0.852401\n",
      "epoch 3; batch 66816; loss 0.073265\n",
      "epoch:3; batch 66816; train accuracy: 0.852459\n",
      "epoch 3; batch 66944; loss 0.050359\n",
      "epoch:3; batch 66944; train accuracy: 0.852520\n",
      "epoch 3; batch 67072; loss 0.084745\n",
      "epoch:3; batch 67072; train accuracy: 0.852577\n",
      "epoch 3; batch 67200; loss 0.081386\n",
      "epoch:3; batch 67200; train accuracy: 0.852631\n",
      "epoch 3; batch 67328; loss 0.068717\n",
      "epoch:3; batch 67328; train accuracy: 0.852688\n",
      "epoch 3; batch 67456; loss 0.046605\n",
      "epoch:3; batch 67456; train accuracy: 0.852752\n",
      "epoch 3; batch 67584; loss 0.095454\n",
      "epoch:3; batch 67584; train accuracy: 0.852802\n",
      "epoch 3; batch 67712; loss 0.129942\n",
      "epoch:3; batch 67712; train accuracy: 0.852848\n",
      "epoch 3; batch 67840; loss 0.050540\n",
      "epoch:3; batch 67840; train accuracy: 0.852913\n",
      "epoch 3; batch 67968; loss 0.067164\n",
      "epoch:3; batch 67968; train accuracy: 0.852977\n",
      "epoch 3; batch 68096; loss 0.080401\n",
      "epoch:3; batch 68096; train accuracy: 0.853034\n",
      "epoch 3; batch 68224; loss 0.071778\n",
      "epoch:3; batch 68224; train accuracy: 0.853098\n",
      "epoch 3; batch 68352; loss 0.133636\n",
      "epoch:3; batch 68352; train accuracy: 0.853144\n",
      "epoch 3; batch 68480; loss 0.068774\n",
      "epoch:3; batch 68480; train accuracy: 0.853204\n",
      "epoch 3; batch 68608; loss 0.049197\n",
      "epoch:3; batch 68608; train accuracy: 0.853264\n",
      "epoch 3; batch 68736; loss 0.078320\n",
      "epoch:3; batch 68736; train accuracy: 0.853314\n",
      "epoch 3; batch 68864; loss 0.040281\n",
      "epoch:3; batch 68864; train accuracy: 0.853378\n",
      "epoch 3; batch 68992; loss 0.064080\n",
      "epoch:3; batch 68992; train accuracy: 0.853427\n",
      "epoch 3; batch 69120; loss 0.051562\n",
      "epoch:3; batch 69120; train accuracy: 0.853487\n",
      "epoch 3; batch 69248; loss 0.033269\n",
      "epoch:3; batch 69248; train accuracy: 0.853554\n",
      "epoch 3; batch 69376; loss 0.142661\n",
      "epoch:3; batch 69376; train accuracy: 0.853600\n",
      "epoch 3; batch 69504; loss 0.098828\n",
      "epoch:3; batch 69504; train accuracy: 0.853656\n",
      "epoch 3; batch 69632; loss 0.057020\n",
      "epoch:3; batch 69632; train accuracy: 0.853720\n",
      "epoch 3; batch 69760; loss 0.035063\n",
      "epoch:3; batch 69760; train accuracy: 0.853783\n",
      "epoch 3; batch 69888; loss 0.051636\n",
      "epoch:3; batch 69888; train accuracy: 0.853843\n",
      "epoch 3; batch 70016; loss 0.097469\n",
      "epoch:3; batch 70016; train accuracy: 0.853899\n",
      "epoch 3; batch 70144; loss 0.048049\n",
      "epoch:3; batch 70144; train accuracy: 0.853958\n",
      "epoch 3; batch 70272; loss 0.080476\n",
      "epoch:3; batch 70272; train accuracy: 0.854018\n",
      "epoch 3; batch 70400; loss 0.141306\n",
      "epoch:3; batch 70400; train accuracy: 0.854070\n",
      "epoch 3; batch 70528; loss 0.147894\n",
      "epoch:3; batch 70528; train accuracy: 0.854112\n",
      "epoch 3; batch 70656; loss 0.081142\n",
      "epoch:3; batch 70656; train accuracy: 0.854171\n",
      "epoch 3; batch 70784; loss 0.134757\n",
      "epoch:3; batch 70784; train accuracy: 0.854217\n",
      "epoch 3; batch 70912; loss 0.061515\n",
      "epoch:3; batch 70912; train accuracy: 0.854269\n",
      "epoch 3; batch 71040; loss 0.118429\n",
      "epoch:3; batch 71040; train accuracy: 0.854317\n",
      "epoch 3; batch 71168; loss 0.188759\n",
      "epoch:3; batch 71168; train accuracy: 0.854359\n",
      "epoch 3; batch 71296; loss 0.114532\n",
      "epoch:3; batch 71296; train accuracy: 0.854414\n",
      "epoch 3; batch 71424; loss 0.054285\n",
      "epoch:3; batch 71424; train accuracy: 0.854470\n",
      "epoch 3; batch 71552; loss 0.094319\n",
      "epoch:3; batch 71552; train accuracy: 0.854522\n",
      "epoch 3; batch 71680; loss 0.085693\n",
      "epoch:3; batch 71680; train accuracy: 0.854581\n",
      "epoch 3; batch 71808; loss 0.093321\n",
      "epoch:3; batch 71808; train accuracy: 0.854636\n",
      "epoch 3; batch 71936; loss 0.070886\n",
      "epoch:3; batch 71936; train accuracy: 0.854692\n",
      "epoch 3; batch 72064; loss 0.071686\n",
      "epoch:3; batch 72064; train accuracy: 0.854751\n",
      "epoch 3; batch 72192; loss 0.060574\n",
      "epoch:3; batch 72192; train accuracy: 0.854802\n",
      "epoch 3; batch 72320; loss 0.078505\n",
      "epoch:3; batch 72320; train accuracy: 0.854847\n",
      "epoch 3; batch 72448; loss 0.110793\n",
      "epoch:3; batch 72448; train accuracy: 0.854895\n",
      "epoch 3; batch 72576; loss 0.091473\n",
      "epoch:3; batch 72576; train accuracy: 0.854950\n",
      "epoch 3; batch 72704; loss 0.099092\n",
      "epoch:3; batch 72704; train accuracy: 0.854995\n",
      "epoch 3; batch 72832; loss 0.057968\n",
      "epoch:3; batch 72832; train accuracy: 0.855050\n",
      "epoch 3; batch 72960; loss 0.115888\n",
      "epoch:3; batch 72960; train accuracy: 0.855105\n",
      "epoch 3; batch 73088; loss 0.056730\n",
      "epoch:3; batch 73088; train accuracy: 0.855160\n",
      "epoch 3; batch 73216; loss 0.064248\n",
      "epoch:3; batch 73216; train accuracy: 0.855218\n",
      "epoch 3; batch 73344; loss 0.042258\n",
      "epoch:3; batch 73344; train accuracy: 0.855273\n",
      "epoch 3; batch 73472; loss 0.058903\n",
      "epoch:3; batch 73472; train accuracy: 0.855331\n",
      "epoch 3; batch 73600; loss 0.057005\n",
      "epoch:3; batch 73600; train accuracy: 0.855389\n",
      "epoch 3; batch 73728; loss 0.032455\n",
      "epoch:3; batch 73728; train accuracy: 0.855451\n",
      "epoch 3; batch 73856; loss 0.151743\n",
      "epoch:3; batch 73856; train accuracy: 0.855499\n",
      "epoch 3; batch 73984; loss 0.034384\n",
      "epoch:3; batch 73984; train accuracy: 0.855560\n",
      "epoch 3; batch 74112; loss 0.102698\n",
      "epoch:3; batch 74112; train accuracy: 0.855611\n",
      "epoch 3; batch 74240; loss 0.034994\n",
      "epoch:3; batch 74240; train accuracy: 0.855669\n",
      "epoch 3; batch 74368; loss 0.028777\n",
      "epoch:3; batch 74368; train accuracy: 0.855727\n",
      "epoch 3; batch 74496; loss 0.077538\n",
      "epoch:3; batch 74496; train accuracy: 0.855782\n",
      "epoch 3; batch 74624; loss 0.040921\n",
      "epoch:3; batch 74624; train accuracy: 0.855839\n",
      "epoch 3; batch 74752; loss 0.022769\n",
      "epoch:3; batch 74752; train accuracy: 0.855901\n",
      "epoch 3; batch 74880; loss 0.066467\n",
      "epoch:3; batch 74880; train accuracy: 0.855955\n",
      "epoch 3; batch 75008; loss 0.064211\n",
      "epoch:3; batch 75008; train accuracy: 0.856016\n",
      "epoch 3; batch 75136; loss 0.011758\n",
      "epoch:3; batch 75136; train accuracy: 0.856081\n",
      "epoch 3; batch 75264; loss 0.129574\n",
      "epoch:3; batch 75264; train accuracy: 0.856128\n",
      "epoch 3; batch 75392; loss 0.066415\n",
      "epoch:3; batch 75392; train accuracy: 0.856179\n",
      "epoch 3; batch 75520; loss 0.038128\n",
      "epoch:3; batch 75520; train accuracy: 0.856239\n",
      "epoch 3; batch 75648; loss 0.081327\n",
      "epoch:3; batch 75648; train accuracy: 0.856286\n",
      "epoch 3; batch 75776; loss 0.043964\n",
      "epoch:3; batch 75776; train accuracy: 0.856344\n",
      "epoch 3; batch 75904; loss 0.162103\n",
      "epoch:3; batch 75904; train accuracy: 0.856380\n",
      "epoch 3; batch 76032; loss 0.068613\n",
      "epoch:3; batch 76032; train accuracy: 0.856434\n",
      "epoch 3; batch 76160; loss 0.103492\n",
      "epoch:3; batch 76160; train accuracy: 0.856491\n",
      "epoch 3; batch 76288; loss 0.040021\n",
      "epoch:3; batch 76288; train accuracy: 0.856545\n",
      "epoch 3; batch 76416; loss 0.103911\n",
      "epoch:3; batch 76416; train accuracy: 0.856602\n",
      "epoch 3; batch 76544; loss 0.118609\n",
      "epoch:3; batch 76544; train accuracy: 0.856652\n",
      "epoch 3; batch 76672; loss 0.148830\n",
      "epoch:3; batch 76672; train accuracy: 0.856688\n",
      "epoch 3; batch 76800; loss 0.132820\n",
      "epoch:3; batch 76800; train accuracy: 0.856742\n",
      "epoch 3; batch 76928; loss 0.103053\n",
      "epoch:3; batch 76928; train accuracy: 0.856799\n",
      "epoch 3; batch 77056; loss 0.064959\n",
      "epoch:3; batch 77056; train accuracy: 0.856856\n",
      "epoch 3; batch 77184; loss 0.044702\n",
      "epoch:3; batch 77184; train accuracy: 0.856912\n",
      "epoch 3; batch 77312; loss 0.074810\n",
      "epoch:3; batch 77312; train accuracy: 0.856962\n",
      "epoch 3; batch 77440; loss 0.064449\n",
      "epoch:3; batch 77440; train accuracy: 0.857016\n",
      "epoch 3; batch 77568; loss 0.134447\n",
      "epoch:3; batch 77568; train accuracy: 0.857069\n",
      "epoch 3; batch 77696; loss 0.070521\n",
      "epoch:3; batch 77696; train accuracy: 0.857115\n",
      "epoch 3; batch 77824; loss 0.031578\n",
      "epoch:3; batch 77824; train accuracy: 0.857175\n",
      "epoch 3; batch 77952; loss 0.147783\n",
      "epoch:3; batch 77952; train accuracy: 0.857228\n",
      "epoch 3; batch 78080; loss 0.046445\n",
      "epoch:3; batch 78080; train accuracy: 0.857288\n",
      "epoch 3; batch 78208; loss 0.075813\n",
      "epoch:3; batch 78208; train accuracy: 0.857341\n",
      "epoch 3; batch 78336; loss 0.068857\n",
      "epoch:3; batch 78336; train accuracy: 0.857391\n",
      "epoch 3; batch 78464; loss 0.055999\n",
      "epoch:3; batch 78464; train accuracy: 0.857450\n",
      "epoch 3; batch 78592; loss 0.127144\n",
      "epoch:3; batch 78592; train accuracy: 0.857493\n",
      "epoch 3; batch 78720; loss 0.030666\n",
      "epoch:3; batch 78720; train accuracy: 0.857553\n",
      "epoch 3; batch 78848; loss 0.067987\n",
      "epoch:3; batch 78848; train accuracy: 0.857609\n",
      "epoch 3; batch 78976; loss 0.068977\n",
      "epoch:3; batch 78976; train accuracy: 0.857655\n",
      "epoch 3; batch 79104; loss 0.099095\n",
      "epoch:3; batch 79104; train accuracy: 0.857697\n",
      "epoch 3; batch 79232; loss 0.062453\n",
      "epoch:3; batch 79232; train accuracy: 0.857750\n",
      "epoch 3; batch 79360; loss 0.150687\n",
      "epoch:3; batch 79360; train accuracy: 0.857792\n",
      "epoch 3; batch 79488; loss 0.061946\n",
      "epoch:3; batch 79488; train accuracy: 0.857841\n",
      "epoch 3; batch 79616; loss 0.049734\n",
      "epoch:3; batch 79616; train accuracy: 0.857900\n",
      "epoch 3; batch 79744; loss 0.025789\n",
      "epoch:3; batch 79744; train accuracy: 0.857960\n",
      "epoch 3; batch 79872; loss 0.034400\n",
      "epoch:3; batch 79872; train accuracy: 0.858015\n",
      "epoch 3; batch 80000; loss 0.020614\n",
      "epoch:3; batch 80000; train accuracy: 0.858075\n",
      "epoch 3; batch 80128; loss 0.066321\n",
      "epoch:3; batch 80128; train accuracy: 0.858127\n",
      "epoch 3; batch 80256; loss 0.034228\n",
      "epoch:3; batch 80256; train accuracy: 0.858186\n",
      "epoch 3; batch 80384; loss 0.053169\n",
      "epoch:3; batch 80384; train accuracy: 0.858238\n",
      "epoch 3; batch 80512; loss 0.020570\n",
      "epoch:3; batch 80512; train accuracy: 0.858297\n",
      "epoch 3; batch 80640; loss 0.024696\n",
      "epoch:3; batch 80640; train accuracy: 0.858356\n",
      "epoch 3; batch 80768; loss 0.049588\n",
      "epoch:3; batch 80768; train accuracy: 0.858412\n",
      "epoch 3; batch 80896; loss 0.040876\n",
      "epoch:3; batch 80896; train accuracy: 0.858471\n",
      "epoch 3; batch 81024; loss 0.047476\n",
      "epoch:3; batch 81024; train accuracy: 0.858529\n",
      "epoch 3; batch 81152; loss 0.071458\n",
      "epoch:3; batch 81152; train accuracy: 0.858585\n",
      "epoch 3; batch 81280; loss 0.047535\n",
      "epoch:3; batch 81280; train accuracy: 0.858640\n",
      "epoch 3; batch 81408; loss 0.018096\n",
      "epoch:3; batch 81408; train accuracy: 0.858699\n",
      "epoch 3; batch 81536; loss 0.114559\n",
      "epoch:3; batch 81536; train accuracy: 0.858737\n",
      "epoch 3; batch 81664; loss 0.085465\n",
      "epoch:3; batch 81664; train accuracy: 0.858785\n",
      "epoch 3; batch 81792; loss 0.110802\n",
      "epoch:3; batch 81792; train accuracy: 0.858827\n",
      "epoch 3; batch 81920; loss 0.097339\n",
      "epoch:3; batch 81920; train accuracy: 0.858878\n",
      "epoch 3; batch 82048; loss 0.063950\n",
      "epoch:3; batch 82048; train accuracy: 0.858933\n",
      "epoch 3; batch 82176; loss 0.093263\n",
      "epoch:3; batch 82176; train accuracy: 0.858985\n",
      "epoch 3; batch 82304; loss 0.030222\n",
      "epoch:3; batch 82304; train accuracy: 0.859043\n",
      "epoch 3; batch 82432; loss 0.124004\n",
      "epoch:3; batch 82432; train accuracy: 0.859098\n",
      "epoch 3; batch 82560; loss 0.048403\n",
      "epoch:3; batch 82560; train accuracy: 0.859149\n",
      "epoch 3; batch 82688; loss 0.068445\n",
      "epoch:3; batch 82688; train accuracy: 0.859208\n",
      "epoch 3; batch 82816; loss 0.015033\n",
      "epoch:3; batch 82816; train accuracy: 0.859269\n",
      "epoch 3; batch 82944; loss 0.062964\n",
      "epoch:3; batch 82944; train accuracy: 0.859327\n",
      "epoch 3; batch 83072; loss 0.148606\n",
      "epoch:3; batch 83072; train accuracy: 0.859375\n",
      "epoch 3; batch 83200; loss 0.040257\n",
      "epoch:3; batch 83200; train accuracy: 0.859430\n",
      "epoch 3; batch 83328; loss 0.041946\n",
      "epoch:3; batch 83328; train accuracy: 0.859484\n",
      "epoch 3; batch 83456; loss 0.041800\n",
      "epoch:3; batch 83456; train accuracy: 0.859539\n",
      "epoch 3; batch 83584; loss 0.030595\n",
      "epoch:3; batch 83584; train accuracy: 0.859593\n",
      "epoch 3; batch 83712; loss 0.039832\n",
      "epoch:3; batch 83712; train accuracy: 0.859651\n",
      "epoch 3; batch 83840; loss 0.035374\n",
      "epoch:3; batch 83840; train accuracy: 0.859709\n",
      "epoch 3; batch 83968; loss 0.031188\n",
      "epoch:3; batch 83968; train accuracy: 0.859766\n",
      "epoch 3; batch 84096; loss 0.034992\n",
      "epoch:3; batch 84096; train accuracy: 0.859824\n",
      "epoch 3; batch 84224; loss 0.011154\n",
      "epoch:3; batch 84224; train accuracy: 0.859885\n",
      "epoch 3; batch 84352; loss 0.094017\n",
      "epoch:3; batch 84352; train accuracy: 0.859936\n",
      "epoch 3; batch 84480; loss 0.048381\n",
      "epoch:3; batch 84480; train accuracy: 0.859983\n",
      "epoch 3; batch 84608; loss 0.040635\n",
      "epoch:3; batch 84608; train accuracy: 0.860034\n",
      "epoch 3; batch 84736; loss 0.082912\n",
      "epoch:3; batch 84736; train accuracy: 0.860088\n",
      "epoch 3; batch 84864; loss 0.018760\n",
      "epoch:3; batch 84864; train accuracy: 0.860145\n",
      "epoch 3; batch 84992; loss 0.037853\n",
      "epoch:3; batch 84992; train accuracy: 0.860199\n",
      "epoch 3; batch 85120; loss 0.022577\n",
      "epoch:3; batch 85120; train accuracy: 0.860260\n",
      "epoch 3; batch 85248; loss 0.067096\n",
      "epoch:3; batch 85248; train accuracy: 0.860303\n",
      "epoch 3; batch 85376; loss 0.017770\n",
      "epoch:3; batch 85376; train accuracy: 0.860364\n",
      "epoch 3; batch 85504; loss 0.035974\n",
      "epoch:3; batch 85504; train accuracy: 0.860418\n",
      "epoch 3; batch 85632; loss 0.067207\n",
      "epoch:3; batch 85632; train accuracy: 0.860471\n",
      "epoch 3; batch 85760; loss 0.076199\n",
      "epoch:3; batch 85760; train accuracy: 0.860525\n",
      "epoch 3; batch 85888; loss 0.019262\n",
      "epoch:3; batch 85888; train accuracy: 0.860582\n",
      "epoch 3; batch 86016; loss 0.016226\n",
      "epoch:3; batch 86016; train accuracy: 0.860642\n",
      "epoch 3; batch 86144; loss 0.027845\n",
      "epoch:3; batch 86144; train accuracy: 0.860696\n",
      "epoch 3; batch 86272; loss 0.071462\n",
      "epoch:3; batch 86272; train accuracy: 0.860752\n",
      "epoch 3; batch 86400; loss 0.068235\n",
      "epoch:3; batch 86400; train accuracy: 0.860796\n",
      "epoch 3; batch 86528; loss 0.145601\n",
      "epoch:3; batch 86528; train accuracy: 0.860832\n",
      "epoch 3; batch 86656; loss 0.051226\n",
      "epoch:3; batch 86656; train accuracy: 0.860886\n",
      "epoch 3; batch 86784; loss 0.113534\n",
      "epoch:3; batch 86784; train accuracy: 0.860932\n",
      "epoch 3; batch 86912; loss 0.050604\n",
      "epoch:3; batch 86912; train accuracy: 0.860985\n",
      "epoch 3; batch 87040; loss 0.051555\n",
      "epoch:3; batch 87040; train accuracy: 0.861035\n",
      "epoch 3; batch 87168; loss 0.046691\n",
      "epoch:3; batch 87168; train accuracy: 0.861085\n",
      "epoch 3; batch 87296; loss 0.012493\n",
      "epoch:3; batch 87296; train accuracy: 0.861145\n",
      "epoch 3; batch 87424; loss 0.043835\n",
      "epoch:3; batch 87424; train accuracy: 0.861198\n",
      "epoch 3; batch 87552; loss 0.060543\n",
      "epoch:3; batch 87552; train accuracy: 0.861251\n",
      "epoch 3; batch 87680; loss 0.056949\n",
      "epoch:3; batch 87680; train accuracy: 0.861300\n",
      "epoch 3; batch 87808; loss 0.061850\n",
      "epoch:3; batch 87808; train accuracy: 0.861347\n",
      "epoch 3; batch 87936; loss 0.033904\n",
      "epoch:3; batch 87936; train accuracy: 0.861403\n",
      "epoch 3; batch 88064; loss 0.041858\n",
      "epoch:3; batch 88064; train accuracy: 0.861449\n",
      "epoch 3; batch 88192; loss 0.130988\n",
      "epoch:3; batch 88192; train accuracy: 0.861492\n",
      "epoch 3; batch 88320; loss 0.073973\n",
      "epoch:3; batch 88320; train accuracy: 0.861541\n",
      "epoch 3; batch 88448; loss 0.056849\n",
      "epoch:3; batch 88448; train accuracy: 0.861590\n",
      "epoch 3; batch 88576; loss 0.070077\n",
      "epoch:3; batch 88576; train accuracy: 0.861643\n",
      "epoch 3; batch 88704; loss 0.146864\n",
      "epoch:3; batch 88704; train accuracy: 0.861686\n",
      "epoch 3; batch 88832; loss 0.079260\n",
      "epoch:3; batch 88832; train accuracy: 0.861728\n",
      "epoch 3; batch 88960; loss 0.076006\n",
      "epoch:3; batch 88960; train accuracy: 0.861777\n",
      "epoch 3; batch 89088; loss 0.118445\n",
      "epoch:3; batch 89088; train accuracy: 0.861816\n",
      "epoch 3; batch 89216; loss 0.040510\n",
      "epoch:3; batch 89216; train accuracy: 0.861869\n",
      "epoch 3; batch 89344; loss 0.031430\n",
      "epoch:3; batch 89344; train accuracy: 0.861925\n",
      "epoch 3; batch 89472; loss 0.026249\n",
      "epoch:3; batch 89472; train accuracy: 0.861977\n",
      "epoch 3; batch 89600; loss 0.070072\n",
      "epoch:3; batch 89600; train accuracy: 0.862026\n",
      "epoch 3; batch 89728; loss 0.065684\n",
      "epoch:3; batch 89728; train accuracy: 0.862082\n",
      "epoch 3; batch 89856; loss 0.096746\n",
      "epoch:3; batch 89856; train accuracy: 0.862137\n",
      "epoch 3; batch 89984; loss 0.046186\n",
      "epoch:3; batch 89984; train accuracy: 0.862186\n",
      "epoch 3; batch 90112; loss 0.069570\n",
      "epoch:3; batch 90112; train accuracy: 0.862235\n",
      "epoch 3; batch 90240; loss 0.034140\n",
      "epoch:3; batch 90240; train accuracy: 0.862290\n",
      "epoch 3; batch 90368; loss 0.034162\n",
      "epoch:3; batch 90368; train accuracy: 0.862342\n",
      "epoch 3; batch 90496; loss 0.057118\n",
      "epoch:3; batch 90496; train accuracy: 0.862397\n",
      "epoch 3; batch 90624; loss 0.060754\n",
      "epoch:3; batch 90624; train accuracy: 0.862446\n",
      "epoch 3; batch 90752; loss 0.063862\n",
      "epoch:3; batch 90752; train accuracy: 0.862495\n",
      "epoch 3; batch 90880; loss 0.057335\n",
      "epoch:3; batch 90880; train accuracy: 0.862550\n",
      "epoch 3; batch 91008; loss 0.038436\n",
      "epoch:3; batch 91008; train accuracy: 0.862602\n",
      "epoch 3; batch 91136; loss 0.096903\n",
      "epoch:3; batch 91136; train accuracy: 0.862653\n",
      "epoch 3; batch 91264; loss 0.031377\n",
      "epoch:3; batch 91264; train accuracy: 0.862709\n",
      "epoch 3; batch 91392; loss 0.017375\n",
      "epoch:3; batch 91392; train accuracy: 0.862767\n",
      "epoch 3; batch 91520; loss 0.085410\n",
      "epoch:3; batch 91520; train accuracy: 0.862812\n",
      "epoch 3; batch 91648; loss 0.074799\n",
      "epoch:3; batch 91648; train accuracy: 0.862867\n",
      "epoch 3; batch 91776; loss 0.018210\n",
      "epoch:3; batch 91776; train accuracy: 0.862925\n",
      "epoch 3; batch 91904; loss 0.032080\n",
      "epoch:3; batch 91904; train accuracy: 0.862976\n",
      "epoch 3; batch 92032; loss 0.021688\n",
      "epoch:3; batch 92032; train accuracy: 0.863031\n",
      "epoch 3; batch 92160; loss 0.137243\n",
      "epoch:3; batch 92160; train accuracy: 0.863069\n",
      "epoch 3; batch 92288; loss 0.089875\n",
      "epoch:3; batch 92288; train accuracy: 0.863117\n",
      "epoch 3; batch 92416; loss 0.026276\n",
      "epoch:3; batch 92416; train accuracy: 0.863172\n",
      "epoch 3; batch 92544; loss 0.078589\n",
      "epoch:3; batch 92544; train accuracy: 0.863217\n",
      "epoch 3; batch 92672; loss 0.058553\n",
      "epoch:3; batch 92672; train accuracy: 0.863268\n",
      "epoch 3; batch 92800; loss 0.068095\n",
      "epoch:3; batch 92800; train accuracy: 0.863313\n",
      "epoch 3; batch 92928; loss 0.032325\n",
      "epoch:3; batch 92928; train accuracy: 0.863364\n",
      "epoch 3; batch 93056; loss 0.029603\n",
      "epoch:3; batch 93056; train accuracy: 0.863415\n",
      "epoch 3; batch 93184; loss 0.056329\n",
      "epoch:3; batch 93184; train accuracy: 0.863466\n",
      "epoch 3; batch 93312; loss 0.024937\n",
      "epoch:3; batch 93312; train accuracy: 0.863520\n",
      "epoch 3; batch 93440; loss 0.033532\n",
      "epoch:3; batch 93440; train accuracy: 0.863571\n",
      "epoch 3; batch 93568; loss 0.064311\n",
      "epoch:3; batch 93568; train accuracy: 0.863622\n",
      "epoch 3; batch 93696; loss 0.048994\n",
      "epoch:3; batch 93696; train accuracy: 0.863673\n",
      "epoch 3; batch 93824; loss 0.064902\n",
      "epoch:3; batch 93824; train accuracy: 0.863717\n",
      "epoch 3; batch 93952; loss 0.109845\n",
      "epoch:3; batch 93952; train accuracy: 0.863758\n",
      "epoch 3; batch 94080; loss 0.086609\n",
      "epoch:3; batch 94080; train accuracy: 0.863806\n",
      "epoch 3; batch 94208; loss 0.032465\n",
      "epoch:3; batch 94208; train accuracy: 0.863857\n",
      "epoch 3; batch 94336; loss 0.016118\n",
      "epoch:3; batch 94336; train accuracy: 0.863911\n",
      "epoch 3; batch 94464; loss 0.118559\n",
      "epoch:3; batch 94464; train accuracy: 0.863955\n",
      "epoch 3; batch 94592; loss 0.068742\n",
      "epoch:3; batch 94592; train accuracy: 0.864002\n",
      "epoch 3; batch 94720; loss 0.026988\n",
      "epoch:3; batch 94720; train accuracy: 0.864056\n",
      "epoch 3; batch 94848; loss 0.098164\n",
      "epoch:3; batch 94848; train accuracy: 0.864093\n",
      "epoch 3; batch 94976; loss 0.054157\n",
      "epoch:3; batch 94976; train accuracy: 0.864141\n",
      "epoch 3; batch 95104; loss 0.021187\n",
      "epoch:3; batch 95104; train accuracy: 0.864194\n",
      "epoch 3; batch 95232; loss 0.056352\n",
      "epoch:3; batch 95232; train accuracy: 0.864241\n",
      "epoch 3; batch 95360; loss 0.041367\n",
      "epoch:3; batch 95360; train accuracy: 0.864292\n",
      "epoch 3; batch 95488; loss 0.033806\n",
      "epoch:3; batch 95488; train accuracy: 0.864342\n",
      "epoch 3; batch 95616; loss 0.018074\n",
      "epoch:3; batch 95616; train accuracy: 0.864399\n",
      "epoch 3; batch 95744; loss 0.097889\n",
      "epoch:3; batch 95744; train accuracy: 0.864436\n",
      "epoch 3; batch 95872; loss 0.128367\n",
      "epoch:3; batch 95872; train accuracy: 0.864480\n",
      "epoch 3; batch 96000; loss 0.068990\n",
      "epoch:3; batch 96000; train accuracy: 0.864520\n",
      "epoch 3; batch 96128; loss 0.031154\n",
      "epoch:3; batch 96128; train accuracy: 0.864570\n",
      "epoch 3; batch 96256; loss 0.113674\n",
      "epoch:3; batch 96256; train accuracy: 0.864614\n",
      "epoch 3; batch 96384; loss 0.020862\n",
      "epoch:3; batch 96384; train accuracy: 0.864670\n",
      "epoch 3; batch 96512; loss 0.051978\n",
      "epoch:3; batch 96512; train accuracy: 0.864717\n",
      "epoch 3; batch 96640; loss 0.030200\n",
      "epoch:3; batch 96640; train accuracy: 0.864767\n",
      "epoch 3; batch 96768; loss 0.029181\n",
      "epoch:3; batch 96768; train accuracy: 0.864817\n",
      "epoch 3; batch 96896; loss 0.037289\n",
      "epoch:3; batch 96896; train accuracy: 0.864867\n",
      "epoch 3; batch 97024; loss 0.059743\n",
      "epoch:3; batch 97024; train accuracy: 0.864917\n",
      "epoch 3; batch 97152; loss 0.034315\n",
      "epoch:3; batch 97152; train accuracy: 0.864967\n",
      "epoch 3; batch 97280; loss 0.074695\n",
      "epoch:3; batch 97280; train accuracy: 0.865020\n",
      "epoch 3; batch 97408; loss 0.046606\n",
      "epoch:3; batch 97408; train accuracy: 0.865069\n",
      "epoch 3; batch 97536; loss 0.038533\n",
      "epoch:3; batch 97536; train accuracy: 0.865119\n",
      "epoch 3; batch 97664; loss 0.067281\n",
      "epoch:3; batch 97664; train accuracy: 0.865169\n",
      "epoch 3; batch 97792; loss 0.170732\n",
      "epoch:3; batch 97792; train accuracy: 0.865215\n",
      "epoch 3; batch 97920; loss 0.076118\n",
      "epoch:3; batch 97920; train accuracy: 0.865261\n",
      "epoch 3; batch 98048; loss 0.012364\n",
      "epoch:3; batch 98048; train accuracy: 0.865314\n",
      "epoch 3; batch 98176; loss 0.113296\n",
      "epoch:3; batch 98176; train accuracy: 0.865357\n",
      "epoch 3; batch 98304; loss 0.023581\n",
      "epoch:3; batch 98304; train accuracy: 0.865406\n",
      "epoch 3; batch 98432; loss 0.031075\n",
      "epoch:3; batch 98432; train accuracy: 0.865459\n",
      "epoch 3; batch 98560; loss 0.075677\n",
      "epoch:3; batch 98560; train accuracy: 0.865495\n",
      "epoch 3; batch 98688; loss 0.045340\n",
      "epoch:3; batch 98688; train accuracy: 0.865541\n",
      "epoch 3; batch 98816; loss 0.048928\n",
      "epoch:3; batch 98816; train accuracy: 0.865591\n",
      "epoch 3; batch 98944; loss 0.015731\n",
      "epoch:3; batch 98944; train accuracy: 0.865646\n",
      "epoch 3; batch 99072; loss 0.043295\n",
      "epoch:3; batch 99072; train accuracy: 0.865699\n",
      "epoch 3; batch 99200; loss 0.111521\n",
      "epoch:3; batch 99200; train accuracy: 0.865741\n",
      "epoch 3; batch 99328; loss 0.091026\n",
      "epoch:3; batch 99328; train accuracy: 0.865781\n",
      "epoch 3; batch 99456; loss 0.074253\n",
      "epoch:3; batch 99456; train accuracy: 0.865817\n",
      "epoch 3; batch 99584; loss 0.054330\n",
      "epoch:3; batch 99584; train accuracy: 0.865863\n",
      "epoch 3; batch 99712; loss 0.111432\n",
      "epoch:3; batch 99712; train accuracy: 0.865896\n",
      "epoch 3; batch 99840; loss 0.025399\n",
      "epoch:3; batch 99840; train accuracy: 0.865951\n",
      "epoch 3; batch 99968; loss 0.061003\n",
      "epoch:3; batch 99968; train accuracy: 0.866000\n",
      "epoch 3; batch 100096; loss 0.102850\n",
      "epoch:3; batch 100096; train accuracy: 0.866042\n",
      "epoch 3; batch 100224; loss 0.040192\n",
      "epoch:3; batch 100224; train accuracy: 0.866091\n",
      "epoch 3; batch 100352; loss 0.083869\n",
      "epoch:3; batch 100352; train accuracy: 0.866140\n",
      "epoch 3; batch 100480; loss 0.079592\n",
      "epoch:3; batch 100480; train accuracy: 0.866182\n",
      "epoch 3; batch 100608; loss 0.089200\n",
      "epoch:3; batch 100608; train accuracy: 0.866228\n",
      "epoch 3; batch 100736; loss 0.029119\n",
      "epoch:3; batch 100736; train accuracy: 0.866280\n",
      "epoch 3; batch 100864; loss 0.028941\n",
      "epoch:3; batch 100864; train accuracy: 0.866332\n",
      "epoch 3; batch 100992; loss 0.056646\n",
      "epoch:3; batch 100992; train accuracy: 0.866374\n",
      "epoch 3; batch 101120; loss 0.025060\n",
      "epoch:3; batch 101120; train accuracy: 0.866426\n",
      "epoch 3; batch 101248; loss 0.038514\n",
      "epoch:3; batch 101248; train accuracy: 0.866477\n",
      "epoch 3; batch 101376; loss 0.039352\n",
      "epoch:3; batch 101376; train accuracy: 0.866526\n",
      "epoch 3; batch 101504; loss 0.037589\n",
      "epoch:3; batch 101504; train accuracy: 0.866577\n",
      "epoch 3; batch 101632; loss 0.081820\n",
      "epoch:3; batch 101632; train accuracy: 0.866623\n",
      "epoch 3; batch 101760; loss 0.072750\n",
      "epoch:3; batch 101760; train accuracy: 0.866671\n",
      "epoch 3; batch 101888; loss 0.091550\n",
      "epoch:3; batch 101888; train accuracy: 0.866716\n",
      "epoch 3; batch 102016; loss 0.096316\n",
      "epoch:3; batch 102016; train accuracy: 0.866758\n",
      "epoch 3; batch 102144; loss 0.026330\n",
      "epoch:3; batch 102144; train accuracy: 0.866809\n",
      "epoch 3; batch 102272; loss 0.107812\n",
      "epoch:3; batch 102272; train accuracy: 0.866848\n",
      "epoch 3; batch 102400; loss 0.093188\n",
      "epoch:3; batch 102400; train accuracy: 0.866893\n",
      "epoch 3; batch 102528; loss 0.092251\n",
      "epoch:3; batch 102528; train accuracy: 0.866928\n",
      "epoch 3; batch 102656; loss 0.076096\n",
      "epoch:3; batch 102656; train accuracy: 0.866967\n",
      "epoch 3; batch 102784; loss 0.122398\n",
      "epoch:3; batch 102784; train accuracy: 0.867002\n",
      "epoch 3; batch 102912; loss 0.039986\n",
      "epoch:3; batch 102912; train accuracy: 0.867053\n",
      "epoch 3; batch 103040; loss 0.115966\n",
      "epoch:3; batch 103040; train accuracy: 0.867095\n",
      "epoch 3; batch 103168; loss 0.020470\n",
      "epoch:3; batch 103168; train accuracy: 0.867149\n",
      "epoch 3; batch 103296; loss 0.079490\n",
      "epoch:3; batch 103296; train accuracy: 0.867191\n",
      "epoch 3; batch 103424; loss 0.076459\n",
      "epoch:3; batch 103424; train accuracy: 0.867229\n",
      "epoch 3; batch 103552; loss 0.038831\n",
      "epoch:3; batch 103552; train accuracy: 0.867280\n",
      "epoch 3; batch 103680; loss 0.034537\n",
      "epoch:3; batch 103680; train accuracy: 0.867328\n",
      "epoch 3; batch 103808; loss 0.022615\n",
      "epoch:3; batch 103808; train accuracy: 0.867379\n",
      "epoch 3; batch 103936; loss 0.037305\n",
      "epoch:3; batch 103936; train accuracy: 0.867430\n",
      "epoch 3; batch 104064; loss 0.084024\n",
      "epoch:3; batch 104064; train accuracy: 0.867471\n",
      "epoch 3; batch 104192; loss 0.041675\n",
      "epoch:3; batch 104192; train accuracy: 0.867515\n",
      "epoch 3; batch 104320; loss 0.021709\n",
      "epoch:3; batch 104320; train accuracy: 0.867569\n",
      "epoch 3; batch 104448; loss 0.027026\n",
      "epoch:3; batch 104448; train accuracy: 0.867620\n",
      "epoch 3; batch 104576; loss 0.036246\n",
      "epoch:3; batch 104576; train accuracy: 0.867671\n",
      "epoch 3; batch 104704; loss 0.024198\n",
      "epoch:3; batch 104704; train accuracy: 0.867718\n",
      "epoch 3; batch 104832; loss 0.185230\n",
      "epoch:3; batch 104832; train accuracy: 0.867756\n",
      "epoch 3; batch 104960; loss 0.030214\n",
      "epoch:3; batch 104960; train accuracy: 0.867807\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 37150 ; rate: 0.353944\n",
      "y_true_label_1_num: 8747 ; rate: 0.083337\n",
      "y_true_label_2_num: 10204 ; rate: 0.097218\n",
      "y_true_label_3_num: 48859 ; rate: 0.465501\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.977468\n",
      "valid avg_precision: 0.979932\n",
      "valid avg_recall: 0.976420\n",
      "valid avg_f1: 0.977836\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 5391 ; rate: 0.359976\n",
      "y_true_label_1_num: 1214 ; rate: 0.081063\n",
      "y_true_label_2_num: 1418 ; rate: 0.094685\n",
      "y_true_label_3_num: 6953 ; rate: 0.464276\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.741386\n",
      "valid avg_precision: 0.779826\n",
      "valid avg_recall: 0.736912\n",
      "valid avg_f1: 0.755097\n",
      "epoch 4\n",
      "epoch 4; batch 128; loss 0.192242\n",
      "epoch:4; batch 128; train accuracy: 0.867829\n",
      "epoch 4; batch 256; loss 0.037111\n",
      "epoch:4; batch 256; train accuracy: 0.867873\n",
      "epoch 4; batch 384; loss 0.163849\n",
      "epoch:4; batch 384; train accuracy: 0.867904\n",
      "epoch 4; batch 512; loss 0.099106\n",
      "epoch:4; batch 512; train accuracy: 0.867939\n",
      "epoch 4; batch 640; loss 0.108216\n",
      "epoch:4; batch 640; train accuracy: 0.867977\n",
      "epoch 4; batch 768; loss 0.192130\n",
      "epoch:4; batch 768; train accuracy: 0.868008\n",
      "epoch 4; batch 896; loss 0.041833\n",
      "epoch:4; batch 896; train accuracy: 0.868052\n",
      "epoch 4; batch 1024; loss 0.070242\n",
      "epoch:4; batch 1024; train accuracy: 0.868090\n",
      "epoch 4; batch 1152; loss 0.194301\n",
      "epoch:4; batch 1152; train accuracy: 0.868124\n",
      "epoch 4; batch 1280; loss 0.068541\n",
      "epoch:4; batch 1280; train accuracy: 0.868162\n",
      "epoch 4; batch 1408; loss 0.040867\n",
      "epoch:4; batch 1408; train accuracy: 0.868209\n",
      "epoch 4; batch 1536; loss 0.034773\n",
      "epoch:4; batch 1536; train accuracy: 0.868259\n",
      "epoch 4; batch 1664; loss 0.099835\n",
      "epoch:4; batch 1664; train accuracy: 0.868309\n",
      "epoch 4; batch 1792; loss 0.041502\n",
      "epoch:4; batch 1792; train accuracy: 0.868356\n",
      "epoch 4; batch 1920; loss 0.045580\n",
      "epoch:4; batch 1920; train accuracy: 0.868400\n",
      "epoch 4; batch 2048; loss 0.080574\n",
      "epoch:4; batch 2048; train accuracy: 0.868440\n",
      "epoch 4; batch 2176; loss 0.026785\n",
      "epoch:4; batch 2176; train accuracy: 0.868487\n",
      "epoch 4; batch 2304; loss 0.016915\n",
      "epoch:4; batch 2304; train accuracy: 0.868537\n",
      "epoch 4; batch 2432; loss 0.061065\n",
      "epoch:4; batch 2432; train accuracy: 0.868577\n",
      "epoch 4; batch 2560; loss 0.018058\n",
      "epoch:4; batch 2560; train accuracy: 0.868627\n",
      "epoch 4; batch 2688; loss 0.011220\n",
      "epoch:4; batch 2688; train accuracy: 0.868680\n",
      "epoch 4; batch 2816; loss 0.026806\n",
      "epoch:4; batch 2816; train accuracy: 0.868730\n",
      "epoch 4; batch 2944; loss 0.044583\n",
      "epoch:4; batch 2944; train accuracy: 0.868776\n",
      "epoch 4; batch 3072; loss 0.038974\n",
      "epoch:4; batch 3072; train accuracy: 0.868820\n",
      "epoch 4; batch 3200; loss 0.047963\n",
      "epoch:4; batch 3200; train accuracy: 0.868866\n",
      "epoch 4; batch 3328; loss 0.054139\n",
      "epoch:4; batch 3328; train accuracy: 0.868916\n",
      "epoch 4; batch 3456; loss 0.137714\n",
      "epoch:4; batch 3456; train accuracy: 0.868956\n",
      "epoch 4; batch 3584; loss 0.011525\n",
      "epoch:4; batch 3584; train accuracy: 0.869009\n",
      "epoch 4; batch 3712; loss 0.032201\n",
      "epoch:4; batch 3712; train accuracy: 0.869052\n",
      "epoch 4; batch 3840; loss 0.014020\n",
      "epoch:4; batch 3840; train accuracy: 0.869101\n",
      "epoch 4; batch 3968; loss 0.036519\n",
      "epoch:4; batch 3968; train accuracy: 0.869148\n",
      "epoch 4; batch 4096; loss 0.042740\n",
      "epoch:4; batch 4096; train accuracy: 0.869194\n",
      "epoch 4; batch 4224; loss 0.034094\n",
      "epoch:4; batch 4224; train accuracy: 0.869243\n",
      "epoch 4; batch 4352; loss 0.022761\n",
      "epoch:4; batch 4352; train accuracy: 0.869293\n",
      "epoch 4; batch 4480; loss 0.025750\n",
      "epoch:4; batch 4480; train accuracy: 0.869342\n",
      "epoch 4; batch 4608; loss 0.022923\n",
      "epoch:4; batch 4608; train accuracy: 0.869388\n",
      "epoch 4; batch 4736; loss 0.036357\n",
      "epoch:4; batch 4736; train accuracy: 0.869434\n",
      "epoch 4; batch 4864; loss 0.053612\n",
      "epoch:4; batch 4864; train accuracy: 0.869477\n",
      "epoch 4; batch 4992; loss 0.038125\n",
      "epoch:4; batch 4992; train accuracy: 0.869526\n",
      "epoch 4; batch 5120; loss 0.027536\n",
      "epoch:4; batch 5120; train accuracy: 0.869572\n",
      "epoch 4; batch 5248; loss 0.042903\n",
      "epoch:4; batch 5248; train accuracy: 0.869615\n",
      "epoch 4; batch 5376; loss 0.016392\n",
      "epoch:4; batch 5376; train accuracy: 0.869664\n",
      "epoch 4; batch 5504; loss 0.034058\n",
      "epoch:4; batch 5504; train accuracy: 0.869713\n",
      "epoch 4; batch 5632; loss 0.063851\n",
      "epoch:4; batch 5632; train accuracy: 0.869752\n",
      "epoch 4; batch 5760; loss 0.072117\n",
      "epoch:4; batch 5760; train accuracy: 0.869795\n",
      "epoch 4; batch 5888; loss 0.008720\n",
      "epoch:4; batch 5888; train accuracy: 0.869847\n",
      "epoch 4; batch 6016; loss 0.019270\n",
      "epoch:4; batch 6016; train accuracy: 0.869896\n",
      "epoch 4; batch 6144; loss 0.013869\n",
      "epoch:4; batch 6144; train accuracy: 0.869944\n",
      "epoch 4; batch 6272; loss 0.008065\n",
      "epoch:4; batch 6272; train accuracy: 0.869996\n",
      "epoch 4; batch 6400; loss 0.033905\n",
      "epoch:4; batch 6400; train accuracy: 0.870045\n",
      "epoch 4; batch 6528; loss 0.008166\n",
      "epoch:4; batch 6528; train accuracy: 0.870097\n",
      "epoch 4; batch 6656; loss 0.024335\n",
      "epoch:4; batch 6656; train accuracy: 0.870145\n",
      "epoch 4; batch 6784; loss 0.011338\n",
      "epoch:4; batch 6784; train accuracy: 0.870197\n",
      "epoch 4; batch 6912; loss 0.014748\n",
      "epoch:4; batch 6912; train accuracy: 0.870245\n",
      "epoch 4; batch 7040; loss 0.018584\n",
      "epoch:4; batch 7040; train accuracy: 0.870297\n",
      "epoch 4; batch 7168; loss 0.020075\n",
      "epoch:4; batch 7168; train accuracy: 0.870345\n",
      "epoch 4; batch 7296; loss 0.063109\n",
      "epoch:4; batch 7296; train accuracy: 0.870391\n",
      "epoch 4; batch 7424; loss 0.020044\n",
      "epoch:4; batch 7424; train accuracy: 0.870439\n",
      "epoch 4; batch 7552; loss 0.077392\n",
      "epoch:4; batch 7552; train accuracy: 0.870484\n",
      "epoch 4; batch 7680; loss 0.005644\n",
      "epoch:4; batch 7680; train accuracy: 0.870536\n",
      "epoch 4; batch 7808; loss 0.011013\n",
      "epoch:4; batch 7808; train accuracy: 0.870587\n",
      "epoch 4; batch 7936; loss 0.015302\n",
      "epoch:4; batch 7936; train accuracy: 0.870638\n",
      "epoch 4; batch 8064; loss 0.040109\n",
      "epoch:4; batch 8064; train accuracy: 0.870683\n",
      "epoch 4; batch 8192; loss 0.083830\n",
      "epoch:4; batch 8192; train accuracy: 0.870719\n",
      "epoch 4; batch 8320; loss 0.027517\n",
      "epoch:4; batch 8320; train accuracy: 0.870767\n",
      "epoch 4; batch 8448; loss 0.034167\n",
      "epoch:4; batch 8448; train accuracy: 0.870812\n",
      "epoch 4; batch 8576; loss 0.019471\n",
      "epoch:4; batch 8576; train accuracy: 0.870860\n",
      "epoch 4; batch 8704; loss 0.047667\n",
      "epoch:4; batch 8704; train accuracy: 0.870902\n",
      "epoch 4; batch 8832; loss 0.030950\n",
      "epoch:4; batch 8832; train accuracy: 0.870950\n",
      "epoch 4; batch 8960; loss 0.053202\n",
      "epoch:4; batch 8960; train accuracy: 0.870989\n",
      "epoch 4; batch 9088; loss 0.084496\n",
      "epoch:4; batch 9088; train accuracy: 0.871027\n",
      "epoch 4; batch 9216; loss 0.023282\n",
      "epoch:4; batch 9216; train accuracy: 0.871075\n",
      "epoch 4; batch 9344; loss 0.028673\n",
      "epoch:4; batch 9344; train accuracy: 0.871120\n",
      "epoch 4; batch 9472; loss 0.015775\n",
      "epoch:4; batch 9472; train accuracy: 0.871168\n",
      "epoch 4; batch 9600; loss 0.003718\n",
      "epoch:4; batch 9600; train accuracy: 0.871219\n",
      "epoch 4; batch 9728; loss 0.023587\n",
      "epoch:4; batch 9728; train accuracy: 0.871266\n",
      "epoch 4; batch 9856; loss 0.010696\n",
      "epoch:4; batch 9856; train accuracy: 0.871317\n",
      "epoch 4; batch 9984; loss 0.023834\n",
      "epoch:4; batch 9984; train accuracy: 0.871365\n",
      "epoch 4; batch 10112; loss 0.070462\n",
      "epoch:4; batch 10112; train accuracy: 0.871403\n",
      "epoch 4; batch 10240; loss 0.005854\n",
      "epoch:4; batch 10240; train accuracy: 0.871454\n",
      "epoch 4; batch 10368; loss 0.008319\n",
      "epoch:4; batch 10368; train accuracy: 0.871504\n",
      "epoch 4; batch 10496; loss 0.099034\n",
      "epoch:4; batch 10496; train accuracy: 0.871549\n",
      "epoch 4; batch 10624; loss 0.012595\n",
      "epoch:4; batch 10624; train accuracy: 0.871596\n",
      "epoch 4; batch 10752; loss 0.016753\n",
      "epoch:4; batch 10752; train accuracy: 0.871643\n",
      "epoch 4; batch 10880; loss 0.069505\n",
      "epoch:4; batch 10880; train accuracy: 0.871675\n",
      "epoch 4; batch 11008; loss 0.029987\n",
      "epoch:4; batch 11008; train accuracy: 0.871720\n",
      "epoch 4; batch 11136; loss 0.027188\n",
      "epoch:4; batch 11136; train accuracy: 0.871767\n",
      "epoch 4; batch 11264; loss 0.003409\n",
      "epoch:4; batch 11264; train accuracy: 0.871817\n",
      "epoch 4; batch 11392; loss 0.033190\n",
      "epoch:4; batch 11392; train accuracy: 0.871862\n",
      "epoch 4; batch 11520; loss 0.013484\n",
      "epoch:4; batch 11520; train accuracy: 0.871909\n",
      "epoch 4; batch 11648; loss 0.005632\n",
      "epoch:4; batch 11648; train accuracy: 0.871959\n",
      "epoch 4; batch 11776; loss 0.060792\n",
      "epoch:4; batch 11776; train accuracy: 0.872003\n",
      "epoch 4; batch 11904; loss 0.007138\n",
      "epoch:4; batch 11904; train accuracy: 0.872053\n",
      "epoch 4; batch 12032; loss 0.007378\n",
      "epoch:4; batch 12032; train accuracy: 0.872103\n",
      "epoch 4; batch 12160; loss 0.008344\n",
      "epoch:4; batch 12160; train accuracy: 0.872153\n",
      "epoch 4; batch 12288; loss 0.025059\n",
      "epoch:4; batch 12288; train accuracy: 0.872200\n",
      "epoch 4; batch 12416; loss 0.002887\n",
      "epoch:4; batch 12416; train accuracy: 0.872250\n",
      "epoch 4; batch 12544; loss 0.048100\n",
      "epoch:4; batch 12544; train accuracy: 0.872297\n",
      "epoch 4; batch 12672; loss 0.004356\n",
      "epoch:4; batch 12672; train accuracy: 0.872347\n",
      "epoch 4; batch 12800; loss 0.031342\n",
      "epoch:4; batch 12800; train accuracy: 0.872394\n",
      "epoch 4; batch 12928; loss 0.033084\n",
      "epoch:4; batch 12928; train accuracy: 0.872441\n",
      "epoch 4; batch 13056; loss 0.025356\n",
      "epoch:4; batch 13056; train accuracy: 0.872487\n",
      "epoch 4; batch 13184; loss 0.005111\n",
      "epoch:4; batch 13184; train accuracy: 0.872537\n",
      "epoch 4; batch 13312; loss 0.065649\n",
      "epoch:4; batch 13312; train accuracy: 0.872575\n",
      "epoch 4; batch 13440; loss 0.014417\n",
      "epoch:4; batch 13440; train accuracy: 0.872624\n",
      "epoch 4; batch 13568; loss 0.016705\n",
      "epoch:4; batch 13568; train accuracy: 0.872671\n",
      "epoch 4; batch 13696; loss 0.026270\n",
      "epoch:4; batch 13696; train accuracy: 0.872717\n",
      "epoch 4; batch 13824; loss 0.026459\n",
      "epoch:4; batch 13824; train accuracy: 0.872761\n",
      "epoch 4; batch 13952; loss 0.050787\n",
      "epoch:4; batch 13952; train accuracy: 0.872807\n",
      "epoch 4; batch 14080; loss 0.014055\n",
      "epoch:4; batch 14080; train accuracy: 0.872854\n",
      "epoch 4; batch 14208; loss 0.016731\n",
      "epoch:4; batch 14208; train accuracy: 0.872900\n",
      "epoch 4; batch 14336; loss 0.009674\n",
      "epoch:4; batch 14336; train accuracy: 0.872950\n",
      "epoch 4; batch 14464; loss 0.012550\n",
      "epoch:4; batch 14464; train accuracy: 0.872996\n",
      "epoch 4; batch 14592; loss 0.033237\n",
      "epoch:4; batch 14592; train accuracy: 0.873042\n",
      "epoch 4; batch 14720; loss 0.008993\n",
      "epoch:4; batch 14720; train accuracy: 0.873092\n",
      "epoch 4; batch 14848; loss 0.031436\n",
      "epoch:4; batch 14848; train accuracy: 0.873135\n",
      "epoch 4; batch 14976; loss 0.042965\n",
      "epoch:4; batch 14976; train accuracy: 0.873178\n",
      "epoch 4; batch 15104; loss 0.005651\n",
      "epoch:4; batch 15104; train accuracy: 0.873227\n",
      "epoch 4; batch 15232; loss 0.008024\n",
      "epoch:4; batch 15232; train accuracy: 0.873276\n",
      "epoch 4; batch 15360; loss 0.014060\n",
      "epoch:4; batch 15360; train accuracy: 0.873322\n",
      "epoch 4; batch 15488; loss 0.009400\n",
      "epoch:4; batch 15488; train accuracy: 0.873372\n",
      "epoch 4; batch 15616; loss 0.055642\n",
      "epoch:4; batch 15616; train accuracy: 0.873415\n",
      "epoch 4; batch 15744; loss 0.040883\n",
      "epoch:4; batch 15744; train accuracy: 0.873457\n",
      "epoch 4; batch 15872; loss 0.030544\n",
      "epoch:4; batch 15872; train accuracy: 0.873503\n",
      "epoch 4; batch 16000; loss 0.017005\n",
      "epoch:4; batch 16000; train accuracy: 0.873549\n",
      "epoch 4; batch 16128; loss 0.020880\n",
      "epoch:4; batch 16128; train accuracy: 0.873595\n",
      "epoch 4; batch 16256; loss 0.010330\n",
      "epoch:4; batch 16256; train accuracy: 0.873641\n",
      "epoch 4; batch 16384; loss 0.017368\n",
      "epoch:4; batch 16384; train accuracy: 0.873690\n",
      "epoch 4; batch 16512; loss 0.002670\n",
      "epoch:4; batch 16512; train accuracy: 0.873739\n",
      "epoch 4; batch 16640; loss 0.017436\n",
      "epoch:4; batch 16640; train accuracy: 0.873781\n",
      "epoch 4; batch 16768; loss 0.028346\n",
      "epoch:4; batch 16768; train accuracy: 0.873824\n",
      "epoch 4; batch 16896; loss 0.011376\n",
      "epoch:4; batch 16896; train accuracy: 0.873870\n",
      "epoch 4; batch 17024; loss 0.011985\n",
      "epoch:4; batch 17024; train accuracy: 0.873918\n",
      "epoch 4; batch 17152; loss 0.020323\n",
      "epoch:4; batch 17152; train accuracy: 0.873964\n",
      "epoch 4; batch 17280; loss 0.060013\n",
      "epoch:4; batch 17280; train accuracy: 0.873997\n",
      "epoch 4; batch 17408; loss 0.008276\n",
      "epoch:4; batch 17408; train accuracy: 0.874046\n",
      "epoch 4; batch 17536; loss 0.027071\n",
      "epoch:4; batch 17536; train accuracy: 0.874092\n",
      "epoch 4; batch 17664; loss 0.034932\n",
      "epoch:4; batch 17664; train accuracy: 0.874134\n",
      "epoch 4; batch 17792; loss 0.027268\n",
      "epoch:4; batch 17792; train accuracy: 0.874179\n",
      "epoch 4; batch 17920; loss 0.013238\n",
      "epoch:4; batch 17920; train accuracy: 0.874228\n",
      "epoch 4; batch 18048; loss 0.055947\n",
      "epoch:4; batch 18048; train accuracy: 0.874270\n",
      "epoch 4; batch 18176; loss 0.028783\n",
      "epoch:4; batch 18176; train accuracy: 0.874312\n",
      "epoch 4; batch 18304; loss 0.066719\n",
      "epoch:4; batch 18304; train accuracy: 0.874352\n",
      "epoch 4; batch 18432; loss 0.027020\n",
      "epoch:4; batch 18432; train accuracy: 0.874397\n",
      "epoch 4; batch 18560; loss 0.024446\n",
      "epoch:4; batch 18560; train accuracy: 0.874442\n",
      "epoch 4; batch 18688; loss 0.023557\n",
      "epoch:4; batch 18688; train accuracy: 0.874487\n",
      "epoch 4; batch 18816; loss 0.011365\n",
      "epoch:4; batch 18816; train accuracy: 0.874533\n",
      "epoch 4; batch 18944; loss 0.064713\n",
      "epoch:4; batch 18944; train accuracy: 0.874569\n",
      "epoch 4; batch 19072; loss 0.014943\n",
      "epoch:4; batch 19072; train accuracy: 0.874617\n",
      "epoch 4; batch 19200; loss 0.009919\n",
      "epoch:4; batch 19200; train accuracy: 0.874665\n",
      "epoch 4; batch 19328; loss 0.026858\n",
      "epoch:4; batch 19328; train accuracy: 0.874707\n",
      "epoch 4; batch 19456; loss 0.018870\n",
      "epoch:4; batch 19456; train accuracy: 0.874752\n",
      "epoch 4; batch 19584; loss 0.014701\n",
      "epoch:4; batch 19584; train accuracy: 0.874797\n",
      "epoch 4; batch 19712; loss 0.012038\n",
      "epoch:4; batch 19712; train accuracy: 0.874842\n",
      "epoch 4; batch 19840; loss 0.009158\n",
      "epoch:4; batch 19840; train accuracy: 0.874886\n",
      "epoch 4; batch 19968; loss 0.046983\n",
      "epoch:4; batch 19968; train accuracy: 0.874928\n",
      "epoch 4; batch 20096; loss 0.035166\n",
      "epoch:4; batch 20096; train accuracy: 0.874970\n",
      "epoch 4; batch 20224; loss 0.010935\n",
      "epoch:4; batch 20224; train accuracy: 0.875018\n",
      "epoch 4; batch 20352; loss 0.035671\n",
      "epoch:4; batch 20352; train accuracy: 0.875060\n",
      "epoch 4; batch 20480; loss 0.040111\n",
      "epoch:4; batch 20480; train accuracy: 0.875098\n",
      "epoch 4; batch 20608; loss 0.019335\n",
      "epoch:4; batch 20608; train accuracy: 0.875143\n",
      "epoch 4; batch 20736; loss 0.047435\n",
      "epoch:4; batch 20736; train accuracy: 0.875182\n",
      "epoch 4; batch 20864; loss 0.033307\n",
      "epoch:4; batch 20864; train accuracy: 0.875220\n",
      "epoch 4; batch 20992; loss 0.016071\n",
      "epoch:4; batch 20992; train accuracy: 0.875265\n",
      "epoch 4; batch 21120; loss 0.042021\n",
      "epoch:4; batch 21120; train accuracy: 0.875304\n",
      "epoch 4; batch 21248; loss 0.029181\n",
      "epoch:4; batch 21248; train accuracy: 0.875348\n",
      "epoch 4; batch 21376; loss 0.147445\n",
      "epoch:4; batch 21376; train accuracy: 0.875387\n",
      "epoch 4; batch 21504; loss 0.137735\n",
      "epoch:4; batch 21504; train accuracy: 0.875428\n",
      "epoch 4; batch 21632; loss 0.015281\n",
      "epoch:4; batch 21632; train accuracy: 0.875475\n",
      "epoch 4; batch 21760; loss 0.029520\n",
      "epoch:4; batch 21760; train accuracy: 0.875517\n",
      "epoch 4; batch 21888; loss 0.003697\n",
      "epoch:4; batch 21888; train accuracy: 0.875564\n",
      "epoch 4; batch 22016; loss 0.010964\n",
      "epoch:4; batch 22016; train accuracy: 0.875609\n",
      "epoch 4; batch 22144; loss 0.009561\n",
      "epoch:4; batch 22144; train accuracy: 0.875656\n",
      "epoch 4; batch 22272; loss 0.015647\n",
      "epoch:4; batch 22272; train accuracy: 0.875700\n",
      "epoch 4; batch 22400; loss 0.030411\n",
      "epoch:4; batch 22400; train accuracy: 0.875744\n",
      "epoch 4; batch 22528; loss 0.007110\n",
      "epoch:4; batch 22528; train accuracy: 0.875791\n",
      "epoch 4; batch 22656; loss 0.045097\n",
      "epoch:4; batch 22656; train accuracy: 0.875830\n",
      "epoch 4; batch 22784; loss 0.012170\n",
      "epoch:4; batch 22784; train accuracy: 0.875877\n",
      "epoch 4; batch 22912; loss 0.050107\n",
      "epoch:4; batch 22912; train accuracy: 0.875915\n",
      "epoch 4; batch 23040; loss 0.024152\n",
      "epoch:4; batch 23040; train accuracy: 0.875956\n",
      "epoch 4; batch 23168; loss 0.013380\n",
      "epoch:4; batch 23168; train accuracy: 0.876000\n",
      "epoch 4; batch 23296; loss 0.066802\n",
      "epoch:4; batch 23296; train accuracy: 0.876041\n",
      "epoch 4; batch 23424; loss 0.011877\n",
      "epoch:4; batch 23424; train accuracy: 0.876088\n",
      "epoch 4; batch 23552; loss 0.008106\n",
      "epoch:4; batch 23552; train accuracy: 0.876135\n",
      "epoch 4; batch 23680; loss 0.059774\n",
      "epoch:4; batch 23680; train accuracy: 0.876176\n",
      "epoch 4; batch 23808; loss 0.036612\n",
      "epoch:4; batch 23808; train accuracy: 0.876219\n",
      "epoch 4; batch 23936; loss 0.002864\n",
      "epoch:4; batch 23936; train accuracy: 0.876266\n",
      "epoch 4; batch 24064; loss 0.015466\n",
      "epoch:4; batch 24064; train accuracy: 0.876310\n",
      "epoch 4; batch 24192; loss 0.026737\n",
      "epoch:4; batch 24192; train accuracy: 0.876351\n",
      "epoch 4; batch 24320; loss 0.018099\n",
      "epoch:4; batch 24320; train accuracy: 0.876392\n",
      "epoch 4; batch 24448; loss 0.051353\n",
      "epoch:4; batch 24448; train accuracy: 0.876432\n",
      "epoch 4; batch 24576; loss 0.011947\n",
      "epoch:4; batch 24576; train accuracy: 0.876479\n",
      "epoch 4; batch 24704; loss 0.017241\n",
      "epoch:4; batch 24704; train accuracy: 0.876522\n",
      "epoch 4; batch 24832; loss 0.033649\n",
      "epoch:4; batch 24832; train accuracy: 0.876563\n",
      "epoch 4; batch 24960; loss 0.009057\n",
      "epoch:4; batch 24960; train accuracy: 0.876610\n",
      "epoch 4; batch 25088; loss 0.005162\n",
      "epoch:4; batch 25088; train accuracy: 0.876656\n",
      "epoch 4; batch 25216; loss 0.032761\n",
      "epoch:4; batch 25216; train accuracy: 0.876697\n",
      "epoch 4; batch 25344; loss 0.044789\n",
      "epoch:4; batch 25344; train accuracy: 0.876737\n",
      "epoch 4; batch 25472; loss 0.037448\n",
      "epoch:4; batch 25472; train accuracy: 0.876781\n",
      "epoch 4; batch 25600; loss 0.011478\n",
      "epoch:4; batch 25600; train accuracy: 0.876824\n",
      "epoch 4; batch 25728; loss 0.003150\n",
      "epoch:4; batch 25728; train accuracy: 0.876870\n",
      "epoch 4; batch 25856; loss 0.011436\n",
      "epoch:4; batch 25856; train accuracy: 0.876913\n",
      "epoch 4; batch 25984; loss 0.029639\n",
      "epoch:4; batch 25984; train accuracy: 0.876957\n",
      "epoch 4; batch 26112; loss 0.014654\n",
      "epoch:4; batch 26112; train accuracy: 0.877000\n",
      "epoch 4; batch 26240; loss 0.023077\n",
      "epoch:4; batch 26240; train accuracy: 0.877043\n",
      "epoch 4; batch 26368; loss 0.012557\n",
      "epoch:4; batch 26368; train accuracy: 0.877086\n",
      "epoch 4; batch 26496; loss 0.010602\n",
      "epoch:4; batch 26496; train accuracy: 0.877133\n",
      "epoch 4; batch 26624; loss 0.024220\n",
      "epoch:4; batch 26624; train accuracy: 0.877176\n",
      "epoch 4; batch 26752; loss 0.004775\n",
      "epoch:4; batch 26752; train accuracy: 0.877222\n",
      "epoch 4; batch 26880; loss 0.020539\n",
      "epoch:4; batch 26880; train accuracy: 0.877262\n",
      "epoch 4; batch 27008; loss 0.019645\n",
      "epoch:4; batch 27008; train accuracy: 0.877305\n",
      "epoch 4; batch 27136; loss 0.012457\n",
      "epoch:4; batch 27136; train accuracy: 0.877348\n",
      "epoch 4; batch 27264; loss 0.006619\n",
      "epoch:4; batch 27264; train accuracy: 0.877394\n",
      "epoch 4; batch 27392; loss 0.019681\n",
      "epoch:4; batch 27392; train accuracy: 0.877440\n",
      "epoch 4; batch 27520; loss 0.018483\n",
      "epoch:4; batch 27520; train accuracy: 0.877482\n",
      "epoch 4; batch 27648; loss 0.026857\n",
      "epoch:4; batch 27648; train accuracy: 0.877525\n",
      "epoch 4; batch 27776; loss 0.012824\n",
      "epoch:4; batch 27776; train accuracy: 0.877568\n",
      "epoch 4; batch 27904; loss 0.041384\n",
      "epoch:4; batch 27904; train accuracy: 0.877611\n",
      "epoch 4; batch 28032; loss 0.132357\n",
      "epoch:4; batch 28032; train accuracy: 0.877654\n",
      "epoch 4; batch 28160; loss 0.008594\n",
      "epoch:4; batch 28160; train accuracy: 0.877699\n",
      "epoch 4; batch 28288; loss 0.022958\n",
      "epoch:4; batch 28288; train accuracy: 0.877742\n",
      "epoch 4; batch 28416; loss 0.045329\n",
      "epoch:4; batch 28416; train accuracy: 0.877779\n",
      "epoch 4; batch 28544; loss 0.001577\n",
      "epoch:4; batch 28544; train accuracy: 0.877824\n",
      "epoch 4; batch 28672; loss 0.010736\n",
      "epoch:4; batch 28672; train accuracy: 0.877867\n",
      "epoch 4; batch 28800; loss 0.013868\n",
      "epoch:4; batch 28800; train accuracy: 0.877910\n",
      "epoch 4; batch 28928; loss 0.019821\n",
      "epoch:4; batch 28928; train accuracy: 0.877952\n",
      "epoch 4; batch 29056; loss 0.011480\n",
      "epoch:4; batch 29056; train accuracy: 0.877995\n",
      "epoch 4; batch 29184; loss 0.002792\n",
      "epoch:4; batch 29184; train accuracy: 0.878040\n",
      "epoch 4; batch 29312; loss 0.011077\n",
      "epoch:4; batch 29312; train accuracy: 0.878083\n",
      "epoch 4; batch 29440; loss 0.016248\n",
      "epoch:4; batch 29440; train accuracy: 0.878125\n",
      "epoch 4; batch 29568; loss 0.010156\n",
      "epoch:4; batch 29568; train accuracy: 0.878170\n",
      "epoch 4; batch 29696; loss 0.015053\n",
      "epoch:4; batch 29696; train accuracy: 0.878213\n",
      "epoch 4; batch 29824; loss 0.020330\n",
      "epoch:4; batch 29824; train accuracy: 0.878255\n",
      "epoch 4; batch 29952; loss 0.001992\n",
      "epoch:4; batch 29952; train accuracy: 0.878300\n",
      "epoch 4; batch 30080; loss 0.013107\n",
      "epoch:4; batch 30080; train accuracy: 0.878342\n",
      "epoch 4; batch 30208; loss 0.012055\n",
      "epoch:4; batch 30208; train accuracy: 0.878385\n",
      "epoch 4; batch 30336; loss 0.003095\n",
      "epoch:4; batch 30336; train accuracy: 0.878430\n",
      "epoch 4; batch 30464; loss 0.045631\n",
      "epoch:4; batch 30464; train accuracy: 0.878469\n",
      "epoch 4; batch 30592; loss 0.011670\n",
      "epoch:4; batch 30592; train accuracy: 0.878511\n",
      "epoch 4; batch 30720; loss 0.000853\n",
      "epoch:4; batch 30720; train accuracy: 0.878556\n",
      "epoch 4; batch 30848; loss 0.031132\n",
      "epoch:4; batch 30848; train accuracy: 0.878598\n",
      "epoch 4; batch 30976; loss 0.009417\n",
      "epoch:4; batch 30976; train accuracy: 0.878643\n",
      "epoch 4; batch 31104; loss 0.031806\n",
      "epoch:4; batch 31104; train accuracy: 0.878685\n",
      "epoch 4; batch 31232; loss 0.020994\n",
      "epoch:4; batch 31232; train accuracy: 0.878727\n",
      "epoch 4; batch 31360; loss 0.021910\n",
      "epoch:4; batch 31360; train accuracy: 0.878766\n",
      "epoch 4; batch 31488; loss 0.024477\n",
      "epoch:4; batch 31488; train accuracy: 0.878808\n",
      "epoch 4; batch 31616; loss 0.113276\n",
      "epoch:4; batch 31616; train accuracy: 0.878844\n",
      "epoch 4; batch 31744; loss 0.024443\n",
      "epoch:4; batch 31744; train accuracy: 0.878886\n",
      "epoch 4; batch 31872; loss 0.041062\n",
      "epoch:4; batch 31872; train accuracy: 0.878928\n",
      "epoch 4; batch 32000; loss 0.049774\n",
      "epoch:4; batch 32000; train accuracy: 0.878967\n",
      "epoch 4; batch 32128; loss 0.025440\n",
      "epoch:4; batch 32128; train accuracy: 0.879006\n",
      "epoch 4; batch 32256; loss 0.034843\n",
      "epoch:4; batch 32256; train accuracy: 0.879042\n",
      "epoch 4; batch 32384; loss 0.000403\n",
      "epoch:4; batch 32384; train accuracy: 0.879086\n",
      "epoch 4; batch 32512; loss 0.025857\n",
      "epoch:4; batch 32512; train accuracy: 0.879122\n",
      "epoch 4; batch 32640; loss 0.031317\n",
      "epoch:4; batch 32640; train accuracy: 0.879164\n",
      "epoch 4; batch 32768; loss 0.007865\n",
      "epoch:4; batch 32768; train accuracy: 0.879208\n",
      "epoch 4; batch 32896; loss 0.051911\n",
      "epoch:4; batch 32896; train accuracy: 0.879244\n",
      "epoch 4; batch 33024; loss 0.023706\n",
      "epoch:4; batch 33024; train accuracy: 0.879286\n",
      "epoch 4; batch 33152; loss 0.038418\n",
      "epoch:4; batch 33152; train accuracy: 0.879324\n",
      "epoch 4; batch 33280; loss 0.007489\n",
      "epoch:4; batch 33280; train accuracy: 0.879369\n",
      "epoch 4; batch 33408; loss 0.077835\n",
      "epoch:4; batch 33408; train accuracy: 0.879402\n",
      "epoch 4; batch 33536; loss 0.003287\n",
      "epoch:4; batch 33536; train accuracy: 0.879446\n",
      "epoch 4; batch 33664; loss 0.018344\n",
      "epoch:4; batch 33664; train accuracy: 0.879487\n",
      "epoch 4; batch 33792; loss 0.008311\n",
      "epoch:4; batch 33792; train accuracy: 0.879532\n",
      "epoch 4; batch 33920; loss 0.002792\n",
      "epoch:4; batch 33920; train accuracy: 0.879576\n",
      "epoch 4; batch 34048; loss 0.006079\n",
      "epoch:4; batch 34048; train accuracy: 0.879620\n",
      "epoch 4; batch 34176; loss 0.016218\n",
      "epoch:4; batch 34176; train accuracy: 0.879661\n",
      "epoch 4; batch 34304; loss 0.010901\n",
      "epoch:4; batch 34304; train accuracy: 0.879702\n",
      "epoch 4; batch 34432; loss 0.037415\n",
      "epoch:4; batch 34432; train accuracy: 0.879735\n",
      "epoch 4; batch 34560; loss 0.048940\n",
      "epoch:4; batch 34560; train accuracy: 0.879776\n",
      "epoch 4; batch 34688; loss 0.092163\n",
      "epoch:4; batch 34688; train accuracy: 0.879809\n",
      "epoch 4; batch 34816; loss 0.002365\n",
      "epoch:4; batch 34816; train accuracy: 0.879853\n",
      "epoch 4; batch 34944; loss 0.013699\n",
      "epoch:4; batch 34944; train accuracy: 0.879897\n",
      "epoch 4; batch 35072; loss 0.029093\n",
      "epoch:4; batch 35072; train accuracy: 0.879938\n",
      "epoch 4; batch 35200; loss 0.012855\n",
      "epoch:4; batch 35200; train accuracy: 0.879982\n",
      "epoch 4; batch 35328; loss 0.010950\n",
      "epoch:4; batch 35328; train accuracy: 0.880026\n",
      "epoch 4; batch 35456; loss 0.076550\n",
      "epoch:4; batch 35456; train accuracy: 0.880058\n",
      "epoch 4; batch 35584; loss 0.010082\n",
      "epoch:4; batch 35584; train accuracy: 0.880102\n",
      "epoch 4; batch 35712; loss 0.037393\n",
      "epoch:4; batch 35712; train accuracy: 0.880140\n",
      "epoch 4; batch 35840; loss 0.008396\n",
      "epoch:4; batch 35840; train accuracy: 0.880184\n",
      "epoch 4; batch 35968; loss 0.001298\n",
      "epoch:4; batch 35968; train accuracy: 0.880227\n",
      "epoch 4; batch 36096; loss 0.116945\n",
      "epoch:4; batch 36096; train accuracy: 0.880265\n",
      "epoch 4; batch 36224; loss 0.049434\n",
      "epoch:4; batch 36224; train accuracy: 0.880303\n",
      "epoch 4; batch 36352; loss 0.033854\n",
      "epoch:4; batch 36352; train accuracy: 0.880341\n",
      "epoch 4; batch 36480; loss 0.032073\n",
      "epoch:4; batch 36480; train accuracy: 0.880379\n",
      "epoch 4; batch 36608; loss 0.006384\n",
      "epoch:4; batch 36608; train accuracy: 0.880423\n",
      "epoch 4; batch 36736; loss 0.034399\n",
      "epoch:4; batch 36736; train accuracy: 0.880463\n",
      "epoch 4; batch 36864; loss 0.052804\n",
      "epoch:4; batch 36864; train accuracy: 0.880498\n",
      "epoch 4; batch 36992; loss 0.007799\n",
      "epoch:4; batch 36992; train accuracy: 0.880542\n",
      "epoch 4; batch 37120; loss 0.008394\n",
      "epoch:4; batch 37120; train accuracy: 0.880582\n",
      "epoch 4; batch 37248; loss 0.004831\n",
      "epoch:4; batch 37248; train accuracy: 0.880626\n",
      "epoch 4; batch 37376; loss 0.013668\n",
      "epoch:4; batch 37376; train accuracy: 0.880666\n",
      "epoch 4; batch 37504; loss 0.041952\n",
      "epoch:4; batch 37504; train accuracy: 0.880701\n",
      "epoch 4; batch 37632; loss 0.003305\n",
      "epoch:4; batch 37632; train accuracy: 0.880744\n",
      "epoch 4; batch 37760; loss 0.015480\n",
      "epoch:4; batch 37760; train accuracy: 0.880785\n",
      "epoch 4; batch 37888; loss 0.062028\n",
      "epoch:4; batch 37888; train accuracy: 0.880825\n",
      "epoch 4; batch 38016; loss 0.040212\n",
      "epoch:4; batch 38016; train accuracy: 0.880863\n",
      "epoch 4; batch 38144; loss 0.050330\n",
      "epoch:4; batch 38144; train accuracy: 0.880900\n",
      "epoch 4; batch 38272; loss 0.023864\n",
      "epoch:4; batch 38272; train accuracy: 0.880941\n",
      "epoch 4; batch 38400; loss 0.089500\n",
      "epoch:4; batch 38400; train accuracy: 0.880973\n",
      "epoch 4; batch 38528; loss 0.002027\n",
      "epoch:4; batch 38528; train accuracy: 0.881016\n",
      "epoch 4; batch 38656; loss 0.014375\n",
      "epoch:4; batch 38656; train accuracy: 0.881059\n",
      "epoch 4; batch 38784; loss 0.131147\n",
      "epoch:4; batch 38784; train accuracy: 0.881099\n",
      "epoch 4; batch 38912; loss 0.010699\n",
      "epoch:4; batch 38912; train accuracy: 0.881142\n",
      "epoch 4; batch 39040; loss 0.033155\n",
      "epoch:4; batch 39040; train accuracy: 0.881177\n",
      "epoch 4; batch 39168; loss 0.004831\n",
      "epoch:4; batch 39168; train accuracy: 0.881220\n",
      "epoch 4; batch 39296; loss 0.003079\n",
      "epoch:4; batch 39296; train accuracy: 0.881262\n",
      "epoch 4; batch 39424; loss 0.005997\n",
      "epoch:4; batch 39424; train accuracy: 0.881305\n",
      "epoch 4; batch 39552; loss 0.034186\n",
      "epoch:4; batch 39552; train accuracy: 0.881345\n",
      "epoch 4; batch 39680; loss 0.017870\n",
      "epoch:4; batch 39680; train accuracy: 0.881385\n",
      "epoch 4; batch 39808; loss 0.003847\n",
      "epoch:4; batch 39808; train accuracy: 0.881428\n",
      "epoch 4; batch 39936; loss 0.013966\n",
      "epoch:4; batch 39936; train accuracy: 0.881468\n",
      "epoch 4; batch 40064; loss 0.012847\n",
      "epoch:4; batch 40064; train accuracy: 0.881508\n",
      "epoch 4; batch 40192; loss 0.019186\n",
      "epoch:4; batch 40192; train accuracy: 0.881548\n",
      "epoch 4; batch 40320; loss 0.008233\n",
      "epoch:4; batch 40320; train accuracy: 0.881591\n",
      "epoch 4; batch 40448; loss 0.027129\n",
      "epoch:4; batch 40448; train accuracy: 0.881628\n",
      "epoch 4; batch 40576; loss 0.025777\n",
      "epoch:4; batch 40576; train accuracy: 0.881667\n",
      "epoch 4; batch 40704; loss 0.004873\n",
      "epoch:4; batch 40704; train accuracy: 0.881710\n",
      "epoch 4; batch 40832; loss 0.051805\n",
      "epoch:4; batch 40832; train accuracy: 0.881747\n",
      "epoch 4; batch 40960; loss 0.019903\n",
      "epoch:4; batch 40960; train accuracy: 0.881787\n",
      "epoch 4; batch 41088; loss 0.049032\n",
      "epoch:4; batch 41088; train accuracy: 0.881824\n",
      "epoch 4; batch 41216; loss 0.007058\n",
      "epoch:4; batch 41216; train accuracy: 0.881866\n",
      "epoch 4; batch 41344; loss 0.059075\n",
      "epoch:4; batch 41344; train accuracy: 0.881903\n",
      "epoch 4; batch 41472; loss 0.020983\n",
      "epoch:4; batch 41472; train accuracy: 0.881943\n",
      "epoch 4; batch 41600; loss 0.020816\n",
      "epoch:4; batch 41600; train accuracy: 0.881982\n",
      "epoch 4; batch 41728; loss 0.039353\n",
      "epoch:4; batch 41728; train accuracy: 0.882016\n",
      "epoch 4; batch 41856; loss 0.005583\n",
      "epoch:4; batch 41856; train accuracy: 0.882058\n",
      "epoch 4; batch 41984; loss 0.019393\n",
      "epoch:4; batch 41984; train accuracy: 0.882101\n",
      "epoch 4; batch 42112; loss 0.004655\n",
      "epoch:4; batch 42112; train accuracy: 0.882143\n",
      "epoch 4; batch 42240; loss 0.022790\n",
      "epoch:4; batch 42240; train accuracy: 0.882177\n",
      "epoch 4; batch 42368; loss 0.089921\n",
      "epoch:4; batch 42368; train accuracy: 0.882213\n",
      "epoch 4; batch 42496; loss 0.135755\n",
      "epoch:4; batch 42496; train accuracy: 0.882250\n",
      "epoch 4; batch 42624; loss 0.012471\n",
      "epoch:4; batch 42624; train accuracy: 0.882292\n",
      "epoch 4; batch 42752; loss 0.051441\n",
      "epoch:4; batch 42752; train accuracy: 0.882332\n",
      "epoch 4; batch 42880; loss 0.058990\n",
      "epoch:4; batch 42880; train accuracy: 0.882368\n",
      "epoch 4; batch 43008; loss 0.062428\n",
      "epoch:4; batch 43008; train accuracy: 0.882407\n",
      "epoch 4; batch 43136; loss 0.071081\n",
      "epoch:4; batch 43136; train accuracy: 0.882444\n",
      "epoch 4; batch 43264; loss 0.042552\n",
      "epoch:4; batch 43264; train accuracy: 0.882483\n",
      "epoch 4; batch 43392; loss 0.086719\n",
      "epoch:4; batch 43392; train accuracy: 0.882511\n",
      "epoch 4; batch 43520; loss 0.004618\n",
      "epoch:4; batch 43520; train accuracy: 0.882553\n",
      "epoch 4; batch 43648; loss 0.015564\n",
      "epoch:4; batch 43648; train accuracy: 0.882595\n",
      "epoch 4; batch 43776; loss 0.026328\n",
      "epoch:4; batch 43776; train accuracy: 0.882634\n",
      "epoch 4; batch 43904; loss 0.029856\n",
      "epoch:4; batch 43904; train accuracy: 0.882668\n",
      "epoch 4; batch 44032; loss 0.024044\n",
      "epoch:4; batch 44032; train accuracy: 0.882707\n",
      "epoch 4; batch 44160; loss 0.023908\n",
      "epoch:4; batch 44160; train accuracy: 0.882743\n",
      "epoch 4; batch 44288; loss 0.006916\n",
      "epoch:4; batch 44288; train accuracy: 0.882785\n",
      "epoch 4; batch 44416; loss 0.016811\n",
      "epoch:4; batch 44416; train accuracy: 0.882824\n",
      "epoch 4; batch 44544; loss 0.017769\n",
      "epoch:4; batch 44544; train accuracy: 0.882860\n",
      "epoch 4; batch 44672; loss 0.070736\n",
      "epoch:4; batch 44672; train accuracy: 0.882893\n",
      "epoch 4; batch 44800; loss 0.004001\n",
      "epoch:4; batch 44800; train accuracy: 0.882935\n",
      "epoch 4; batch 44928; loss 0.024808\n",
      "epoch:4; batch 44928; train accuracy: 0.882974\n",
      "epoch 4; batch 45056; loss 0.008043\n",
      "epoch:4; batch 45056; train accuracy: 0.883013\n",
      "epoch 4; batch 45184; loss 0.023564\n",
      "epoch:4; batch 45184; train accuracy: 0.883051\n",
      "epoch 4; batch 45312; loss 0.013216\n",
      "epoch:4; batch 45312; train accuracy: 0.883093\n",
      "epoch 4; batch 45440; loss 0.017757\n",
      "epoch:4; batch 45440; train accuracy: 0.883132\n",
      "epoch 4; batch 45568; loss 0.028350\n",
      "epoch:4; batch 45568; train accuracy: 0.883168\n",
      "epoch 4; batch 45696; loss 0.008114\n",
      "epoch:4; batch 45696; train accuracy: 0.883209\n",
      "epoch 4; batch 45824; loss 0.015723\n",
      "epoch:4; batch 45824; train accuracy: 0.883248\n",
      "epoch 4; batch 45952; loss 0.011246\n",
      "epoch:4; batch 45952; train accuracy: 0.883289\n",
      "epoch 4; batch 46080; loss 0.033790\n",
      "epoch:4; batch 46080; train accuracy: 0.883325\n",
      "epoch 4; batch 46208; loss 0.014627\n",
      "epoch:4; batch 46208; train accuracy: 0.883364\n",
      "epoch 4; batch 46336; loss 0.014270\n",
      "epoch:4; batch 46336; train accuracy: 0.883402\n",
      "epoch 4; batch 46464; loss 0.043891\n",
      "epoch:4; batch 46464; train accuracy: 0.883438\n",
      "epoch 4; batch 46592; loss 0.006937\n",
      "epoch:4; batch 46592; train accuracy: 0.883479\n",
      "epoch 4; batch 46720; loss 0.006463\n",
      "epoch:4; batch 46720; train accuracy: 0.883520\n",
      "epoch 4; batch 46848; loss 0.040094\n",
      "epoch:4; batch 46848; train accuracy: 0.883559\n",
      "epoch 4; batch 46976; loss 0.010195\n",
      "epoch:4; batch 46976; train accuracy: 0.883600\n",
      "epoch 4; batch 47104; loss 0.006311\n",
      "epoch:4; batch 47104; train accuracy: 0.883641\n",
      "epoch 4; batch 47232; loss 0.015893\n",
      "epoch:4; batch 47232; train accuracy: 0.883680\n",
      "epoch 4; batch 47360; loss 0.046771\n",
      "epoch:4; batch 47360; train accuracy: 0.883718\n",
      "epoch 4; batch 47488; loss 0.010430\n",
      "epoch:4; batch 47488; train accuracy: 0.883759\n",
      "epoch 4; batch 47616; loss 0.018800\n",
      "epoch:4; batch 47616; train accuracy: 0.883797\n",
      "epoch 4; batch 47744; loss 0.007341\n",
      "epoch:4; batch 47744; train accuracy: 0.883838\n",
      "epoch 4; batch 47872; loss 0.004464\n",
      "epoch:4; batch 47872; train accuracy: 0.883879\n",
      "epoch 4; batch 48000; loss 0.002210\n",
      "epoch:4; batch 48000; train accuracy: 0.883920\n",
      "epoch 4; batch 48128; loss 0.005414\n",
      "epoch:4; batch 48128; train accuracy: 0.883961\n",
      "epoch 4; batch 48256; loss 0.016749\n",
      "epoch:4; batch 48256; train accuracy: 0.884002\n",
      "epoch 4; batch 48384; loss 0.119463\n",
      "epoch:4; batch 48384; train accuracy: 0.884040\n",
      "epoch 4; batch 48512; loss 0.001459\n",
      "epoch:4; batch 48512; train accuracy: 0.884081\n",
      "epoch 4; batch 48640; loss 0.012857\n",
      "epoch:4; batch 48640; train accuracy: 0.884122\n",
      "epoch 4; batch 48768; loss 0.007775\n",
      "epoch:4; batch 48768; train accuracy: 0.884163\n",
      "epoch 4; batch 48896; loss 0.005272\n",
      "epoch:4; batch 48896; train accuracy: 0.884203\n",
      "epoch 4; batch 49024; loss 0.003088\n",
      "epoch:4; batch 49024; train accuracy: 0.884244\n",
      "epoch 4; batch 49152; loss 0.006541\n",
      "epoch:4; batch 49152; train accuracy: 0.884285\n",
      "epoch 4; batch 49280; loss 0.009769\n",
      "epoch:4; batch 49280; train accuracy: 0.884323\n",
      "epoch 4; batch 49408; loss 0.019804\n",
      "epoch:4; batch 49408; train accuracy: 0.884361\n",
      "epoch 4; batch 49536; loss 0.023255\n",
      "epoch:4; batch 49536; train accuracy: 0.884396\n",
      "epoch 4; batch 49664; loss 0.028938\n",
      "epoch:4; batch 49664; train accuracy: 0.884431\n",
      "epoch 4; batch 49792; loss 0.005120\n",
      "epoch:4; batch 49792; train accuracy: 0.884472\n",
      "epoch 4; batch 49920; loss 0.005277\n",
      "epoch:4; batch 49920; train accuracy: 0.884512\n",
      "epoch 4; batch 50048; loss 0.016525\n",
      "epoch:4; batch 50048; train accuracy: 0.884550\n",
      "epoch 4; batch 50176; loss 0.003228\n",
      "epoch:4; batch 50176; train accuracy: 0.884590\n",
      "epoch 4; batch 50304; loss 0.028218\n",
      "epoch:4; batch 50304; train accuracy: 0.884628\n",
      "epoch 4; batch 50432; loss 0.040701\n",
      "epoch:4; batch 50432; train accuracy: 0.884663\n",
      "epoch 4; batch 50560; loss 0.001039\n",
      "epoch:4; batch 50560; train accuracy: 0.884703\n",
      "epoch 4; batch 50688; loss 0.003761\n",
      "epoch:4; batch 50688; train accuracy: 0.884744\n",
      "epoch 4; batch 50816; loss 0.002616\n",
      "epoch:4; batch 50816; train accuracy: 0.884784\n",
      "epoch 4; batch 50944; loss 0.033808\n",
      "epoch:4; batch 50944; train accuracy: 0.884822\n",
      "epoch 4; batch 51072; loss 0.012328\n",
      "epoch:4; batch 51072; train accuracy: 0.884862\n",
      "epoch 4; batch 51200; loss 0.001500\n",
      "epoch:4; batch 51200; train accuracy: 0.884902\n",
      "epoch 4; batch 51328; loss 0.003413\n",
      "epoch:4; batch 51328; train accuracy: 0.884942\n",
      "epoch 4; batch 51456; loss 0.030153\n",
      "epoch:4; batch 51456; train accuracy: 0.884980\n",
      "epoch 4; batch 51584; loss 0.021356\n",
      "epoch:4; batch 51584; train accuracy: 0.885017\n",
      "epoch 4; batch 51712; loss 0.005665\n",
      "epoch:4; batch 51712; train accuracy: 0.885058\n",
      "epoch 4; batch 51840; loss 0.057343\n",
      "epoch:4; batch 51840; train accuracy: 0.885095\n",
      "epoch 4; batch 51968; loss 0.020920\n",
      "epoch:4; batch 51968; train accuracy: 0.885132\n",
      "epoch 4; batch 52096; loss 0.009648\n",
      "epoch:4; batch 52096; train accuracy: 0.885172\n",
      "epoch 4; batch 52224; loss 0.009054\n",
      "epoch:4; batch 52224; train accuracy: 0.885212\n",
      "epoch 4; batch 52352; loss 0.003614\n",
      "epoch:4; batch 52352; train accuracy: 0.885252\n",
      "epoch 4; batch 52480; loss 0.006746\n",
      "epoch:4; batch 52480; train accuracy: 0.885292\n",
      "epoch 4; batch 52608; loss 0.051821\n",
      "epoch:4; batch 52608; train accuracy: 0.885327\n",
      "epoch 4; batch 52736; loss 0.032607\n",
      "epoch:4; batch 52736; train accuracy: 0.885364\n",
      "epoch 4; batch 52864; loss 0.005142\n",
      "epoch:4; batch 52864; train accuracy: 0.885404\n",
      "epoch 4; batch 52992; loss 0.016027\n",
      "epoch:4; batch 52992; train accuracy: 0.885444\n",
      "epoch 4; batch 53120; loss 0.019472\n",
      "epoch:4; batch 53120; train accuracy: 0.885478\n",
      "epoch 4; batch 53248; loss 0.008193\n",
      "epoch:4; batch 53248; train accuracy: 0.885518\n",
      "epoch 4; batch 53376; loss 0.031144\n",
      "epoch:4; batch 53376; train accuracy: 0.885552\n",
      "epoch 4; batch 53504; loss 0.012374\n",
      "epoch:4; batch 53504; train accuracy: 0.885589\n",
      "epoch 4; batch 53632; loss 0.012589\n",
      "epoch:4; batch 53632; train accuracy: 0.885626\n",
      "epoch 4; batch 53760; loss 0.016922\n",
      "epoch:4; batch 53760; train accuracy: 0.885664\n",
      "epoch 4; batch 53888; loss 0.017565\n",
      "epoch:4; batch 53888; train accuracy: 0.885701\n",
      "epoch 4; batch 54016; loss 0.035126\n",
      "epoch:4; batch 54016; train accuracy: 0.885735\n",
      "epoch 4; batch 54144; loss 0.026036\n",
      "epoch:4; batch 54144; train accuracy: 0.885772\n",
      "epoch 4; batch 54272; loss 0.008027\n",
      "epoch:4; batch 54272; train accuracy: 0.885811\n",
      "epoch 4; batch 54400; loss 0.020042\n",
      "epoch:4; batch 54400; train accuracy: 0.885848\n",
      "epoch 4; batch 54528; loss 0.003617\n",
      "epoch:4; batch 54528; train accuracy: 0.885888\n",
      "epoch 4; batch 54656; loss 0.007992\n",
      "epoch:4; batch 54656; train accuracy: 0.885927\n",
      "epoch 4; batch 54784; loss 0.038301\n",
      "epoch:4; batch 54784; train accuracy: 0.885961\n",
      "epoch 4; batch 54912; loss 0.031913\n",
      "epoch:4; batch 54912; train accuracy: 0.885993\n",
      "epoch 4; batch 55040; loss 0.068851\n",
      "epoch:4; batch 55040; train accuracy: 0.886024\n",
      "epoch 4; batch 55168; loss 0.037620\n",
      "epoch:4; batch 55168; train accuracy: 0.886058\n",
      "epoch 4; batch 55296; loss 0.020546\n",
      "epoch:4; batch 55296; train accuracy: 0.886095\n",
      "epoch 4; batch 55424; loss 0.028933\n",
      "epoch:4; batch 55424; train accuracy: 0.886129\n",
      "epoch 4; batch 55552; loss 0.011163\n",
      "epoch:4; batch 55552; train accuracy: 0.886165\n",
      "epoch 4; batch 55680; loss 0.032555\n",
      "epoch:4; batch 55680; train accuracy: 0.886202\n",
      "epoch 4; batch 55808; loss 0.011087\n",
      "epoch:4; batch 55808; train accuracy: 0.886239\n",
      "epoch 4; batch 55936; loss 0.001824\n",
      "epoch:4; batch 55936; train accuracy: 0.886278\n",
      "epoch 4; batch 56064; loss 0.015593\n",
      "epoch:4; batch 56064; train accuracy: 0.886314\n",
      "epoch 4; batch 56192; loss 0.004742\n",
      "epoch:4; batch 56192; train accuracy: 0.886354\n",
      "epoch 4; batch 56320; loss 0.057638\n",
      "epoch:4; batch 56320; train accuracy: 0.886385\n",
      "epoch 4; batch 56448; loss 0.015070\n",
      "epoch:4; batch 56448; train accuracy: 0.886421\n",
      "epoch 4; batch 56576; loss 0.074494\n",
      "epoch:4; batch 56576; train accuracy: 0.886452\n",
      "epoch 4; batch 56704; loss 0.022718\n",
      "epoch:4; batch 56704; train accuracy: 0.886489\n",
      "epoch 4; batch 56832; loss 0.027012\n",
      "epoch:4; batch 56832; train accuracy: 0.886525\n",
      "epoch 4; batch 56960; loss 0.045730\n",
      "epoch:4; batch 56960; train accuracy: 0.886561\n",
      "epoch 4; batch 57088; loss 0.051106\n",
      "epoch:4; batch 57088; train accuracy: 0.886598\n",
      "epoch 4; batch 57216; loss 0.053135\n",
      "epoch:4; batch 57216; train accuracy: 0.886631\n",
      "epoch 4; batch 57344; loss 0.006814\n",
      "epoch:4; batch 57344; train accuracy: 0.886670\n",
      "epoch 4; batch 57472; loss 0.066507\n",
      "epoch:4; batch 57472; train accuracy: 0.886701\n",
      "epoch 4; batch 57600; loss 0.033234\n",
      "epoch:4; batch 57600; train accuracy: 0.886732\n",
      "epoch 4; batch 57728; loss 0.032992\n",
      "epoch:4; batch 57728; train accuracy: 0.886766\n",
      "epoch 4; batch 57856; loss 0.067235\n",
      "epoch:4; batch 57856; train accuracy: 0.886802\n",
      "epoch 4; batch 57984; loss 0.018761\n",
      "epoch:4; batch 57984; train accuracy: 0.886838\n",
      "epoch 4; batch 58112; loss 0.043672\n",
      "epoch:4; batch 58112; train accuracy: 0.886869\n",
      "epoch 4; batch 58240; loss 0.013155\n",
      "epoch:4; batch 58240; train accuracy: 0.886902\n",
      "epoch 4; batch 58368; loss 0.016780\n",
      "epoch:4; batch 58368; train accuracy: 0.886938\n",
      "epoch 4; batch 58496; loss 0.027441\n",
      "epoch:4; batch 58496; train accuracy: 0.886975\n",
      "epoch 4; batch 58624; loss 0.005136\n",
      "epoch:4; batch 58624; train accuracy: 0.887013\n",
      "epoch 4; batch 58752; loss 0.010716\n",
      "epoch:4; batch 58752; train accuracy: 0.887052\n",
      "epoch 4; batch 58880; loss 0.034890\n",
      "epoch:4; batch 58880; train accuracy: 0.887083\n",
      "epoch 4; batch 59008; loss 0.038557\n",
      "epoch:4; batch 59008; train accuracy: 0.887116\n",
      "epoch 4; batch 59136; loss 0.006994\n",
      "epoch:4; batch 59136; train accuracy: 0.887155\n",
      "epoch 4; batch 59264; loss 0.010469\n",
      "epoch:4; batch 59264; train accuracy: 0.887193\n",
      "epoch 4; batch 59392; loss 0.054958\n",
      "epoch:4; batch 59392; train accuracy: 0.887226\n",
      "epoch 4; batch 59520; loss 0.011000\n",
      "epoch:4; batch 59520; train accuracy: 0.887265\n",
      "epoch 4; batch 59648; loss 0.004275\n",
      "epoch:4; batch 59648; train accuracy: 0.887303\n",
      "epoch 4; batch 59776; loss 0.084344\n",
      "epoch:4; batch 59776; train accuracy: 0.887337\n",
      "epoch 4; batch 59904; loss 0.041174\n",
      "epoch:4; batch 59904; train accuracy: 0.887372\n",
      "epoch 4; batch 60032; loss 0.041059\n",
      "epoch:4; batch 60032; train accuracy: 0.887406\n",
      "epoch 4; batch 60160; loss 0.008167\n",
      "epoch:4; batch 60160; train accuracy: 0.887444\n",
      "epoch 4; batch 60288; loss 0.066556\n",
      "epoch:4; batch 60288; train accuracy: 0.887480\n",
      "epoch 4; batch 60416; loss 0.015529\n",
      "epoch:4; batch 60416; train accuracy: 0.887518\n",
      "epoch 4; batch 60544; loss 0.009423\n",
      "epoch:4; batch 60544; train accuracy: 0.887554\n",
      "epoch 4; batch 60672; loss 0.007348\n",
      "epoch:4; batch 60672; train accuracy: 0.887592\n",
      "epoch 4; batch 60800; loss 0.012452\n",
      "epoch:4; batch 60800; train accuracy: 0.887628\n",
      "epoch 4; batch 60928; loss 0.011112\n",
      "epoch:4; batch 60928; train accuracy: 0.887663\n",
      "epoch 4; batch 61056; loss 0.054584\n",
      "epoch:4; batch 61056; train accuracy: 0.887696\n",
      "epoch 4; batch 61184; loss 0.011016\n",
      "epoch:4; batch 61184; train accuracy: 0.887735\n",
      "epoch 4; batch 61312; loss 0.027716\n",
      "epoch:4; batch 61312; train accuracy: 0.887767\n",
      "epoch 4; batch 61440; loss 0.008176\n",
      "epoch:4; batch 61440; train accuracy: 0.887806\n",
      "epoch 4; batch 61568; loss 0.034543\n",
      "epoch:4; batch 61568; train accuracy: 0.887841\n",
      "epoch 4; batch 61696; loss 0.009127\n",
      "epoch:4; batch 61696; train accuracy: 0.887877\n",
      "epoch 4; batch 61824; loss 0.003938\n",
      "epoch:4; batch 61824; train accuracy: 0.887915\n",
      "epoch 4; batch 61952; loss 0.004134\n",
      "epoch:4; batch 61952; train accuracy: 0.887953\n",
      "epoch 4; batch 62080; loss 0.033921\n",
      "epoch:4; batch 62080; train accuracy: 0.887988\n",
      "epoch 4; batch 62208; loss 0.006646\n",
      "epoch:4; batch 62208; train accuracy: 0.888026\n",
      "epoch 4; batch 62336; loss 0.043487\n",
      "epoch:4; batch 62336; train accuracy: 0.888061\n",
      "epoch 4; batch 62464; loss 0.046580\n",
      "epoch:4; batch 62464; train accuracy: 0.888092\n",
      "epoch 4; batch 62592; loss 0.024190\n",
      "epoch:4; batch 62592; train accuracy: 0.888121\n",
      "epoch 4; batch 62720; loss 0.028510\n",
      "epoch:4; batch 62720; train accuracy: 0.888154\n",
      "epoch 4; batch 62848; loss 0.004434\n",
      "epoch:4; batch 62848; train accuracy: 0.888192\n",
      "epoch 4; batch 62976; loss 0.015690\n",
      "epoch:4; batch 62976; train accuracy: 0.888225\n",
      "epoch 4; batch 63104; loss 0.011437\n",
      "epoch:4; batch 63104; train accuracy: 0.888260\n",
      "epoch 4; batch 63232; loss 0.017570\n",
      "epoch:4; batch 63232; train accuracy: 0.888295\n",
      "epoch 4; batch 63360; loss 0.012684\n",
      "epoch:4; batch 63360; train accuracy: 0.888333\n",
      "epoch 4; batch 63488; loss 0.058538\n",
      "epoch:4; batch 63488; train accuracy: 0.888368\n",
      "epoch 4; batch 63616; loss 0.030715\n",
      "epoch:4; batch 63616; train accuracy: 0.888400\n",
      "epoch 4; batch 63744; loss 0.011593\n",
      "epoch:4; batch 63744; train accuracy: 0.888435\n",
      "epoch 4; batch 63872; loss 0.010023\n",
      "epoch:4; batch 63872; train accuracy: 0.888473\n",
      "epoch 4; batch 64000; loss 0.000930\n",
      "epoch:4; batch 64000; train accuracy: 0.888511\n",
      "epoch 4; batch 64128; loss 0.004400\n",
      "epoch:4; batch 64128; train accuracy: 0.888549\n",
      "epoch 4; batch 64256; loss 0.018843\n",
      "epoch:4; batch 64256; train accuracy: 0.888581\n",
      "epoch 4; batch 64384; loss 0.024198\n",
      "epoch:4; batch 64384; train accuracy: 0.888616\n",
      "epoch 4; batch 64512; loss 0.003157\n",
      "epoch:4; batch 64512; train accuracy: 0.888653\n",
      "epoch 4; batch 64640; loss 0.025581\n",
      "epoch:4; batch 64640; train accuracy: 0.888688\n",
      "epoch 4; batch 64768; loss 0.027298\n",
      "epoch:4; batch 64768; train accuracy: 0.888723\n",
      "epoch 4; batch 64896; loss 0.009180\n",
      "epoch:4; batch 64896; train accuracy: 0.888761\n",
      "epoch 4; batch 65024; loss 0.030248\n",
      "epoch:4; batch 65024; train accuracy: 0.888793\n",
      "epoch 4; batch 65152; loss 0.044083\n",
      "epoch:4; batch 65152; train accuracy: 0.888822\n",
      "epoch 4; batch 65280; loss 0.003464\n",
      "epoch:4; batch 65280; train accuracy: 0.888860\n",
      "epoch 4; batch 65408; loss 0.034247\n",
      "epoch:4; batch 65408; train accuracy: 0.888892\n",
      "epoch 4; batch 65536; loss 0.022887\n",
      "epoch:4; batch 65536; train accuracy: 0.888927\n",
      "epoch 4; batch 65664; loss 0.001431\n",
      "epoch:4; batch 65664; train accuracy: 0.888964\n",
      "epoch 4; batch 65792; loss 0.017242\n",
      "epoch:4; batch 65792; train accuracy: 0.888999\n",
      "epoch 4; batch 65920; loss 0.002519\n",
      "epoch:4; batch 65920; train accuracy: 0.889036\n",
      "epoch 4; batch 66048; loss 0.013937\n",
      "epoch:4; batch 66048; train accuracy: 0.889071\n",
      "epoch 4; batch 66176; loss 0.006677\n",
      "epoch:4; batch 66176; train accuracy: 0.889108\n",
      "epoch 4; batch 66304; loss 0.001239\n",
      "epoch:4; batch 66304; train accuracy: 0.889145\n",
      "epoch 4; batch 66432; loss 0.015164\n",
      "epoch:4; batch 66432; train accuracy: 0.889183\n",
      "epoch 4; batch 66560; loss 0.014616\n",
      "epoch:4; batch 66560; train accuracy: 0.889220\n",
      "epoch 4; batch 66688; loss 0.013053\n",
      "epoch:4; batch 66688; train accuracy: 0.889254\n",
      "epoch 4; batch 66816; loss 0.007727\n",
      "epoch:4; batch 66816; train accuracy: 0.889291\n",
      "epoch 4; batch 66944; loss 0.016458\n",
      "epoch:4; batch 66944; train accuracy: 0.889326\n",
      "epoch 4; batch 67072; loss 0.005611\n",
      "epoch:4; batch 67072; train accuracy: 0.889363\n",
      "epoch 4; batch 67200; loss 0.030707\n",
      "epoch:4; batch 67200; train accuracy: 0.889398\n",
      "epoch 4; batch 67328; loss 0.040685\n",
      "epoch:4; batch 67328; train accuracy: 0.889429\n",
      "epoch 4; batch 67456; loss 0.031822\n",
      "epoch:4; batch 67456; train accuracy: 0.889464\n",
      "epoch 4; batch 67584; loss 0.006007\n",
      "epoch:4; batch 67584; train accuracy: 0.889501\n",
      "epoch 4; batch 67712; loss 0.004062\n",
      "epoch:4; batch 67712; train accuracy: 0.889538\n",
      "epoch 4; batch 67840; loss 0.001342\n",
      "epoch:4; batch 67840; train accuracy: 0.889575\n",
      "epoch 4; batch 67968; loss 0.028357\n",
      "epoch:4; batch 67968; train accuracy: 0.889609\n",
      "epoch 4; batch 68096; loss 0.035923\n",
      "epoch:4; batch 68096; train accuracy: 0.889643\n",
      "epoch 4; batch 68224; loss 0.048533\n",
      "epoch:4; batch 68224; train accuracy: 0.889675\n",
      "epoch 4; batch 68352; loss 0.009106\n",
      "epoch:4; batch 68352; train accuracy: 0.889712\n",
      "epoch 4; batch 68480; loss 0.028301\n",
      "epoch:4; batch 68480; train accuracy: 0.889743\n",
      "epoch 4; batch 68608; loss 0.065833\n",
      "epoch:4; batch 68608; train accuracy: 0.889778\n",
      "epoch 4; batch 68736; loss 0.002939\n",
      "epoch:4; batch 68736; train accuracy: 0.889814\n",
      "epoch 4; batch 68864; loss 0.013343\n",
      "epoch:4; batch 68864; train accuracy: 0.889848\n",
      "epoch 4; batch 68992; loss 0.045800\n",
      "epoch:4; batch 68992; train accuracy: 0.889883\n",
      "epoch 4; batch 69120; loss 0.012332\n",
      "epoch:4; batch 69120; train accuracy: 0.889917\n",
      "epoch 4; batch 69248; loss 0.003269\n",
      "epoch:4; batch 69248; train accuracy: 0.889953\n",
      "epoch 4; batch 69376; loss 0.024305\n",
      "epoch:4; batch 69376; train accuracy: 0.889987\n",
      "epoch 4; batch 69504; loss 0.002065\n",
      "epoch:4; batch 69504; train accuracy: 0.890024\n",
      "epoch 4; batch 69632; loss 0.020116\n",
      "epoch:4; batch 69632; train accuracy: 0.890058\n",
      "epoch 4; batch 69760; loss 0.012122\n",
      "epoch:4; batch 69760; train accuracy: 0.890095\n",
      "epoch 4; batch 69888; loss 0.002049\n",
      "epoch:4; batch 69888; train accuracy: 0.890131\n",
      "epoch 4; batch 70016; loss 0.002956\n",
      "epoch:4; batch 70016; train accuracy: 0.890168\n",
      "epoch 4; batch 70144; loss 0.002406\n",
      "epoch:4; batch 70144; train accuracy: 0.890204\n",
      "epoch 4; batch 70272; loss 0.017847\n",
      "epoch:4; batch 70272; train accuracy: 0.890238\n",
      "epoch 4; batch 70400; loss 0.018765\n",
      "epoch:4; batch 70400; train accuracy: 0.890269\n",
      "epoch 4; batch 70528; loss 0.022235\n",
      "epoch:4; batch 70528; train accuracy: 0.890301\n",
      "epoch 4; batch 70656; loss 0.095852\n",
      "epoch:4; batch 70656; train accuracy: 0.890332\n",
      "epoch 4; batch 70784; loss 0.001772\n",
      "epoch:4; batch 70784; train accuracy: 0.890368\n",
      "epoch 4; batch 70912; loss 0.010081\n",
      "epoch:4; batch 70912; train accuracy: 0.890402\n",
      "epoch 4; batch 71040; loss 0.002037\n",
      "epoch:4; batch 71040; train accuracy: 0.890438\n",
      "epoch 4; batch 71168; loss 0.055733\n",
      "epoch:4; batch 71168; train accuracy: 0.890472\n",
      "epoch 4; batch 71296; loss 0.006127\n",
      "epoch:4; batch 71296; train accuracy: 0.890508\n",
      "epoch 4; batch 71424; loss 0.020388\n",
      "epoch:4; batch 71424; train accuracy: 0.890545\n",
      "epoch 4; batch 71552; loss 0.014863\n",
      "epoch:4; batch 71552; train accuracy: 0.890578\n",
      "epoch 4; batch 71680; loss 0.013130\n",
      "epoch:4; batch 71680; train accuracy: 0.890612\n",
      "epoch 4; batch 71808; loss 0.005264\n",
      "epoch:4; batch 71808; train accuracy: 0.890648\n",
      "epoch 4; batch 71936; loss 0.004200\n",
      "epoch:4; batch 71936; train accuracy: 0.890684\n",
      "epoch 4; batch 72064; loss 0.007400\n",
      "epoch:4; batch 72064; train accuracy: 0.890721\n",
      "epoch 4; batch 72192; loss 0.006214\n",
      "epoch:4; batch 72192; train accuracy: 0.890757\n",
      "epoch 4; batch 72320; loss 0.026784\n",
      "epoch:4; batch 72320; train accuracy: 0.890790\n",
      "epoch 4; batch 72448; loss 0.003918\n",
      "epoch:4; batch 72448; train accuracy: 0.890826\n",
      "epoch 4; batch 72576; loss 0.041576\n",
      "epoch:4; batch 72576; train accuracy: 0.890855\n",
      "epoch 4; batch 72704; loss 0.013874\n",
      "epoch:4; batch 72704; train accuracy: 0.890888\n",
      "epoch 4; batch 72832; loss 0.008656\n",
      "epoch:4; batch 72832; train accuracy: 0.890922\n",
      "epoch 4; batch 72960; loss 0.023895\n",
      "epoch:4; batch 72960; train accuracy: 0.890955\n",
      "epoch 4; batch 73088; loss 0.012076\n",
      "epoch:4; batch 73088; train accuracy: 0.890991\n",
      "epoch 4; batch 73216; loss 0.002402\n",
      "epoch:4; batch 73216; train accuracy: 0.891027\n",
      "epoch 4; batch 73344; loss 0.028737\n",
      "epoch:4; batch 73344; train accuracy: 0.891060\n",
      "epoch 4; batch 73472; loss 0.002303\n",
      "epoch:4; batch 73472; train accuracy: 0.891096\n",
      "epoch 4; batch 73600; loss 0.024474\n",
      "epoch:4; batch 73600; train accuracy: 0.891127\n",
      "epoch 4; batch 73728; loss 0.002226\n",
      "epoch:4; batch 73728; train accuracy: 0.891163\n",
      "epoch 4; batch 73856; loss 0.021340\n",
      "epoch:4; batch 73856; train accuracy: 0.891196\n",
      "epoch 4; batch 73984; loss 0.029309\n",
      "epoch:4; batch 73984; train accuracy: 0.891229\n",
      "epoch 4; batch 74112; loss 0.001446\n",
      "epoch:4; batch 74112; train accuracy: 0.891265\n",
      "epoch 4; batch 74240; loss 0.022306\n",
      "epoch:4; batch 74240; train accuracy: 0.891296\n",
      "epoch 4; batch 74368; loss 0.019274\n",
      "epoch:4; batch 74368; train accuracy: 0.891331\n",
      "epoch 4; batch 74496; loss 0.015687\n",
      "epoch:4; batch 74496; train accuracy: 0.891365\n",
      "epoch 4; batch 74624; loss 0.103916\n",
      "epoch:4; batch 74624; train accuracy: 0.891395\n",
      "epoch 4; batch 74752; loss 0.022586\n",
      "epoch:4; batch 74752; train accuracy: 0.891428\n",
      "epoch 4; batch 74880; loss 0.015125\n",
      "epoch:4; batch 74880; train accuracy: 0.891461\n",
      "epoch 4; batch 75008; loss 0.000760\n",
      "epoch:4; batch 75008; train accuracy: 0.891497\n",
      "epoch 4; batch 75136; loss 0.007201\n",
      "epoch:4; batch 75136; train accuracy: 0.891533\n",
      "epoch 4; batch 75264; loss 0.010215\n",
      "epoch:4; batch 75264; train accuracy: 0.891568\n",
      "epoch 4; batch 75392; loss 0.028913\n",
      "epoch:4; batch 75392; train accuracy: 0.891601\n",
      "epoch 4; batch 75520; loss 0.005642\n",
      "epoch:4; batch 75520; train accuracy: 0.891637\n",
      "epoch 4; batch 75648; loss 0.004660\n",
      "epoch:4; batch 75648; train accuracy: 0.891672\n",
      "epoch 4; batch 75776; loss 0.035310\n",
      "epoch:4; batch 75776; train accuracy: 0.891705\n",
      "epoch 4; batch 75904; loss 0.033102\n",
      "epoch:4; batch 75904; train accuracy: 0.891738\n",
      "epoch 4; batch 76032; loss 0.017223\n",
      "epoch:4; batch 76032; train accuracy: 0.891771\n",
      "epoch 4; batch 76160; loss 0.002291\n",
      "epoch:4; batch 76160; train accuracy: 0.891806\n",
      "epoch 4; batch 76288; loss 0.048259\n",
      "epoch:4; batch 76288; train accuracy: 0.891832\n",
      "epoch 4; batch 76416; loss 0.022937\n",
      "epoch:4; batch 76416; train accuracy: 0.891864\n",
      "epoch 4; batch 76544; loss 0.018492\n",
      "epoch:4; batch 76544; train accuracy: 0.891897\n",
      "epoch 4; batch 76672; loss 0.009001\n",
      "epoch:4; batch 76672; train accuracy: 0.891930\n",
      "epoch 4; batch 76800; loss 0.001856\n",
      "epoch:4; batch 76800; train accuracy: 0.891965\n",
      "epoch 4; batch 76928; loss 0.001587\n",
      "epoch:4; batch 76928; train accuracy: 0.892001\n",
      "epoch 4; batch 77056; loss 0.012885\n",
      "epoch:4; batch 77056; train accuracy: 0.892033\n",
      "epoch 4; batch 77184; loss 0.005303\n",
      "epoch:4; batch 77184; train accuracy: 0.892069\n",
      "epoch 4; batch 77312; loss 0.019880\n",
      "epoch:4; batch 77312; train accuracy: 0.892101\n",
      "epoch 4; batch 77440; loss 0.006464\n",
      "epoch:4; batch 77440; train accuracy: 0.892134\n",
      "epoch 4; batch 77568; loss 0.004523\n",
      "epoch:4; batch 77568; train accuracy: 0.892169\n",
      "epoch 4; batch 77696; loss 0.002024\n",
      "epoch:4; batch 77696; train accuracy: 0.892204\n",
      "epoch 4; batch 77824; loss 0.004219\n",
      "epoch:4; batch 77824; train accuracy: 0.892239\n",
      "epoch 4; batch 77952; loss 0.001223\n",
      "epoch:4; batch 77952; train accuracy: 0.892275\n",
      "epoch 4; batch 78080; loss 0.005053\n",
      "epoch:4; batch 78080; train accuracy: 0.892310\n",
      "epoch 4; batch 78208; loss 0.073371\n",
      "epoch:4; batch 78208; train accuracy: 0.892335\n",
      "epoch 4; batch 78336; loss 0.003652\n",
      "epoch:4; batch 78336; train accuracy: 0.892370\n",
      "epoch 4; batch 78464; loss 0.019963\n",
      "epoch:4; batch 78464; train accuracy: 0.892400\n",
      "epoch 4; batch 78592; loss 0.008988\n",
      "epoch:4; batch 78592; train accuracy: 0.892435\n",
      "epoch 4; batch 78720; loss 0.006868\n",
      "epoch:4; batch 78720; train accuracy: 0.892470\n",
      "epoch 4; batch 78848; loss 0.031040\n",
      "epoch:4; batch 78848; train accuracy: 0.892502\n",
      "epoch 4; batch 78976; loss 0.014915\n",
      "epoch:4; batch 78976; train accuracy: 0.892534\n",
      "epoch 4; batch 79104; loss 0.002152\n",
      "epoch:4; batch 79104; train accuracy: 0.892569\n",
      "epoch 4; batch 79232; loss 0.003872\n",
      "epoch:4; batch 79232; train accuracy: 0.892604\n",
      "epoch 4; batch 79360; loss 0.005788\n",
      "epoch:4; batch 79360; train accuracy: 0.892639\n",
      "epoch 4; batch 79488; loss 0.049785\n",
      "epoch:4; batch 79488; train accuracy: 0.892669\n",
      "epoch 4; batch 79616; loss 0.003872\n",
      "epoch:4; batch 79616; train accuracy: 0.892704\n",
      "epoch 4; batch 79744; loss 0.008212\n",
      "epoch:4; batch 79744; train accuracy: 0.892738\n",
      "epoch 4; batch 79872; loss 0.002317\n",
      "epoch:4; batch 79872; train accuracy: 0.892773\n",
      "epoch 4; batch 80000; loss 0.010684\n",
      "epoch:4; batch 80000; train accuracy: 0.892808\n",
      "epoch 4; batch 80128; loss 0.026453\n",
      "epoch:4; batch 80128; train accuracy: 0.892840\n",
      "epoch 4; batch 80256; loss 0.024404\n",
      "epoch:4; batch 80256; train accuracy: 0.892870\n",
      "epoch 4; batch 80384; loss 0.010473\n",
      "epoch:4; batch 80384; train accuracy: 0.892902\n",
      "epoch 4; batch 80512; loss 0.006818\n",
      "epoch:4; batch 80512; train accuracy: 0.892937\n",
      "epoch 4; batch 80640; loss 0.029314\n",
      "epoch:4; batch 80640; train accuracy: 0.892969\n",
      "epoch 4; batch 80768; loss 0.023299\n",
      "epoch:4; batch 80768; train accuracy: 0.892998\n",
      "epoch 4; batch 80896; loss 0.014484\n",
      "epoch:4; batch 80896; train accuracy: 0.893030\n",
      "epoch 4; batch 81024; loss 0.011528\n",
      "epoch:4; batch 81024; train accuracy: 0.893065\n",
      "epoch 4; batch 81152; loss 0.003136\n",
      "epoch:4; batch 81152; train accuracy: 0.893100\n",
      "epoch 4; batch 81280; loss 0.005405\n",
      "epoch:4; batch 81280; train accuracy: 0.893134\n",
      "epoch 4; batch 81408; loss 0.007852\n",
      "epoch:4; batch 81408; train accuracy: 0.893166\n",
      "epoch 4; batch 81536; loss 0.007499\n",
      "epoch:4; batch 81536; train accuracy: 0.893201\n",
      "epoch 4; batch 81664; loss 0.024483\n",
      "epoch:4; batch 81664; train accuracy: 0.893233\n",
      "epoch 4; batch 81792; loss 0.006846\n",
      "epoch:4; batch 81792; train accuracy: 0.893267\n",
      "epoch 4; batch 81920; loss 0.031591\n",
      "epoch:4; batch 81920; train accuracy: 0.893296\n",
      "epoch 4; batch 82048; loss 0.016936\n",
      "epoch:4; batch 82048; train accuracy: 0.893328\n",
      "epoch 4; batch 82176; loss 0.001739\n",
      "epoch:4; batch 82176; train accuracy: 0.893363\n",
      "epoch 4; batch 82304; loss 0.001665\n",
      "epoch:4; batch 82304; train accuracy: 0.893397\n",
      "epoch 4; batch 82432; loss 0.091514\n",
      "epoch:4; batch 82432; train accuracy: 0.893429\n",
      "epoch 4; batch 82560; loss 0.015479\n",
      "epoch:4; batch 82560; train accuracy: 0.893461\n",
      "epoch 4; batch 82688; loss 0.038035\n",
      "epoch:4; batch 82688; train accuracy: 0.893490\n",
      "epoch 4; batch 82816; loss 0.018010\n",
      "epoch:4; batch 82816; train accuracy: 0.893522\n",
      "epoch 4; batch 82944; loss 0.005348\n",
      "epoch:4; batch 82944; train accuracy: 0.893556\n",
      "epoch 4; batch 83072; loss 0.002082\n",
      "epoch:4; batch 83072; train accuracy: 0.893590\n",
      "epoch 4; batch 83200; loss 0.053478\n",
      "epoch:4; batch 83200; train accuracy: 0.893619\n",
      "epoch 4; batch 83328; loss 0.001871\n",
      "epoch:4; batch 83328; train accuracy: 0.893654\n",
      "epoch 4; batch 83456; loss 0.004268\n",
      "epoch:4; batch 83456; train accuracy: 0.893688\n",
      "epoch 4; batch 83584; loss 0.015320\n",
      "epoch:4; batch 83584; train accuracy: 0.893717\n",
      "epoch 4; batch 83712; loss 0.036563\n",
      "epoch:4; batch 83712; train accuracy: 0.893749\n",
      "epoch 4; batch 83840; loss 0.019278\n",
      "epoch:4; batch 83840; train accuracy: 0.893780\n",
      "epoch 4; batch 83968; loss 0.021474\n",
      "epoch:4; batch 83968; train accuracy: 0.893812\n",
      "epoch 4; batch 84096; loss 0.055432\n",
      "epoch:4; batch 84096; train accuracy: 0.893838\n",
      "epoch 4; batch 84224; loss 0.015897\n",
      "epoch:4; batch 84224; train accuracy: 0.893870\n",
      "epoch 4; batch 84352; loss 0.002579\n",
      "epoch:4; batch 84352; train accuracy: 0.893904\n",
      "epoch 4; batch 84480; loss 0.015036\n",
      "epoch:4; batch 84480; train accuracy: 0.893938\n",
      "epoch 4; batch 84608; loss 0.000806\n",
      "epoch:4; batch 84608; train accuracy: 0.893972\n",
      "epoch 4; batch 84736; loss 0.004252\n",
      "epoch:4; batch 84736; train accuracy: 0.894006\n",
      "epoch 4; batch 84864; loss 0.007480\n",
      "epoch:4; batch 84864; train accuracy: 0.894040\n",
      "epoch 4; batch 84992; loss 0.003642\n",
      "epoch:4; batch 84992; train accuracy: 0.894074\n",
      "epoch 4; batch 85120; loss 0.004233\n",
      "epoch:4; batch 85120; train accuracy: 0.894108\n",
      "epoch 4; batch 85248; loss 0.033790\n",
      "epoch:4; batch 85248; train accuracy: 0.894136\n",
      "epoch 4; batch 85376; loss 0.001242\n",
      "epoch:4; batch 85376; train accuracy: 0.894170\n",
      "epoch 4; batch 85504; loss 0.003825\n",
      "epoch:4; batch 85504; train accuracy: 0.894204\n",
      "epoch 4; batch 85632; loss 0.017880\n",
      "epoch:4; batch 85632; train accuracy: 0.894235\n",
      "epoch 4; batch 85760; loss 0.001263\n",
      "epoch:4; batch 85760; train accuracy: 0.894269\n",
      "epoch 4; batch 85888; loss 0.034986\n",
      "epoch:4; batch 85888; train accuracy: 0.894300\n",
      "epoch 4; batch 86016; loss 0.015064\n",
      "epoch:4; batch 86016; train accuracy: 0.894332\n",
      "epoch 4; batch 86144; loss 0.008843\n",
      "epoch:4; batch 86144; train accuracy: 0.894365\n",
      "epoch 4; batch 86272; loss 0.003092\n",
      "epoch:4; batch 86272; train accuracy: 0.894399\n",
      "epoch 4; batch 86400; loss 0.010560\n",
      "epoch:4; batch 86400; train accuracy: 0.894430\n",
      "epoch 4; batch 86528; loss 0.034169\n",
      "epoch:4; batch 86528; train accuracy: 0.894462\n",
      "epoch 4; batch 86656; loss 0.035036\n",
      "epoch:4; batch 86656; train accuracy: 0.894490\n",
      "epoch 4; batch 86784; loss 0.011754\n",
      "epoch:4; batch 86784; train accuracy: 0.894521\n",
      "epoch 4; batch 86912; loss 0.018490\n",
      "epoch:4; batch 86912; train accuracy: 0.894552\n",
      "epoch 4; batch 87040; loss 0.024357\n",
      "epoch:4; batch 87040; train accuracy: 0.894584\n",
      "epoch 4; batch 87168; loss 0.003144\n",
      "epoch:4; batch 87168; train accuracy: 0.894617\n",
      "epoch 4; batch 87296; loss 0.007299\n",
      "epoch:4; batch 87296; train accuracy: 0.894651\n",
      "epoch 4; batch 87424; loss 0.077636\n",
      "epoch:4; batch 87424; train accuracy: 0.894682\n",
      "epoch 4; batch 87552; loss 0.013502\n",
      "epoch:4; batch 87552; train accuracy: 0.894713\n",
      "epoch 4; batch 87680; loss 0.005146\n",
      "epoch:4; batch 87680; train accuracy: 0.894746\n",
      "epoch 4; batch 87808; loss 0.012781\n",
      "epoch:4; batch 87808; train accuracy: 0.894780\n",
      "epoch 4; batch 87936; loss 0.005794\n",
      "epoch:4; batch 87936; train accuracy: 0.894813\n",
      "epoch 4; batch 88064; loss 0.011020\n",
      "epoch:4; batch 88064; train accuracy: 0.894846\n",
      "epoch 4; batch 88192; loss 0.027994\n",
      "epoch:4; batch 88192; train accuracy: 0.894877\n",
      "epoch 4; batch 88320; loss 0.027801\n",
      "epoch:4; batch 88320; train accuracy: 0.894908\n",
      "epoch 4; batch 88448; loss 0.004252\n",
      "epoch:4; batch 88448; train accuracy: 0.894942\n",
      "epoch 4; batch 88576; loss 0.054913\n",
      "epoch:4; batch 88576; train accuracy: 0.894970\n",
      "epoch 4; batch 88704; loss 0.019643\n",
      "epoch:4; batch 88704; train accuracy: 0.895001\n",
      "epoch 4; batch 88832; loss 0.069825\n",
      "epoch:4; batch 88832; train accuracy: 0.895027\n",
      "epoch 4; batch 88960; loss 0.041398\n",
      "epoch:4; batch 88960; train accuracy: 0.895052\n",
      "epoch 4; batch 89088; loss 0.015721\n",
      "epoch:4; batch 89088; train accuracy: 0.895083\n",
      "epoch 4; batch 89216; loss 0.113190\n",
      "epoch:4; batch 89216; train accuracy: 0.895107\n",
      "epoch 4; batch 89344; loss 0.004398\n",
      "epoch:4; batch 89344; train accuracy: 0.895140\n",
      "epoch 4; batch 89472; loss 0.019713\n",
      "epoch:4; batch 89472; train accuracy: 0.895168\n",
      "epoch 4; batch 89600; loss 0.011208\n",
      "epoch:4; batch 89600; train accuracy: 0.895199\n",
      "epoch 4; batch 89728; loss 0.008798\n",
      "epoch:4; batch 89728; train accuracy: 0.895232\n",
      "epoch 4; batch 89856; loss 0.008134\n",
      "epoch:4; batch 89856; train accuracy: 0.895265\n",
      "epoch 4; batch 89984; loss 0.004826\n",
      "epoch:4; batch 89984; train accuracy: 0.895298\n",
      "epoch 4; batch 90112; loss 0.002516\n",
      "epoch:4; batch 90112; train accuracy: 0.895331\n",
      "epoch 4; batch 90240; loss 0.009827\n",
      "epoch:4; batch 90240; train accuracy: 0.895364\n",
      "epoch 4; batch 90368; loss 0.023659\n",
      "epoch:4; batch 90368; train accuracy: 0.895395\n",
      "epoch 4; batch 90496; loss 0.035742\n",
      "epoch:4; batch 90496; train accuracy: 0.895423\n",
      "epoch 4; batch 90624; loss 0.014948\n",
      "epoch:4; batch 90624; train accuracy: 0.895456\n",
      "epoch 4; batch 90752; loss 0.001457\n",
      "epoch:4; batch 90752; train accuracy: 0.895489\n",
      "epoch 4; batch 90880; loss 0.008458\n",
      "epoch:4; batch 90880; train accuracy: 0.895522\n",
      "epoch 4; batch 91008; loss 0.008317\n",
      "epoch:4; batch 91008; train accuracy: 0.895555\n",
      "epoch 4; batch 91136; loss 0.011883\n",
      "epoch:4; batch 91136; train accuracy: 0.895585\n",
      "epoch 4; batch 91264; loss 0.040064\n",
      "epoch:4; batch 91264; train accuracy: 0.895613\n",
      "epoch 4; batch 91392; loss 0.006904\n",
      "epoch:4; batch 91392; train accuracy: 0.895646\n",
      "epoch 4; batch 91520; loss 0.001944\n",
      "epoch:4; batch 91520; train accuracy: 0.895679\n",
      "epoch 4; batch 91648; loss 0.004045\n",
      "epoch:4; batch 91648; train accuracy: 0.895712\n",
      "epoch 4; batch 91776; loss 0.012385\n",
      "epoch:4; batch 91776; train accuracy: 0.895745\n",
      "epoch 4; batch 91904; loss 0.033440\n",
      "epoch:4; batch 91904; train accuracy: 0.895775\n",
      "epoch 4; batch 92032; loss 0.012908\n",
      "epoch:4; batch 92032; train accuracy: 0.895805\n",
      "epoch 4; batch 92160; loss 0.017444\n",
      "epoch:4; batch 92160; train accuracy: 0.895836\n",
      "epoch 4; batch 92288; loss 0.086637\n",
      "epoch:4; batch 92288; train accuracy: 0.895866\n",
      "epoch 4; batch 92416; loss 0.002732\n",
      "epoch:4; batch 92416; train accuracy: 0.895899\n",
      "epoch 4; batch 92544; loss 0.147016\n",
      "epoch:4; batch 92544; train accuracy: 0.895927\n",
      "epoch 4; batch 92672; loss 0.013051\n",
      "epoch:4; batch 92672; train accuracy: 0.895957\n",
      "epoch 4; batch 92800; loss 0.004029\n",
      "epoch:4; batch 92800; train accuracy: 0.895989\n",
      "epoch 4; batch 92928; loss 0.006221\n",
      "epoch:4; batch 92928; train accuracy: 0.896022\n",
      "epoch 4; batch 93056; loss 0.018678\n",
      "epoch:4; batch 93056; train accuracy: 0.896052\n",
      "epoch 4; batch 93184; loss 0.056565\n",
      "epoch:4; batch 93184; train accuracy: 0.896082\n",
      "epoch 4; batch 93312; loss 0.011845\n",
      "epoch:4; batch 93312; train accuracy: 0.896115\n",
      "epoch 4; batch 93440; loss 0.001430\n",
      "epoch:4; batch 93440; train accuracy: 0.896148\n",
      "epoch 4; batch 93568; loss 0.030104\n",
      "epoch:4; batch 93568; train accuracy: 0.896175\n",
      "epoch 4; batch 93696; loss 0.005210\n",
      "epoch:4; batch 93696; train accuracy: 0.896208\n",
      "epoch 4; batch 93824; loss 0.002803\n",
      "epoch:4; batch 93824; train accuracy: 0.896240\n",
      "epoch 4; batch 93952; loss 0.012856\n",
      "epoch:4; batch 93952; train accuracy: 0.896270\n",
      "epoch 4; batch 94080; loss 0.012211\n",
      "epoch:4; batch 94080; train accuracy: 0.896300\n",
      "epoch 4; batch 94208; loss 0.017816\n",
      "epoch:4; batch 94208; train accuracy: 0.896330\n",
      "epoch 4; batch 94336; loss 0.005592\n",
      "epoch:4; batch 94336; train accuracy: 0.896363\n",
      "epoch 4; batch 94464; loss 0.002814\n",
      "epoch:4; batch 94464; train accuracy: 0.896395\n",
      "epoch 4; batch 94592; loss 0.002702\n",
      "epoch:4; batch 94592; train accuracy: 0.896428\n",
      "epoch 4; batch 94720; loss 0.009371\n",
      "epoch:4; batch 94720; train accuracy: 0.896457\n",
      "epoch 4; batch 94848; loss 0.002813\n",
      "epoch:4; batch 94848; train accuracy: 0.896490\n",
      "epoch 4; batch 94976; loss 0.001180\n",
      "epoch:4; batch 94976; train accuracy: 0.896522\n",
      "epoch 4; batch 95104; loss 0.064722\n",
      "epoch:4; batch 95104; train accuracy: 0.896552\n",
      "epoch 4; batch 95232; loss 0.004551\n",
      "epoch:4; batch 95232; train accuracy: 0.896584\n",
      "epoch 4; batch 95360; loss 0.005468\n",
      "epoch:4; batch 95360; train accuracy: 0.896617\n",
      "epoch 4; batch 95488; loss 0.007345\n",
      "epoch:4; batch 95488; train accuracy: 0.896649\n",
      "epoch 4; batch 95616; loss 0.014133\n",
      "epoch:4; batch 95616; train accuracy: 0.896679\n",
      "epoch 4; batch 95744; loss 0.044080\n",
      "epoch:4; batch 95744; train accuracy: 0.896706\n",
      "epoch 4; batch 95872; loss 0.039909\n",
      "epoch:4; batch 95872; train accuracy: 0.896733\n",
      "epoch 4; batch 96000; loss 0.078730\n",
      "epoch:4; batch 96000; train accuracy: 0.896763\n",
      "epoch 4; batch 96128; loss 0.012027\n",
      "epoch:4; batch 96128; train accuracy: 0.896795\n",
      "epoch 4; batch 96256; loss 0.010823\n",
      "epoch:4; batch 96256; train accuracy: 0.896827\n",
      "epoch 4; batch 96384; loss 0.032467\n",
      "epoch:4; batch 96384; train accuracy: 0.896857\n",
      "epoch 4; batch 96512; loss 0.004115\n",
      "epoch:4; batch 96512; train accuracy: 0.896889\n",
      "epoch 4; batch 96640; loss 0.002259\n",
      "epoch:4; batch 96640; train accuracy: 0.896921\n",
      "epoch 4; batch 96768; loss 0.014557\n",
      "epoch:4; batch 96768; train accuracy: 0.896948\n",
      "epoch 4; batch 96896; loss 0.039529\n",
      "epoch:4; batch 96896; train accuracy: 0.896976\n",
      "epoch 4; batch 97024; loss 0.013956\n",
      "epoch:4; batch 97024; train accuracy: 0.897008\n",
      "epoch 4; batch 97152; loss 0.010500\n",
      "epoch:4; batch 97152; train accuracy: 0.897037\n",
      "epoch 4; batch 97280; loss 0.008893\n",
      "epoch:4; batch 97280; train accuracy: 0.897069\n",
      "epoch 4; batch 97408; loss 0.008467\n",
      "epoch:4; batch 97408; train accuracy: 0.897099\n",
      "epoch 4; batch 97536; loss 0.006982\n",
      "epoch:4; batch 97536; train accuracy: 0.897131\n",
      "epoch 4; batch 97664; loss 0.004569\n",
      "epoch:4; batch 97664; train accuracy: 0.897162\n",
      "epoch 4; batch 97792; loss 0.048410\n",
      "epoch:4; batch 97792; train accuracy: 0.897192\n",
      "epoch 4; batch 97920; loss 0.052533\n",
      "epoch:4; batch 97920; train accuracy: 0.897219\n",
      "epoch 4; batch 98048; loss 0.016886\n",
      "epoch:4; batch 98048; train accuracy: 0.897248\n",
      "epoch 4; batch 98176; loss 0.023835\n",
      "epoch:4; batch 98176; train accuracy: 0.897278\n",
      "epoch 4; batch 98304; loss 0.017154\n",
      "epoch:4; batch 98304; train accuracy: 0.897305\n",
      "epoch 4; batch 98432; loss 0.004156\n",
      "epoch:4; batch 98432; train accuracy: 0.897337\n",
      "epoch 4; batch 98560; loss 0.000903\n",
      "epoch:4; batch 98560; train accuracy: 0.897368\n",
      "epoch 4; batch 98688; loss 0.007575\n",
      "epoch:4; batch 98688; train accuracy: 0.897398\n",
      "epoch 4; batch 98816; loss 0.008977\n",
      "epoch:4; batch 98816; train accuracy: 0.897430\n",
      "epoch 4; batch 98944; loss 0.003267\n",
      "epoch:4; batch 98944; train accuracy: 0.897461\n",
      "epoch 4; batch 99072; loss 0.000979\n",
      "epoch:4; batch 99072; train accuracy: 0.897493\n",
      "epoch 4; batch 99200; loss 0.007111\n",
      "epoch:4; batch 99200; train accuracy: 0.897525\n",
      "epoch 4; batch 99328; loss 0.003680\n",
      "epoch:4; batch 99328; train accuracy: 0.897556\n",
      "epoch 4; batch 99456; loss 0.001617\n",
      "epoch:4; batch 99456; train accuracy: 0.897588\n",
      "epoch 4; batch 99584; loss 0.053636\n",
      "epoch:4; batch 99584; train accuracy: 0.897617\n",
      "epoch 4; batch 99712; loss 0.016469\n",
      "epoch:4; batch 99712; train accuracy: 0.897646\n",
      "epoch 4; batch 99840; loss 0.017555\n",
      "epoch:4; batch 99840; train accuracy: 0.897676\n",
      "epoch 4; batch 99968; loss 0.007825\n",
      "epoch:4; batch 99968; train accuracy: 0.897705\n",
      "epoch 4; batch 100096; loss 0.080036\n",
      "epoch:4; batch 100096; train accuracy: 0.897731\n",
      "epoch 4; batch 100224; loss 0.004347\n",
      "epoch:4; batch 100224; train accuracy: 0.897763\n",
      "epoch 4; batch 100352; loss 0.006364\n",
      "epoch:4; batch 100352; train accuracy: 0.897794\n",
      "epoch 4; batch 100480; loss 0.059524\n",
      "epoch:4; batch 100480; train accuracy: 0.897824\n",
      "epoch 4; batch 100608; loss 0.002237\n",
      "epoch:4; batch 100608; train accuracy: 0.897855\n",
      "epoch 4; batch 100736; loss 0.003935\n",
      "epoch:4; batch 100736; train accuracy: 0.897887\n",
      "epoch 4; batch 100864; loss 0.043350\n",
      "epoch:4; batch 100864; train accuracy: 0.897916\n",
      "epoch 4; batch 100992; loss 0.010884\n",
      "epoch:4; batch 100992; train accuracy: 0.897945\n",
      "epoch 4; batch 101120; loss 0.030105\n",
      "epoch:4; batch 101120; train accuracy: 0.897974\n",
      "epoch 4; batch 101248; loss 0.022097\n",
      "epoch:4; batch 101248; train accuracy: 0.898003\n",
      "epoch 4; batch 101376; loss 0.004288\n",
      "epoch:4; batch 101376; train accuracy: 0.898034\n",
      "epoch 4; batch 101504; loss 0.006470\n",
      "epoch:4; batch 101504; train accuracy: 0.898065\n",
      "epoch 4; batch 101632; loss 0.021011\n",
      "epoch:4; batch 101632; train accuracy: 0.898094\n",
      "epoch 4; batch 101760; loss 0.006332\n",
      "epoch:4; batch 101760; train accuracy: 0.898125\n",
      "epoch 4; batch 101888; loss 0.002060\n",
      "epoch:4; batch 101888; train accuracy: 0.898157\n",
      "epoch 4; batch 102016; loss 0.004388\n",
      "epoch:4; batch 102016; train accuracy: 0.898188\n",
      "epoch 4; batch 102144; loss 0.032807\n",
      "epoch:4; batch 102144; train accuracy: 0.898215\n",
      "epoch 4; batch 102272; loss 0.087436\n",
      "epoch:4; batch 102272; train accuracy: 0.898239\n",
      "epoch 4; batch 102400; loss 0.048545\n",
      "epoch:4; batch 102400; train accuracy: 0.898265\n",
      "epoch 4; batch 102528; loss 0.005175\n",
      "epoch:4; batch 102528; train accuracy: 0.898296\n",
      "epoch 4; batch 102656; loss 0.010391\n",
      "epoch:4; batch 102656; train accuracy: 0.898325\n",
      "epoch 4; batch 102784; loss 0.044285\n",
      "epoch:4; batch 102784; train accuracy: 0.898354\n",
      "epoch 4; batch 102912; loss 0.017415\n",
      "epoch:4; batch 102912; train accuracy: 0.898382\n",
      "epoch 4; batch 103040; loss 0.013708\n",
      "epoch:4; batch 103040; train accuracy: 0.898414\n",
      "epoch 4; batch 103168; loss 0.041660\n",
      "epoch:4; batch 103168; train accuracy: 0.898440\n",
      "epoch 4; batch 103296; loss 0.002767\n",
      "epoch:4; batch 103296; train accuracy: 0.898471\n",
      "epoch 4; batch 103424; loss 0.003576\n",
      "epoch:4; batch 103424; train accuracy: 0.898502\n",
      "epoch 4; batch 103552; loss 0.002216\n",
      "epoch:4; batch 103552; train accuracy: 0.898533\n",
      "epoch 4; batch 103680; loss 0.004538\n",
      "epoch:4; batch 103680; train accuracy: 0.898564\n",
      "epoch 4; batch 103808; loss 0.037863\n",
      "epoch:4; batch 103808; train accuracy: 0.898593\n",
      "epoch 4; batch 103936; loss 0.039079\n",
      "epoch:4; batch 103936; train accuracy: 0.898621\n",
      "epoch 4; batch 104064; loss 0.051953\n",
      "epoch:4; batch 104064; train accuracy: 0.898648\n",
      "epoch 4; batch 104192; loss 0.024121\n",
      "epoch:4; batch 104192; train accuracy: 0.898676\n",
      "epoch 4; batch 104320; loss 0.026670\n",
      "epoch:4; batch 104320; train accuracy: 0.898705\n",
      "epoch 4; batch 104448; loss 0.014950\n",
      "epoch:4; batch 104448; train accuracy: 0.898736\n",
      "epoch 4; batch 104576; loss 0.002773\n",
      "epoch:4; batch 104576; train accuracy: 0.898767\n",
      "epoch 4; batch 104704; loss 0.011838\n",
      "epoch:4; batch 104704; train accuracy: 0.898795\n",
      "epoch 4; batch 104832; loss 0.006184\n",
      "epoch:4; batch 104832; train accuracy: 0.898826\n",
      "epoch 4; batch 104960; loss 0.010140\n",
      "epoch:4; batch 104960; train accuracy: 0.898857\n",
      "模型在训练集上的评价指标：\n",
      "y_true_label_0_num: 37148 ; rate: 0.353925\n",
      "y_true_label_1_num: 8748 ; rate: 0.083346\n",
      "y_true_label_2_num: 10206 ; rate: 0.097237\n",
      "y_true_label_3_num: 48858 ; rate: 0.465492\n",
      "y_true_label_num_total: 104960\n",
      "valid accuracy: 0.992292\n",
      "valid avg_precision: 0.992523\n",
      "valid avg_recall: 0.992140\n",
      "valid avg_f1: 0.992325\n",
      "模型在验证集上的评价指标：\n",
      "y_true_label_0_num: 5387 ; rate: 0.359709\n",
      "y_true_label_1_num: 1214 ; rate: 0.081063\n",
      "y_true_label_2_num: 1418 ; rate: 0.094685\n",
      "y_true_label_3_num: 6957 ; rate: 0.464543\n",
      "y_true_label_num_total: 14976\n",
      "valid accuracy: 0.767628\n",
      "valid avg_precision: 0.769686\n",
      "valid avg_recall: 0.764690\n",
      "valid avg_f1: 0.764125\n"
     ]
    }
   ],
   "source": [
    "#训练过程\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "for num in range(epochs):\n",
    "    print('epoch %d' % (num+1))\n",
    "    train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=10000).batch(batch_size,drop_remainder=True)\n",
    "    index = 0\n",
    "    for x,y in train:\n",
    "        index +=1\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(x)\n",
    "            #print(y.shape)\n",
    "            #print(labels_pred.shape)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            sparse_categorical_accuracy.update_state(y_true = y, y_pred= labels_pred)\n",
    "            #loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=labels_pred)\n",
    "            print(\"epoch %d; batch %d; loss %f\" % (num+1, index*batch_size, loss.numpy()))\n",
    "            print(\"epoch:%d; batch %d; train accuracy: %f\" % (num+1, index*batch_size, sparse_categorical_accuracy.result()))\n",
    "            grads = tape.gradient(loss, model.trainable_variables) #梯度\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables)) #梯度和变量\n",
    "    \n",
    "    # 模型在训练集上 的评估参数,batch_size 128\n",
    "    print('模型在训练集上的评价指标：')\n",
    "    eval_train = eval_of_experiment((train_x,train_y), batch_size ,model)\n",
    "    eval_train.eval_of_valid()\n",
    "    eval_train.result()\n",
    "    #  模型在验证集上 的评估参数，batch_size 128\n",
    "    print('模型在验证集上的评价指标：')\n",
    "    eval_valid = eval_of_experiment((valid_x,valid_y), batch_size ,model)\n",
    "    eval_valid.eval_of_valid()\n",
    "    eval_valid.result()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8  dish 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "embedding_matrix = load_obj('./data2/train_sales_embedding_matrix.pkl')\n",
    "\n",
    "model = single_attention_aspect(\n",
    "                maxlen=maxlen, \n",
    "                embedding_matrix=embedding_matrix, \n",
    "                # aspect=loc,\n",
    "                # aspect=ser,\n",
    "                # aspect=pri,\n",
    "                # aspect=env,\n",
    "                aspect=dis,\n",
    "                embedding_dim=200,\n",
    "                aspect_len=20,\n",
    "                hidden_size=100,\n",
    "                activation=None,\n",
    "                output_size=4\n",
    "            )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
