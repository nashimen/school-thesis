import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
from pylab import *
mpl.rcParams['font.sans-serif'] = ['SimHei']

# æ˜¾ç¤ºæ‰€æœ‰åˆ—
pd.set_option('display.max_columns', None)


def dictTest():
    dictionary = {"2star": ["a"], "3star": ["b", "d"]}
    print(dictionary)
    dictionary["4star"] = ["dd", "bb"]
    print(dictionary)
    dictionary["2star"].append("df")
    print(dictionary)
    ll = ["a", "b"]
    np.save("test.npy", ll)
    ll_load = np.load("test.npy", allow_pickle=True).tolist()
    print(ll_load)
    print(type(ll_load))

    name = ["ç™½ç‰å…°é…’åº—(ä¸Šæµ·ç£æ‚¬æµ®æ€»ç«™åº—)"]
    path = "ä¸Šæµ·3æ˜Ÿ_hotel_done_file.npy"
    np.save(path, name)


def dfTest():
    path = "data/test/è¯„è®º-åŒ—äº¬2æ˜Ÿ.xlsx"
    data = pd.read_excel(path, nrows=2000)
    # print(data.head())
    hotelNames = set(data["åç§°"].tolist())
    print(hotelNames)
    current_hotel = data.loc[data['åç§°'] == str("O2è½»å¥¢é…’åº—(åŒ—äº¬ç«‹æ°´æ¡¥åœ°é“ç«™åº—)")]
    # print(current_hotel.head())
    # for name in hotelNames:
    #     print(name)
    for index, row in data.iterrows():
        # print("row's type = ", type(row))
        # print("row:", row)
        print(row["åç§°"])


# åˆ¤æ–­æ˜¯å¦å…¨éƒ¨ä¸ºéä¸­æ–‡å­—ç¬¦
def is_Chinese(word):
    for ch in word:
        if '\u4e00' <= ch <= '\u9fa5':
            return True
    return False


def ChineseTest():
    '''
    ss = "ä¸­å›½äºº"
    sss = "ä¸­å›½äººğŸ˜±ëƒ‰"
    ssss = "ğŸ˜±ëƒ‰ëƒ‰"
    print(no_Chinese(ss))
    print(no_Chinese(sss))
    print(no_Chinese(sss))
    '''
    docs = ["ä¸­å›½äºº", "ä¸­å›½äººğŸ˜±", "ğŸ˜±ëƒ‰ëƒ‰", "ğŸ˜±ëƒ‰ëƒ‰ä¸­"]
    for doc in docs:
        print("doc = ", doc)
        if not is_Chinese(doc):
            # print("delete", doc)
            docs.remove(doc)
    print(docs)

    print("åˆ é™¤éä¸­æ–‡å­—ç¬¦ã€‚ã€‚ã€‚")
    for doc in docs:
        find_Chinese(doc)


def find_Chinese(doc):
    pattern = re.compile(r'[^\u4e00-\u9fa5]')
    un_Chinese = re.sub(pattern, "", doc)
    print(un_Chinese)


def indexTest():
    texts = ["a", "CC", "abc", "defss", "fefsa"]
    for i, text in enumerate(texts):
        print(i, text)


def plotting():
    names = ['5', '10', '15', '20', '25']
    x = range(len(names))
    y = [0.855, 0.84, 0.835, 0.815, 0.81]
    y1 = [0.86, 0.85, 0.853, 0.849, 0.83]
    # plt.plot(x, y, 'ro-')
    # plt.plot(x, y1, 'bo-')
    # pl.xlim(-1, 11)  # é™å®šæ¨ªè½´çš„èŒƒå›´
    # pl.ylim(-1, 110)  # é™å®šçºµè½´çš„èŒƒå›´
    plt.plot(x, y, marker='o', mec='r', mfc='w', label=u'y=x^2æ›²çº¿å›¾')
    plt.plot(x, y1, marker='*', ms=10, label=u'y=x^3æ›²çº¿å›¾')
    plt.legend()  # è®©å›¾ä¾‹ç”Ÿæ•ˆ
    plt.xticks(x, names, rotation=45)
    plt.margins(0)
    plt.subplots_adjust(bottom=0.15)
    plt.xlabel(u"time(s)é‚»å±…")  # Xè½´æ ‡ç­¾
    plt.ylabel("RMSE")  # Yè½´æ ‡ç­¾
    plt.title("A simple plot")  # æ ‡é¢˜

    plt.show()


import datetime
def dateTransform():
    day = "2021/12/41"
    day_time = datetime.datetime.strptime(day, "%Y/%m/%d")
    print(day_time)
    day_change = day_time.strftime("%Y%m")
    print(day_change)


def silhouetteTest():
    #åˆå§‹åŒ–åŸå§‹æ•°å­—ç‚¹
    x1 = np.array([1, 2, 3, 1, 5, 6, 5, 5, 6, 7, 8, 9, 7, 9])
    x2 = np.array([1,3,2,2,8,6,7,6,7,1,2,1,1,3])
    #X = np.array([x1,x2])
    X = np.array(list(zip(x1,x2))).reshape(len(x1), 2)
    labels = [1, 3, 2, 2, 2, 3, 2, 3, 2, 1, 2, 1, 1, 3]
    sc_score = silhouette_score(X, labels, metric='euclidean')
    print(sc_score)


def time_fix(day):
    # print("day's type = ", type(day))
    # day_time = datetime.datetime.strftime(day)
    # print("day = ", day)
    # day_time = datetime.datetime.strptime(day, "%Y/%m/%d")
    # print(day_time)
    day_change = day.strftime("%Y%m")
    return day_change


def year_fix(day):
    # day_time = datetime.datetime.strptime(day, "%Y/%m/%d")
    year = day.strftime("%Y")

    return year


def season_fix(day):
    # day_time = datetime.datetime.strptime(day, "%Y/%m/%d")
    year = day.strftime("%Y")
    month = day.strftime("%m")
    if month in ["01", "02", "03"]:
        season = year + "S1"
    elif month in ["04", "05", "06"]:
        season = year + "S2"
    elif month in ["07", "08", "09"]:
        season = year + "S3"
    else:
        season = year + "S4"
    return season


def findStarLevel(hotels, datasets):
    economy = []
    luxury = []
    df = datasets[["åç§°", "æ˜Ÿçº§"]]
    df = df.drop_duplicates(subset="åç§°", keep="first")
    for index, row in df.iterrows():
        # print(row)
        # print(type(row["æ˜Ÿçº§"]))
        name = row["åç§°"]
        star = str(row["æ˜Ÿçº§"])
        if name not in hotels:
            continue
        if star in ["2", "3"]:
            economy.append(name)
        else:
            luxury.append(name)
    return economy, luxury


debug = False
debugLength = 1000
def hotelStatistics():
    print("æ­£åœ¨è¯»å–æ•°æ®ã€‚ã€‚ã€‚")
    path = "data/original/merged_file+è¡¥å……æ•°æ®.xlsx"
    df = pd.read_excel(path, engine="openpyxl", nrows=debugLength if debug else None)
    print("æ•°æ®è¯»å–å®Œæ¯•ã€‚ã€‚ã€‚")
    print("length = ", len(df))

    # df = pd.read_csv(path)
    df["å…¥ä½æœˆä»½"] = df.apply(lambda row: time_fix(row["å…¥ä½æ—¥æœŸ"]), axis=1)
    df["å…¥ä½å¹´ä»½"] = df.apply(lambda row: year_fix(row["å…¥ä½æ—¥æœŸ"]), axis=1)
    df["å…¥ä½å­£èŠ‚"] = df.apply(lambda row: season_fix(row["å…¥ä½æ—¥æœŸ"]), axis=1)
    # print("æŒ‰ç…§åœ°åŒº*æ˜Ÿçº§ç»Ÿè®¡æ¯æœˆéƒ½æœ‰çš„é…’åº—ã€‚ã€‚ã€‚")
    print("æ—¶é—´æ ¼å¼å¤„ç†å®Œæ¯•ã€‚ã€‚ã€‚")

    # print(df.head())
    df = df.loc[df["å…¥ä½å¹´ä»½"].isin(["2020", "2019", "2021"])]
    print("*" * 50, "æŒ‰å¹´ç»Ÿè®¡", "*" * 50)
    years = list(set(df["å…¥ä½å¹´ä»½"].tolist()))
    print("years = ", years)
    hotels = list(set(df.loc[df["å…¥ä½å¹´ä»½"] == years[0]]["åç§°"]))
    # print("hotels:", hotels)
    print("hotels' length = ", len(hotels))
    for i, year in enumerate(years):
        if i == 0:
            continue
        current = set(df.loc[df["å…¥ä½å¹´ä»½"] == year]["åç§°"])
        hotels = list(set(hotels) & current)
    # print("final hotels:", hotels)
    print("final hotels' length:", len(hotels))
    # 2æ˜Ÿ&3æ˜Ÿã€4æ˜Ÿ&5æ˜Ÿé…’åº—éƒ½æœ‰å“ªäº›
    economy, luxury = findStarLevel(hotels, df)
    # print("economy:", economy)
    # print("luxury:", luxury)

    print("*" * 50, "æŒ‰å­£èŠ‚ç»Ÿè®¡", "*" * 50)
    seasons = list(set(df["å…¥ä½å­£èŠ‚"].tolist()))
    print(seasons)
    hotels = list(set(df.loc[df["å…¥ä½å­£èŠ‚"] == seasons[0]]["åç§°"]))
    # print("hotels:", hotels)
    print("hotels' length = ", len(hotels))
    for i, season in enumerate(seasons):
        if i == 0:
            continue
        current = set(df.loc[df["å…¥ä½å­£èŠ‚"] == season]["åç§°"])
        hotels = list(set(hotels) & current)
    # print("final hotels:", hotels)
    print("final hotels' length:", len(hotels))
    # 2æ˜Ÿ&3æ˜Ÿã€4æ˜Ÿ&5æ˜Ÿé…’åº—éƒ½æœ‰å“ªäº›
    economy, luxury = findStarLevel(hotels, df)
    print("economy:", economy)
    print("economy's length = ", len(economy))
    print("luxury:", luxury)
    print("luxury's length = ", len(luxury))

    print("*" * 50, "æŒ‰æœˆç»Ÿè®¡", "*" * 50)
    months = list(set(df["å…¥ä½æœˆä»½"].tolist()))
    hotels = list(set(df.loc[df["å…¥ä½æœˆä»½"] == months[0]]["åç§°"]))
    # print("hotels:", hotels)
    print("hotels' length = ", len(hotels))
    for i, month in enumerate(months):
        if i == 0:
            continue
        current = set(df.loc[df["å…¥ä½æœˆä»½"] == month]["åç§°"])
        hotels = list(set(hotels) & current)
    # print("final hotels:", hotels)
    print("final hotels' length:", len(hotels))
    # 2æ˜Ÿ&3æ˜Ÿã€4æ˜Ÿ&5æ˜Ÿé…’åº—éƒ½æœ‰å“ªäº›
    economy, luxury = findStarLevel(hotels, df)
    # print("economy:", economy)
    # print("luxury:", luxury)


def meanTest():
    score = [0.1, 0.3, 0.5, 0.1, 0.2]
    print(np.mean(score))
    texts = ["hehe", "haha", "hello"]
    texts2 = []
    texts3 = ["hehe"]
    print(",".join(texts3))


def judgeTopic():
    num = randint(1, 6)
    print(num)
    if num == 1:
        print("ROOM")
    elif num == 2:
        print("SERVICE")
    elif num == 3:
        print("LOCATION")
    elif num == 4:
        print("SLEEP_QUALITY")
    else:
        print("VALUE")


import xiangshi as xs
from snownlp import SnowNLP
def similarityTest():
    input1 = ["æ¥¼ä¸‹é¤å…å£å‘³ä¹Ÿä¸é”™"]
    input2 = ["é¤å…"]
    input3 = ["æˆ¿é—´"]
    print(xs.cossim(input1, input2))
    # print(xs.cossim(input2, input1))
    # print(xs.cossim(input2, input3))
    print(xs.cossim(input1, input3))

    # s = SnowNLP([[input1, input2], [input1, input3]])
    s = SnowNLP(input1)
    print(s.sim(u'å£å‘³'))


def maxTest():
    a = max(0.1, 0.5)
    print(a)


def dfTest():
    df = pd.DataFrame()
    dictionary = {"a": [1, 2, 3], "b": [3, 2, 1], "c": [4, 5, 1]}
    for key, value in dictionary.items():
        df[key] = pd.Series(value)
    print(df)


from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
def tfidfTest():
    '''
    corpus = [
        'é£Ÿç‰© ä½ç½® è½¦ç«™',
        'æ—©é¤ é¤å…',
        'é¤å… å…¬äº¤ç«™',
        'æ—©é¤ è½¦ç«™',
    ]
    '''
    corpus = ['é£Ÿç‰© ä½ç½® è½¦ç«™ è½¦ç«™ æ—©é¤ é¤å… æ—©é¤ è½¦ç«™']
    # corpus = corpus[0].split(" ")
    vectorizer = CountVectorizer()  # è¯¥ç±»ä¼šå°†æ–‡æœ¬ä¸­çš„è¯è¯­è½¬æ¢ä¸ºè¯é¢‘çŸ©é˜µï¼ŒçŸ©é˜µå…ƒç´ a[i][j] è¡¨ç¤ºjè¯åœ¨iç±»æ–‡æœ¬ä¸‹çš„è¯é¢‘
    transformer = TfidfTransformer()  # è¯¥ç±»ä¼šç»Ÿè®¡æ¯ä¸ªè¯è¯­çš„tf-idfæƒå€¼
    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))  # ç¬¬ä¸€ä¸ªfit_transformæ˜¯è®¡ç®—tf-idfï¼Œç¬¬äºŒä¸ªfit_transformæ˜¯å°†æ–‡æœ¬è½¬ä¸ºè¯é¢‘çŸ©é˜µ
    word = vectorizer.get_feature_names()  # è·å–è¯è¢‹æ¨¡å‹ä¸­çš„æ‰€æœ‰è¯è¯­
    weight = tfidf.toarray()  # å°†tf-idfçŸ©é˜µæŠ½å–å‡ºæ¥ï¼Œå…ƒç´ a[i][j]è¡¨ç¤ºjè¯åœ¨iç±»æ–‡æœ¬ä¸­çš„tf-idfæƒé‡
    print(word)
    print(weight)
    print("æ‰“å°tfidfæœ€é«˜çš„è¯è¯­")
    for i in range(len(weight)):
        df = pd.DataFrame()
        tfidf_temp = []
        for j in range(len(word)):
            tfidf_temp.append(weight[i][j])
        df["word"] = pd.Series(word)
        df["tfidf"] = pd.Series(tfidf_temp)
        df = df.sort_values(by="tfidf", ascending=False)
        print(df[:5])
        print("*" * 50)


def frequencyTest():
    corpus = [
        'é…’åº— ä½ç½® éå¸¸å¥½ ä» è¥¿ç«™ åŒ—å¹¿åœº å‡ºæ¥ å³è½¬ å³ åˆ° ç´§é‚» å…¬äº¤ å§‹å‘ç«™',
        'é…’åº— æœ‰ é¤å… æœ‰ æ—©é¤ å¥½åƒ è¦ ä»˜è´¹ æˆ‘ä»¬ æ²¡åœ¨ é…’åº— ç”¨é¤',
        'é…’åº— æ¡ä»¶ è¿˜ å¯ä»¥ æœåŠ¡ è´¨é‡ ä¸€èˆ¬  å°±æ˜¯ å¤–é¢ æœ‰ç‚¹ å¤ª åµé—¹ æ—©é¤ åº”è¯¥ æ˜¯ å–ç‚¹ èŠ±æ · è¾ƒå¤š å¸Œæœ› æœ¬åº— æœåŠ¡ è´¨é‡ æœ‰å¾… æé«˜',
        'æ€§ä»·æ¯” å¾ˆé«˜ å®¢æˆ¿ æœåŠ¡ ä¹Ÿ å¾ˆå¥½ å…¥ä½ ä½“éªŒ è¿˜ ä¸é”™',
    ]
    vectorizer = CountVectorizer()  # è¯¥ç±»ä¼šå°†æ–‡æœ¬ä¸­çš„è¯è¯­è½¬æ¢ä¸ºè¯é¢‘çŸ©é˜µï¼ŒçŸ©é˜µå…ƒç´ a[i][j] è¡¨ç¤ºjè¯åœ¨iç±»æ–‡æœ¬ä¸‹çš„è¯é¢‘
    X = vectorizer.fit_transform(corpus)
    print(vectorizer.get_feature_names())
    print(X.toarray())


def dictTest():
    myDict = {"a": 1, "b": 1.5, "d": 0.6, "aa": 1.1}
    print(myDict.values())


def listSum():
    ll = [1, 2, 3, 5, 1]
    print(sum(ll))
    final_result = pd.DataFrame(columns=["location", "star", "hotel", "quarter", "review_number", "like_number",
                                         "hot_words", "hot", "frequent_words", "frequency", "new_words", "new"])


def digitTest():
    a = '100km'
    b = '12l'
    c = '2l'
    d = "121"
    e = "21"
    print(is_number(a))
    print(is_number(a) and a.isdigit())
    print(is_number(b))
    print(is_number(c))
    print(is_number(d))
    print(is_number(e))


def is_number(num):
    pattern = re.compile(r'^[-+]?[-0-9]\d*\.\d*|[-+]?\.?[0-9]\d*$')
    result = pattern.match(num)
    if result:
        return True
    else:
        return False


def is_number2(s):
    try:
        complex(s)  # for int, long, float and complex
    except ValueError:
        return False
    return True


import paddlehub as hub
def sentiment_calculating():
    senta = hub.Module(name='senta_bilstm')
    texts = ["è…Šè‚‰éå¸¸å¥½åƒ", "ç‚’èœå‘³é“ä¸é”™", "æ•´ä½“ä¸€èˆ¬å§", "æˆ‘ä»¬å»äº†å…¬å›­"]
    res = senta.sentiment_classify(data={"text": texts})
    print(res)
    # for text in texts:

import random
from pyecharts import options as opts
from pyecharts.charts import HeatMap
from pyecharts.faker import Faker

import seaborn as sns
sns.set()
sns.set_style("whitegrid", {"font.sans-serif": ["simhei", "Arial"]})
def heatMapTest():
    df = pd.DataFrame(
        np.random.rand(4, 7),
        index=["å¤©å®‰é—¨", "æ•…å®«", "å¥¥æ—åŒ¹å…‹æ£®æ—å…¬å›­", "å…«è¾¾å²­é•¿åŸ"],
        columns=["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"]
    )
    plt.figure(figsize=(10, 4))
    sns.heatmap(df, annot=True, fmt=".1f", cmap="coolwarm")


# RGBæ ¼å¼é¢œè‰²è½¬æ¢ä¸º16è¿›åˆ¶é¢œè‰²æ ¼å¼
def RGB_to_Hex(rgb):
    RGB = rgb.split(',')  # å°†RGBæ ¼å¼åˆ’åˆ†å¼€æ¥
    color = '#'
    for i in RGB:
        num = int(i)
        # å°†Rã€Gã€Båˆ†åˆ«è½¬åŒ–ä¸º16è¿›åˆ¶æ‹¼æ¥è½¬æ¢å¹¶å¤§å†™  hex() å‡½æ•°ç”¨äºå°†10è¿›åˆ¶æ•´æ•°è½¬æ¢æˆ16è¿›åˆ¶ï¼Œä»¥å­—ç¬¦ä¸²å½¢å¼è¡¨ç¤º
        color += str(hex(num))[-2:].replace('x', '0').upper()
    print(color)
    return color


# RGBæ ¼å¼é¢œè‰²è½¬æ¢ä¸º16è¿›åˆ¶é¢œè‰²æ ¼å¼
def RGB_list_to_Hex(RGB):
    # RGB = rgb.split(',')  # å°†RGBæ ¼å¼åˆ’åˆ†å¼€æ¥
    color = '#'
    for i in RGB:
        num = int(i)
        # å°†Rã€Gã€Båˆ†åˆ«è½¬åŒ–ä¸º16è¿›åˆ¶æ‹¼æ¥è½¬æ¢å¹¶å¤§å†™  hex() å‡½æ•°ç”¨äºå°†10è¿›åˆ¶æ•´æ•°è½¬æ¢æˆ16è¿›åˆ¶ï¼Œä»¥å­—ç¬¦ä¸²å½¢å¼è¡¨ç¤º
        color += str(hex(num))[-2:].replace('x', '0').upper()
    print(color)
    return color


# 16è¿›åˆ¶é¢œè‰²æ ¼å¼é¢œè‰²è½¬æ¢ä¸ºRGBæ ¼å¼
def Hex_to_RGB(hex):
    r = int(hex[1:3], 16)
    g = int(hex[3:5], 16)
    b = int(hex[5:7], 16)
    rgb = str(r) + ',' + str(g) + ',' + str(b)
    print(rgb)
    return rgb, [r, g, b]


def gradient_color(color_list, color_sum=15):
    color_center_count = len(color_list)
    # if color_center_count == 2:
    #     color_center_count = 1
    color_sub_count = int(color_sum / (color_center_count - 1))
    color_index_start = 0
    color_map = []
    for color_index_end in range(1, color_center_count):
        color_rgb_start = Hex_to_RGB(color_list[color_index_start])[1]
        color_rgb_end = Hex_to_RGB(color_list[color_index_end])[1]
        r_step = (color_rgb_end[0] - color_rgb_start[0]) / color_sub_count
        g_step = (color_rgb_end[1] - color_rgb_start[1]) / color_sub_count
        b_step = (color_rgb_end[2] - color_rgb_start[2]) / color_sub_count
        # ç”Ÿæˆä¸­é—´æ¸å˜è‰²
        now_color = color_rgb_start
        color_map.append(RGB_list_to_Hex(now_color))
        for color_index in range(1, color_sub_count):
            now_color = [now_color[0] + r_step, now_color[1] + g_step, now_color[2] + b_step]
            color_map.append(RGB_list_to_Hex(now_color))
        color_index_start = color_index_end
    return color_map

from haishoku.haishoku import Haishoku
def extract_color():
    image = "data/color.jpg"
    haishoku = Haishoku.loadHaishoku(image)
    print(haishoku.palette)
    print(type(haishoku.palette))
    haishoku.showPalette(image)

from colormap import rgb2hex
def produce_color():
    color_names = []
    sample = [0, 0.5, 1]  # å¯ä»¥æ ¹æ®è‡ªå·±æƒ…å†µè¿›è¡Œè®¾ç½®
    # sample = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]  # å¯ä»¥æ ¹æ®è‡ªå·±æƒ…å†µè¿›è¡Œè®¾ç½®
    for i in sample:
        for j in sample:
            for k in sample:
                col = rgb2hex(i, j, k, normalised=True)
                color_names.append(col)
    print(color_names)
    x = np.random.random((1000, 1))
    j = 0
    for i in range(len(color_names)):
        y = j * 3 + np.random.random((1000, 1)) * 2
        j += 1
        plt.plot(x, y, 'o', color=color_names[i])
    plt.show()


def resize_subplot():
    figure = plt.figure(1)

    quadrant = "121"
    # ax = figure.add_subplot(quadrant, projection='3d')
    ax2 = figure.add_axes([0, 0.2, 0.5, 0.6], projection='3d')
    ax3 = figure.add_axes([0.5, 0.2, 0.4, 0.6])

    plt.show()


if __name__ == "__main__":
    print("start...")

    resize_subplot()

    print("end...")

